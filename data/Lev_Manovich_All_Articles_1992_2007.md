**Lev Manovich: All Articles, 1992 - 2007**


# Table of contents:

["Real" Wars: Esthetics and Professionalism in Computer Animation (1991)](#"Real" Wars: Esthetics and Professionalism in Computer Animation)

[Assembling Reality: Myths of Computer Graphics (1992)](#Assembling Reality: Myths of Computer Graphics)

[The Mapping of Space: Perspective, Radar, and 3-D Computer Graphics (1993)](#The Mapping of Space: Perspective, Radar, and 3-D Computer Graphics)

[The Paradoxes of Digital Photography (1993)](#The Paradoxes of Digital Photography)

[The Engineering of Vision and the Aesthetics of Computer Art (1994)](#The Engineering of Vision and the Aesthetics of Computer Art)

[From the Externalization of the Psyche to the Implantation of Technology (1994)](#From the Externalization of the Psyche to the Implantation of Technology)

[The Labor of Perception (1995)](#The Labor of Perception) 

[Cinema and Digital Media (1995)](#Cinema and Digital Media)

[What is Digital Cinema? (1995)](#What is Digital Cinema?)

[To Lie and to Act: Cinema and Telecommunication (1995)](#To Lie and to Act: Cinema and Telecommunication)

[Archeology of a Computer Screen (1995)](#Archeology of a Computer Screen)

[Reading New Media Art (1995)](#Reading New Media Art)

[Virtual Worlds: Report from Los Angeles (1995)](#Virtual Worlds: Report from Los Angeles)

[Zeuxis meets RealityEngine -  Digital Realism and Virtual Worlds (1996)](#Zeuxis meets RealityEngine -  Digital Realism and Virtual Worlds)

[On Totalitarian Interactivity (1996)](#On Totalitarian Interactivity)

[Automation of Sight from Photography to Computer Vision (1997)](#Automation of Sight from Photography to Computer Vision)

[Behind the Screen / Russian New Media (1997)](#Behind the Screen / Russian New Media)

[Detour to the East (1997)](#Detour to the East)

[Jump over Proust (1997)](#Jump over Proust)

[Cinema as a Cultural Interface (1997)](#Cinema as a Cultural Interface)

[The Genealogy of the Interface (1997)](#The Genealogy of the Interface)

[Navigable Space (1998)](#Navigable Space)

[The Language of New Media (article) (1998)](#The Language of New Media (article))

[Filters, Plug-ins, and Menus - from Creation to Selection (1998)](#Filters, Plug-ins, and Menus - from Creation to Selection)

[Database as a Symbolic Form (1998)](#Database as a Symbolic Form)

[The Camera and the World (1998)](#The Camera and the World)

[Cinema by Numbers (1998)](#Cinema by Numbers)

[Review of Stars Wars: Episode 1 (1999)](#Review of Stars Wars: Episode 1)

[New Media: a User's Guide (1999)](#New Media: a User's Guide)

[Avant-garde as Software (1999)](#Avant-garde as Software)

[Computer Simulation and the History of Illusion (1999)](#Computer Simulation and the History of Illusion)

[Macro-media and Micro-media (2000)](#Macro-media and Micro-media)

[Information and Form: Electrolobby at Ars Electronica 2000 (2000)](#Information and Form: Electrolobby at Ars Electronica 2000)

[Fashion Sites (2001)](#Fashion Sites)

[From DV Realism to a Universal Recording Machine (2001)](#From DV Realism to a Universal Recording Machine)

[Post-media Aesthetics (2001)](#Post-media Aesthetics)

[New Media from Borges to HTML (2001)](#New Media from Borges to HTML)

[The Poetics of Augmented Space (2002)](#The Poetics of Augmented Space)

[Who is the Author? Sampling / Remixing / Open Source (2002)](#Who is the Author? Sampling / Remixing / Open Source)

[10 Key Texts on New Media Art, 1970-2000 (2002)](#10 Key Texts on New Media Art, 1970-2000)

[Generation Flash (2002)](#Generation Flash)

[Metadata, Mon Amour (2002)](#Metadata, Mon Amour)

[Data Visualization as New Abstraction and Anti-Sublime (2002)](#Data Visualization as New Abstraction and Anti-Sublime)

[Don’t Call it Art: Ars Electronica 2003 (2003)](#Don’t Call it Art: Ars Electronica 2003)

[Introduction to Korean Edition of _The Language of New Media_ (2003)](#Introduction to Korean Edition of _The Language of New Media_)

[Moving Image after Computerization - Extending Traditional Elements of Cinema. An Outline (2003)](#Moving Image after Computerization - Extending Traditional Elements of Cinema. An Outline)

[Abstraction and Complexity (2004)](#Abstraction and Complexity)

[The Shape of Information (2005)](#The Shape of Information)

[Remixability and Modularity (2005)](#Remixability and Modularity)

[Scale Effects (2005)](#Scale Effects)

[Introduction to Info-Aesthetics (2005)](#Introduction to Info-Aesthetics)

[Image Future (2006)](#Image Future)

[Friendly Alien: Object and Interface (2006)](#Friendly Alien: Object and Interface)

[Social Data Browsing (2006)](#Social Data Browsing)

[Import/Export: Design Workflow and Contemporary Aesthetics (2006)](#Import/Export: Design Workflow and Contemporary Aesthetics)

[After Effects, or Velvet Revolution. Part I (2006)](#After Effects, or Velvet Revolution. Part I)

[After Effects, or Velvet Revolution. Part II (2006)](#After Effects, or Velvet Revolution. Part II)

[Alan Kay’s Universal Media Machine (2006)](#Alan Kay’s Universal Media Machine)

[Understanding Hybrid Media (2007)](#Understanding Hybrid Media)

[Information as an Aesthetic Event (2007)](#Information as an Aesthetic Event)

[What Comes After Remix? (2007)](#What Comes After Remix?)

[Designing Shanghai, or Why East Is the New West (2007)](#Designing Shanghai, or Why East Is the New West)

[The Practice of Everyday (Media) Life (2008)](#The Practice of Everyday (Media) Life)

# "Real" Wars: Esthetics and Professionalism in Computer Animation

_author: Lev Manovich_
_year: 1991_

## Introduction

The rise of modern mass culture has created a new profession — a designer. Like other professionals, designers must satisfy their clients while upholding a professional identity. Unlike the others, designers also establish their professionalism through the appearances of their products. The appearance of every design product not only reflects the client’s desires but also signals the designer’s excellence. 

I will follow this idea on the example of a particular design field — three-dimensional computer animation created for broadcast and corporate markets. Such animations include companies’ and networks’ logos as well as simulated products and environments as used in commercials and corporate presentations. The distinguishing feature of this design practice is its over-determining concern with illusionism. In the words of the pioneers of computer animation technology, this technology aims to provide simulations of visual reality, "virtually indistinguishable from live-action motion picture photography" and "visually rich as real scenes". [1] Therefore, illusionism in computer animation refers to the simulation of perceptual properties of real-life objects and environments (shape, shading, texture, atmospheric effects) as seen through the simulated codes of traditional cinematography (composition, lighting, choice of lens and camera movement.)

The strive for illusionism in computer animation should not be taken for granted as a natural progress. The notion of illusionism acts as an umbrella for a number of distinct aesthetic standards, such as the smoothness of image and complexity of motion. The role played by these standards is not to make computer-generated images more illusionistic, more life-like, or more persuasive to the viewers. Rather, they allow the designers to signal their professional status, thus serving as the tools of competition within the industry. The struggle for the simulation of the real masks another struggle — the real war for professional survival.

The starting point of my analysis is the theory of culture developed by French sociologist Pierre Bourdieu in his influential Distinction. [2] For Bourdieu, aesthetic choices and judgments are never divorced from practical life, as Kant has it, but, on the contrary, have crucial social functions. Aesthetic preferences have the power to legitimate social distinctions precisely because they are proclaimed to be free of social values, to be grounded in a universal experience of beauty. In particular, the aesthetic taste of the ruling classes served to legitimate their social privilege:

"The denial of low, of course, vulgar ...in a word, natural enjoyment, which constitutes the sacred sphere of culture, implies an affirmation of the superiority of those who can be satisfied with the sublimated, refined, disinterested...pleasures forever closed to the profane. This is why art and cultural consumption are predisposed, consciously and deliberately or not, to fulfill a social function of legitimating social differences." [3]

Bourdieu shows how this aristocratic taste defines itself structurally, by inverting the preferences of popular aesthetics of lower classes. And within the grand hierarchy of high and low aesthetics, every social class distinguishes itself by adopting particular aesthetic values, separating itself from the classes below and above. Thus, the logic of aesthetic choices is one of distinction. This logic justifies both meanings of this word — to distinguish, to separate something from the rest, and at the same time to subordinate, to create the relations of hierarchy.

Although Bourdieu talks about the consumers of cultural products, his insights can be extended to account for the mechanisms of distinction among the producers of culture, including design professionals. Following the argument of Distinction, we can expect that the aesthetics of computer animation is exactly the domain where the distinction between the professionals and the amateurs is legitimated.

In its emphasis on illusionism, the aesthetics of computer animation can be seen as a successor of earlier photographic and film technology. Before confronting computer animation directly, it is useful to see how the mechanisms of professional distinction historically developed in these industries. 

## Amateur-Professional

Recent writings demonstrated the importance of the amateur-professional distinction in shaping photographic [4] and film industries [5] around the turn of the 20th century. The professional and the amateur come into existence simultaneously and are defined in relation to each other, as separate markets. Crucial for the creation of this division is the adaptation of a particular technical standard, requiring considerable investment, as a professional format (such as 35 mm in the film industry). Unable to afford the cost of professional equipment, amateurs are excluded from the competition in the professional’s market. As Yefimov says:

Amateurs were defined in purely technical terms and were those practitioners who were not yet able to see a financial return on their inventions or practices but, through the system of patents, had a real possibility and a great chance of becoming entrepreneurs. The aesthetic and social terms of amateurism appear only much later. At the next stage, the professional and amateur markets are separated, when a few companies establish professional standards for equipment, film, and distribution, and thus control access to a large industry segment. The amateurs still compete in the arena of "substandard" cost-cutting technology, hoping to eventually hold patents and introduce standards on their own. Eventually, the professional segment of the market subsumes the amateur market by turning competitors into consumers. [6]

Today, the importance of technical standards in keeping amateurs from entering into professional markets can be seen even more clearly in the video industry. The acceptable video signal for broadcasting is explicitly regulated by the Federal Communications Commission. This specification excludes those who can’t afford the high initial investment of professional NTSC equipment from entering into the video production for broadcast. The amateur users are defined by their own standards (VHS, 8 mm). The professional video market is itself segmented into sub-markets, each competing for different clients, each defined through the use of a particular tape format. Companies in the industrial market use 3/4 or BETA, those who produce for broadcast —1 inch or component BETA; and top players use digital formats. The difference in quality between the formats in not very substantial, but the cost differential is steep. 

## Aesthetic and technical standard

Industry defined technical standards (such as NTSC video signal in broadcast and 35 mm gauge in film) are just one mechanism by which the professional — amateur distinction is sustained in film, video and photography. There are also unspoken standards on the level of the aesthetic.

Re-examining the history of these industries, we can see that the creation of the aesthetic standards went along with the adaptation of technical ones. For instance, in the era of classical Hollywood cinema, just eight companies controlled 95% of the US box office receipts. As Roy Armes points out, "outsiders were kept from this selected "club" not by patent rights and restrictions, but by the level of investment required". [7] The prohibitive cost of individual film production included not only the cost of professional equipment but also the costs of elaborate sets and costumes, services of numerous technicians, actor’s fees, etc.

Thus, the particular aesthetic standard of movie industry, rather than catering to a pre-existent taste of audiences for grandiose spectacles, was deliberately constructed by those who controlled the market to legitimate their monopoly. This, of course, is in perfect agreement with Bourdieu’s proposition that aesthetic ideals, rather than being grounded in essentialist experiences of pleasure and beauty, serve to legitimate social privileges.

In a similar fashion, in computer animation, the competition among the professionals and the exclusion of amateurs is also supported by aesthetic standards. These aesthetic standards function like the technical standards in that they justify the high investment required to produce a truly professional production. The widely held notion of illusionism of computer animation conceals these standards by claiming them to be the features of perceptual reality, rather than the mechanisms of professional identification.

In the next three sections, I will consider some of these aesthetic standards and the ways in which they are achieved by those who commission and produce computer animations. 

## Smoothness

The separation between amateurs and professionals in computer animation is, first of all, achieved through the aesthetic standard of smoothness. Professionals in computer animation are recognized as those who can produce smooth images — made possible with more costly equipment.

Inexpensive software/hardware for 3D modeling and animation uses the same algorithms as the most expensive professional packages and it provides the user with the same design tools. Yet, while the amateur can design 3D images as sophisticated as the ones produced by professional companies, the final images will unmistakably look non-professional, with jagged lines and continuous tones broken into visible stripes. In contrast to this characteristic "low-res" look, the images produced on expensive equipment have sharp lines and smooth transitions between colors. This happens not only because more expensive displays have higher resolution and larger color pallets, which automatically make any picture smoother. Most importantly, professional 3D software includes antialiasing — special algorithms which compensate for the limited resolution of displays, producing smooth images even with low-res displays. In addition, while both cheap and expensive packages offer the ability to do texture mapping (the technique of wrapping images around objects, which is the primary tool in modeling natural-looking scenes), only the expensive ones can antialiased maps properly, resulting in smooth images. Thus, amateur CG equipment excludes exactly those features which are needed to produce professional-looking images. The "clean" look of properly antialiased images is presented as more illusionistic and then the "jaggies" of inexpensive equipment, functions to separate professionals from the amateur users, at the same time legitimating their difference.

Among professionals who already use expensive equipment with antialiasing capacity, the criterion of smoothness takes on a metaphorical dimension. The competition for the sleekest images is carried out not just through more expensive technology but also through the choice of what the images represent. Commercial 3D designs uniformly model particular types of materials. While some man-made materials are smooth (metal, glass, plastic), most natural objects have irregular textures, rough edges, uneven color, etc. Although the techniques to model this irregularity of natural phenomena are well known, they are very rarely used in commercial 3D animations. Instead, the typical logo animation presents an environment made from super-smooth "high-tech" surfaces, a world of glass, plastic and reflecting chrome. Such an image becomes a visible metaphor of smoothness, clearly signaling a designer’s professionalism.

And when natural phenomena are simulated, the criterion of smoothness is still maintained. Many computer graphics researchers spend their careers perfecting algorithms to simulate the geometry of natural phenomena, atmospheric lighting effects, environmental reflections and so on. They devote months and even years polishing a single image or a short animation which will demonstrate the state-of-the-art in digital simulation of nature. The images incorporate a tremendous amount of detail, yet the "nature" in them looks too clean, too airbrushed, too unnatural. [8] The authors stop short of putting in their images the full irregularity and "dirtiness" of real scenes — because it may threaten their professional identity, associated with the ability to produce "clean" images. To present truly irregular images is risky, since the imperfections may be attributed not to the intentional modeling of natural randomness, but to cheap equipment or animator’s mistakes!

## Complexity

If any single still in a computer animation strives for smoothness, the most general aesthetic standard of the entire animation is complexity. It is related to the fact that animation, by default, involves a sequence of images in which some changes take place. The standard of complexity means that the more changes take place from frame to frame, the higher an animation is valued. 

As with smoothness, to understand the function of complexity, we should ask what it signifies for the designers and the patrons of computer animation. The smoothness of an image points to the cost of equipment since expensive equipment automatically produces smoother images. Therefore, a sleek image signifies both designer’s professionalism (since the designer who owns expensive equipment is automatically a pro) and the prestige of a client who can afford to pay for the use of this expensive equipment. With complexity, the difference between hardware and software is not that important. To make the animation more complex requires more labor; thus, the visual complexity of a 3D animation is directly related to its cost to the client, and functions as a sign of client’s wealth.

The quest for complexity is reflected in numerous design strategies adopted by designers of 3D computer animations. These strategies can be grouped in two categories: increasing the amount of all kinds of movement and complicating the geometry of the scene.

In a sophisticated animation, all objects in the scene move, changing their positions, shapes, or colors. The complex motion of an object or the complicated camera move is preferred over simple ones; changing background (for instance, moving clouds) is favored over a static one. But movement is not limited to the physical motion of objects. It is desirable to have moving lights and/or use reflection maps, so that in the course of an animation highlights on the object and its colors constantly change. Recently, it has also become trendy to use "stretch" and "squash" of traditional Disney animation, making all objects change their shapes as they move. While the aesthetics of anthropomorphism of classical Disney code deserves a special analysis, one reason for its comeback in computer animation is that it makes possible more complex animations [9], separating top companies which, through the use of custom software, can incorporate "stretch" and "squash" effects, from the users of turnkey systems, limited to rigid objects.

Adding all kinds of motion makes a final animation more complex. Alternatively, visual complexity of an animation can be increased by complicating the geometry of the scene itself. Individual objects should have as much detail as possible. This is achieved by making objects more detailed geometrically, but also by choosing one shading algorithm over another. Simple algorithms shade surfaces flat, while more complex ones turn them into continuous gradations, resulting in "rich" looking objects. Indeed, such objects are literally richer, since more complex algorithms take more time to execute, thus costing a client more.

Apart from the degree of detail in single objects, the design of the scene as a whole should point to the amount of labor which went into it. This works in an interesting way in the animations of logos, which are the bread and butter of the commercial 3D animation field. The default (and least expensive) animation involves bringing the logo into its final position on the screen in some way. Typically, a logo as a whole flies in, or its pieces assemble into the whole. But in top-of-the-line animations, the designer builds a whole architectonic construction, involving many pieces, with the logo being just one of them. Such designs can be explained through the competitive pressure to increase the complexity of an animation, since there is just so much one can do with a logo itself.

## Computer look

The aesthetic standards of smoothness and complexity, discussed so far, function to signify the professionalism of computer animation designers and the prestige of their clients. A client distinguishes himself by commissioning the state-of-the-art computer animation, complex and sleek. Similarly, the choice of computer animation over other media is already significant, already prestigious in itself. Currently, a 3D animation is still the most expensive video effect. Besides signifying monetary investment, computer-generated visuals readily function as signs of high-tech and scientific progress — something every company wants to be identified with. The use of computer animation signifies that the client is rich enough to pay for it and modern enough to use it. Thus, the use of computer animation is already a message — but only if the appearance of the images clearly reveals their computer origin.

3D animation, in principle, can be quite photo-realistic and hardly distinguishable from a real scene recorded on film. But if designed in this fashion, its main symbolic value is gone. Accordingly, commercial 3D pieces incorporate various strategies that obviate their artificiality, even though separate elements can be quite illusionistic. These design strategies include the choice of very wide lenses, rollercoaster-like camera moves, the placement of objects in neutral space rather than in a familiar environment, the use of abstract design elements, exaggerated reflections, and so on. The overall geometric look in even top-of-the-line animations, which professional discourses blame on the hardware/software limitations, also can be seen as quite intentional: actually downplaying the current technical possibilities and using newer technology to simulate the old, familiar "computer look."

## Conclusion

In considering the mechanisms of professional distinction in the field of computer animation, I established that the aesthetic standards function like the technical standards. The conjunction "like" implies both similarity and difference, and we now may be in a better position to see what these similarities and differences are.

Both aesthetic and technical standards establish the requirement for a considerable investment needed to compete in the professional market. But while they have the same function, aesthetic standards appear to be the more effective mechanism to legitimate the status of design professionals.

As Bourdieu proposed, it is the proclaimed disinterestedness and universality of the aesthetic that makes it the most powerful mechanism to legitimate social distinctions. As with other aesthetic judgments and tastes, the aesthetic standards are justified with the reference to the universal aesthetic experiences of beauty and pleasure and thought to be divorced from the pragmatic interests of social life. In contrast, the technical standards are explicitly formulated by the industry organizations and enforced by government regulations. Accordingly, while the connection between technical standards and the interests of professional agencies is obvious, the aesthetic standards are thought to be unrelated to the interests of any agency. Thus, the standards of smoothness and complexity in 3D computer animation are so effective in protecting the status of the professionals because they figure in the realm of the aesthetic.

## References:

[1] R. Cook, L. Carpenter and E. Catwill, "The Reys Image Rendering Architecture," Computer Graphics, 21, No. 4 (1987), p. 95.

[2] Pierre Bourdieu, Distinction. A Social Critique of the Judgement of Taste (Cambridge: Harvard University Press, 1984.)

[3] Bourdieu, p. 7.

[4] Alla Yefimov, The Borders of Representation: the Amateur Described, unpublished manuscript, 1989.

[5] Patricia Zimmerman, "Entrepreneurs, Engineers, and Hobbyists: The Formation of a Definition of Amateur Film, 1897-1923," in Current Research in Film, ed. Bruce Austin (Norwood, N.J.: Ablex Publishing Company, 1987), pp.163-188.

[6] Yefimov, pp. 5-6.

[7] Roy Armes, On Video (London: Routledge, 1988), p. 52.

[8] For example, see Jules Bloomenthal, "Modeling the Mighty Maple," Computer Graphics, 19, No. 3 (1985), pp. 305-312; Geoffrey Gardner, "Visual Simulation of Clouds," Computer Graphics, 19, No. 3 (1985), pp. 297-304; Kenton Musgrave, Craig Kolb and Robert Mace, "The Synthesis and Rendering of Eroded Fractal Terrains," Computer Graphics, 23, No. 3 (1989), pp. 41-50.

[9] John Lasseter, "Principles of Traditional Animation as Applied to 3D Computer Animation," Computer Graphics, 21, No. 4 (1987), pp. 35-44.

---
# Assembling Reality: Myths of Computer Graphics

_author: Lev Manovich_
_year: 1992_

## Giotto, the Inventor of 3D

This is how Frederick Hartt, the author of Art. History of Painting, Sculpture, Architecture, a widely used textbook, describes the importance of Giotto di Bondone, "the first giant in the long history of Italian painting": "In contemporary Italian eyes the step from Cimabue to Giotto was immense in that weight and mass, light and inward extension were suddenly introduced in a direct and convincing manner" (503). "Giotto's miracle lay in being able to produce for the first time on a flat surface three-dimensional forms, which the French could achieve only in sculpture." "For the first time since antiquity, a painter has truly conquered solid form" (504).

When the students in the introductory art history survey course at the University of Rochester which uses Hartt's textbook were asked to compare Giotto and Cimabue, they described Giotto's achievements in a somewhat different language: "Giotto first achieves strong 3D effect"; "Cimabue is still 2D, while Giotto has much more of 3D".

In his recent history of vision in the 19th century, Jonathan Crary suggests that the rapid development and diffusion of various computer graphics technologies in the last decade constitutes a "transformation in the nature of visuality probably more profound than the break that separates medieval imagery from Renaissance perspective" (Crary 1). What makes computer graphics so valuable to education and advertising, business and the military, science and entertainment industry is their ability to simulate three-dimensional images of both existent and imagined objects and environments. Usually, the viewer sees these simulated objects as images on a flat screen, however, new interfaces are being developed (virtual reality, computer holography) to enhance the illusion of their three-dimensional presence.

"Realism" is the concept that inevitably accompanies the development and assimilation of computer graphics technologies. In media, trade publications and research papers, the history of technological innovation and research are presented as a progression toward realism — the ability to simulate any object in such a way that its computer image is indistinguishable from a photograph. At the same time, it is constantly pointed out that this realism is qualitatively different from the realism of optically based image technologies (photography, film), for the simulated reality is not indexically related to the existing world. 

Despite this difference, the ability to generate three-dimensional stills does not represent a radical break in the history of visual representation of the multitude comparable to the achievements of Giotto. A Renaissance painting and a computer image employ the same technique (a set of consistent depth cues) to create an illusion of space — existent or imaginary. The real break is the introduction of a moving synthetic image — interactive 3D computer graphics and computer animation. With these technologies, a viewer has an experience of moving around the simulated 3D space — something one can't do with a painting.

In order to better understand the nature of "realism" of the synthetic moving image, it is relevant to consider a contiguous practice of the moving image — the cinema. I will approach the problem of "realism" in 3D computer animation starting from the arguments advanced in film theory in regard to cinematic realism. First, I review the key accounts which situate the realism of film in the histories of cinematic technology and style. The next section tests the models suggested in these accounts on the history of computer animation and computer graphics research. The third section shifts emphasis, considering realism in computer animation as an effect of subject matter.

## Technology and Style in Cinema

The idea of cinematic realism is first of all associated with André Bazin, for whom cinematic technology and style move toward a "total and complete representation of reality" (20). In "The Myth of Total Cinema" Bazin claims that the idea of cinema existed long before the medium had actually appeared and that the development of cinema technology "little by little made a reality out of original "myth" (21). In this account, the modern technology of cinema is a realization of an ancient myth of mimesis, just as the development of aviation is a realization of the myth of Icarus. In another influential essay, "The Evolution of the Language of Cinema," Bazin reads the history of film style in similar teleological terms: the introduction of the depth of field style at the end of the 1930s and the subsequent innovations of Italian neorealists in 1940s gradually bring a spectator "into a relation with the image closer to that which he enjoys with reality". The essays differ not only in that the first interprets film technology while the second concentrates on film style but also in their distinct approaches to the problem of realism. In the first essay, realism stands for the approximation of phenomenological qualities of reality, "the reconstruction of a perfect illusion of the outside world in sound, color and relief" (20). In the second essay, Bazin emphasizes that a realistic representation should also approximate the perceptual and cognitive dynamics of natural vision. For Bazin, this dynamics involves the active exploration of visual reality. Consequently, he interprets the introduction of the depth of field as a step toward realism, because now the viewer can freely explore the space of the film image (36-37).

Against Bazin's "idealist" and evolutionary account, Jean-Louis Comolli proposes a "materialist" and fundamentally non-linear reading of the history of cinematic technology and style. The cinema, Comolli tells us, "is born immediately as a social machine...from the anticipation and confirmation of its social profitability; economic, ideological and symbolic" (Comolli 122). Comolli thus proposes to read the history of cinema techniques as an intersection of technical, aesthetic, social and ideological determinations; however, his analyses clearly privilege an ideological function of the cinema. For Comolli, this function is an "objective" duplication of the "real" itself conceived as a "specular reflection" (133). Along with other representational cultural practices, cinema works to endlessly reduplicate the visible thus sustaining the illusion that it is the phenomenal forms (such as the commodity form) which constitute the social "real" — rather than "invisible" to the eye relations of productions. To fulfill its function, cinema must maintain and constantly update its "realism". Comolli sketches this process using two alternative figures — addition and substitution.

In terms of technological developments, the history of realism in the cinema is one of additions. First, additions are necessary to maintain the process of disavowal, which for Comolli defines the nature of cinematic spectatorship (132). Each new technological development (sound, panchromatic stock, color) points to the viewers just how "unrealistic" the previous image was and also reminds them that the present image, even though more realistic, will be superseded in the future — thus constantly sustaining the state of disavowal. Secondly, since cinema functions in a structure with other visual media, it has to keep up with its changing level of realism. For instance, by the 1920s the spread of photography with its finely gradated image made cinematic image seem harsh by comparison, and film industry was forced to change to the panchromatic stock to keep up with the standard of photographic realism (131). This example is a good illustration of Comolli's reliance on Althusserian structuralist Marxism. Unprofitable economically for the film industry, this change is "profitable" in more abstract terms for the social structure as a whole, helping to sustain the ideology of the real/visible.

In terms of cinematic style, the history of realism in cinema is one of the substitutions of cinematic techniques. For instance, while the change to panchromatic stock adds to the image quality, it leads to other losses. If earlier cinematic realism was maintained through the effects of depth, now "depth(perspective) loses its importance in the production of "reality effects" in favor of shade, range, color" (131). So theorized, realistic effect in the cinema appears as a constant sum in an equation with a few variables which change historically and have equal weight: if more shading or color is "put in," perspective can be "taken out". Comolli follows the same logic of substitution/subtraction in sketching the development of cinematic style in its first two decades: the early cinematographic image announces its realism through an abundance of moving figures and the use of deep focus; later these devices fade away and others, such as fictional logic, psychological characters, coherent space-time of narration, take over (130).

While for Bazin realism functions as an Idea (in a Hegelian sense), for Comolli it plays an ideological role (in a Marxist sense); for David Bordwell and Janet Staiger, realism in film is first of all connected with the industrial organization of cinema. Put differently, Bazin draws the idea of realism from mythological utopian thinking. For him, realism is found in the space between reality and a transcendental spectator. Comolli sees it as an effect, produced between the image and the historical viewer and continuously sustained through the ideologically determined additions and substitutions of cinematic technologies and techniques. Bordwell and Staiger locate realism within the institutional discourses of film industries, implying that it is a rational and pragmatic tool in industrial competition.

Emphasizing that cinema is an industry like any other, Bordwell and Staiger attribute the changes in cinematic technology to the factors shared by all modern industries — efficiency, product differentiation, maintenance of a standard of quality (247). One of the advantages of adopting an industrial model is that it allows the authors to look at specific agents — manufacturing and supplying firms and professional associations (250). The latter is particularly important since it is in their discourses (conferences, trade meetings and publications) that the standards and goals of stylistic and technical innovations are articulated.

Bordwell and Staiger agree with Comolli that the development of cinematic technology is not linear, however, they claim that it is not random either, as the professional discourses articulate goals of the research and set the limits for permissible innovations (260). According to Bordwell and Staiger, realism is one of these goals. They believe that such definition of realism is specific to Hollywood:

"Showmanship", realism, invisibility: such cannons guided the SMPE members toward understanding the acceptable and unacceptable choices in technical innovations, and these too became teleological. In another industry, the engineer's goal might be un unbreakable glass or a lighter alloy. In the film industry, the goals were not only increased efficiency, economy, and flexibility but also spectacle, concealment of artifice, and what Goldsmith [1934 president of SMPE] called "the production of an acceptance semblance of reality". (258)

Bordwell and Staiger are satisfied with Goldsmith's definition of realism as "the production of an acceptance semblance of reality". However, such general and transhistorical definition does not seem to have any specificity for Hollywood and thus can't really account for the direction of technological innovation. Moreover, although they claim to have successfully reduced realism to a rational and a functional notion, in fact, they have not managed to eliminate Bazin's idealism. It reappears in the comparison between the goals of innovation in film and other industries. "Lighter alloy" is used in aviation industry which can be thought of as the realization of the myth of Icarus, and is there not something mythical and fairytale-like about "unbreakable glass"?

## Technology and Style in Computer Animation

How can these three influential accounts of cinematic realism be used to approach the problem of realism in computer animation? Bazin, Comolli, and Bordwell and Staiger offer us three different strategies, three different starting points. Bazin builds his argument by comparing the changing quality of the cinematic image with the phenomenological impression of visual reality. Comolli's analysis suggests a different strategy: to think of the history of computer graphics technologies and the changing stylistic conventions as a chain of substitutions functioning to sustain the reality effect for audiences. Finally, to follow Bordwell and Staiger's approach is to analyze the relationship between the character of realism in computer animation and the particular industrial organization of the computer graphics industry. (For instance, we can ask how this character is affected by the cost difference between hardware and software development.) Further, we should pay attention to professional organizations in the field and their discourses which articulate the goals of research and where we may expect to find "admonitions about the range and nature of permissible innovations" (Bordwell and Staiger 260). I will try the three strategies in turn.

If we follow Bazin's approach and compare images drawn from the two decades of the history of computer graphics with the visual perception of natural reality, his evolutionary narrative appears to be confirmed. Images progress towards fuller and fuller illusion of reality: from wireframe displays to smooth shadows, intricate textures, aerial perspective; from geometric shapes to moving animal and human figures; from Cimabue to Giotto to Leonardo and beyond. Bazin's idea that deep focus cinematography allowed the spectator a more active position in relation to film image, thus bringing cinematic perception closer to real-life perception, also finds a recent equivalent in interactive computer graphics, where the user can freely explore the virtual space of the display from different points of view. And with such extensions of computer graphics technology as virtual reality, the promise of Bazin's "total realism" appears to be closer than ever, literally within arms reach of virtual reality's user.

The history of the style and technology of computer animation can also be seen in a different way. Comolli reads the history of realistic media as a constant trade-off of codes, a chain of substitutions producing the reality effect for audiences, rather than as an asymptotic movement toward the axes labeled "reality". His interpretation of the history of film style is first of all supported by the shift he observes between the cinematic style of the 1900s and the 1920s, the example I have already mentioned. Early, film announces its realism by excessive representations of deep space achieved through every possible means: deep focus, moving figures, frame compositions which emphasize the effect of linear perspective. In the 1920s, with the adaptation of panchromatic film stock, "depth(perspective) loses its importance in the production of "reality effects" in favor of shade, range, color" (Comolli, 131). A similar trade-off of codes can be observed during the short history of commercial 3D computer animation. Initially, the single frames of animations were schematic, cartoon-like because the objects could only be rendered in wireframe or facet-shaded form. Illusionism was limited to the indication of objects' volumes. To compensate for this limited illusionism of a single image, computer animations of the early 1980s ubiquitously showed deep space. This was done by emphasizing linear perspective (mostly, through the excessive use of grids) and by building animations around rapid movement in depth in the direction perpendicular to the screen. Toward the end of the 1980s, with commercial availability of such techniques as smooth shading, texture mapping and casted shadows, the individual frames of animations approached much closer the ideal of photorealism. At this time, the codes by which early animation signaled deep space started to disappear. In place of rapid in-depth movements and grids, animations began to feature lateral movements in shallow space.

The observed substitution of realistic codes in the history of computer animation seems to confirm Comolli's argument. The introduction of new illusionistic techniques dislodges old ones. Comolli explains this process of sustaining reality effect from the point of view of audiences. Following Bordwell and Staiger's approach, we can consider the same phenomenon from the producers' point of view. For the production companies, the constant substitution of codes is necessary to stay competitive. 

As in every industry, the producers of computer animation stay competitive by differentiating their products. To attract clients, a company has to be able to offer some novel effects and techniques. But why do the old techniques disappear? The specificity of industrial organization of the computer animation field is that it is driven by software innovation. (In this, the field is closer to the computer industry as a whole, rather than film industry or graphic design.) New algorithms to produce new effects are constantly developed. To stay competitive, a company has to quickly incorporate the new software into its offerings. The animations are designed to show off the latest algorithm. Correspondingly, the effects possible with older algorithms are featured less often — available to everybody else in the field, they no longer signal "state of the art". Thus, the trade-off of codes in the history of computer animation can be related to the competitive pressure to quickly utilize the latest achievements of software research.

While commercial companies employ programmers capable of adopting published algorithms for the production environments, the theoretical work of developing these algorithms mainly takes place in academia. To further pursue the question of realism we need to ask about the direction of this work. Do computer graphics researchers share a common goal?

In analyzing the same question for film industry, Bordwell and Staiger claim that realism "was rationally adopted as an engineering aim" (258). They attempt to discover the specificity of Hollywood's conception of realism in the discourses of professional organizations such as SMPE.

For the computer graphics industry, the major professional organization is SIGGRAPH (Special Interest Group on Computer Graphics of the Association for Computing Machinery). Its annual conventions, attended by tenths of thousands, combine a trade show, a festival of computer animation and a scientific conference where the best new research work is presented. The conferences also serve as the meeting place for the researchers, engineers and commercial designers. If the research has a common direction, we can expect to find its articulations in SIGGRAPH proceedings.

Indeed, a typical research paper includes a reference to realism as the goal of investigations in computer graphics field. For example, a 1987 paper presented by three highly recognized scientists offers this definition of realism:

Reys is an image rendering system developed at Lucasfilm Ltd. and currently in use at Pixar. In designing Reys, our goal was an architecture optimized for fast high-quality rendering of complex animated scenes. By fast we mean being able to compute a feature-length film in about a year; high quality means virtually indistinguishable from live-action motion picture photography; and complex means as visually rich as real scenes. (Cook et al, 95. Emphasis mine — L.M.)

Achieving synthetic realism means attaining two goals: to simulate codes of traditional cinematography and to simulate the perceptual properties of real-life objects and environments.

The first goal, the simulation of cinematographic codes, was in principle solved early on as these codes are well-defined and few in number. Every current professional computer animation system incorporates variable length lens, depth of field effect, motion blur and controllable lights.

The second goal, the simulation of "real scenes," turned out to be more complex. Digital recreation of any object involves solving three separate problems: the representation of an object's shape, the effects of light, and the pattern of movement. To have a general solution for each problem requires the exact simulation of underlying physical properties and processes. This is impossible because of the extreme mathematical complexity. For instance, to fully simulate the shape of a tree would involve mathematically "growing" every leaf, every brunch, every piece of bark; and to fully simulate the color of a tree's surface a programmer has to consider every other object in the scene, from grass to clouds to other trees. In practice, computer graphics researchers have resorted to solving particular local cases, developing a number of unrelated models for simulation of some kinds of shapes, materials and movements. Thus, the photorealistic simulation of "real scenes" is practically impossible as techniques available to commercial animators only cover the particular phenomena of visual reality. The animator can easily create a shape of a human face, but not the hair; the materials such as plastic or metal but not cloth or leather; the flight of a bird but not the jumps of a frog. The realism of computer animation is highly uneven, reflecting the range of problems which were addressed and solved.

What determines which particular problems received priority in research? To a large extent, this was determined by the needs of the early sponsors of this research — the Pentagon and Hollywood. I am not concerned here to fully trace the history of these sponsorships. What is important for my argument is that the requirements of military and entertainment applications determined the concentration of research to simulate the particular phenomena of visual reality, such as landscapes and moving figures.

One of the original motivations behind the development of photorealistic computer graphics was its application for flight simulators and other training technology (Goodman 22, 102). And since simulators require synthetic landscapes, a lot of research went into the techniques to render clouds, rugged terrains, trees, aerial perspective. Thus, the work which led to the development of the famous technique to represent natural shapes (such as mountains) using fractal mathematics was undertaken at Boeing (Carpenter et all). Other well-known algorithms to simulate natural scenes and clouds were developed by the researchers of Grumman Aerospace Corporation (Gardner). The latter technology was used for flight simulators and also was applied to pattern recognition research in target tracking by a missile (Gardner 1984: 19).

Another major sponsor was the entertainment industry, lured by the promise of lowering the costs of film and television production. In 1979 Lucasfilm, Ltd., George Lucas's company, organized a computer animation research division. It hired the best computer scientists in the field to produce animations for special effects. The research for the effects in such films as Star Trek II: The Wrath of Khan and Return of the Jedi have led to the development of important algorithms which became widely used (Reeves 1983). Along with special effects, a lot of research activity has been dedicated to the development of moving humanoid figures and synthetic actors, since commercial film and video productions center around characters. Significantly, the first time computer animation was used in a feature film (Looker, 1980) was to create a three-dimensional model of an actress. One of the early attempts to simulate human facial expressions featured synthetic replicas of Marilyn Monroe and Humphrey Bogart (Magnenat-Thalmann & Thalmann). In another acclaimed animation, produced by Kleiser-Walczak Construction Company in 1988, a synthetic human figure was humorously cast as Nestor Sextone, a candidate for the presidency in the Synthetic Actors Guild. 

## The icons of mimesis

While the privileging of certain areas in research can be attributed to the needs of the sponsors, other areas received consistent attention for a different reason. To support the idea of progress of computer graphics toward realism, researchers privilege particular subjects that culturally connote the mastery of mimetic representation.

Historically, the idea of mimesis has been connected with the success in illusionistic representation of certain subjects. The original episode in the history of Western painting is the story of the competition between Zeuxis and Parrhasiuss. The grapes painted by Zeuxis symbolize his skill to create living nature out of inanimate matter of paint. Further examples in the history of art include the celebration of the mimetic skill of those painters who were able to simulate another symbol of living nature — the human flesh.

While the painting tradition had its own iconography of subjects connoting mimesis, moving image media relied on different sets of subjects. Steven Neale describes how early film demonstrated its authenticity by representing moving nature: "What was lacking [in photographs] was the wind, the very index of real, natural movement. Hence the obsessive contemporary fascination, not just with movement, not just with scale, but also with waves and sea spray, with smoke and spray" (52). Computer graphics researchers resort to similar subjects to signify the realism of animation. "Moving nature" presented at SIGGRAPH conferences has included animations of smoke, fire, sea waves, and moving grass (Perlin; Max; Reeves and Blau). These privileged signs of realism overcompensate for the inability of computer graphics researchers to fully simulate "real scenes". 

## Conclusion

Bazin's Evolution of the Language of Cinema is a compilation of three articles written between 1952 and 1955.

In 1951 the viewers of the popular television show "See it Now" for the first time saw a computer graphics display, generated by M.I.T. computer Whirlwind, built in 1949. One animation was of a bouncing ball, another of a rocket's trajectory (Goodman 18-19).

Comolli's Machines of the Visible was given as a paper at the seminal conference on the cinematic apparatus in 1978.

The same year saw the publication of a crucial paper on the history of computer graphics research. It presented a method to simulate bump textures which is still one of the most powerful techniques of synthetic photorealism (Blinn).

Bordwell and Staiger's chapter Technology, Style and Mode of Production forms a part of the comprehensive The Classical Hollywood Cinema: Film Style & Mode of Production to 1960, published in 1985. By this year, most of the fundamental photorealistic techniques were discovered and turnkey computer animation systems were already employed by media production companies.

By 1991, 3D computer animation becomes the yardstick by which realism of any visual representation is measured. A student in art history class writes: "Renaissance perspective is the first development of realistic 3D".

On the one hand, computer graphics is employed to mimic the existing optically based modes of representation — photography and film. The goal of the research is the achievement of photorealism.

On the other hand, synthetic realism is fundamentally different from the realism of the optical media, being partial and uneven, rather than analog. This difference presents a theoretical challenge to the existing accounts of realism developed on the analyses of optical media. During the 20th century, these media were the culturally dominant modes of visual representation and set the standards of mimetic imagination. Presently, the role of 3D synthetic imagery is rapidly becoming more important. If we want to understand the transformations of contemporary visual culture, the problem of realism has to be studied afresh.

## References:

[1] Bazin, André, trans. What is Cinema? 2 vols. Berkeley: University of California Press, 1967, Vol. 1.

[2] Blinn, J. F. "Simulation of Wrinkled Surfaces". Computer Graphics. (August 1978): 286-92.

[3] Bordwell, David and Janet Staiger. "Technology, Style and Mode of Production". The Classical Hollywood Cinema. David Bordwell, Janet Staiger and Kristin Thompson. New York: Columbia University Press, 1985. 243-261.Carpenter, L., A. Fournier and D. Fussell. "Fractal Surfaces". Communications of the ACM. 1981.

[4] Comolli, Jean-Louis. "Machines of the Visible". The Cinematic Apparatus. Ed. Teresa De Lauretis and Steven Health. New York: St.Martin Press, 1980. 121-142.Cook, R., L. Carpenter and E. Catull. "The Reys Image Rendering Architecture". Computer Graphics. 21.4 (1987): 91-102.

[5] Crary, Jonathan. Techniques of the Observer. Cambridge, MA and London: MIT Press, 1990.

[6] Gardner, Geoffrey Y. "Simulation of Natural Scenes Using Textured Quadric Surfaces". Computer Graphics. 18.3 (1984): 21-30.

[7] Gardner, Geoffrey Y. "Visual Simulation of Clouds". Computer Graphics. 19.3 (1985): 297-304.

[8] Goodman, Cynthia. Digital Visions. New York: Harry N. Abrams, Inc., 1987.

[9] Magnenat-Thalman and Thalman. "The Direction of Synthetic Actors in the Film "Rendezvous a Montreal"." IEEE Computer Graphics and Applications. December 1987.

[10] Manovich, Lev. "Real" Wars: Aesthetics and Professionalism in Computer Animation". Design Issues. 8.1. (Fall 1991): 18-25.

[11] Max, Nelson. "Vectorized procedure models for natural terrain: waves and islands in the sunset". Computer Graphics. 15.3. (1981).

[12] Neale, Steve. Cinema and Technology. Bloomington: Indiana University Press, 1985.

[13] Perlin, Ken. "An Image Synthesizer". Computer Graphics. 19.3 (1985): 287-296.

[14] Reeves, William T. "Particle Systems — A Technique for Modeling a Class of Fuzzy Objects". ACM Transactions on Graphics. 2.3 (1983): 91-108.

[15] Reeves, William T. and Ricki Blau. "Approximate and Probabilistic Algorithms for Shading and Rendering Structured Particle Systems". Computer Graphics. 19.3 (1985): 313-322.

[16] The difference between the earlier and the more recent computer animation images can also be observed in synchronic dimensions. It is the difference between the state of the art professional production and the amateur production which is forced to rely on the outmoded technology. See Manovich 1991. 

---

# The Mapping of Space: Perspective, Radar, and 3-D Computer Graphics

_author: Lev Manovich_
_year: 1993_

1991 saw two events of different importance and seemingly unrelated. One was the long-awaited publication in English of what can probably be called the single most influential essay of modern art history — Erwin Panofsky's "Die Perspektive als "symbolische Form"." [1] The interest generated around the re-emergence of this legendary essay, written in 1924-1925, [2] demonstrates that the problem of perspectival representation is still felt to be relevant to contemporary culture. The second event was the Gulf War, the outcome of which was largely predetermined by Western superiority in the techniques of perspectival representation.

The images, extensively televised during the Gulf War, perfectly confirmed Paul Virilio's thesis that modern warfare became a matter of the "logistics of perception". [3] True, broadcasts have included more traditional views of soldiers, planes and tanks as seen from the outside, by a video camera of a reporter. But what we also saw were not just images of the war, but endless images of the means by which the war was carried out: video images from an infrared camera mounted on a plane; video images from a camera installed on a weapon guided by a laser sensor; video in its role as "battle damage assessment" where a weapon equipped with an imaging device follows a weapon of destruction and records details of the damage. This was no longer a traditional reporter's view of a battle. We saw what the soldiers themselves saw: the images that were their only information about the enemy. More often, in a strange case of identification, we witnessed what was "seen" by a machine, a bomb, or a missile. The Gulf War was the combat of surveillance against camouflage, visibility against invisibility, human eye against computer eye. This warfare was indeed based on the "logistics of perception", but we can describe its visual techniques even more precisely. Visual perception was employed in a limited way as an instrument to capture and represent information about shapes and distances in three-dimensional space. The effectiveness of such war technologies as radar, infrared imaging, laser sensors, and 3-D computer graphics depends on the automation of this function of vision, the automation that began with the Renaissance perspective.

The use of these technologies today extends beyond warfare into all spheres of industry and science. Is there an appropriate term to describe the function of vision which they automate? For Plato, sensible particulars were but a pure reflection of Ideas or Forms. Aristotle criticized Plato declaring that the primary substances were not the Ideas but the individual things such as particular men or animals. These opposing views continued to be debated in scholastic philosophy, Plato's view giving rise to realism, and Aristotle's — to nominalism.

This essay will discuss twentieth-century automation of what can be called visual nominalism — the use of vision to capture the identity of individual objects and spaces by recording distances and shapes. The automation of this function of vision started well before the century with the development of various perspectival techniques and technologies: perspective machines, descriptive and perspective geometry, and photography. But only digital computers made possible mass automation in general, including the automation of visual nominalism.

## The Most Important Event of the Renaissance

According to a widely accepted narrative, perspective was already dead by the time art historians such as Panofsky began writing its history. Such narrative is announced, for instance, in the very title of Pierre Francastel's _Painting and Society. Birth and Destruction of Plastic Space from Renaissance to Cubism_ (1952). The opening section of _The Production of Space_ by Henri Lefebvre is equally authoritative: "The fact is that around 1910 a certain space was shattered. It was a space of common sense, of knowledge (savoir), of social practice, or political power ...a space, too, of classical perspective and geometry, developed from the Renaissance onwards based on the Greek tradition (Euclid, logic) and bodied forth in Western art and philosophy, as in the form of the city and town." [4]

Yet, if perspective disappeared from modern art, it survived as one of the techniques of visual nominalism, a method for precisely representing a three-dimensional world on a two-dimensional surface. In this role, it extended into new domains (the whole of electromagnetic spectrum) and became the foundation of new kinds of automated remote sensory technologies.

To consider perspective in this role we should turn to William Ivins's influential 1939 essay "On the Rationalization of Sight". Ivins's approach stands in sharp contrast to the more traditional art historical analyses of perspective by Panofsky and Francastel. They are concerned with perspective as an artistic form and do not look beyond its history in art. Ivins, on the contrary, is concerned with visual culture — the techniques and technologies of visual representation available to society at a given moment and the fundamental role they play in shaping every aspect of society. Ivins argued that perspective allows to create precise maps of three-dimensional reality, to record the shapes of concrete objects and the layout of concrete spaces. [5] It is the tool of a businessman and a scientist rather than an artist. In Ivins's definition, perspective is "a practical means for securing a rigorous two-way or reciprocal, metrical relationship between the shapes of objects as definitely located in space and their representations". [6]

Thus Ivins singles out the precise relationship established between objects and their representations as the most important principle of perspective. Bruno Latour recently extended this idea by pointing out that this relationship made possible by perspective allows us not only to represent reality but also to control it. [7] Latour sees perspectival representations as the "most powerful instrument of power," defined as the ability to mobilize resources across space and time, to manipulate these resources at a distance. For instance, we can't measure the sun in space directly, but we only need a small ruler to measure it on a photograph (perspectival image par excellence). And even if we could fly around the sun, we would still be better off studying the sun through its representations which we can bring back from the trip — because now we have unlimited time to measure, analyze, and catalog them. We can also move objects from one place to another by simply moving their representations: "You can see a church in Rome, and carry it with you in London in such a way as to reconstruct it in London, or you can go back to Rome and amend the picture." Finally, as Latour points out, "the two ways become a four-lane freeway! Impossible palaces can be drawn realistically, but it is also possible to draw possible objects as if they were utopian ones." Real and imagined objects can meet on a flat space of perspectival representation.

Ivins concludes his essay by stating that the beginning of the rationalization of sight through the discovery and the development of perspective "was the most important event of the Renaissance". The invention of perspective propelled modern empirical science, for instance, biology which could now represent forms of nature with mathematical precision. It also stimulated the rise of modern engineering and manufacturing by making feasible the distribution of identical designs to faraway places. Modern designers, scientists, or engineers, of course, do not simply use perspective in the form in which it was formulated by Alberti in the fifteenth century: they have at their disposal much more sophisticated techniques. According to Ivins, the rationalization of perspectival sight proceeded in two directions. On the one hand, perspective became the foundation for the development of the techniques of descriptive and perspective geometry which became a standard visual language of modern engineers and architects. On the other hand, the photographic technologies automated the creation of perspectival images. Both were the accomplishments of the nineteenth century; in fact, both were developed more or less simultaneously. Indeed, as Ivins points out, Nièpce and Talbot, the founders of photography, were contemporaries of Monge and Poncelet, the decisive figures in the development of descriptive and perspective geometry.

## Radar: Seeing Without Eyes

Writing  _On Rationalization of Sight_ between 1936 and 1938, Ivins mentions such examples of the contemporary use of perspective as aerial photographic surveillance, classification in the field of archeology, and criminal detection. [8] However, all these applications of perspectival techniques already existed in the nineteenth century and, by the 1930s, did not represent the latest developments.

While photo reconnaissance was first employed systematically on a mass scale during World War I, the interest in using photography for aerial surveillance existed since its invention. Nadar succeeded in exposing a photographic plate at 262 feet over Bièvre, France in 1858. He was soon approached by the French Army to attempt photo reconnaissance but rejected the offer. In 1882, unmanned photo balloons were already in the air; a little later, they were joined by photo rockets both in France and in Germany.

The only innovation of World War I was to combine aerial cameras with a superior flying platform — the airplane. [9]

In 1858, Albrecht Meydenbauer, a director of the Government Building Office, published a proposal to use photographs for scale measurement. His proposal was based on the existence of a geometrical relationship between photographic image and the object being photographed. Why, for instance, climb a facade of a cathedral in order to measure it (as Meydenbauer had to do, nearly getting killed once) when it is much safer to measure a photograph? Additionally, wrote Meydenbauer, "some may find it hard to believe, but experience has proven than one can see, not everything, but many things, better in scale measurement than on the spot". In 1885 Royal Prussian Institute for Scale Measurement was founded and the measurement of photographs of historic monuments became a frequent practice. [10]

Is it possible that in the twentieth century the "rationalization of sight" was not responsible for any new applications? In fact, while Ivins was writing his essay on perspective, across the Atlantic, in England, work was already underway to install twenty radar stations on the east and southeast coasts to provide surveillance of these air approaches. [11] These radar installations turned out to be absolutely essential in the coming war, allowing for the severely outnumbered Royal Air Force to defeat the Luftwaffe in the Battle of Britain. Radar, the latest technology of visual nominalism, became Britain's most important weapon. [12]

Radar is an acronym for Radio Detection and Ranging. Just as sound waves, radio waves create echoes when they are reflected by objects in their path. Radar transmits a radio wave in a desired direction. The signal reflected back from the objects is picked up by radar antenna. The time between the transmission and the reception of the echo indicates the distance to the object; the direction the antenna is pointing in when the echo is received reveals the object's position in relation to the radar. Detected objects appear as bright spots on the display watched by the radar operator. [13]

Radar is the best example of the rationalization of sight in the twentieth century.

All it sees and all it shows are the positions of objects, 3-D coordinates of points in space, points which correspond to submarines, aircrafts, birds, or missiles. Color, texture, even shape are disregarded. Instead of Alberti's window, opening onto the full richness of the visible world, a radar operator sees a screen, a dark field with a few bright spots. Here, the function of visual nominalism, which perspectival image performed along with many other functions, is isolated and abstracted.

Radar image serves a single function, but it performs it more efficiently than any previous perspectival technique or technology.

First, vision is no longer limited by the spectral capacity of the human or camera eye. Instead of relying, like photography, on the small region of the electromagnetic spectrum to which our eyes are sensitive, radar uses other regions, sending and receiving waves of different lengths. Vision is extended to include the whole electromagnetic spectrum. The visible becomes a small part of a larger field of sensory exploration of the environment. Consequently, the recording of objects' positions in space is no longer limited by conditions of visibility.

Second, this recording now takes place in real-time. No longer military commanders have to wait until pilots come back from surveillance missions and the film is developed. Now, the imaging is instantaneous. The image changes in real-time reflecting the change in the referent.

Along with radar, many other technologies of visual nominalism came into existence following the advances in electronics and computers during World War II: ultrasonic imaging, multispectral photography, multispectral imaging, infrared, sonar, magnetic-resonance imaging, and so on. As radar, these technologies are effectively used to record distances, positions, layouts, shapes, and volumes. Sonar, for instance, detects objects in the water by using sound waves. Ultrasonic computer tomography uses sound waves and computer graphics to construct images of body tissues. Multispectral photography isolates energy reflected from surfaces in a number of given wavelength bands.

Engineering textbooks and encyclopedias group these technologies under the term "remote sensing," defined as the gathering and imaging of information without actual contact with the object or area being investigated. [14] This definition is helpful in separating the two operations involved in the technologies of remote sensing: the gathering of information and its presentation. The first operation may have nothing to do with what is visible to the human eye but in the second operation, the eye eventually comes into play since the gathered information has to be presented to the human observer in visual form in order to be useful.

However, these technologies do not only perform the role previously played by perspectival representations but also rely on the same principle. Nobody is more clear on this point than Jacque Lacan. In "Of the Gaze as Object Petit a" from _The Four Fundamental Concepts of Psycho-Analysis_ Lacan emphasizes that perspective extends beyond the domain of the visible. [15]

Lacan starts by reminding us that an image is anything defined "by a point-by-point correspondence of two unities in space". To obtain an image of something we do not have to rely on light or to operate in the domain of the visible. Nor do we have to limit images to 2-D representations of 3-D reality. We can represent an object by another object or represent a 2-D form by another form. All that is required is a rule to establish the correspondence between the points of the object being imaged and the points on the image.

Similarly, says Lacan, "what is an issue in geometric perspective is simply the mapping of space, not sight". [16] Perspective is one such rule, a particular method to establish a correspondence between the object and its image. Specifically, the method of perspective consists in connecting a single point in space (usually referred to as a point of view) with a number of points on the object by straight lines; the intersection of these lines with a plane creates an image. The fact that perspective, whether as a part of human sight apparatus or as a part of an apparatus of photography, works through light is coincidental. Light travels in straight lines, therefore it can be used to create perspectival images. But one can construct such images without light: "In Descartes, dioptrics, the action of the eyes, is represented as the conjugated action of two sticks". [17] As Lacan points out further on in the seminar, this idea that perspective is not limited to sight alone but functions in other senses as well defines the classical discourses on perception: "The whole trick, the key presto!, of the classic dialectic around perception, derives from the fact that it deals with geometric vision, that is to say, with vision in so far as it situated in a space that is not in its essence the visual." [18] Lacan's clarification that the principle of perspective is not limited to the visible helps us understand that the technologies of remote sensing function on the principle of perspective. Regardless of their lengths, all waves travel in straight lines, and therefore points in space are connected by straight lines to a point of reception (such as radar antenna) or recording (such as photographic camera). Radar, infrared imaging, sonar, or ultrasound are all part of what Lacan called "geometric vision," perspectival vision which extends beyond the visible.

## 3-D Computer Graphics: Interactive Perspectivalism

From the moment of adaptation of perspective attempts have been made to aid the laborious manual process of creating perspectival images. [19] Between the sixteenth and the nineteenth century, various perspectival machines (more precisely, perspective aid devices) have been invented. They were used to construct particularly challenging perspectival images, to illustrate the principles of perspective, to help students learn it, to impress artists' clients or to serve as intellectual toys. Already in the first decades of the sixteenth century, Dürer described a number of such machines. [20] One device is a net in the form of a rectangular grid, stretched between the artist and the subject.

Another uses a string representing a line of sight. The string is fixed on one end, while another end is moved successively to key points on the subject. The point where the string crosses the projection plane, defined by a wooden frame, is recorded by two crossed strings. For each position, a hinged board attached to the frame is moved and the point of intersection is marked on its surface. Other major types of perspectival machines that appeared subsequently included perspectograph, pantograph, physionotrace, and optigraph.

Why manually move the string imitating the ray of light from point to point? Along with perspectival machines a whole range of optical apparatuses was in use, particularly for depicting landscapes and in topographic surveys. They included the versions of camera obscura from large tents to smaller, easily transportable boxes. After 1800, the artist's ammunition was strengthened by camera lucida, patented in 1806. [21] Camera lucida utilized a prism with two reflecting surfaces at 135 degrees. The draftsman carefully positioned his eye to see both the image and the drawing surface below and traced the outline of the image with a pencil.

The images produced by camera obscura or camera lucida were only ephemeral and considerable effort was still required to fix these images. A draftsman had to meticulously trace the image to transform it into the permanent form of a drawing.

With photography, this time-consuming process was finally eliminated. The process of imaging reality, the creation of perspectival representations of real objects was now mechanized. However, this mechanization did not affect other uses of perspectival representation. According to Latour, perspective establishes a "four-lane freeway" between reality and its representation. We can combine real and imagined objects in a single geometric model and to go back and forth between reality and the model. The process of the creation of a geometric model still remained a manual process, requiring techniques of perspectival and analytical geometry, pencil, ruler, and eraser. Similarly, to construct a perspectival view of the model also required hours of drafting. The mechanization and automation of geometrical modeling and display were yet to come. Nothing perhaps symbolizes mechanization as dramatically as the first assembly lines installed by Henry Ford in 1913. The assembly line relied on two crucial principles. The first principle was the standardization of parts, already employed in production of military uniforms in the nineteenth century. The second, newer principle, was the separation of production process into a set of repetitive, sequential and simple activities that could be executed by workers who did not have to master the entire process and could be easily replaced.

It seemed that mechanical modernity was at its peak. Yet, in the same year the Spanish inventor Leonardo Torres y Quevedo already advocated the industrial use of programmed machines. [22] He pointed out that although automatons existed before, they were never used to perform useful work:

"The ancient automatons...imitate the appearance and movement of living beings, but this has not much practical interest, and what is wanted is a class of apparatus which leaves out the merely visible gestures of man and attempts to accomplish the results which a living person obtains, thus replacing a man by a machine." [23] 

With mechanization, the work is performed by a human, but the physical labor is augmented by a machine. Automation takes mechanization one step further — a machine is programmed to replace the functions of human organs of observation, effort and decision.

The term "automation" was finally coined in 1947, and in 1949 Ford began the construction of the first automated factories. Automation was made possible by the development of digital computers during World War II and thus became synonymous with computerization. A decade later, the automation of the process of constructing perspectival images of both existent and non-existent objects and scenes was well underway. [24] By the early 1960s Boeing designers already relied on 3-D computer graphics for simulation of landings on the runway and of pilot movement in the cockpit. [25]

By automating perspectival imaging digital computers completed the process that began in the Renaissance. The automation became possible because perspectival drawing has always been a step-by-step procedure, an algorithm involving a series of steps required to project coordinates of points in 3-D space onto a plane. Before computers, the steps of the algorithm were executed by human draftsmen and artists. The use of a computer allowed to execute them automatically and, therefore, much more efficiently. [26]

The details of the actual perspective-generating algorithm which could be executed by a computer were published in the early 1960s by Larry G. Roberts, then a graduate student at MIT. [27] The perspective-generating algorithm constructs perspectival images in a manner quite similar to traditional perspectival techniques. In fact, Roberts had to refer to German textbooks on perspectival geometry from the early 1800s to get the mathematics of perspective. [28] The algorithm reduces reality to solid objects, and the objects are further reduced to planes defined by straight lines. The coordinates of the endpoint of each line are stored in a computer. Also stored are the parameters of a virtual camera — the coordinates of a point of view, the direction of sight and the position of a projection plane. Given this information, the algorithm generates a perspectival image of an object, point by point.

Computerization of perspectival construction made possible the automatic generation of a perspectival image of a model as seen from an arbitrary point of view — a picture of a virtual world recorded by a virtual camera. The picture, however, was crude and static. To produce a film of a simulated landing, Boeing had to supplement computer technology with manual labor. As in traditional animation, twenty-four plots were required for each second of film. These plots were computer-generated and consisted of simple lines. Each plot was then hand-colored by an artist. Finished plots were filmed, again manually, on an animation stand.

Gradually, throughout the 1970s and the 1980s, the coloring stage was automated as well. Many algorithms were developed to add the full set of depth cues to a synthetic image — hidden line and hidden surface removal, shading, texture, atmospheric perspective, shadows, reflections, and so on. [29]

In 1962 Ivan Sutherland's designed his legendary Sketchpad program. With Sketchpad, a human operator could create graphics directly on a computer screen by touching the screen with a light pen. In the same year, ITEK began marketing Electronic Drafting Machine similar to Sketchpad. [30] Although both programs only dealt with 2-D graphics, they introduced a new paradigm of interactive graphics: by changing something on the screen, the operator changed data in computer memory. [31]

When this paradigm of interactive editing was combined with the algorithms of 3-D graphics, a fundamentally new way to use perspectival images have emerged. This development was more revolutionary than automation of perspective construction per se. Indeed, traditional draftsman could have accomplished what the computer at Boeing was doing — generating plots in perspective given 3-D database - only more slowly. But now it became possible to change the point of view of a virtual camera and see the corresponding changes in the perspectival image in real-time. It also became possible to interactively build and modify 3-D models and observe the changes on the screen.

The emergence of interactive 3-D computer graphics started the race to eliminate the time delay between the action of an operator and the displayed results. In this race for speed, which accelerated in the 1970s as synthetic images began to be utilized in flight simulators, the algorithms of 3-D graphics were gradually transported from software into hardware, each algorithm becoming a special computer chip. Silicon Graphics, one of the major manufacturers of computer graphics hardware, labeled such a system "geometry engine".

The term appropriately symbolizes the second stage of the automation of perspectival imaging. At the first stage, photographic camera, with perspective physically built into its lens, mechanized the process of creating perspectival images of existing objects. Now, with perspectival algorithms and other necessary geometric operations embedded in silicon, it became possible to display and interactively manipulate models of non-existent objects as well.

This essay argued that in this century automation of visual nominalism entered a new stage. The signs of this automation are multitude of new technologies used to capture and visualize three-dimensional reality that emerged since the middle of the twentieth century such as radar, infrared imaging, laser sensors, CAT scan, magnetic resonance imagining, 3-D computer graphics, and computer holography. Since the early 1960s, the work has also been underway to automate vision completely, to create computer vision systems that would recognize objects and interpret scenes automatically. The development of these technologies has been accompanied by massive research into the general problems of visual nominalism in computer science, experimental psychology, and neuroscience. New formal mathematical techniques were developed to analyze images as a source of depth information and, vice versa, to transform this information into realistic images. The work on automation of visual nominalism has also led to new attention to particular aspects of human vision. In fact, a new paradigm for the study of human vision has emerged during the 1970s at MIT. Within this paradigm, the goal of human vision is taken to be the recognition of shapes, leading researchers to study algorithms by which the brain "computes" shapes of objects from retinal input in the hope that these algorithms can be then used by computer vision systems. [32] The emergence of such a paradigm, which reduces human vision to a particular function, and the accompanying research investment, suggest the economic importance of this function of vision for the contemporary society.

## References:

[1] Erwin Panofsky, Perspective as Symbolic Form (New York: Zone Books, 1991).

[2] Erwin Panofsky, Die Perspektive als "symbolische Form" (Leipzig & Berlin: Vertrage der Bibliothek Warburg, 1927), 258-330.

[3] Paul Virilio, War and Cinema: The Logistics of Perception (London: Verso, 1989).

[4] Henri Lefebvre, The Production of Space (Oxford: Blackwell Publishers, 1991), 25.

[5] William Ivins, On the Rationalization of Sight (New York: Da Capo Press, 1975.

[6] Ibid., 9.

[7] Bruno Latour, "Visualization and Cognition: Thinking with Eyes and Hands," Knowledge and Society: Studies in the Sociology of Culture Past and Present 6 (1986): 1-40.

[8] Ivins, 12, 13.

[9] Beaumont Newhall, Airborne Camera (New York: Hastings House Publishers, 1969). For critical histories of photo reconnaissance see Allan Sekula, "The Instrumental Image: Steichen at War" in Photography against the Grain: Essays and Photo Works, 1973-1983 (Halifax: The Press of the Nova Scotia College of Art and Design, 1984); Paul Virilio, War and Cinema: the Logistics of Perception (London: Verso, 1989); Manuel De Landa, "Policing the Spectrum" in War in the Age of Intelligent Machines (New York: Zone Books, 1991).

[10] Harun Farocki, "Reality Would Have to Begin," Documents 1/2 (1992): 136-146.

[11] This section relies on two sources: Echoes of War (Boston: WGBH Boston), videotape; McGraw-Hill Encyclopedia of Science & Technology: An International Reference Work in Twenty Volumes Including Index (New York: McGraw-Hill, 1992).

[12] Principles and technology of radar were worked out independently by scientists in the United States, England, France and Germany during the 1930s. But after the beginning of the War only the US had the necessary resources to continue radar development. In 1940, at MIT, a team of scientists was gathered to work in the Radiation Laboratory or the "Rad Lab," as it came to be called. The purpose of the lab was radar research and production. The lab's first achievement was the successful completion of microwave radar which was small enough to fit on a plane.

[13] Numerous variations of the basic radar technology exist. For instance, in addition to active radars which send a signal and detect energy reflected by the objects there are also passive radars which do not send a signal themselves. However, all radars have in common the use of electromagnetic radiation (radio waves) to detect and measure objects in their vicinity.

[14] McGraw-Hill Encyclopedia of Science & Technology: an International Reference Work in Twenty Volumes Including Index (New York: McGraw-Hill, 1992), vol. 15, 311.

[15] Jacques Lacan, "On the Gaze as Objet Petit a" in The Four Fundamental Concepts of Psycho-Analysis, ed. Jacques-Alain Miller, trans. Alan Sheridan (New York: W.W.Norton & Company, 1981), 67-122.

[16] Ibid., 86.

[17] Ibid., 87.

[18] Ibid., 94.

[19] For a survey of perspectival instruments, see Martin Kemp, The Science of Art (New Haven: Yale University Press, 1990), 167-220.

[20] Ibid., 171-172.

[21] Ibid., 200.

[22] Charles Eames and Ray Eames, A Computer Perspective: Background to the Computer Age (Cambridge, MA: Harvard University Press, 1990), 65-67.

[23] Qtd. in ibid., 67.

[24] I am not aiming here by any means to provide a full account of the history of 3-D computer graphics or its various uses. I am concerned with computer graphics as one development, among others, in the general move toward the rationalization of perspectival imaging. For a more comprehensive account of 3-D computer graphics techniques, see J. William Mitchell, The Reconfigured Eye: Visual Truth in the Post-Photographic Era (Cambridge, Massachusetts: The MIT Press, 1992), 117-162.

[25] Jasia Reichardt, The Computer in Art (London and New York: Studio Vista and Van Nostrand Reinhold Company, 1971), 15.

[26] MIT became the major early research site for yet another new technology of visual nominalism — computer graphics. The Radiation Laboratory was dismantled after the end of the War, but soon Air Force created another secret laboratory in its place — Lincoln Laboratory. The job of Lincoln Laboratory was to work on human factors and new display technologies for SAGE — the "Semi-Automatic Ground Environment," a command center to control the US air defenses established in the mid-1950s. As part of this research, conducted throughout the 1950s and the 1960s, Lincoln Laboratory developed many key principles and technologies of computer graphics - CRT (cathode-ray tube) display, bit-mapped graphics, interactive control and algorithms for 3-D wireframe graphics. See Paul Edwards, "The Closed World. Systems discourse, military policy and post-World War II US historical consciousness," Cyborg Worlds: the Military Information Society, ed. Les Levidow and Kevin Robins (London: Free Association Books, 1989); Howard Rheingold, Virtual Reality (New York: 1991).

[27] L.G. Roberts, "Machine Perception of Three-Dimensional Solids," MIT Lincoln Laboratory TR 315, 1963; L.G. Roberts, "Homogeneous Matrix Representations and Manipulation of N-Dimensional Constructs," MIT Lincoln Laboratory MS 1405, 1965.

[28] "Retrospectives II: The Early Years in Computer Graphics at MIT, Lincoln Lab, and Harvard," SIGGRAPH '89 Panel Proceedings (Boston, Massachusetts: ACM SIGGRAPH, 1989), 72.

[29] For further discussion of the problem of realism in computer graphics, see Lev Manovich, "Real" Wars: Esthetics and Professionalism in Computer Animation," Design Issues 6, no. 1 (Fall 1991): 18-25; Lev Manovich, "Assembling Reality: Myths of Computer Graphics," Afterimage 20, no. 2 (September 1992): 12-14.

[30] Ibid., 51.

[31] In fact, interactive computer graphics technology appeared earlier, although it was not publicized. Already in the 1950s Air Force used interactive CRT displays and light pens in order to process more efficiently information obtained by radar. Both CRT displays and light pens were designed at Lincoln Laboratory as part of the SAGE project. Using this technology, Lincoln researchers created a number of computer graphics programs. They include programs which allowed to display brain waves (1957), to simulate planet and gravitational activity (1960), and to create 2-D drawings (1958). "Retrospectives II: The Early Years in Computer Graphics at MIT, Lincoln Lab, and Harvard," SIGGRAPH '89 Panel Proceedings (Boston, Massachusetts: ACM SIGGRAPH, 1989), 42-54.

[32] The fundamental statement of this paradigm, David Marr's Vision defines human vision as "the process of discovering from images what is present in the world, and where it is". David Marr, Vision (New York: W.H. Freeman and Company, 1982), 3.

---
# The Paradoxes of Digital Photography

_author: Lev Manovich_
_year: 1994_

## 1. Digital Revolution?

Computerized design systems that flawlessly combine real photographed objects and objects synthesized by the computer. Satellites that can photograph the license plate of your car and read the time on your watch. "Smart" weapons that recognize and follow their targets in effortless pursuit — the kind of new, post-modern, post-industrial dance to which we were all exposed during the televised Gulf war. New medical imaging technologies that map every organ and function of the body. Onfline electronic libraries that enable any designer to acquire not only millions of photographs digitally stored but also dozens of styles which can be automatically applied by a computer to any image.

All of these and many other recently emerged technologies of image-making, image manipulation, and vision depend on digital computers. All of them, as a whole, allow photographs to perform new, unprecedented, and still poorly understood functions. All of them radically change what a photograph is.

Indeed, digital photographs function in an entirely different way from traditional - lens and film-based - photographs. For instance, images are obtained and displayed by sequential scanning; they exist as mathematical data which can be displayed in a variety of modes — sacrificing color, spatial or temporal resolution. Image processing techniques make us realize that any photograph contains more information than can be seen with the human eye. Techniques of 3D computer graphics make possible the synthesis of photo-realistic images — yet, this realism is always partial, since these techniques do not permit the synthesis of any arbitrary scene. [1]

Digital photographs function in an entirely different way from traditional photographs. Or do they? Shall we accept that digital imaging represents a radical rupture with photography? Is an image, mediated by computer and electronic technology, radically different from an image obtained through a photographic lens and embodied in film? If we describe film-based images using such categories as depth of field, zoom, a shot, or montage, what categories should be used to describe digital images? Shall the phenomenon of digital imaging force us to rethink such fundamental concepts as realism or representation? In this essay, I will refrain from taking an extreme position of either fully accepting or fully denying the idea of a digital imaging revolution. Rather, I will present the logic of the digital image as paradoxical; radically breaking with older modes of visual representation while at the same time reinforcing these modes. I will demonstrate this paradoxical logic by examining two questions: alleged physical differences between digital and film-based representation of photographs and the notion of realism in computer-generated synthetic photography.

The logic of the digital photograph is one of historical continuity and discontinuity. The digital image tears apart the net of semiotic codes, modes of display, and patterns of spectatorship in modern visual culture — and, at the same time, weaves this net even stronger. The digital image annihilates photography while solidifying, glorifying and immortalizing the photographic. In short, this logic is that of photography after photography.

## 2. Digital Photography Does Not Exist

It is easiest to see how digital (r)evolution solidifies (rather than destroys) certain aspects of modern visual culture — the culture synonymous with the photographic image — by considering not photography itself but a related film-based medium — cinema. New digital technologies promise to radically reconfigure the basic material components (lens, camera, lighting, film) and the basic techniques (the separation of production and post-production, special effects, the use of human actors and non-human props) of the cinematic apparatus as it has existed for decades. The film camera is increasingly supplemented by the virtual camera of computer graphics which is used to simulate sets and even actors (as in _Terminator 2_ and _Jurassic Park_). Traditional film editing and optical printing are being replaced by digital editing and image processing which blur the lines between production and post-production, between shooting and editing.

At the same time, while the basic technology of film-making is about to disappear being replaced by new digital technologies, cinematic codes find new roles in the digital visual culture. New forms of entertainment based on digital media and even the basic interface between a human and a computer are being increasingly modeled on the metaphors of movie making and movie viewing. With Quicktime technology, built into every Macintosh sold today, the user makes and edits digital "movies" using software packages whose very names (such as Director and Premiere) make a direct reference to cinema. Computer games are also increasingly constructed on the metaphor of a movie, featuring realistic sets and characters, complex camera angles, dissolves, and other codes of traditional filmmaking. Many new CD-ROM games go even further, incorporating actual movie-like scenes with live actors directed by well-known Hollywood directors. Finally, SIGGRAPH, the largest international conference on computer graphics technology, offers a course entitled "Film Craft in User Interface Design" based on the premise that "The rich store of knowledge created in 90 years of filmmaking and animation can contribute to the design of user interfaces of multimedia, graphics applications, and even character displays." [2]

Thus, film may soon disappear — but not cinema. On the contrary, with the disappearance of film due to digital technology, cinema acquires a truly fetishistic status. Classical cinema has turned into the priceless data bank, the stock which is guaranteed never to lose its value as classic films become the content of each new round of electronic and digital distribution media — first video cassette, then laser disk, and, now, CD-ROM (major movie companies are planning to release dozens of classic Hollywood films on CD-ROM by the end of 1994). Even more fetishized is "film look" itself — the soft, grainy, and somewhat blurry appearance of a photographic image which is so different from the harsh and flat image of a video camera or the too clean, too perfect image of computer graphics. The traditional photographic image once represented the inhuman, devilish objectivity of technological vision. Today, however, it looks so human, so familiar, so domesticated — in contrast to the alienating, still unfamiliar appearance of a computer display with its 1280 by 1024 resolution, 32 bits per pixel, 16 million colors, and so on. Regardless of what it signifies, any photographic image also connotes memory and nostalgia, nostalgia for modernity and the twentieth century, the era of the pre-digital, pre-post-modern. Regardless of what it represents, any photographic image today first of all represents photography.

So while digital imaging promises to completely replace the techniques of filmmaking, it at the same time finds new roles and brings new value to the cinematic apparatus, the classic films, and the photographic look. This is the first paradox of digital imaging.

But surely, what digital imaging preserves and propagates are only the cultural codes of film or photography. Underneath, isn't there a fundamental physical difference between a film-based image and a digitally encoded image?

The most systematic answer to this question can be found in William Mitchell's recent book _The Reconfigured Eye: Visual Truth in the Post-photographic Era_. [3] Mitchell's entire analysis of the digital imaging revolution revolves around his claim that the difference between a digital image and a photograph "is grounded in fundamental physical characteristics that have logical and cultural consequences." [4] In other words, the physical difference between photographic and digital technology leads to the difference in the logical status of film-based and digital images and also to the difference in their cultural perception.

How fundamental is this difference? If we limit ourselves by focusing solely, as Mitchell does, on the abstract principles of digital imaging, then the difference between a digital and a photographic image appears enormous. But if we consider concrete digital technologies and their uses, the difference disappears. Digital photography simply does not exist.

1. The first alleged difference concerns the relationship between the original and the copy in analog and in digital cultures. Mitchell writes: "The continuous spatial and tonal variation of analog pictures is not exactly replicable, so such images cannot be transmitted or copied without degradation... But discrete states can be replicated precisely, so a digital image that is a thousand generations away from the original is indistinguishable in quality from any one of its progenitors." [5] Therefore, in digital visual culture, "an image file can be copied endlessly, and the copy is distinguishable from the original by its date since there is no loss of quality". [6] This is all true — in principle. However, in reality, there is actually much more degradation and loss of information between copies of digital images than between copies of traditional photographs. A single digital image consists of millions of pixels. All of this data requires considerable storage space in a computer; it also takes a long time (in contrast to a text file) to transmit over a network. Because of this, the current software and hardware used to acquire, store, manipulate, and transmit digital images uniformly rely on lossy compression — the technique of making image files smaller by deleting some information. [7] The technique involves a compromise between image quality and file size — the smaller the size of a compressed file, the more visible are the visual artifacts introduced in deleting information. Depending on the level of compression, these artifacts range from barely noticeable to quite pronounced. At any rate, each time a compressed file is saved, more information is lost, leading to more degradation.

One may argue that this situation is temporary and once cheaper computer storage and faster networks become commonplace, lossy compression will disappear. However, at the moment, the trend is quite the reverse with lossy compression becoming more and more the norm for representing visual information. If a single digital image already contains a lot of data, then this amount increases dramatically if we want to produce and distribute moving images in a digital form (one second of video, for instance, consists of 30 still images). Digital television with its hundreds of channels and video-on-demand services, the distribution of full-length films on CD-ROM or over the Internet, fully digital post-production of feature films — all of these developments will be made possible by newer compression techniques. [8] So rather than being an aberration, a flaw in the otherwise pure and perfect world of the digital, where even a single bit of information is never lost, lossy compression is increasingly becoming the very foundation of digital visual culture. This is another paradox of digital imaging — while in theory, digital technology entails the flawless replication of data, its actual use in contemporary society is characterized by the loss of data, degradation, and noise; the noise which is even stronger than that of traditional photography.

2. The second commonly cited difference between traditional and digital photography concerns the amount of information contained in an image. Mitchell sums it up as follows: "There is an indefinite amount of information in a continuous-tone photograph, so enlargement usually reveals more detail but yields a fuzzier and grainier picture... A digital image, on the other hand, has precisely limited spatial and tonal resolution and contains a fixed amount of information." [9] Here again, Mitchell is right in principle: a digital image consists of a finite number of pixels, each having a distinct color or a total value, and this number determines the amount of detail an image can represent. Yet in reality, this difference does not matter anymore. Current scanners, even consumer brands, can scan an image or an object with very high resolution: 1200 or 2400 pixels per inch is standard today. True, a digital image is still comprised of a finite number of pixels, but at such resolution, it can record much finer detail than was ever possible with traditional photography. This nullifies the whole distinction between an "indefinite amount of information in a continuous-tone photograph" and a fixed amount of detail in a digital image. The more relevant question is how much information in an image can be useful to the viewer. Current technology has already reached the point where a digital image can easily contain much more information than anybody would ever want. This is yet another paradox of digital imaging.

But even the pixel-based representation, which appears to be the very essence of digital imaging, can no longer be taken for granted. Recent computer graphics software has bypassed the limitations of the traditional pixel grid which limits the amount of information in an image because it has a fixed resolution. Live Picture, an image editing program for the Macintosh, converts a pixel-based image into a set of equations. This allows the user to work with an image of virtually unlimited size. Another paint program Matador makes it possible to paint on a tiny image which may consist of just a few pixels as though it were a high-resolution image (it achieves this by breaking each pixel into a number of smaller sub-pixels). In both programs, the pixel is no longer a "final frontier"; as far as the user is concerned, it simply does not exist.

3. Mitchell's third distinction concerns the inherent mutability of a digital image. While he admits that there has always been a tradition of impure, re-worked photography (he refers to "Henry Peach Robinson's and Oscar G. Reijlander's nineteenth-century "combination prints",John Heartfield's photomontages" [10] as well as numerous political photo fakes of the twentieth century) Mitchell identifies straight, unmanipulated photography as the essential, "normal" photographic practice: "There is no doubt that extensive reworking of photographic images to produce seamless transformations and combinations is technically difficult, time-consuming, and outside the mainstream of photographic practice. When we look at photographs we presume, unless we have some clear indications to the contrary, that they have not been reworked." [11] This equation of "normal" photography with straight photography allows Mitchell to claim that a digital image is radically different because it is inherently mutable: "the essential characteristic of digital information is that it can be manipulated easily and very rapidly by computer. It is simply a matter of substituting new digits for old... Computational tools for transforming, combining, altering, and analyzing images are as essential to the digital artist as brushes and pigments to a painter." [12]

From this allegedly purely technological difference between a photograph and a digital image, Mitchell deduces differences in how the two are culturally perceived. Because of the difficulty involved in manipulating them, photographs "were comfortably regarded as causally generated truthful reports about things in the real world". [13] Digital images, being inherently (and so easily) mutable, call into question "our ontological distinctions between the imaginary and the real" [14] or between photographs and drawings. Furthermore, in a digital image, the essential relationship between signifier and signified is one of uncertainty. [15]

Does this hold? While Mitchell aims to deduce culture from technology, it appears that he is actually doing the reverse. In fact, he simply identifies the pictorial tradition of realism with the essence of photographic technology and the tradition of montage and collage with the essence of digital imaging. Thus, the photographic work of Robert Weston and Ansel Adams, nineteenth and twentieth-century realist painting, and the painting of the Italian Renaissance become the essence of photography; while Robinson's and Reijlander's photo composites, constructivist montage, contemporary advertising imagery (based on constructivist design), and Dutch seventeenth-century painting (with its montage-like emphasis on details over the coherent whole) become the essence of digital imaging. In other words, what Mitchell takes to be the essence of photographic and digital imaging technology are two traditions of visual culture. Both existed before photography, and both span different visual technologies and mediums. Just as its counterpart, the realistic tradition extends beyond photography per se and at the same time accounts for just one of many photographic practices.

If this is so, Mitchell's notion of "normal" unmanipulated photography is problematic. Indeed, unmanipulated "straight" photography can hardly be claimed to dominate the modern uses of photography. Consider, for instance, the following photographic practices. One is Soviet photography of the Stalinist era. All published photographs were not only staged but also retouched so heavily that they can hardly be called photographs at all. These images were not montages, as they maintained the unity of space and time, and yet, having lost any trace of photographic grain due to retouching, they existed somewhere between photography and painting. More precisely, we can say that Stalinist visual culture eliminated the very difference between a photograph and a painting by producing photographs which looked like paintings and paintings (I refer to Socialist Realism) which looked like photographs. If this example can be written off as an aberration of totalitarianism, consider another photographic practice closer to home: the use of photographic images in twentieth-century advertising and publicity design. This practice does not make any attempt to claim that a photographic image is a witness testifying about the unique event which took place in a distinct moment of time (which is how, according to Mitchell, we normally read photography). Instead, a photograph becomes just one graphic element among many: few photographs coexist on a single page; photographs are mixed with type; photographs are separated from each by white space, backgrounds are erased leaving only the figures, and so on. The end result is that here, as well, the difference between a painting and a photograph does not hold. A photograph as used in advertising design does not point to a concrete event or a particular object. It does not say, for example, "this hat was in this room on May 12". Rather, it simply presents "a hat" or "a beach" or "a television set" without any reference to time and location.

Such examples question Mitchell's idea that digital imaging destroys the innocence of straight photography by making all photographs inherently mutable. Straight photography has always represented just one tradition of photography; it always coexisted with equally popular traditions where a photographic image was openly manipulated and was read as such. Equally, there never existed a single dominant way of reading photography; depending on the context the viewer could (and continue to) read photographs as representations of concrete events, or as illustrations which do not claim to correspond to events which have occurred. Digital technology does not subvert "normal" photography because "normal" photography never existed.

## 3. Real, All Too Real: Socialist Realism of _Jurassic Park_

I have considered some of the alleged physical differences between traditional and digital photography. But what is a digital photograph? My discussion has focused on the distinction between a film-based representation of an image versus its representation in a computer as a grid of pixels having a fixed resolution and taking up a certain amount of computer storage space. In short, I highlighted the issue of analog versus digital representation of an image while disregarding the procedure through which this image is produced in the first place. However, if this procedure is considered another meaning of digital photography emerges.

Rather than using the lens to focus the image of actual reality on film and then digitizing the film image (or directly using an array of electronic sensors), we can try to construct three-dimensional reality inside a computer and then take a picture of this reality using a virtual camera also inside a computer. In other words, 3-D computer graphics can also be thought of as digital — or synthetic — photography. I will conclude by considering the current state of the art of 3-D computer graphics. Here we will encounter the final paradox of digital photography. Common opinion holds that synthetic photographs generated by computer graphics are not yet (or perhaps will never be) as precise in rendering visual reality as images obtained through a photographic lens. However, I will suggest that such synthetic photographs are already more realistic than traditional photographs. In fact, they are too real.

1. The achievement of realism is the main goal of research in the 3-D computer graphics field. The field defines realism as the ability to simulate any object in such a way that its computer image is indistinguishable from its photograph. It is this ability to simulate photographic images of real or imagined objects which makes possible the use of 3-D computer graphics in military and medical simulators, in television commercials, in computer games, and, of course, in such movies as _Terminator 2_ or _Jurassic Park_.

These last two movies, which contain the most spectacular 3-D computer graphics scenes to date, dramatically demonstrate that total synthetic realism seems to be in sight. Yet, they also exemplify the triviality of what at first may appear to be an outstanding technical achievement — the ability to fake visual reality. For what is faked is, of course, not reality but photographic reality, reality as seen by the camera lens. In other words, what computer graphics has (almost) achieved is not realism, but only photorealism — the ability to fake not our perceptual and bodily experience of reality but only its photographic image. [16] This image exists outside of our consciousness, on a screen — a window of limited size which presents a still imprint of a small part of outer reality, filtered through the lens with its limited depth of field, filtered through film's grain and its limited tonal range. It is only this film-based image which computer graphics technology has learned to simulate. And the reason we think that computer graphics has succeeded in faking reality is that we, over the course of the last hundred and fifty years, have come to accept the image of photography and film as reality.

What is faked is only a film-based image. Once we came to accept the photographic image as reality the way to its future simulation was open. What remained were small details: the development of digital computers (the 1940s) followed by a perspective-generating algorithm (early 1960s), and then working out how to make a simulated object solid with shadow, reflection and texture (1970s), and finally simulating the artifacts of the lens such as motion blur and depth of field (1980s). So, while the distance from the first computer graphics images circa 1960 to the synthetic dinosaurs of _Jurassic Park_ in the 1990s is tremendous, we should not be too impressed. For, conceptually, photorealistic computer graphics had already appeared with Felix Nadar's photographs in the 1840s and certainly with the first films of the Lumieres in the 1890s. It is they who invented 3-D computer graphics.

2. So the goal of computer graphics is not realism but only photorealism. Has this photorealism been achieved? At the time of this writing (May 1994) dinosaurs of _Jurassic Park_ represent the ultimate triumph of computer simulation, yet this triumph took more than two years of work by dozens of designers, animators, and programmers of Industrial Light and Magic (ILM), probably the premier company specializing in the production of computer animation for feature films in the world today. Because a few seconds of computer animation often requires months and months of work, only the huge budget of a Hollywood blockbuster could pay for such extensive and highly detailed computer-generated scenes as seen in _Jurassic Park_. Most of the 3-D computer animation produced today has a much lower degree of photorealism and this photorealism is uneven, higher for some kinds of objects and lower for others. [17] And even for ILM photorealistic simulation of human beings, the ultimate goal of computer animation still remains impossible.

Typical images produced with 3-D computer graphics still appear unnaturally clean, sharp, and geometric looking. Their limitations especially stand out when juxtaposed with a normal photograph. Thus one of the landmark achievements of _Jurassic Park_ was the seamless integration of film footage of real scenes with computer-simulated objects. To achieve this integration, computer-generated images had to be degraded; their perfection had to be diluted to match the imperfection of film's graininess.

First, the animators needed to figure out the resolution at which to render computer graphics elements. If the resolution were too high, the computer image would have more detail than the film image and its artificiality would become apparent. Just as Medieval masters guarded their painting secrets, now leading computer graphics companies carefully guard the resolution of image they simulate.

Once computer-generated images are combined with film images additional tricks are used to diminish their perfection. With the help of special algorithms, the straight edges of computer-generated objects are softened. Barely visible noise is added to the overall image to blend computer and film elements. Sometimes, as in the final battle between the two protagonists in _Terminator 2_, the scene is staged in a particular location (a smoky factory in this example) which justifies addition of smoke or fog to further blend the film and synthetic elements together. So, while we normally think that synthetic photographs produced through computer graphics are inferior in comparison to real photographs, in fact, they are too perfect. But beyond that we can also say that paradoxically they are also too real.

The synthetic image is free of the limitations of both human and camera vision. It can have unlimited resolution and an unlimited level of detail. It is free of the depth-of-field effect, this inevitable consequence of the lens, so everything is in focus. It is also free of grain — the layer of noise created by film stock and by human perception. Its colors are more saturated, and its sharp lines follow the economy of geometry. From the point of view of human vision, it is hyperreal. And yet, it is completely realistic. It is simply a result of a different, more perfect than human, vision.

Whose vision is it? It is the vision of a cyborg or a computer; a vision of Robocop and of an automatic missile. It is a realistic representation of human vision in the future when it will be augmented by computer graphics and cleansed from noise. It is the vision of a digital grid. Synthetic computer-generated image is not an inferior representation of our reality, but a realistic representation of a different reality.

By the same logic, we should not consider clean, skinless, too flexible, and at the same time too jerky, human figures in 3-D computer animation as unrealistic, as imperfect approximation to the real thing — our bodies. They are perfectly realistic representations of a cyborg body yet to come, of a world reduced to geometry, where efficient representation via a geometric model becomes the basis of reality. The synthetic image simply represents the future. In other words, if a traditional photograph always points to the past event, a synthetic photograph points to the future event.

We are now in a position to characterize the aesthetics of _Jurassic Park_. This aesthetic is one of Soviet Socialist Realism. Socialist Realism wanted to show the future in the present by projecting the perfect world of future socialist society on a visual reality familiar to the viewer — streets, faces, and cities of the 1930s. In other words, it had to retain enough of then everyday reality while showing how that reality would look in the future when everyone's body will be healthy and muscular, every street modern, every face transformed by the spirituality of communist ideology.

Exactly the same happens in _Jurassic Park_. It tries to show the future of sight itself — the perfect cyborg vision free of noise and capable of grasping infinite details — vision exemplified by the original computer graphics images before they were blended with film images. But just as Socialist Realist paintings blended the perfect future with the imperfect reality of the 1930s and never depicted this future directly (there is not a single Socialist Realist work of art set in the future), _Jurassic Park_ blends the future super-vision of computer graphics with the familiar vision of film image. In _Jurassic Park_, the computer image bends down before the film image, its perfection is undermined by every possible means and is also masked by the film's content. This is then, the final paradox of digital photography. Its images are not inferior to the visual realism of traditional photography. They are perfectly real — all too real.

## References:

[1] Lev Manovich, "Assembling Reality: Myths of Computer Graphics," Afterimage 20, no. 2 (September 1992): 12-14.

[2] SIGGRAPH 93. Advance Program (ACM: New York, 1993), 28.

[3] William Mitchell, The Reconfigured Eye: Visual Truth in The Post-Photographic Era (Cambridge, Mass.: The MIT Press, 1992).

[4] Ibid., 4.

[5] Ibid., 6.

[6] Ibid., 49.

[7] Currently, the most widespread technique for compressing digital photographs is JPEG. For instance, every Macintosh comes with JPEG compression software.

[8] For almost a century, our standard of visual fidelity was determined by the film image. A video or television image was always viewed as an imperfect, low-quality substitute for the "real thing" — a film-based image. Today, however, a new even lower quality image is becoming increasingly popular — an image of computer multi-media. Its quality is exemplified by a typical, as of this writing, Quicktime movie: 320 by 240 pixels, 10-15 frames a second. Is the 35 mm film image going to remain the unchallenged standard with computer technology eventually duplicating its quality? Or will a low-quality computer image be gradually accepted by the public as the new standard of visual truth?

[9] Mitchell, The Reconfigured Eye, 6.

[10] Ibid., 7.

[11] Ibid.

[12] Ibid.

[13] Ibid., 225.

[14] Ibid.

[15] Ibid., 17.

[16] The research in virtual reality aims to go beyond the screen image in order to simulate both the perceptual and bodily experience of reality.

[17] See Manovich, "Assembling Reality".

---
# The Engineering of Vision and the Aesthetics of Computer Art

_author: Lev Manovich_
_year: 1994_

## I.

Just as it would be futile to consider video art in isolation from television, it would be equally unproductive to theorize new emerging forms of computer art without considering their uneasy connections to contemporary image industries, such as the computer graphics industry. Computer artists need this industry to provide them with the latest technological toys which will set them apart from their colleagues still working in the traditional, pre-industrial mediums. The industry uses the artists as beta-testers for new software and hardware. More importantly, the industry uses the mythology of art — our Romantic-modernist belief that an artist is a unique person, a visionary who transcends everyday reality and pushes the boundaries, etc. — as the most effective sales tool. What better way to market a piece of software than to have an endorsement from the artist? (Thus, paradoxically, a computer artist is somebody who transcends the here and now in the act of creation but can do so only with the help of the very latest tools, the tools of here and now).

If computer art does not exist in isolation from computer graphics industry, let us examine the history and the direction of the industry. Why did computer graphics — the industry concerned with finding more effective ways to produce, store, distribute and present images — achieve such importance? Why is it that today new disciplines which study images and vision continue to expand: image processing, computer vision, research on human-computer interfaces, vision science, and so on? What are the reasons these currently prominent image industries and image sciences have acquired such prominence? Let us begin with three images (figures 1, 2, 3). 

The first image: a portrait of Tatlin by a fellow Soviet designer El Lissitzky (figure 1). Time: early 1920s. A compass, extending straight from Tatlin's eye, a metaphor of vision for work.

The second image: SAGE (the "Semi-Automatic Ground Environment") — the first human-machine interactive display system (figure 2). Time: mid-1950s.

The third image: a virtual reality interface designed at NASA/Ames Human Factors Research Center (figure 3). Time: now. Instead of the metaphor of the eye compass, a reality: video monitors strapped to the eyes. The notion of vision as work is now fully realized: the operator wearing the gear works by mentally processing visually presented information. The gear is designed using all the available knowledge accumulated by experimental psychology about human vision. In the photograph, we see the last leftover from the age of manual labor — an arm in a DataGlove. It will soon disappear since through gaze tracking the operator can control the system by merely looking at different points in virtual space.

## II.

Modernization brought with it a special discipline concerned with efficiency — engineering. The job of an engineer was to ensure maximum performance with a minimum investment of energy, materials, and time, be it the performance of machines (mechanical engineering), communication systems (communication engineering) or human bodies (scientific management, time and motion studies). Inspired by modern engineering, the avant-garde of the 1920s tried to systematically apply its principles to vision.

To engineer vision meant to eliminate waste, to use minimal material resources. Thus, constructivist graphic design streamlined typography, eliminating complicated typefaces in favor of block letters consisting of straight lines; it also eliminated illustrations and "wasteful" decorations by making type itself the main element of design. The goal: maximum impact with minimum use of ink (figure 4).

To engineer vision also meant to minimize the psycho-physical resources required of the viewer. Dziga Vertov writes in his famous 1923 manifesto: "The least advantageous, the least economical communication of a scene is theatrical communication". [1] In contrast, montage forces the eye to see the right thing at the right time, thus eliminating the visual waste of theater, ballet, painting, and other traditional forms. In montage, "camera drags the eyes of a film viewer from hands to legs, from legs to eyes and the rest in the most advantageous order..". [2]

To engineer vision also meant to ensure perception in the shortest possible time. Here as well, the avant-garde promoted montage as an example of possible economy, in this case, economy of time. Maud Lavin describes the 1930 manifesto of the group of leading German designers headed by Kurt Schwitters: "Walter Dexel writes that modern man has the right to expect communications in the shortest possible time. Willi Baumeister points out that photomontage is efficient, allowing for the quick grasp of several images at once [3]

Finally, to engineer vision also meant to be able to measure its efficiency, or, to use the language of a communication engineer, to measure "system performance". Eisenstein, fresh from engineering school, invented his first theory of artistic communication, the famous "montage of attractions": "Let us search for the unit which will measure the influence exerted by art! Science has its "ions", its "electrons", its "neutrons". Art will have — attractions!" [4]

To summarize: The job of the avant-garde artist was to engineer vision and to engineer vision meant to affect the viewer with engineering precision, predictability, and effectiveness.

## III.

In its desire to engineer vision, the avant-garde was ahead of its time. The systematic engineering of vision took place only after World War II with the shift to post-industrial society.

For post-industrial society, mental labor of information processing is more important than manual labor. In contrast to a manual worker of the industrial age (figure 5), an operator in a human-machine system (figure 6) is primarily engaged in the observation of displays which present information in real-time about the changing status of a system or an environment, real or virtual: a radar screen tracking a surrounding space; a computer screen updating the prices of stocks; a video screen of a computer game presenting an imaginary battlefield; a control panel of an automobile showing its speed, etc. In short, vision becomes the major instrument of labor, the most productive organ of a worker in a human-machine system. And this is why following World War II we witness unprecedented amount of research into imaging and vision.

The figure which stands at the gates to this post-industrial society of perceptual labor is a radar operator of World War II.

1. First of all, in order to ensure the maximum performance of such a human-machine system as radar, it became necessary to engineer it around the capacities and the limitations of human vision. At the end of World War II, a new field emerges — human engineering. Let me quote from the description of its history found in a 1965 overview of the field: "The primary emphasis in time-and-motion engineering has been on man as a worker; that is, as a source of mechanical power. It was not until World War II that a new category of machines appeared — machines that made demands not upon the operator's muscular power, but upon his sensory, perceptual, judgmental, and decision-making abilities. The job of a radar operator, for example, requires virtually no muscular effort but makes severe demands on sensory capacity, vigilance, and decision-making ability. This new class of machines raised some intricate and unusual questions about human abilities: How much information can a man absorb from a radar screen?" [5]

Already before the war, experimental psychologists assisted in selecting military personnel for such jobs as pilot or airplane observer by administering special aptitude tests. During the war, a much greater number of pilots, radar operators and other similar personnel became needed. The emphasis was shifted, therefore, from selecting personnel with particularly good perceptual and motor skills to designing the equipment (controls, radar screens, dials, warning lights) to match the sensory capacities of an average person. [6] And it was the field of experimental psychology that possessed the knowledge about the sensory capacities of an average, statistical person: how visibility and acuity vary between day and night; how the ability to distinguish colors and brightness varies with illumination or distance; what the smallest amount of light is which can be reliably noticed; and so on. [7] All this data was now utilized for designing better displays and controls of the first modern human-machine systems such as radar installations or high-speed aircrafts.

The term "human engineering" was eventually replaced by another term standard today — "human factors". The radar operator who in the 1940s and 1950s was the prototypical example of a human-machine system, was replaced by the 1980s by a new prototypical figure, the computer user. Thus, references to "human-machine systems" became references to "human-computer systems". The same amount of intellectual energy and research which in the middle of the century went into theorizing the performance of a radar operator and adapting him and radar display to each other, today goes into the work on new computer interfaces, such as NASA/Ames VR system (figure 3).

2. The work on radar also directly leads to the development of interactive computer graphics. Next to photography, radar provided a superior way to gather information about enemy locations. In fact, it provided too much information, more information than one person could deal with. Was there a way to process and display information gathered by radars more effectively? The key principles and technologies of computer graphics — CRT (cathode-ray tube) display, bit-mapped graphics, interactive control, were developed as a way of solving this problem. The research took place at MIT. After the end of the War, Air Force created a secret Lincoln Laboratory. The job of Lincoln Laboratory was to work on human factors and new display technologies for SAGE — the "Semi-Automatic Ground Environment," a command center to control the U.S. air defenses established in the mid-1950s. [8] The earlier version of the center, called Cape Cod network, was operating right out of the Barta Building at MIT.

Each of 82 Air Force officers was monitoring his own computer display which showed the outlines of the New England Coast and locations of key radars (figures 6,7). Whenever an officer would notice a dot indicating a moving plane, he would use a light gun to tell the computer to track this dot. [9]

This was the first human-machine interactive computer graphic display system, developed to alleviate the mental labor of information processing. Vision, enhanced by computer graphics technology, became the only means to deal with information overflow.

## IV.

Computer graphics helped to process radar information more efficiently, but was there a way to take the human, who was too slow to keep up with the computers, completely out of the loop? This is the third crucial development in engineering of vision — the work on computer vision.

In 1961, the National Photographic Interpretation Center (NPIC) was created to produce photo analysis for the rest of the U.S. intelligence community and, as Manual De Landa points out, by the end of the next decade computers "were routinely used to correct for distortions made by satellite's imaging sensors and by atmospheric effects, sharpen out-of-focus images, extract particular features..." Computer analysis of photographic imagery also became the only way to deal with the pure volume of intelligence being gathered.

The techniques of image processing, which can automatically increase an image's contrast, remove the effects of blur, extract edges, record differences between two images, and so on, greatly eased the job of human photo analysts. But was it possible to completely replace them by computers?

Roberts' 1965 paper "Machine Recognition of Three-dimensional Solids" is considered to be the first attempt at solving the general task of automatically recognizing three-dimensional objects. [10] His program was designed to understand the artificial world composed solely of polyhedral blocks (figures 8, 9). Using image processing techniques, a photograph of a scene was first converted into a line drawing. Next, the techniques of 3-D computer graphics were used, also developed by Roberts. Thus, the two fields were born simultaneously: 3-D computer graphics and computer vision, automation of imaging and of sight. In summary, the rise of modern image industries and image sciences, such as computer graphics, human-factors research, or computer vision, can be seen as a part of the shift to the post-industrial society of perceptual labor. This shift involves two processes - two stages of automation.

The first stage of automation: human and machine are integrated in new human-machine systems which increasingly came to dominate both the battlefield and the workplace after World War II (radar screen, aircraft controls, computer terminals of the automated factory). Human vision became the key instrument of post-industrial labor as the channel of communication between human and machine. This leads into research into more efficient human-machine interfaces - from Ivan Sutherland's Sketchpad to today's VR.

The second stage of automation: the complete replacement of human cognitive functions by a computer, such as the substitution of human vision by computer vision. What does it mean to teach a computer how to see? In the field of computer vision, "vision" refers to two goals. First, it means the identification of various objects represented in an image. Second, it means reconstruction of three-dimensional space from the image. For instance, a missile not only has to identify a target but also to determine the position of this target in three-dimensional space. Here, vision is not meant for the contemplation of a sunset or appreciation of art; instead, it is reduced to the common denominator shared by humans and low-level organisms: to detect an obstacle, a predator, a prey.

I believe that most of the new research into vision and imaging after World War II can be understood as following these two directions: on the one hand, making human vision in its new role of a human-machine interface as efficient, as productive as possible; on the other hand, transferring vision and other human cognitive capacities from human to a computer.

## V.

What does this analysis entail for forming aesthetic criteria by which we can judge computer art? Let us look at the two paradigms in turn.

First, as I pointed out, in a post-industrial society vision acquires a new role of the human-machine interface - from radar screens of World War II to such contemporary developments as VR. The industry aims to make human vision as productive, as efficient as possible. If we still believe that art is something which is anti-productive, anti-utilitarian, the computer artist can be defined as designer of bad interfaces: interfaces which are inefficient, wasteful, confusing. One example of such "bad" interfaces is a display where, instead of usual modernist clarity, or "good form," the viewer encounters formlessness, chaos, "the madness of vision" (figure 10). [11]

Another example can be a pseudo-interactive work: a screen with a menu where every choice gets you to the same place.

Second, since we are also witnessing a movement towards the complete automation, including the replacement of human vision by computer vision, we need to completely reevaluate the very term "computer art". The term presently refers to the making of art with the help of a computer, the art to be enjoyed by human observers. The artist is the one who makes the creative choices. This Romantic paradigm reaches its extreme in the recent trend of artificial life art, where the computer is programmed to simulate the laws of evolution, mutating images to create endless new combinations; while the artist assumes the role of God, selecting which of these images will survive.

I suggest redefining "computer art" to mean "art for computers," art to be enjoyed not by humans but by computers. Moreover, using the tools of expert systems, artificial life and neural networks, we can evolve not only computer artists — the programs to create images — but also computer critics, the programs to evaluate them. What kind of images will be pleasurable for a computer? It is hard to make predictions, but I can guess that following its human master, the computer will adopt efficiency as the main aesthetic criteria. Thus, the computer may prefer images which are efficient in terms of storage — images which compress well. Rewriting art history from this perspective, the computer critic will prefer minimalist abstraction to Jackson Pollock and will champion Malevich as the most important artist of the twentieth century — the artist who anticipated the aesthetics of compression, and thus was already ahead of today's computer artists who still try to resist the poetics of the productive, functional, industrial (figures 11, 12).

As Dziga Vertov wrote in 1923, "I am a mechanical eye". [12]

## VI.

The preceding examples, of course, should be taken only half seriously. My main point is to urge computer artists to examine their relationship to the computer graphics industry, and to address the impact of this and other contemporary image industries not just on art practice but on society at large.

The notion that the artist functions outside of society, history and industry is a modernist myth. Modernist artists were not only the pioneers of the utilitarian aesthetics of modern industrial design or the pioneers of the techniques of modern advertisement and political propaganda; as I suggested in this essay, they have also pioneered post-modern engineering of vision, the integration of human and machine in human-machine systems and the replacement of human by computer vision. Today computer graphics industry is one of sites of this engineering. Whether computer artists acknowledge or ignore their relationship to this industry, it exists. Acknowledging rather than ignoring this is the first step toward a critical computer art practice.

## References:

[1] Dziga Vertov, "Kinoki. Perevorot" (Kinoki. A revolution), LEF 3 (1923): 139.

[2] Ibid., 139. The emphasis in the original — L.M.

[3] Maud Lavin, "Photomontage, Mass Culture, and Modernity. Utopianism in the Circle of New Advertising Designers," in Montage and Modern Life: 1919-1942, ed. Matthew Teitelbaum (Cambridge: The MIT Press, 1992), 54.

[4] Qtd. in Jacques Aumont, Montage Eisenstein (London and Bloomington: BFI Publishing and Indiana University Press, 1987), 41. Emphasis mine — L.M.

[5] Alphonse Chapanis, Man-Machine Engineering (Belmont, CA: Wadsworth Publishing Company, Inc., 1965), 9-10.

[6] Ibid., 8.

[7] William Estes, "Experimental Psychology: an Overview," in The First Century of Experimental Psychology, ed. Eliot Hearst (Hillsdale, NJ: Lawrence Erlbaum Associates, Publishers, 1979), 630.

[8] See Paul Edwards, "The Closed World. Systems discourse, military policy and post-World War II US historical consciousness," in Cyborg Worlds: The Military Information Society, ed. Les Levidow and Kevin Robins (London: Free Association Books, 1989); Howard Rheingold, Virtual Reality (New York: 1991).

[9] Panel proceedings of SIGGRAPH '89 (Boston, Mass., July 31-August 4, 1989), in Computer Graphics 23, 5 (ACM SIGGRAPH: New York, 1989), 22-24.

[10] L.G. Roberts, "Machine perception of three-dimensional solids," in Optical and Electro-Optical Information Processing, ed. J.T. Tippett (Cambridge: The MIT Press, 1965).

[11] The notion of "the madness of vision" is explored by the French philosopher Cristine Buci-Glucksmann. Describing her work, Martin Jay writes: "Resistant to any totalizing vision from above, the baroque explored what Buci-Glucksmann calls "the madness of vision", the overloading of the visual apparatus with a surplus of images in a plurality of spatial planes. As a result, it dazzles and distorts rather than presents a clear and tranquil perspective on the truth of the external world Martin Jay, Downcast Eyes: The Denigration of Vision In Twentieth-Century French Thought (Berkeley: University of California Press, 1933), 47-48.

[12] Vertov, "Kinoki," 141.

---

# From the Externalization of the Psyche to the Implantation of Technology

_author: Lev Manovich_
_year: 1995_

## 1.

In 1877 Sir Francis Galton, a statistician and a cousin of Charles Darwin, a founder of eugenics (a project of social betterment through planned breeding), and the author of highly influential psychological texts, pioneered a procedure of making composite photographs which proliferated widely in the next three decades. [1] Fabricated by a process of successive registration and exposure of portraits onto a single plate, Galton's composites were thought to constitute true statistic averages, representing human types — a criminal, a prostitute, an Englishman, a Jew, and others. Galton wrote about his composite pictures that they are "much more than averages; they are rather the equivalents of those large statistical tables whose totals, divided by the number of classes and entered on the bottom line, are the averages. They are real generalizations, because they include the whole of the material under consideration [2]

Galton not only claimed that "the ideal faces obtained by the method of composite portraiture appear to have a great deal in common with...so-called abstract ideas" but in fact, he proposed to rename abstract ideas "cumulative ideas". In contrast to the human mind, "a most imperfect apparatus for the elaboration of general ideas," Galton championed his composite photographs, which, being mechanical and precise, were much more reliable for arriving at abstract representations. [3]

With his photographs, Galton not only proposed that universals may be represented through generic images; he actually objectified and materialized them. Plato's ideas were given concrete form: they could now be touched, copied, fabricated, multiplied, distributed, etc.

Galton's belief that his composite photographs gave abstract ideas material tangible form is just one example of a more general modern phenomenon. This phenomenon can be called externalization of the mind. It shows itself in two ways. On the one hand, we witness recurrent claims by the users of new visual technologies, from Galton to Jaron Lanier, that these technologies externalize and objectify the mind. On the other hand, modern psychological theories of the mind, from Freud to cognitive psychology, also equate mental processes with external, technologically generated visual forms.

What to make of this desire to externalize the mind? In this essay, I will relate it to the demand of modern mass society for standardization. The subjects have to be standardized, and the means by which they are standardized need to be standardized as well. Hence, the objectification of internal, private mental processes, and their equation with external visual forms which can be easily manipulated, mass-produced, and standardized on their own. The private and individual are translated into the public and become regulated.

What before was a mental process, a uniquely individual state, now became part of a public sphere. Unobservable and interior processes and representations were taken out of individual heads and put outside — as drawings, photographs and other visual forms. Now they could be discussed in public, employed in teaching and propaganda, standardized, and mass distributed. What was private became public. What was unique became mass-produced. What was hidden in an individual's mind became shared.

## 2.

Galton saw photography as a machine for externalization of ideas. Even stronger claims were made about the next visual technology — film.

Indeed, the revolutionary new medium, the medium of mass society par excellence — film — was immediately proclaimed to be the machine for the externalization of private mental functions and states. In 1916 Hugo Münsterberg, a Professor of Psychology at Harvard University and one of the founders of the fields of industrial and applied psychology, published The Film: A Psychological Study, today canonized as one of the earliest theoretical treatments of cinema. [4] According to Münsterberg, the essence of the new medium lies in its ability to reproduce, or "objectify" various mental functions on the screen: "The photoplay obeys the laws of the mind rather than those of the outer world." [5] In contrast to the theater, where the action is constrained by the limitations of physical reality, film is free to shape arbitrarily its material, closely approximating flashes of memory, the flights of imagination, and other mental acts. For instance, while in theater events have to follow each other corresponding to the progression of time, in film the action can suddenly jump back and forth, just as in an act of imagination.

Münsterberg was not content to point out the analogy between film and mental life; in an astounding analysis, he correlated the main cinematic techniques to different mental functions such as attention and memory, one-to-one. For example, in the close-up, "everything which our mind wants to disregard has been suddenly banished from our sight and has disappeared," analogous to how our attention selects a particular object from the environment. Similarly, the "cut-back" technique objectifies the mental function of memory.

"In both cases," Münsterberg wrote, "the act which in the ordinary theater would go on in our mind alone is here in the photography projected into the pictures themselves." [6] The psychological laboratory became indistinguishable from the movie house; the textbook of experimental psychology — from the cinematographer's manual. The mind was projected on the screen; the inside became the outside.

Münsterberg admired the power of film to externalize the functions of consciousness. The next logical step was taken by German psychologist Kurt Levin who, in 1924-25, was the first to use film in his experiments. He wrote that "fiction film attempts to objectify certain psychological processes for the viewer. Psychological (scientific) film studies to what extent these psychological processes can be objectified [7] Soviet psychologist A.P. Luria, who planned to establish a psychological laboratory in Moscow in cooperation with the State Film Academy, acquainted Levin with Eisenstein, who attended the shooting of one of Levin's films and advised him. [8]

The figure of Eisenstein is particularly important because it reveals the historical connection between the desire to externalize the mind and the rise of mass communication, of which film was a major vehicle. The emergence of new mass societies in the earlier part of this century dictated the necessity to communicate ideological concepts to mass populations which were often illiterate. [9] In the 1920s Eisenstein boldly conceived a screen adaptation of Marx's _Capital_ as a way to efficiently bring about the political enlightenment of Russian audiences, especially the peasants who would not sit through a political lecture but, attracted by the "novelty" of a movie projector, would come to see movies, regardless of what was shown. [10] Unprecedented as his project was, its radicalism lay not only in the decision to visualize the abstract notions and logic of _Capital_ but in the method employed, which, according to Eisenstein, would directly provoke dialectical thinking in audiences. [11] Jacques Aumont concludes that for Eisenstein, "the object privileged in Marx's work is not a theoretical one, like any of the key concepts from _Capital_. It is at another level entirely that Eisenstein selects his true object — the Marxist method itself [12] Thus it was not simply a matter of the modern redeployment of the directions of the 1492 sermon: "...Images of the Virgin and the Saints were introduced...on account of the ignorance of simple people, so that those who are not able to read the scriptures can yet learn by seeing the sacraments of our salvation and faith in pictures." [13] The viewers of _Capital_ were not only to learn the scriptures of the new atheistic religion; they were to learn the process of reasoning.

It is significant that the most categorical statement by Eisenstein on the possibility of "filmic reasoning," reasoning through images, appears in the context of his discussion of the sequence known as For God and Country from October (1929):

"Maintaining the denotation of "God," the images increasingly disagree with our concept of God, inevitably leading to individual conclusions about the nature of all deities ...a chain of images attempted to achieve a purely intellectual resolution, resulting from a conflict between a preconception and a gradual discrediting of its purposeful steps. Step by step ...power is accumulated behind a process that can be formally identified with that of a logical deduction...The conventional descriptive form for film leads to the formal possibility of a kind of filmic reasoning. While conventional film directs emotions, this suggests an opportunity to encourage and direct the whole thought process as well." [14]

Far from simply representing God or deities, as they did for centuries, here images serve a totally new function — to provoke and direct reasoning, reasoning of a particular kind — "Marxist dialectics In accordance with its principles, as canonized by the official Soviet philosophy, Eisenstein wants to present the viewer with the visual equivalents of thesis and anti-thesis so that the viewer can then proceed to arrive at synthesis, i.e. the correct conclusion, pre-programmed by Eisenstein.

"The content of _Capital_ (its aim) is now formulated: to teach the worker to think dialectically," Eisenstein writes enthusiastically in April of 1928. [15] Schooled by the film, viewers would become self-sufficient thinkers, learning the skill of "Communist decoding of the world," each a walking camera, snapping pictures of visual thesis and anti-thesis, the brain automatically executing cognitive operations of montage, thinking through images, efficiently and effectively.

Eisenstein claims the radical novelty of his concept of "filmic reasoning":

"The proclamation that I'm going to make a movie of Marx's _Das Kapital_ is not a publicity stunt. I believe that the films of the future will be found going in this direction (or else they'll be filming things like The Idea of Christianity from the bourgeois point of view!) In any case, they will have to do with philosophy...the field is absolutely untouched. Tabula rasa". [16]

Yet, Einstein's theory was not an isolated development. Many in the artistic left of the 1920s shared a similar belief in the cognitive power of new visual forms such as montage. In the late 1920s, Alexander Rodchenko promoted the use of montage sequences in graphic design and, like Eisenstein, he saw montage as being equivalent to "dialectical" reasoning. In this formulation, an individual image corresponded to a single concept, and thinking was thought to be provoked when a number of images were juxtaposed in a series. [17]

Galton, Münsterberg, Eisenstein. Composite photographs, cinematic devices of close-up, cut-back, and montage. These developments are symptoms of a single social imaginary at work: to make the mind more controllable by externalizing it and rendering it visible.

The recurrent claims that new visual technologies externalize and objectify reasoning, and that they can be used to augment or control it, are based on the assumption of the isomorphism of mental representations and operations with external visual effects such as dissolves, composite images, and edited sequences. This assumption, which I so far have not questioned, on a closer examination appears to be highly problematic. Whatever mental representations and operations really are, the mind surely does not contain pictures, photographs or film clips which some mental homunculus looks at. The external images presented to the mind are not magically transplanted inside it as ready-made ideas and arguments. Regardless of what visual forms can be presented before the eye — diagrams, photographs, film images — as any other visual input, they are subjected to the complicated processing by the nervous system which constructs its own internal representations.

Yet, the assumption of such an isomorphism continues to persist in modern thinking about vision, ignited by every new round of visualization technology: photography, film, computer animation, and virtual reality. Consider the claims which surround the new field of scientific visualization — visualization of data sets, their relationships and their dynamic behavior using computer graphics. Richard Mark Friedhoff and William Benson proclaim that computer visualization techniques constitute the second computer revolution because they act as the direct "extension of preconscious visual processes". [18] They assume that the images on a computer screen do not simply function as an aid for reasoning but that they are equivalent to the mental representations the mind may construct while thinking — and this is the source of their power.

Or consider the technology, which, even more so than scientific visualization, is seen as capable of completely objectifying, better yet, transparently merging with mental processes — virtual reality (VR). Again, the descriptions of its capabilities do not distinguish between internal mental functions, events and processes, and externally presented images. This is how, according to Jaron Lanier, VR can take over human memory: "You can play back your memory through time and classify your memories in various ways. You'd be able to run back through the experiential places you've been in order to be able to find people, tools [19] Lanier also claims that VR will lead to the age of "post-symbolic communication," communication without language or any other symbols. Indeed, why should there be any need for linguistic symbols, if everybody, rather than being locked into a "prison-house of language" (Jameson), will happily live in the ultimate nightmare of democracy — the single mental space which is shared by everybody, and where every communicative act is always ideal (Habermas). This is Lanier's example of how post-symbolic communication will function: "you can make a cup that someone else can pick when there wasn't a cup before, without having to use a picture of the word "cup". [20] Here, as with the earlier technology of film, the fantasy of objectifying and augmenting consciousness, extending the powers of reason, goes hand in hand with the desire to see in technology a return to the primitive happy age of pre-language, pre-misunderstanding. Locked in virtual reality caves, with language taken away, we will communicate through gestures, body movements, and grimaces, like our primitive ancestors... [21]

What can one make of this apparently unsound, yet irresistible, assumption of isomorphism between the mental process of reasoning and external, technologically generated visual forms, haunting us at least since the end of the nineteenth century? The conflation of outside and inside is, of course, symptomatic of the desire to project the inside onto the outside, to make it objective and public. But is this all? To really understand the persistence of this assumption, we should turn to the history of ideas about the nature of mental processes.

It is well known that technologies have historically provided and continue to provide the models according to which people imagine the mind. In the seventeenth century, it was the clock, in the nineteenth — the motor, in the second half of the twentieth — digital computers. More precisely, the paradigms were provided not by the technologies themselves, but by theories which made them possible. To take the last paradigm as an example, cognitive psychology, born in the 1950s and gaining prominence ever since, approaches the mind as an information-processing system, as software which runs on the hardware of the brain. But what gave cognitive psychology its epistemological basis was not the new technology itself (computers), but the information theory accompanying it. It was this paradigm which substituted the discussions of mind and brain with the notion of "human information processing". And before information theory, theories of the mind were influenced by thermodynamics (as in Freud) and mechanics (Hobbes). [22]

It can be also claimed, however, that human imagination about the mind's operations is limited by the current visual technologies. Consider the current views about the nature of mental processes. The linguist George Lakoff asserted that "natural reasoning makes use of at least some unconscious and automatic image-based processes such as superimposing images, scanning them, focusing on part of them" [23] while the psychologist Philip Johnson-Laird proposed that logical reasoning is a matter of scanning visual models. [24] Such notions would have been impossible before the emergence of television and computer graphics. These visual technologies made operations on images such as scanning, focusing, and superimposition seem natural. Even more telling are the models of cognitive psychologists who, in the last two decades, have systematically scrutinized the role played by mental images in reasoning. These models, which define mental images in terms of such characteristics as spatial resolution, speed of access, basic graphic operations (rotation, translation, copy), seem to be described first of all computer imaging systems. Psychologists argue among themselves which imaging systems better resemble mental processes, but they do not doubt the basic metaphor. As Paul Virilio notes, "now the virtual images of the computer screen seem to confirm not only the existence of certain forms of representation but, more immediately, the objective presence of mental images". [25]

Similarly, in the earlier period, when Freud, in _The Interpretation of Dreams_, described the mechanisms by which dream thoughts and the logical relations between them are represented in dreams, he and his fellow psychologists relied on available visual technology for their understanding of the mind. Not surprisingly, Galton's composites, the earliest form of image processing before digital computers, provided a particularly attractive model. Freud compared the process of condensation with one of Francis Galton's procedures which became especially famous: making family portraits by overlaying a different negative image for each member of the family and then making a single print. [26] Writing in the same decade, the American psychologist Edward Titchener opened the discussion of the nature of abstract ideas in his textbook of psychology by noting that "the suggestion has been made that an abstract idea is a sort of composite photograph, a mental picture which results from the superimposition of many particular perceptions or ideas, and which therefore shows the common elements distinct and the individual elements blurred". [27] He then proceeds to consider the pros and cons of this view. We should not wonder why Titchener, Freud and other psychologists take the comparison for granted rather than presenting it as a simple metaphor — contemporary cognitive psychologists also do not question why their models of the mind are so similar to the computer workstations on which they are constructed.

## 4.

As psychologists begin to furiously take on the questions philosophers only wondered about, subjecting mental processes to controlled, scientific, laboratory study, their models begin to reflect, more and more, the external visual forms made possible by whatever visual technology dominates the period.

Given the reliance of psychological theories of the mind on contemporary visual technologies, is there any "progress" between the turn of the century and today, except that the imagination of contemporary psychologists depends on the more sophisticated visual technologies of computer graphics? In fact, during this century, the assumption of an isomorphism between the mental and the objective became even more prominent; and externalization of reasoning has been taken much further, both technologically and theoretically.

On the one hand, the refinement of various medical imagining techniques in the 1980s made possible an increasingly precise imaging of brain activity, including the visualization of reasoning — in a literal sense. It is now possible to ask the experimental subject to concentrate on solving a problem and to see which parts of the cerebral cortex are active. The question of whether reasoning in fact depends on the operations normally involved in perception becomes more and more the question which, according to a number of researchers, can be answered through experimentation — it is enough to show that the part of a cortex normally dedicated to the processing of visual information is activated in the process of reasoning. [28]

More importantly, in their theories, many cognitive psychologists have accepted as given the equivalence between internal mental processes and the operations which can be done with external, objectively existing visual representations and objects. Consider the debates about the nature and role of mental imagery, which have constituted one of the most active areas of research in cognitive psychology in the last two decades. [29]

On the one hand, there are those (such as Zenon Pylyshyn) who argue that mental imagery simply consists of the use of general thought processes to simulate physical perceptual events. In this view, if the subjects report the presence of mental imagery during reasoning and problem-solving, this is simply a side effect, a by-product of real mental computations which do not involve visual representations. On the other hand, there are those psychologists and neurophysiologists who, through experiments and direct imaging of brain activity, want to prove that reasoning takes place through the construction and manipulation of mental images (Alan Pavio, Roger Shepard, Stephen Kosslyn, Martha Farah).

One of the most well-known experiments in defense of the latter view has been done by Roger Shepard and Jacqueline Metzler of Harvard University. [30] They presented subjects with pairs of perspective line drawings of three-dimensional forms constructed from small cubes. The subjects' task was to determine whether or not the forms were identical in shape, despite the difference in orientation. Shepard and Metzler have found that the reaction time was proportional to the degree of rotation which is required to bring the two objects into a similar position. These results were taken as proof that in solving the problem, the subjects mentally rotated representations of three-dimensional objects, and that imagined rotations corresponded to actual physical rotations of objects: "Imagined rotations and physical transformations exhibit corresponding dynamic characteristics and are governed by the same laws of motion". [31] Thus, a mental process was equated with an operation one would perform with real, objectively existing objects.

Other experiments in defense of the position that many kinds of reasoning involve manipulation of mental imagery entail the comparison of abstract qualities. When subjects were asked to recall two animals and to judge which one was larger, the reaction time decreased proportionally to the difference in estimated size. In another experiment, one group of subjects was asked to rate animals in intelligence on a scale from one to ten, while another group had to compare the intelligence of pairs of animals. Again, the reaction time decreased as the distance in rated intelligence increased. It was concluded that when the subjects tried to discriminate between two objects, reaction time was shorter the greater the difference between two objects, regardless of whether the objects were really presented (for instance, two lines of different length), or were imagined (size of animals), or whether the qualities to be compared were abstract (intelligence of animals). [32]

We shall leave to psychologists the debates whether these and numerous related experiments indeed prove that internal mental processes involve the manipulation of pictures similar in their qualities to real images. But for my purpose, it is significant in itself that in imagining what mental processes are like, contemporary psychologists have assumed, without any reservations, an equivalence between the internal and the external, between the mental objects and the real ones.

Modernization, visualization, externalization. In order to externalize the internal, the invisible, it was first equated with the visual. Once this was accomplished, it was simple and only logical to equate the visual inside with the visual outside, the objectivity and standardization of images drawn on a classroom blackboard, on the screen of a movie theater, or, most recently, on the computer terminal.

## 5.

I have traced different ways in which the mind was externalized in the last century and a half. Abstract ideas and the process of condensation were equated with composite photographs (Galton and Freud respectively); mental functions such as attention and memory — with cinematic devices of close-up and cut-back (Münsterberg); the process of thinking — with montage (Eisenstein). More recently, Lanier and Friedhoff similarly have linked mental processes with VR and scientific visualization techniques ( and Benson). And finally, cognitive scientists have described mental processes and functions in terms of operations only possible with computer imaging systems.

The overall trajectory which I followed is from the inside to the outside, from the private and inaccessible mental states to the public, external, technologically generated visual forms or the latest imaging technologies themselves. One can even say that to a large extent it is this very desire to objectify the psyche which gave birth to modern imaging technologies such as photography, cinema and VR. Indeed, is not the whole idea of photography to objectify private memories and private mental images?

This trajectory still continues. In fact, the advances in electronics, computers and neuroscience now allow us to externalize mental processes in real-time. One example of this which I mentioned is medical imagining of brain activity. Another is the recent work to control computers by thinking commands. The 1993 issue of a computer journal reports:

NTT Researchers in Japan have created methods to use brainwaves to determine which direction a person will move a joystick; University of Illinois psychologists developed a system that types when words are spelled out mentally; and the New York State Health Dept. devised a system that lets users take a cursor up-and-down or side-to-side by visualizing the moves. [33]

However, the same technological advances (as well as work in nanotechnology and neural networks) also make it possible to take the next step: to go from the outside to inside, to internalize external technologies by putting the machines back into the brain. We are now witnessing the birth of neurotechnology: complete computers the size of neurons which can be implanted under the skull or tiny neural network circuits which can be merged with real neural networks.

We may only guess how far such research has already advanced in military labs (is it possible that much science fiction in this century is not about the future but simply an accurate description of contemporary military research?). For now, futuristic movies provide us with the best examples of how such implants could function. Two such movies are Hollywood's _Terminator 2_ and _Robocop_. In both films the main character's vision is enhanced by a sophisticated computer imaging circuitry. The circuitry functions as combination of a video camera and a robotic vision system. It allows both heroes to zoom on the objects, to see in the dark, to record and to play back what they see, to bring up stored images which can be compared with what they presently see and so on.

We used to dream of flying carpets and magic castles; now we dream of tiny video recorders implanted in our retinas and computer RAM inside our sculls supplementing our own short-term memory. In short, we dream of becoming neuro-cyborgs.

We used to flock to movie houses where our mental mechanisms were projected on a huge screen. Soon each of us will be able to put back this screen inside her or his head.

## References:

[1] Allan Sekula, "The Body and the Archive," October 39 (1987): 40.

[2] Qtd. in Ibid., 47.

[3] Qtd. in Ibid., 51.

[4] Hugo Münsterberg, The Photoplay: A Psychological Study (New York: D. Aplleton & Co., 1916).

[5] Ibid., 41.

[6] Ibid., 41.

[7] Qtd. in Olga Bulgakova, "Sergei Eisenshtein i ego "psikhologicheskiy Berlin" — mezhdu psikhoanalizom i strukturnoy psikhologiey" (Sergei Eisenstein and his "psychological Berlin": between psychoanalysis and structural psychology), Kinovedcheskie Zapiski 2 (1988): 187.

[8] Ibid., 177.

[9] For instance, according to the 1926 census, out of every 1,000 citizens of the U.S.S.R., only 445 were literate. Peter Kenez, The Birth of the Propaganda State. Soviet Methods of Mass Mobilization, 1917-1929 (Cambridge: Cambridge University Press, 1985), 157.

[10] Kenez, The Birth of the Propaganda State, 220.

[11] The pioneering work of Annette Michelson was important in bringing my attention to Eisenstein's Capital project. Annette Michelson, "Reading Eisenstein Reading "Capital"," October 2 (1976): 27-38; October 3 (1977): 82-88.

[12] Jacques Aumont, Montage Eisenstein (London and Bloomington: BFI Publishing and Indiana University Press, 1987), 163.

[13] Qtd. in Michael Baxandall, Painting and Experience in Fifteenth Century Italy (Oxford: Oxford University Press, 1972), 41.

[14] Sergei Eisenstein, "A Dialectical Approach to Film Form," in Film Form: Essays in Film Theory, ed. Jay Leyda (New York: Harcourt Brace and World, 1949), 62. Emphasis mine — L.M.

[15] Sergei Eisenstein, "Notes for a Film of "Capital", trans. Maciej Sliwowski, Jay Leyda, and Annette Michelson, October 2 (1976): 10.

[16] Qtd. in Michelson, "Reading Eisenstein," October 2: 28.

[17] While at first Rodchenko practiced juxtaposition of many separate photographs and fragments within the space of a single image, at the end of the 1920s his photomontages became multi-page layouts composed of a number of more "traditional" photographs, more like a film montage sequence.

[18] Richard Mark Friedhoff and William Benson, The Second Computer Revolution: Visualization (W.H. Freeman and Company, 1991), 13.

[19] Timothy Druckrey, "Revenge of the Nerds. An Interview with Jaron Lanier," Afterimage (May 1991), 9.

[20] Ibid., 6.

[21] At SIGGRAPH 1992, the premier annual conference on Computer Graphics and Interactive Techniques in the U.S., attended by nearly 30,000 people, about a dozen VR exhibits always had long lines of visitors. However, the lines to two of these exhibits were much longer than any of the others. One was called Dome, the other — Virtual Reality Cave; in both cases, to see the show, the visitors had to go inside some cave-line structures. It did not matter that one of the exhibits featured a scientific visualization display, of interest only to specialists. Clearly, the fact that in order to see the spectacle one had to go inside a dark, cave-like space, different from normal space, provided enough attraction at the end of this century, just as it did at its beginning, when millions flocked into the dark caves of movie theaters.

[22] While this theory is well known and widely accepted, other facts suggest that, at least, sometimes, the influence runs in the opposite direction — biological and psychological theories of body and mind providing paradigms for theories of mechanisms. For instance, it appears that Norbert Wiener's cybernetics was inspired by the concept of homeostasis developed in biology: "Physiologist Walter B. Cannon viewed the animal body as a self-regulating machine. Building on the work done by Claude Bernard in the nineteenth century, Cannon developed the concept of "homeostasis" — the process by which the body maintains itself in a state of internal equilibrium. Cannon's ideas were well known to Norbert Wiener. In fact, Cannon's Wisdom of the Body (1932) may be read as sort of an introduction to Wiener's Cybernetics (1948). Charles Eames and Ray Eames, A Computer Perspective: Background to the Computer Age (Cambridge: Harvard University Press, 1990), 99. Another example is provided by the turn of Artificial Intelligence (AI) in the 1980s from trying to simulate the disembodied mind to the simulation of a collective of primitive organisms, having the functionality of insects. Drawing directly on research in biology, the researchers in AI hope that intelligence will emerge as a product of the collective behavior of machines simulating simple biological organisms.

[23] George Lakoff, "Cognitive Linguistics," Versus 44/45 (1986): 149.

[24] Philip Johnson-Laird, Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness (Cambridge: Cambridge University Press, 1983).

[25] Paul Virilio, Lost Dimension (New York: Semiotext(e), 1991), 114.

[26] Sigmund Freud, Standard Edition of the Complete Psychological Works (London: Hogarth Press, 1953), 4: 293.

[27] Edward Bradford Titchener, A Beginner's Psychology (New York: The Macmillan Company, 1915), 114.

[28] Martha Farah, "Is Visual Imagery Really Visual? Overlooked Evidence from Neuropsychology," Psychological Review 95, no. 3 (1988): 307-317.

[29] For a summary of different positions, see Ronald A. Finke, Principles of Mental Imagery (Cambridge: The MIT Press, 1989).

[30] S. Shepard and J. Metzler, "Mental Rotations of Three-dimensional Objects," Science 171 (1971): 701-703.

[31] Finke, Principles of Mental Imagery, 93.

[32] John Robert Anderson, "Mental Imagery," in Cognitive Psychology and Its Implications, (W.H. Freeman and Company, 1980).

[33] Communications of the ACM 36, no. 5 (May 1993): 11.

---
# The Labor of Perception

_author: Lev Manovich_
_year: 1995_

## Work or Play

Walter Benjamin's writings kept coming back to the prototypical perceptual spaces of modernity: the factory, the movie theater, the shopping arcade. Scrutinizing these new spaces, Benjamin insisted on the continuity between the perceptual experiences in the workplace and outside of it:

"Whereas Poe's passers-by cast glances in all directions which still appeared to be aimless, today's pedestrians are obliged to do so in order to keep abreast of traffic signals. Thus, technology has subjected the human sensorium to a complex kind of training. There came a day when a new and urgent need for stimuli was met by the film. In a film, perception in the form of shocks was established as a formal principle. That which determines the rhythm of production on a conveyer belt is the basis of the rhythm of reception in the film. [1]

For Benjamin, the modern regime of perceptual labor, where the eye is constantly asked to process stimuli, equally manifests itself in work and leisure. The eye is trained to keep pace with the rhythm of industrial production at the factory and to navigate through the complex visual semiosphere beyond the factory gates.

What would be the equivalents of film and conveyer belt for the perceptual experience of post-modernity? The most direct equivalents are an arcade-type computer game and a military training simulator. But now, not only the two experiences provide the same stimuli, but they also share the same technology.

In fact, since the early 1990s, many companies which before supplied very expensive simulators to the military are busy converting them into entertainment arcade-based systems. One of the first such systems already commercially operating in a number of major cities, including Chicago and Tokyo — Battletech Center from Virtual World Entertainment, Inc. — is directly modeled on SIMNET (Simulation Network) developed by DARPA (Defense Advanced Research Projects Agency). SIMNET can be thought of as the first model of cyberspace, the very first collaborative VR environment. SIMNET consists of a number of individual simulators, networked together, each containing a copy of the world database and the virtual representation of all other participants in the conflict such as the Kuwaiti theater of operations. Similarly, a Battletech Center comprises a networked collection of futuristic cockpit models with VR gear. For seven dollars each, seven players can fight each other in a simulated environment. In another example, in 1992 Lucas Arts teamed up with Hughes Aircraft, combining the expertise in computer games of the former with the expertise in building actual flight simulators of the latter, in a joint venture aimed at theme-park type rides. [2]

A computer game and a flight simulator (or an actual cockpit) are only the most obvious examples of how contemporary visual culture is increasingly permeated by interactive computer graphic information displays. Their presence points to an essential feature of the post-industrial society in which the human, both at work and at play, functions as a part of human-machine systems where vision acts are the main interface between the human and the machine. This article will consider some historical aspects of this phenomenon.

Human-machine system is defined as "an equipment system, in which at least one of the components is a human being who interacts with or intervenes in the operation of the machine components of the system from time to time". [3] In contrast to a manual worker of the industrial age, an operator in a human-machine system is primarily engaged in the observation of displays which present information in real-time about the changing status of a system or an environment, real or virtual: a radar screen tracking a surrounding space; a computer screen updating the prices of stocks; a video screen of a computer game presenting an imaginary battlefield; a control panel of an automobile showing its speed, etc. [4] From time to time, some information causes an operator to make a decision and to intervene in the system's operation: tell the computer to track an enemy bomber noticed on the radar screen; buy or sell a stock; press a joystick; change gears. In some situations, these interventions may be required every second (a pilot engaged with an enemy, a computer game player, a financial analysis monitoring stock prices), while in others they are needed very rarely (a technician monitoring an automated plant, power station, a nuclear reactor; a radar operator monitoring a radar screen, waiting for potential enemy planes).

The first kind of situation can be seen as a direct continuation of the experience described by Benjamin. In the quoted passage Benjamin characterized modern experience as a constant periodic rhythm of perceptual shocks; the experience shared by an assembly line worker, by a pedestrian, and by a film viewer. This experience is also characteristic of the cybernetic workplace: the constant overwhelming amount of information; the constant cascade of cognitive shocks which require immediate interventions (a pilot engaged with an enemy, a player of a computer game). [5] The second kind of situation, however, points to another work experience, new to post-industrial society: work as waiting for something to happen. A radar operator waiting for a tiny dot to appear on the screen; a technician monitoring an automated plant, power station, or nuclear reactor, knowing that a software bug will eventually manifest itself, making a pointer on one of numerous dials shoot into the red...

## From Taylorism to Cognitive Science

Industrial society was characterized by the centrality of the concepts of manual labor, production of goods, and fatigue. Between 1940 and 1960, these were gradually replaced by new concepts of cognitive labor, information processing, and noise. Taylorism, Gilberts' motion studies, and behaviorism gave way to engineering psychology, "human information processing," and cognitive science. In short, with the transformation of industrial society into post-industrial society, the disciplines of the efficiency of the body were replaced by the disciplines concerned with the efficiency of the new instrument of labor — the mind.

In _The Human Motor: Energy, Fatigue and the Origins of Modernity_ Anson Rabinbach demonstrated how the scientific ideas of thermodynamics, formulated in the middle of the nineteenth century, became central for the conception of work in modernity. Helmholtz, who discovered the law of the conservation of energy, promoted this law as the universal principle which equally applies to nature, machines, and humans. Helmholtz "portrayed the movements of the planets, the forces of nature, the productive force of machines, and of course, human labor power as examples of the principle of conservation of energy". [6] All work was understood as the expenditure of energy, with a crucial consequence of redefining human labor as labor power, the expenditure of the energy of a body. Thus a worker was redefined as a "human motor". This, in turn, lead to the emergence, towards the end of the century, of the movement which Rabinbach calls the European science of work, "the search for the precise laws of muscles, nerves, and the efficient expenditure of energy centered on the physiology of labor". [7] In manual labor, the energy stored in the body where it was accumulated through the intake of food, sleep, and rest is transferred into muscular force — hammerer striking a blow, filer filing a machine part, and so on. Therefore, psychologists, physiologists and industrial experts searched for methods to maximize both the accumulation of a worker's energy (through proper nutrition, shorter working hours, appropriate breaks) and its expenditure on labor. Just as an engineer designing an engine was concerned with the most efficient transfer of fuel energy into movement, European work experts aimed to maximize worker efficiency and to eliminate possible waste. Central to the quest for the efficiency of the human motor was the struggle against fatigue, understood as the equivalent of entropy. "As entropy revealed the loss of energy involved in any transfer of force, so fatigue revealed the loss of energy in the conservation of Kraft to socially useful production. As energy was the transcendental, "objective" force in nature, fatigue became the objective nemesis of a society founded on labor power. [8]

The European science of work may appear to be very similar to the American scientific management movement pioneered by Frederick Winslow Taylor, a former engineer turned management consultant. As a part of his program, Taylor aimed to minimize and standardize the time required by a worker to perform each operation. He employed the method of time studies whereby the best workers were timed and the results became the norm to be followed by the rest. [9] Later, Frank and Lilian Gilberts (he — an engineer, she — a psychologist) popularized another method of motion study. [10] They argued that maximizing worker productivity is best achieved by the elimination of unnecessary movements and making the necessary more efficient. Although both time and motion studies and the European science of work were concerned with the efficiency of manual work, there was a fundamental difference between the two approaches. [11] Taylorism aimed for maximum productivity and had no concern for the exhaustion and deterioration of the human motor. In contrast, European scientists aimed for optimum productivity, and therefore were concerned not only with the rationalization of the workplace, but also with the workers' health, nutrition, safety, and the optimal length of a workday. In short, Taylorism had no reservations about replacing one exhausted human motor with another — the philosophy which in the U.S. seems to go hand in hand with the emerging ethics of the consumer society and with immigration policies which assured the constant supply of a cheap labor force. Europeans, on the other hand, were committed to caring for and servicing the human motor. The two paradigms converged after World War I, when European industrialists partly adopted the more brutal, but ultimately more effective Taylorist methods, while U.S. management experts became more sensitive to workers' physiology and psychology.

Taylorism reduced the worker's body to a mechanical machine and had no concern for her or his mind. Indeed, as Marta Braun points out, Taylorism aimed to systematically rob the worker of any degree of independence or even understanding of the overall work process by "separating responsibility for the execution of work from its planning or conception". [12] This disdain for the mind was shared by behaviorism, which matured at the same time as the European science of work and Taylorism, and which equally well characterizes the imaginary of hard-edged social engineering of the first half of the twentieth century. In 1913, J.B. Watson, the founder of behaviorism, explicitly defined it as the science of social control: "Psychology as the behaviorist views it is a purely objective experimental branch of natural science. Its theoretical goal is the prediction and control of behavior. [13] Behaviorism approached the human subject as an input-output system of stimulus and response to be controlled through conditioning. Concerned with controlling the body, it almost completely suppressed any studies of perceptual or mental processes between 1920 and 1950 in the U.S. It was psychology well suited for controlling the subject already reduced to the brainless human motor.

In the 1950s cognitive psychology begins to displace then dominant behaviorism. Since then, what comes under scrutiny of psychologists are mental functions: perception, attention, text comprehension, memory, and problem-solving.

I read this as one of the most important signs of the shift from industrial to post-industrial society. The point is not whether corporeal labor was indeed universally displaced by mental labor: this is different from country to country, from industry to industry. What is important is that the obsession with the rationalization of corporeal work (Taylorism, European science of work, psychotechnics) disappeared, displaced by new obsession with the rationalization of the mind (cognitive psychology, artificial intelligence, cognitive engineering). Regardless of the percentage of the workforce that still may be engaged in manual labor, society is no longer concerned with spending more intellectual resources to perfect workers' movements.

What Taylor's scientific management was for the age of industrialization, cognitive sciences became for the age of automation. In the 1940s, Herbert Simon worked on theories of management, the field of research originated by Taylor. Having recognized the increasing importance of mental skills in the corporate workplace, Simon became one of the pioneers of cognitive science with his work on automatic reasoning by computer. In 1964 he wrote that "the bulk of productive wealth consists of programs...stored in human minds". [14] Another pioneer of cognitive science was Jerome Bruner. Reflecting back on his work in the 1950s, he noted in 1983: "It seems plain to me now that the "cognitive revolution"...was a response to the technological demands of the "post-industrial revolution". You cannot properly conceive of managing a complex world of information without a workable concept of mind. [15]

The replacement of manual work by cognitive work is directly related to automation. Already in 1961, in an influential study of automation in French industry, Pierre Naville and his fellow sociologists had described the transition from the "work of the laborer to the work of communication," work which became primarily "cognitive or semiotic". [16] In his summary of this study Rabinbach writes, "The appearance of the cerebral worker whose material and product is "information" is emblematic of the vast distance traversed between the worker who surveys complex technologies of communication and the "man-beef" of Taylor." [17]

It is important to note that automation does not lead to the replacement of human by machine. Rather, the worker's role becomes one of monitoring and regulation: watching displays, analyzing incoming information, making decisions, and operating controls. And it is the corresponding human functions of perception, attention, memory, and problem solving which become the subject of research by new cognitive sciences.

The rise of cognitive sciences is one aspect of the larger shift from industrial to post-industrial society and the corresponding new image of work and play: visual and mental processing of information rather than corporeal activity. A complimentary development is the emergence, during World War II, of the new discipline of applied experimental psychology, or, as it was also called, "human engineering".

## Human Engineering

The gradual expansion of the practical applications of experimental psychology provides a precise map of the new occupations and new conditions of modern experience which call for perceptual skills. During World War I, England, Germany, and France utilized experimental psychologists to design and administer tests for aviation pilots, aeronautical, and airplane observers, hydrophone operators, and submarine "listeners-in". [18] During peacetime, a number of psychologists published papers on the readability of written text and of highway signs and on the visibility of lights at sea. [19] However, in the industrial world which conceived of the worker as a human motor and was largely concerned with the productivity of manual rather than perceptual labor, these studies were an exception rather than the mainstream rule.

It was World War II which finally put to use the expertise of experimental psychologists. Why did this happen? The first textbook on applied experimental psychology (1949) opens by describing the recent origins of the field:

"For years experimental psychologists have worked diligently in academic laboratories studying man's capacities to perceive, to work, and to learn. Only very slowly, however, have the facts and methods which they have assembled been put to use in everyday life. A particularly glaring gap in modern technology, both industrial and military, is the lack of human engineering — engineering of machines for human use and engineering of human tasks for operating machines. Motion-and-time engineers have been at work on many of these problems, but the experimental psychologist is also needed for his fundamental knowledge of human capacities and his methods of measuring human performance.

The recent war put the spotlight on this gap. The war needed, and produced, many complex machines, and it taxed the resources of both the designer and operator in making them practical for human use. The war also brought together psychologists, physiologists, physicists, design engineers, and motion-and-time engineers to solve some of these problems. Though much of their work began too late to do any real good, it has continued on a rather large scale into the piece.

Today, there are many groups busy with research on man-machine problems. They use different names to describe the work in its various aspects: biotechnology, biomechanics, psychoacoustics, human engineering, and systems research. Other names may be appropriate and may appear in the future. In casting about for a title for this book, we tried to select one that would describe the subject matter without the restrictive connotations attaching to some of the names mentioned above. "Applied Experimental Psychology seems best to fill these requirements, because the traditional data and subject of experimental psychology are fundamental to this field." [20]

Already before the war, experimental psychologists assisted in selecting military personnel for such jobs as pilot or airplane observer by administering special aptitude tests. During the war, a much greater number of pilots, radar operators and other similar personnel became needed. The emphasis was shifted, therefore, from selecting personnel with particularly good perceptual and motor skills to designing the equipment (controls, radar screens, dials, warning lights) to match the sensory capacities of an average person. [21] And it was the field of experimental psychology that possessed the knowledge about the sensory capacities of an average, statistical person [22]: how visibility and acuity vary between day and night; how the ability to distinguish colors and brightness vary with illumination or distance; what the smallest amount of light is which can be reliably noticed; and so on. [23] All this data was now utilized for designing better displays and controls of the first modern human-machine systems such as high-speed aircrafts or radar installations.

The development of these new human-machine systems during the war pushed human perceptual and mental performance to the limit and this was the second reason why experimental psychologists were called in. The performance of a human-machine system was limited by human information capacity to process information. In the words of the authors of _Applied Experimental Psychology_:

"We can make a machine that will do almost anything, given enough time and enough engineers. But man has limits to his developments, at least as far as we can see it. When we think how much a single radar can do in a small fraction of a second, and then realize by comparison that even the simplest form of reaction for a human being requires about a fifth of a second, we realize what we are up against... The full potential of radar, for example, lagged far behind physical developments because human operators could not master the complex operation of this machine system. "We had to worry about such things as a new kind of visual signal — very small and not very bright." [24]

Considering that the authors described the work of time-and-motion engineers as directly leading to applied experimental psychology, this rhetoric can be expected. Taylor was impatient with the limitations of the body; now there was a similar impatience with the limitations of human information processing. With Taylor, it was the question of the speed of muscular movements; now, it became the question of reaction time: the minimum time in milliseconds required for an operator to detect a signal, to identify it, to press a control.

In order to measure normal human sensory capacities, experimental psychologists have always put subjects in, so to speak, boundary conditions. They measured sensory thresholds, such as the least amount of light which can be detected. They also measured just noticeable differences (j.n.d.), the smallest difference between two stimuli which can be detected. Finally, they measured reaction times, the measure which became the main tool to deduce the time taken by different mental processes. In order to measure these characteristics, a number of standard experiments were designed, and they remained largely unchanged from the times of Weber, Fechner, and Wundt. In a detection experiment, the task of an observer is to detect the presence of barely visible stimuli, for instance, a tiny light briefly flashed in the dark (did I see something?). In an identification experiment, the task is to identify which of possible stimuli were presented, for instance, which of two colors (which one did I see?). In a recognition experiment, the task is to not only detect something, but to recognize what it is, for instance: what was the shape that briefly appeared (what did I see?).

During World War II, the radar operator, the anti-aircraft gunner, the aircraft pilot found themselves in the same situations in which nineteenth-century psychologists put their experimental subjects. The setups of psychophysical experiments became, in all details, the conditions of military work; the tasks devised by psychologists to study human vision became the actual tasks faced by the operators of human-machine systems. Like the subject of a detection experiment, a radar operator scans the radar screen for a barely noticeable dot of light. [25] Like the subject of an identification experiment, he has to try to guess whether this dot is the same or different from another dot which from his previous experience he knows to correspond to a friendly airplane. An anti-aircraft gunner is subjected to a recognition experiment, trying to identify a plane by its shape. And all of them, especially the pilot, are engaged in a sort of reaction time experiment.

Thus, nineteenth-century psychophysical setups became the military, and soon, civilian workplaces of post-industrial society; from there, they traveled back into laboratories, leading to such close interrelations between basic research in experimental psychology and its practical applications that they were no longer separable. For example, a 1947 article in _American Psychologist_ describes the work of Naval Research Laboratory as following these three directions: "the design of gun fire control and missile control instruments from the point of view of ease and efficiency of operation; the design and evaluation of synthetic gunnery and missile control trainers; and basic psychological research". But what is meant here by "basic research"? We read that "at present, all basic research studies are aimed at the eye-hand coordination problem involved in target tracking". "Target tracking" is just one example of a military task which traveled into a psychological laboratory, and gradually become a standard psychophysical experiment. 

The terms "applied experimental psychology," "human engineering" and "man-machine engineering" were replaced by another term standard today — "human factors". The radar operator who in the 1940s and 1950s was the prototypical example of a human-machine system, was replaced by the 1980s by a new prototypical figure, the computer user. Thus, references to "human-machine systems" became references to "human-computer systems". The same amount of intellectual energy and research which in the middle of the century went into theorizing the performance of a radar operator and adapting him and radar display to each other, today goes into the work on computer interfaces. In retrospect then, we should recognize the radar operator as the central figure standing at the origins of post-industrial society, the figure which put directly into motion the new disciplines of the efficiency of the mind: engineering psychology, human information processing, and cognitive science.

If radar screen of the 1940s was the first modern visual human-machine interface, VR gear is the most recent. While VR is commonly associated with the notions of escape from reality, unrestricted play and fantasy, in fact it is yet another development in the history of "human engineering". As an example, consider a popular photograph from the late 1980s which showcased virtual reality interface designed at NASA/Ames Human Factors Research Center. The gear was constructed by human factors specialists, the direct descendants of the "human engineers" of the 1940s. The specialists utilized all the knowledge accumulated by psychology about the human vision in order to employ most efficiently.

In the photograph, we see the last leftover from the age of manual labor — an arm in a DataGlove. It will soon disappear since through gaze tracking the operator can control the system by merely looking at different points in virtual space. Perceptual labor became the foundation of both work and play.

## References:

[1] Walter Benjamin, "On Some Motives in Baudelaire," in Illuminations, ed. Hannah Arendt (New York: Schochen Books, 1969), 175.

[2] On the connection between SIMNET and Battletech Centers, see Tony Reveaux, "Virtual Reality Gets Real," New Media (January 1993): 36-41. On VR entertainment systems in the context of location-based entertainment — arcades and theme parks — see Richard Cook, "Serious Entertainment," Computer Graphics World (May 1992): 40-48.

[3] Alphonse Chapanis, Man-Machine Engineering (Belmont, CA: Wadsworth Publishing Company, Inc., 1965), 16.

[4] A 1965 textbook on human-machine systems calls an automobile "a first-rate example of a true man-machine system...a highly complex system in which the operator plays a commanding role or actively intervenes in the system from time to time". Chapanis, Man-Machine Engineering, 16.

[5] Now, however, these shocks arrive exclusively through the visual channel (dials, computer screen, head-mounted display). Thus of the roles mentioned by Benjamin, it is the film viewer rather than the assembly line worker who directly anticipates the experience of an operator in this type of human-machine situation.

[6] Anson Rabinbach, The Human Motor: Energy, Fatigue, and the Origins of Modernity (Basic Books, Inc., 1990), 3.

[7] Ibid., 10.

[8] Ibid., 68.

[9] Frederick Winslow Taylor, The Principles of Scientific Management (New York, 1967).

[10] William R. Spriegel and Clark E. Myers, eds., The Writings of the Gilbreths (Homewood, IL., 1953).

[11] Rabinbach, The Human Motor, 117, 277.

[12] Marta Braun, Picturing Time: the Work of Etienne-Jules Marey (1830-1904) (Chicago: The University of Chicago Press, 1992), 337.

[13] Qtd. in Eliot Hearst, "One Hundred Years: Themes and Perspectives," in The First Century of Experimental Psychology, ed. Eliot Hearst (Hillsdale, NJ: Lawrence Erlbaum Associates, Publishers, 1979), 27.

[14] Qtd. in Douglas Noble, "Mental Materiel: The Militarization of Learning and Intelligence in U.S. Education," in Cyborg Worlds: The Military Information Society, ed. Les Levidov and Kevin Robins (London: Free Association Books, 1989), 34.

[15] Qtd. in ibid., 34-35.

[16] Qtd. in Rabinbach, The Human Motor, 298. 

[17] Ibid., 298.

[18] Morris Viteles, Industrial Psychology (New York: W.W. Norton & Company, Inc., 1932), 43.

[19] Paul Fitts, "Engineering Psychology and Equipment Design," in Handbook of Experimental Psychology, ed. S.S. Stevens (New York and London: John Wiley & Sons, Inc., 1951), 1287-1340.

[20] Alphonse Chapanis, Wendell R. Garner, and Clifford T. Morgan, Applied Experimental Psychology (New York: John Wiley & Sons, Inc., 1949), v. Ibid., 8.

[21] William Estes, "Experimental Psychology: an Overview," in The First Century of Experimental Psychology, ed. Eliot Hearst (Hillsdale, NJ: Lawrence Erlbaum Associates, Publishers, 1979), 630.

[22] Chapanis, Applied Experimental Psychology, 7-8.

[23] As Paul Fitts notes in his 1951 overview of engineering psychology, "radar operators are often forced to search for weak signals at near-threshold levels". Fitts, "Engineering Psychology and Equipment Design," 1290.

[24] Franklin Taylor, "Psychology at the Naval Research Laboratory," American Psychologist 2, no. 3 (1947): 87, 91.

[25] On NASA/Ames virtual reality research in the 1980s, see Scott S. Fisher, "Virtual Interface Environments," in The Art of Human-Computer Interface Design, ed. Brenda Laurel (Reading, Mass.: Addison-Wesley Publishing Company, 1990): 423-438.

---
# Cinema and Digital Media

_author: Lev Manovich_
_year: 1995_

## 1. Cinema Gives Birth to a Computer

Let us reverse a well-known wisdom: that a modern digital computer is a typical wartime technology developed for the purposes of calculation and real-time control and that its current use to create moving images is a rather specialized and recent application. Not only were computers used to create moving images within a few years of their "birth" but, in fact, the modern digital computer was born from cinema.

What is cinema? If we believe the word itself (cinematograph means "writing movement"), its essence is recording and storing visible data in a material form. A film camera records data on film; a film projector reads it off. This cinematic apparatus is similar to a computer in one key respect: a computer is controlled by a program stored externally in some medium. Therefore, it is not accidental that a diagram of the Universal Turing Machine looks suspiciously like a film projector. In fact, the development of a suitable storage medium and a method for coding data represent important parts of both cinema and computer pre-histories. As we know, the former eventually settled on discrete images recorded on a strip of celluloid; the latter — which needed much greater speed of access as well as the ability to quickly read and write data — on storing it electronically in a binary code.

So why was the digital computer born from cinema? 

### 1.1. Jacquard Loom

Around 1800 J.M. Jacquard invented a loom which was automatically controlled by punched paper cards. The loom was used to weave intricate figurative images, including Jacquard's portrait. This specialized graphics computer inspired Charles Babbage in his work on the Analytical Engine, a general computer for numerical calculations. As Ada Augusta, the daughter of Lord Byron and the first computer programmer put it, "the Analytical Engine weaves algebraical patterns just as the Jacquard loom weaves flowers and leaves". [1]

Thus, a programmed machine was already synthesizing images even before it was put to process numbers. 

### 1.2. Zuse's Film

Even more interesting is the case of Konrad Zuse. Starting in 1936 and continuing into the Second World War, Zuse had been building a computer in the living room of his parents' apartment in Berlin. Zuse's computer pioneered some of the basic ideas of computing: binary arithmetic, floating decimal point and program control by punched tape. For the tape, he used discarded 35 mm movie film. [2]

One of these surviving pieces of film shows the abstract program codes punched over the original frames of some interior shots. The iconic code of cinema is discarded in favor of the more efficient binary one. In a technological remake of the Oedipal complex, a son murders his father. But the story has a new twist — a happy one. Zuse's film with its strange superimposition of the binary over iconic anticipates the process which gets underway half a century later: the convergence of all media, including film, to digital code. Cinema and computer — the Jacquard loom and the Analytical Engine — merge into one. 

### 1.3. Digital Media

This story can be summarized as follows. A modern digital computer is developed to perform calculations on numerical data more efficiently; it takes over from numerous mechanical tabulators and calculators already widely employed by companies and governments since the turn of the century. In parallel, we witness the rise of modern media which allow the storage of images, image sequences, sounds and text in different material forms: a photographic plate, a film stock, a gramophone record, etc. The synthesis of these two histories? The translation, which is taking place today, of all existing media into numerical data accessible for the computers. The result: digital media — graphics, moving images, sounds, shapes, spaces and text which become computable, i.e. simply another set of computer data.

If before a computer would read in a row of numbers outputting a statistical result or a gun trajectory, now it can read in pixel values, blurring the image, adjusting its contrast, or checking whether it contains an outline of a gun. The iconic — Barthes's famous "message without a code" — finally became securely codified. (It is interesting that image processing and semiotic analysis of iconic signs both develop at the same time — the second half of the 1950s.) And while the numeric coding of an image did not, of course, fulfill the semiotic desire to divide an image into units of meaning, it did come just at the right time for the enormous economic, ideological and military interests already dependent on the instrumental use of the visible and therefore looking for a more efficient way for it to be recorded, stored, manipulated, reproduced, transmitted and displayed. The society of the Spectacle was destined to embrace digital media.

## 2. Cinema Prepares Digital Media

Cinema not only plays a special role in the history of the computer. Since the late nineteenth century, cinema was also preparing us for digital media in a more direct way. It worked to make familiar such digital concepts as sampling, random access, or a database — in order to allow us to swallow the digital revolution as painlessly as possible. Gradually, cinema taught us to accept the manipulation of time and space, the arbitrary coding of the visible, the mechanization of vision, and the reduction of reality to a moving image as a given. As a result, today the conceptual shock of the digital revolution is not experienced as a real shock — because we were ready for it for a long time. 

### 2.1. Sampling

Any digital representation consists of a limited number of samples, a fact which is usually illustrated by a grid of pixels — a sampling of two-dimensional space. Cinema prepares us for digital media because it is already based on sampling — the sampling of time. Cinema samples time twenty-four times per second. All that remains is to take this already discrete representation and to quantify it. But this is simply a mechanical step; what cinema accomplished is a much more difficult conceptual break from the continuous to the discrete. Cinema is not the only media technology which, emerging towards the end of the nineteenth century, is dependent on a discrete representation. If cinema samples time, fax transmission of images, starting in 1907, samples two-dimensional space; even earlier, the first television experiments (Carey, 1875; Nipkow, 1884) already involve sampling of both. [3] However, reaching mass popularity much earlier than these other technologies, cinema is the first to make the principle of a discreet representation of the iconic public knowledge.

### 2.2. Random Access

Another key quality of digital media is random access. For instance, once a film is digitized and loaded in the computer memory, any frame can be accessed equally fast. Therefore, if film samples time but still preserves its linear ordering (subsequent moments of time become subsequent frames), digital media abandons this "human-centered" representation altogether in order to put time fully under our control. Time is mapped onto two-dimensional space, where it can be managed, analyzed and manipulated more easily.

Such mapping was already widely used in nineteenth-century cinema machines. The Phenakisticope, the Zoetrope, the Zoopraxiscope, the Tachyscope, and Marey's photographic gun were all based on placing a number of slightly different images around the perimeter of a circle. Even more striking is the case of Thomas Edison's first cinema apparatus. In 1887 Edison and his assistant, William Dickson began experiments to adopt the already proven technology of a phonograph record for recording and displaying motion pictures. Using a special picture-recording camera, tiny pinpoint-sized photographs were placed in spirals on a cylindrical cell similar in size to the phonography cylinder. A cylinder was to hold 42,000 images, each so small (1/32 inch wide) that a viewer would have to look at them through a microscope. [4] The storage capacity of this medium was twenty-eight minutes — twenty-eight minutes of continuous time taken apart, flattened on a surface and mapped into a two-dimensional grid. In short, time was prepared to be recreated, manipulated and reordered.

## 3. Simulation

It won't be difficult to show how cinema has been preparing other concepts associated with digital media, but, given the limitations of space, I want to focus on the most important one: simulation. Digital media makes commonplace the simulation of non-existent realistic worlds. Examples include military simulators, Virtual Reality, computer games, television ("virtual sets" technology), and, of course, special effects of Hollywood films such as "Terminator 2," "Jurassic Park" and "Caspar". These latter films seem to demonstrate that, given enough time and money, almost anything can be simulated. Yet, they also exemplify the triviality of what at first may appear to be an outstanding technical achievement — the ability to fake visual reality. What is faked, of course, is not reality but photographic reality, reality as seen by the camera lens. In other words, what digital simulation has (almost) achieved is not realism, but only photorealism — the ability to fake not our perceptual and bodily experience of reality but only its film image. This image exists outside of our consciousness, on a screen — a window of limited size which presents a still imprint of a small part of outer reality, filtered through the lens with its limited depth of field, filtered through film's grain and its limited tonal range. It is only this film-based image which digital technology has learned to simulate. And the reason we think that this technology has succeeded in faking reality is that cinema, over the course of the last hundred years, has taught us to accept its particular representational form as reality.

What is faked is only a cinematic image. Once we came to accept a moving photograph as reality, the way to its future simulation was open. Conceptually, digitally simulated worlds already appeared with the first films of the Lumieres and Georges Melies in the 1890s. It is they who invented digital simulation. 

It is hundred years later and the simulation techniques are fully perfected. And it is becoming clear that it is ultimately more advantageous to simulate the world than to film it directly. A simulated image can represent non-existent reality, it can be endlessly modified, it is more manageable, and so on. Because of this, our society will try to use digital simulations whenever possible.

Cinema, which was the key method to represent the world throughout the twentieth century, is destined to be replaced by digital media: the numeric, the computable, the simulated. This was the historical role played by cinema: to prepare us to live comfortably in the world of two-dimensional moving simulations. Having played this role well, cinema exits the stage. Enters the computer.

## References:

[1] Charles Eames, A COMPUTER PERSPECTIVE: BACKGROUND TO THE COMPUTER AGE (Cambridge, Mass.: Harvard University Press, 1990), 18.

[2] Eames, 120.

[3] Albert Abramson, ELECTRONIC MOTION PICTURES. A HISTORY OF TELEVISION CAMERA (Berkeley: University of California Press, 1955), 15-24.

[4] Charles Musser, THE EMERGENCE OF CINEMA: THE AMERICAN SCREEN TO 1907 (Berkeley: University of California Press, 1994), 65.

---
# What is Digital Cinema?

_author: Lev Manovich_
_year: 1995_

## Cinema, the Art of the Index

Thus far, most discussions of cinema in the digital age have focused on the possibilities of interactive narrative. It is not hard to understand why: since the majority of viewers and critics equate cinema with storytelling, digital media is understood as something which will let cinema tell its stories in a new way. Yet as exciting as the ideas of a viewer participating in a story, choosing different paths through the narrative space, and interacting with characters may be, they only address one aspect of cinema which is neither unique nor, as many will argue, essential to it: narrative.

The challenge which digital media poses to cinema extends far beyond the issue of narrative. Digital media redefines the very identity of cinema. In a symposium which took place in Hollywood in the Spring of 1996, one of the participants provocatively referred to movies as "flatties" and to human actors as "organics" and "soft fuzzies". [1] As these terms accurately suggest, what used to be cinema's defining characteristics have become just the default options, with many others available. When one can "enter" a virtual three-dimensional space, to view flat images projected on the screen is hardly the only option. When, given enough time and money, almost everything can be simulated in a computer, to film physical reality is just one possibility.

This "crisis" of cinema's identity also affects the terms and the categories used to theorize cinema's past. French film theorist Christian Metz wrote in the 1970s that "Most films shot today, good or bad, original or not, "commercial" or not, have as a common characteristic that they tell a story; in this measure, they all belong to one and the same genre, which is, rather, a sort of "super-genre" ["sur-genre"]." [2] In identifying fictional films as a "super-genre" of twentieth-century cinema, Metz did not bother to mention another characteristic of this genre because at that time it was too obvious: fictional films are **live-action** films, i.e. they largely consist of unmodified photographic recordings of real events which took place in real physical space. Today, in the age of computer simulation and digital compositing, invoking this characteristic becomes crucial in defining the specificity of twentieth-century cinema. From the perspective of a future historian of visual culture, the differences between classical Hollywood films, European art films and avant-garde films (apart from abstract ones) may appear less significant than this common feature: that they relied on lens-based recordings of reality. This essay is concerned with the effect of the so-called digital revolution on cinema as defined by its "super genre" as a fictional live-action film. [3]

During cinema's history, a whole repertoire of techniques (lighting, art direction, the use of different film stocks and lenses, etc.) was developed to modify the basic record obtained by a film apparatus. And yet behind even the most stylized cinematic images we can discern the bluntness, the sterility, the banality of early nineteenth-century photographs. No matter how complex its stylistic innovations, the cinema has found its base in these deposits of reality, these samples obtained by a methodical and prosaic process. Cinema emerged out of the same impulse which engendered naturalism, court stenography and wax museums. Cinema is the art of the index; it is an attempt to make art out of a footprint. 

Even for Andrey Tarkovsky, film-painter par excellence, cinema's identity lay in its ability to record reality. Once, during a public discussion in Moscow sometime in the 1970s, he was asked the question as to whether he was interested in making abstract films. He replied that there can be no such thing. Cinema's most basic gesture is to open the shutter and to start the film rolling, recording whatever happens to be in front of the lens. For Tarkovsky, an abstract cinema is thus impossible.

But what happens to cinema's indexical identity if it is now possible to generate photorealistic scenes entirely on a computer using 3-D computer animation; to modify individual frames or whole scenes with the help of a digital paint program; to cut, bend, stretch and stitch digitized film images into something which has perfect photographic credibility, although it was never actually filmed? 

This essay will address the meaning of these changes in the filmmaking process from the point of view of the larger cultural history of the moving image. Seen in this context, the manual construction of images in digital cinema represents a return to nineteenth-century pre-cinematic practices, when images were hand-painted and hand-animated. At the turn of the twentieth century, cinema was to delegate these manual techniques to animation and define itself as a recording medium. As cinema enters the digital age, these techniques are again becoming commonplace in the filmmaking process. Consequently, cinema can no longer be clearly distinguished from animation. It is no longer an indexical media technology but, rather, a sub-genre of painting. 

This argument will be developed in three stages. I will first follow a historical trajectory from nineteenth-century techniques for creating moving images to twentieth-century cinema and animation. Next, I will arrive at a definition of digital cinema by abstracting the common features and interface metaphors of a variety of computer software and hardware which are currently replacing traditional film technology. Seen together, these features and metaphors suggest a distinct logic of a digital moving image. This logic subordinates the photographic and the cinematic to the painterly and the graphic, destroying cinema's identity as media art. Finally, I will examine different production contexts which already use digital moving images — Hollywood films, music videos, CD-ROM games and artworks — in order to see if and how this logic has begun to manifest itself.

## A Brief Archeology of Moving Pictures

As testified by its original names (kinetoscope, cinematograph, moving pictures), cinema was understood, from its birth, as the art of motion, the art which finally succeeded in creating a convincing illusion of dynamic reality. If we approach cinema in this way (rather than the art of audio-visual narrative, or the art of a projected image, or the art of collective spectatorship, etc.), we can see it superseding previous techniques for creating and displaying moving images. 

These earlier techniques shared a number of common characteristics. First, they all relied on hand-painted or hand-drawn images. The magic lantern slides were painted at least until the 1850s; so were the images used in the Phenakistiscope, the Thaumatrope, the Zoetrope, the Praxinoscope, the Choreutoscope and numerous other nineteenth-century pro-cinematic devices. Even Muybridge's celebrated Zoopraxiscope lectures of the 1880s featured not actual photographs but colored drawings painted after the photographs. [4]

Not only were the images created manually, they were also manually animated. In Robertson's Phantasmagoria, which premiered in 1799, magic lantern operators moved behind the screen in order to make projected images appear to advance and withdraw. [5] More often, an exhibitor used only his hands, rather than his whole body, to put the images into motion. One animation technique involved using mechanical slides consisting of a number of layers. An exhibitor would slide the layers to animate the image. [6] Another technique was to slowly move a long slide containing separate images in front of a magic lantern lens. Nineteenth-century optical toys enjoyed in private homes also required manual action to create movement — twirling the strings of the Thaumatrope, rotating the Zootrope's cylinder, turning the Viviscope's handle. 

It was not until the last decade of the nineteenth century that the automatic generation of images and their automatic projection were finally combined. A mechanical eye became coupled with a mechanical heart; photography met the motor. As a result, cinema — a very particular regime of the visible — was born. Irregularity, non-uniformity, the accident, and other traces of the human body, which previously inevitably accompanied moving image exhibitions, were replaced by the uniformity of machine vision. [7] A machine, which was like a conveyer belt, was now spitting out images, all sharing the same appearance, all the same size, all moving at the same speed, like a line of marching soldiers. 

Cinema also eliminated the discrete character of both space and movement in moving images. Before cinema, the moving element was visually separated from the static background as with a mechanical slide show or Reynaud's Praxinoscope Theater (1892). [8] The movement itself was limited in range and affected only a clearly defined figure rather than the whole image. Thus, typical actions would include a bouncing ball, a raised hand or eyes, a butterfly moving back and forth over the heads of fascinated children — simple vectors charted across still fields. 

Cinema's most immediate predecessors share something else. As the nineteenth-century obsession with movement intensified, devices which could animate more than just a few images became increasingly popular. All of them — the Zoetrope, the Phonoscope, the Tachyscope, the Kinetoscope — were based on loops, sequences of images featuring complete actions which can be played repeatedly. The Thaumatrope (1825), in which a disk with two different images painted on each face was rapidly rotated by twirling a string attached to it, was in its essence a loop in its most minimal form: two elements replacing one another in succession. In the Zoetrope (1867) and its numerous variations, approximately a dozen images were arranged around the perimeter of a circle. [9] The Mutoscope, popular in America throughout the 1890s, increased the duration of the loop by placing a larger number of images radially on an axle. [10] Even Edison's Kinetoscope (1892-1896), the first modern cinematic machine to employ film, continued to arrange images in a loop. [11] 50 feet of film translated to an approximately 20-second-long presentation — a genre whose potential development was cut short when cinema adopted a much longer narrative form. 

## From Animation to Cinema

Once the cinema was stabilized as a technology, it cut all references to its origins in artifice. Everything which characterized moving pictures before the twentieth century — the manual construction of images, loop actions, the discrete nature of space and movement — all this was delegated to cinema's bastard relative, its supplement, its shadow — animation. Twentieth-century animation became a depository for nineteenth-century moving image techniques left behind by cinema.

The opposition between the styles of animation and cinema defined the culture of the moving image in the twentieth century. Animation foregrounds its artificial character, openly admitting that its images are mere representations. Its visual language is more aligned to the graphic than to the photographic. It is discrete and self-consciously discontinuous: crudely rendered characters moving against a stationary and detailed background; sparsely and irregularly sampled motion (in contrast to the uniform sampling of motion by a film camera — recall Jean-Luc Godard's definition of cinema as "truth 24 frames per second"), and finally space constructed from separate image layers.

In contrast, cinema works hard to erase any traces of its own production process, including any indication that the images which we see could have been constructed rather than recorded. It denies that the reality it shows often does not exist outside of the film image, the image which was arrived at by photographing an already impossible space, itself put together with the use of models, mirrors, and matte paintings, and which was then combined with other images through optical printing. It pretends to be a simple recording of an already existing reality — both to a viewer and to itself. [12] Cinema's public image stressed the aura of reality "captured" on film, thus implying that cinema was about photographing what existed before the camera, rather than "creating the 'never-was" of special effects. [13] Rear projection and blue screen photography, matte paintings and glass shots, mirrors, and miniatures, push development, optical effects and other techniques which allowed filmmakers to construct and alter the moving images, and thus could reveal that cinema was not really different from animation, were pushed to cinema's periphery by its practitioners, historians and critics. [14]

Today, with the shift to digital media, these marginalized techniques move to the center.

## What is Digital Cinema?

A visible sign of this shift is the new role which computer-generated special effects have come to play in the Hollywood industry in the last few years. Many recent blockbusters have been driven by special effects; feeding on their popularity. Hollywood has even created a new-mini genre of "The Making of..." videos and books which reveal how special effects are created. 

I will use special effects from few recent Hollywood films for illustrations of some of the possibilities of digital filmmaking. Until recently, Hollywood studios were the only ones who had the money to pay for digital tools and for the labor involved in producing digital effects. However, the shift to digital media affects not just Hollywood, but filmmaking as a whole. As traditional film technology is universally being replaced by digital technology, the logic of the filmmaking process is being redefined. What I describe below are the new principles of digital filmmaking which are equally valid for individual or collective film productions, regardless of whether they are using the most expensive professional hardware and software or its amateur equivalents. 

Consider, then, the following principles of digital filmmaking: 

1. Rather than filming physical reality it is now possible to generate film-like scenes directly on a computer with the help of 3-D computer animation. Therefore, live-action footage is displaced from its role as the only possible material from which the finished film is constructed.

2. Once live-action footage is digitized (or directly recorded in a digital format), it loses its privileged indexical relationship to pro-filmic reality. The computer does not distinguish between an image obtained through the photographic lens, an image created in a paint program or an image synthesized in a 3-D graphics package, since they are made from the same material — pixels. And pixels, regardless of their origin, can be easily altered, substituted one for another, and so on. Live action footage is reduced to be just another graphic, no different than images which were created manually. [15]

3. If live-action footage was left intact in traditional filmmaking, now it functions as raw material for further compositing, animating and morphing. As a result, while retaining visual realism unique to the photographic process, film obtains the plasticity which was previously only possible in painting or animation. To use the suggestive title of a popular morphing software, digital filmmakers work with "elastic reality". For example, the opening shot of **Forest Gump** (Robert Zemeckis, Paramount Pictures, 1994; special effects by Industrial Light and Magic) tracks an unusually long and extremely intricate flight of a feather. To create the shot, the real feather was filmed against a blue background in different positions; this material was then animated and composited against shots of a landscape. [16] The result: a new kind of realism, which can be described as "something which looks is intended to look exactly as if it could have happened, although it really could not."

4. Previously, editing and special effects were strictly separate activities. An editor worked on ordering sequences of images together; any intervention within an image was handled by special effects specialists. The computer collapses this distinction. The manipulation of individual images via a paint program or algorithmic image processing becomes as easy as arranging sequences of images in time. Both simply involve "cut and paste". As this basic computer command exemplifies, modification of digital images (or other digitized data) is not sensitive to distinctions of time and space or of differences in scale. So, re-ordering sequences of images in time, compositing them together in space, modifying parts of an individual image, and changing individual pixels become the same operation, conceptually and practically.

5. Given the preceding principles, we can define digital film in this way:  digital film = live-action material + painting + image processing + compositing + 2-D computer animation + 3-D computer animation

Live action material can either be recorded on film or video or directly in a digital format. [17] Painting, image processing and computer animation refer to the processes of modifying already existent images as well as creating new ones. In fact, the very distinction between creation and modification, so clear in film-based media (shooting versus darkroom processes in photography, production versus post-production in cinema) no longer applies to digital cinema, since each image, regardless of its origin, goes through a number of programs before making it to the final film. [18]

Let us summarize the principles discussed thus far. Live action footage is now only raw material to be manipulated by hand: animated, combined with 3-D computer-generated scenes and painted over. The final images are constructed manually from different elements; and all the elements are either created entirely from scratch or modified by hand. 

We can finally answer the question "what is digital cinema?" Digital cinema is a particular case of animation which uses live-action footage as one of its many elements. 

This can be re-read in view of the history of the moving image sketched earlier. Manual construction and animation of images gave birth to cinema and slipped into the margins...only to re-appear as the foundation of digital cinema. The history of the moving image thus makes a full circle. **Born from animation, cinema pushed animation to its boundary, only to become one particular case of animation in the end.** 

The relationship between "normal" filmmaking and special effects is similarly reversed. Special effects, which involved human intervention into machine recorded footage and which were therefore delegated to cinema's periphery throughout its history, become the norm of digital filmmaking.

The same applies to the relationship between production and post-production. Cinema traditionally involved arranging physical reality to be filmed through the use of sets, models, art direction, cinematography, etc. Occasional manipulation of recorded film (for instance, through optical printing) was negligible compared to the extensive manipulation of reality in front of a camera. In digital filmmaking, shot footage is no longer the final point but just raw material to be manipulated in a computer where the real construction of a scene will take place. In short, the production becomes just the first stage of post-production. 

The following examples illustrate this shift from re-arranging reality to re-arranging its images. From the analog era: for a scene in **Zabriskie Point** (1970), Michelangelo Antonioni, trying to achieve a particularly saturated color, ordered a field of grass to be painted. From the digital era: to create the launch sequence in **Apollo 13** (Universal Studious, 1995; special effects by Digital Domain), the crew shot footage at the original location of the launch at Cape Canaveral. The artists at Digital Domain scanned the film and altered it on computer workstations, removing recent building construction, adding grass to the launch pad and painting the skies to make them more dramatic. This altered film was then mapped onto 3D planes to create a virtual set which was animated to match a 180-degree dolly movement of a camera following a rising rocket. [19]

The last example brings us to yet another conceptualization of digital cinema — as painting. In his book-length study of digital photography, William J. Mitchell focuses our attention on what he calls the inherent mutability of a digital image: "The essential characteristic of digital information is that it can be manipulated easily and very rapidly by computer. It is simply a matter of substituting new digits for old... Computational tools for transforming, combining, altering, and analyzing images are as essential to the digital artist as brushes and pigments to a painter." [20] As Mitchell points out, this inherent mutability erases the difference between a photograph and a painting. Since a film is a series of photographs, it is appropriate to extend Mitchell's argument to digital film. With an artist being able to easily manipulate digitized footage either as a whole or frame by frame, a film in a general sense becomes a series of paintings. [21] 

Hand-painting digitized film frames, made possible by a computer, is probably the most dramatic example of the new status of cinema. No longer strictly locked in the photographic, it opens itself towards the painterly. It is also the most obvious example of the return of cinema to its nineteenth-century origins — in this case, to hand-crafted images of magic lantern slides, the Phenakistiscope, the Zoetrope.

We usually think of computerization as automation, but here the result is the reverse: what was previously automatically recorded by a camera now has to be painted one frame at a time. But not just a dozen images, as in the nineteenth century, but thousands and thousands. We can draw another parallel with the practice, common in the early days of silent cinema, of manually tinting film frames in different colors according to a scene's mood. [22] Today, some of the most visually sophisticated digital effects are often achieved using the same simple method: painstakingly altering by hand thousands of frames. The frames are painted over either to create mattes ("hand-drawn matte extraction") or to directly change the images, as, for instance, in **Forest Gump**, where President Kennedy was made to speak new sentences by altering the shape of his lips, one frame at a time. [23] In principle, given enough time and money, one can create what will be the ultimate digital film: 90 minutes, i.e., 129600 frames completely painted by hand from scratch, but indistinguishable in appearance from live photography. [24]

## Multimedia as "Primitive" Digital Cinema

3-D animation, compositing, mapping, paint retouching: in commercial cinema, these radical new techniques are mostly used to solve technical problems while traditional cinematic language is preserved unchanged. Frames are hand-painted to remove wires which supported an actor during shooting; a flock of birds is added to a landscape; a city street is filled with crowds of simulated extras. Although most Hollywood releases now involve digitally manipulated scenes, the use of computers is always carefully hidden. [25]

Commercial narrative cinema still continues to hold on to the classical realist style where images function as un-retouched photographic records of some events which took place in front of the camera. [26] Cinema refuses to give up its unique cinema effect, an effect which, according to Christian Metz's penetrating analysis made in the 1970s, depends upon narrative form, the reality effect and cinema's architectural arrangement all working together. [27]

Towards the end of his essay, Metz wonders whether in the future non-narrative films may become more numerous; if this happens, he suggests that cinema will no longer need to manufacture its reality effect. Electronic and digital media have already brought about this transformation. Beginning in the 1980s, new cinematic forms have emerged which are not linear narratives, which are exhibited on a television or a computer screen, rather than in a movie theater — and which simultaneously give up cinematic realism.

What are these forms? First of all, there is the music video. Probably not by accident, the genre of music video came into existence exactly at the time when electronic video effects devices were entering editing studios. Importantly, just as music videos often incorporate narratives within them, but are not linear narratives from start to finish, they rely on film (or video) images but change them beyond the norms of traditional cinematic realism. The manipulation of images through hand-painting and image processing, hidden in Hollywood cinema, is brought into the open on a television screen. Similarly, the construction of an image from heterogeneous sources is not subordinated to the goal of photorealism but functions as an aesthetic strategy. The genre of music video has been a laboratory for exploring numerous new possibilities of manipulating photographic images made possible by computers — the numerous points which exist in the space between the 2-D and the 3-D, cinematography and painting, photographic realism and collage. In short, it is a living and constantly expanding textbook for digital cinema.

A detailed analysis of the evolution of music video imagery (or, more generally, broadcast graphics in the electronic age) deserves a separate treatment and I will not try to take it up here. Instead, I will discuss another new cinematic non-narrative form, CD-ROM games, which, in contrast to music video, relied on the computer for storage and distribution from the very beginning. And, unlike music video designers who were consciously pushing traditional film or video images into something new, the designers of CD-ROMs arrived at a new visual language unintentionally while attempting to emulate traditional cinema.  

In the late 1980s, Apple began to promote the concept of computer multimedia; and in 1991 it released QuickTime software to enable an ordinary personal computer to play movies. However, for the next few years, the computer did not perform its new role very well. First, CD-ROMs could not hold anything close to the length of a standard theatrical film. Secondly, the computer would not smoothly play a movie larger than the size of a stamp. Finally, the movies had to be compressed, degrading their visual appearance. Only in the case of still images was the computer able to display photographic-like detail at full-screen size. 

Because of these particular hardware limitations, the designers of CD-ROMs had to invent a different kind of cinematic language in which a range of strategies, such as discrete motion, loops, and superimposition, previously used in nineteenth-century moving image presentations, in twentieth-century animation, and in the avant-garde tradition of graphic cinema, were applied to photographic or synthetic images. This language synthesized cinematic illusionism and the aesthetics of graphic collage, with its characteristic heterogeneity and discontinuity. The photographic and the graphic, divorced when cinema and animation went their separate ways, met again on a computer screen.

The graphic also met the cinematic. The designers of CD-ROMs were aware of the techniques of twentieth-century cinematography and film editing, but they had to adopt these techniques both to an interactive format and to hardware limitations. As a result, the techniques of modern cinema and of nineteenth-century moving image have merged in a new hybrid language. 

We can trace the development of this language by analyzing a few well-known CD-ROM titles. The best-selling game **Myst** (Broderbund, 1993) unfolds its narrative strictly through still images, a practice which takes us back to magic lantern shows (and to Chris Marker's **La Jetée**). [28] But in other ways **Myst** relies on the techniques of twentieth-century cinema. For instance, the CD-ROM uses simulated camera turns to switch from one image to the next. It also employs the basic technique of film editing to subjectively speed up or slow down time. In the course of the game, the user moves around a fictional island by clicking on a mouse. Each click advances a virtual camera forward, revealing a new view of a 3-D environment. When the user begins to descend into the underground chambers, the spatial distance between the points of view of each two consecutive views sharply decreases. If before the user was able to cross a whole island with just a few clicks, now it takes a dozen clicks to get to the bottom of the stairs! In other words, just as in traditional cinema, **Myst** slows down time to create suspense and tension.

In **Myst**, miniature animations are sometimes embedded within the still images. In the next best-selling CD-ROM **7th Guest** (Virgin Games, 1993), the user is presented with video clips of live actors superimposed over static backgrounds created with 3-D computer graphics. The clips are looped, and the moving human figures clearly stand out against the backgrounds. Both of these features connect the visual language of **7th Guest** to nineteenth-century pro-cinematic devices and twentieth-century cartoons rather than to cinematic verisimilitude. But like **Myst**, **7th Guest** also evokes distinctly modern cinematic codes. The environment where all action takes place (an interior of a house) is rendered using a wide-angle lens; to move from one view to the next a camera follows a complex curve, as though mounted on a virtual dolly.  

Next, consider the CD-ROM **Johnny Mnemonic** (Sony Imagesoft, 1995). Produced to complement the fiction film of the same title, marketed not as a "game" but as an "interactive movie," and featuring full-screen video throughout, it comes closer to cinematic realism than the previous CD-ROMs — yet it is still quite distinct from it. With all action shot against a green screen and then composited with graphic backgrounds, its visual style exists within a space between cinema and collage.

It would be not entirely inappropriate to read this short history of the digital moving image as a teleological development which replays the emergence of cinema a hundred years earlier. Indeed, as computers' speed keeps increasing, the CD-ROM designers have been able to go from a slide show format to the superimposition of small moving elements over static backgrounds and finally to full frame moving images. This evolution repeats the nineteenth-century progression: from sequences of still images (magic lantern slides presentations) to moving characters over static backgrounds (for instance, in Reynaud's Praxinoscope Theater) to full motion (the Lumieres' cinematograph). Moreover, the introduction of QuickTime in 1991 can be compared to the introduction of the Kinetoscope in 1892: both were used to present short loops, both featured images approximately two by three inches in size, both called for private viewing rather than a collective exhibition. Finally, the Lumieres' first film screenings of 1895 which shocked their audiences with huge moving images found their parallel in 1995 CD-ROM titles where the moving image finally fills the entire computer screen. Thus, exactly a hundred years after cinema was officially "born," it was reinvented on a computer screen.

But this is only one reading. We no longer think of the history of cinema as a linear march towards only one possible language, or as a progression towards more and more accurate verisimilitude. Rather, we have come to see its history as a succession of distinct and equally expressive languages, each with its own aesthetic variables, each new language closing off some of the possibilities of the previous one — a cultural logic not dissimilar to Kuhn's analysis of scientific paradigms. [29] Similarly, instead of dismissing visual strategies of early multimedia titles as a result of technological limitations, we may want to think of them as an alternative to traditional cinematic illusionism, as a beginning of digital cinema's new language. 

For the computer/entertainment industry, these strategies represent only a temporary limitation, an annoying drawback that needs to be overcome. This is one important difference between the situation at the end of the nineteenth and the end of the twentieth centuries: if cinema was developing towards the still open horizon of many possibilities, the development of commercial multimedia, and of corresponding computer hardware (compression boards, storage formats such as Digital Video Disk), is driven by a clearly defined goal: the exact duplication of cinematic realism. So if a computer screen, more and more, emulates cinema's screen, this not an accident but a result of conscious planning. 

## The Loop

A number of artists, however, have approached these strategies not as limitations but as a source of new cinematic possibilities. As an example, I will discuss the use of the loop in Jean-Louis Boissier's **Flora petrinsularis** (1993) and Natalie Bookchin's **The Databank of the Everyday** (1996). [30]

As already mentioned, all nineteenth-century pro-cinematic devices, up to Edison's Kinetoscope, were based on short loops. As "the seventh art" began to mature, it banished the loop to the low-art realms of the instructional film, the pornographic peepshow and the animated cartoon. In contrast, narrative cinema has avoided repetitions; as modern Western fictional forms in general, it put forward a notion of human existence as a linear progression through numerous unique events.

Cinema's birth from a loop form was reenacted at least once during its history. In one of the sequences of the revolutionary Soviet montage film, **A Man with a Movie Camera** (1929), Dziga Vertov shows us a cameraman standing in the back of a moving automobile. As he is being carried forward by an automobile, he cranks the handle of his camera. A loop, a repetition, created by the circular movement of the handle, gives birth to a progression of events — a very basic narrative which is also quintessentially modern: a camera moving through space recording whatever is in its way. In what seems to be a reference to cinema's primal scene, these shots are intercut with the shots of a moving train. Vertov even re-stages the terror which Lumieres's film supposedly provoked in its audience; he positions his camera right along the train track, so the train runs over our point of view a number of times, crushing us again and again.

Early digital movies share the same limitations of storage as nineteenth-century pro-cinematic devices. This is probably why the loop playback function was built into the QuickTime interface, thus giving it the same weight as the VCR-style "play forward" function. So, in contrast to films and videotapes, QuickTime movies are supposed to be played forward, backward or looped. Flora petrinsularis realizes some of the possibilities contained in the loop form, suggesting a new temporal aesthetics for digital cinema. 

The CD-ROM, which is based on Rousseau's **Confessions**, opens with a white screen, containing a numbered list. Clicking on each item leads us to a screen containing two frames, positioned side by side. Both frames show the same video loop but are slightly offset from each other in time. Thus, the images appearing in the left frame reappear in a moment on the right and vice versa, as though an invisible wave is running through the screen. This wave soon becomes materialized: when we click on one of the frames we are taken to a new screen showing a loop of a rhythmically vibrating water surface. As each mouse click reveals another loop, the viewer becomes an editor, but not in a traditional sense. Rather than constructing a singular narrative sequence and discarding material which is not used, here the viewer brings to the forefront, one by one, numerous layers of looped actions which seem to be taking place all at once, a multitude of separate but co-existing temporalities. The viewer is not cutting but re-shuffling. In a reversal of Vertov's sequence where a loop generated a narrative, viewer's attempt to create a story in **Flora petrinsularis** leads to a loop. 

The loop which structures **Flora petrinsularis** on a number of levels becomes a metaphor for human desire which can never achieve resolution. It can be also read as a comment on cinematic realism. What are the minimal conditions necessary to create the impression of reality? As Boissier demonstrates, in the case of a field of grass, a close-up of a plant or a stream, just a few looped frames become sufficient to produce the illusion of life and of linear time. 

Steven Neale describes how early film demonstrated its authenticity by representing moving nature: "What was lacking [in photographs] was the wind, the very index of real, natural movement. Hence, the obsessive contemporary fascination, not just with movement, not just with scale, but also with waves and sea spray, with smoke and spray." [31] What for early cinema was its biggest pride and achievement — a faithful documentation of nature's movement — becomes for Boissier a subject of ironic and melancholic simulation. As the few frames are looped over and over, we see blades of grades shifting slightly back and forth, rhythmically responding to the blow of non-existent wind which is almost approximated by the noise of a computer reading data from a CD-ROM. 

Something else is being simulated here as well, perhaps unintentionally. As you watch the CD-ROM, the computer periodically staggers, unable to maintain consistent data rate. As a result, the images on the screen move in uneven bursts, slowing and speeding up with human-like irregularity. It is as though they are brought to life not by a digital machine but by a human operator, cranking the handle of the Zoetrope a century and a half ago... 

If Flora petrinsularis uses the loop to comment on cinema's visual realism, The Databank of the Everyday suggests that the loop can be a new narrative form appropriate for the computer age. In her ironic manifesto which parodies the avant-garde manifestos from the earlier part of the century, Bookchin reminds us that the loop gave birth not only to cinema but also to computer programming. Programming involves altering the linear flow of data through control structures, such as "if/then" and "repeat/while"; the loop is the most elementary of these control structures. Bookchin writes:

As digital media replaces film and photography, it is only logical that the computer program's loop should replace photography's frozen moment and cinema's linear narrative. The Databank champions the loop as a new form of digital storytelling; there is no true beginning or end, only a series of the loops with their endless repetitions, halted by a users' selection or a power shortage.

The computer program's loop makes its first "screen debut" in one particularly effective image from The Databank of the Everyday. The screen is divided into two frames, one showing a video loop of a woman shaving her leg, another - a loop of a computer program in execution. Program statements repeating over and over mirror the woman's arm methodically moving back and forth. This image represents one of the first attempts in computer art to apply a Brechtian strategy; that is, to show the mechanisms by which the computer produces its illusions as a part of the artwork. Stripped of its usual interface, the computer turns out to be another version of Ford's factory, with a loop as its conveyer belt. 

Like Boissier, Bookchin also explores alternatives to cinematic montage, in her case replacing its traditional sequential mode with a spatial one. Ford's assembly line relied on the separation of the production process into a set of repetitive, sequential, and simple activities. The same principle made computer programming possible: a computer program breaks tasks into a series of elemental operations to be executed one at a time. Cinema followed this principle as well: it replaced all other modes of narration with a sequential narrative, an assembly line of shots which appear on the screen one at a time. A sequential narrative turned out to be particularly incompatible with a spatialized narrative which played a prominent role in European visual culture for centuries. From Giotto's fresco cycle at Capella degli Scrovegni in Padua to Courbet's A Burial at Ornans, artists presented a multitude of separate events (which sometimes were even separated by time) within a single composition. In contrast to cinema's narrative, here all the "shots" were accessible to a viewer at one. 

Cinema has elaborated complex techniques of montage between different images replacing each other in time; but the possibility of what can be called "spatial montage" between simultaneously co-existing images were not explored. The Databank of the Everyday begins to explore this direction, thus opening up again the tradition of spatialized narrative suppressed by cinema. In one section we are presented with a sequence of pairs of short clips of everyday actions which function as antonyms, for instance, opening and closing a door, or pressing up and down buttons in an elevator. In another section the user can choreograph a number of miniature actions appearing in small windows positioned throughout the screen.

## Conclusion: From Kino-Eye to Kino-Brush

In the twentieth century, cinema has played two roles at once. As a media technology, cinema's role was to capture and to store visible reality. The difficulty of modifying images once they were recorded was exactly what gave cinema its value as a document, assuring its authenticity. The same rigidity of the film image has defined the limits of cinema as I defined it earlier, i.e. the super-genre of live-action narrative. Although it includes within itself a variety of styles — the result of the efforts of many directors, designers and cinematographers — these styles share a strong family resemblance. They are all children of the recording process which uses a lens, regular sampling of time and photographic media. They are all children of a machine vision. 

The mutability of digital data impairs the value of cinema recordings as a document of reality. In retrospect, we can see that twentieth-century cinema's regime of visual realism, the result of automatically recording visual reality, was only an exception, an isolated accident in the history of visual representation which has always involved, and now again involves the manual construction of images. Cinema becomes a particular branch of painting — painting in time. No longer a kino-eye, but a kino-brush. [32]

The privileged role played by the manual construction of images in digital cinema is one example of a larger trend: the return of pre-cinematic moving images techniques. Marginalized by the twentieth-century institution of live-action narrative cinema which relegated them to the realms of animation and special effects, these techniques reemerge as the foundation of digital filmmaking. What was supplemental to cinema becomes its norm; what was at its boundaries comes into the center. Digital media returns to us the repressed of the cinema.

As the examples discussed in this essay suggest, the directions which were closed off at the turn of the century when cinema came to dominate the modern moving image culture are now again beginning to be explored. Moving image culture is being redefined once again; the cinematic realism is being displaced from being its dominant mode to become only one option among many. 

## References:

[1] Scott Billups, presentation during "Casting from Forest Lawn (Future of Performers) panel at "The Artists Rights Digital Technology Symposium '96," Los Angeles, Directors Guild of America, February 16, 1996. Billups was a major figure in bringing Hollywood and Silicon Valley together by way of the American Film Institute's Apple Laboratory and Advanced Technologies Programs in the late 1980s and early 1990s. See Paula Parisi, "The New Hollywood Silicon Stars," **Wired** 3.12 (December, 1995) pp. 142-145; pp. 202-210.

[2] Christian Metz, "The Fiction Film and its Spectator: A Metaphychological Study," in **Apparatus**, edited by Theresa Hak Kyung Cha (New York: Tanam Press, 1980), p. 402.

[3] Cinema as defined by its "super-genre" of fictional live-action film belongs to media arts which, in contrast to traditional arts, rely on recordings of reality as their basis. Another term which is not as popular as "media arts" but perhaps is more precise is "recording arts". For the use of this term, see James Monaco, **How to Read a Film**, revised edition (New York and Oxford: Oxford University Press, 1981), 7. 

[4] Charles Musser, **The Emergence of Cinema: The American Screen to 1907** (Berkeley and Los Angeles: University of California Press, 1990), pp. 49-50.

[5] Musser, **The Emergence of Cinema**, p. 25.

[6] C.W. Ceram, **Archeology of the Cinema** (New York: Harcourt, Brace & World, Inc., 1965), pp. 44-45.

[7] The birth of cinema in the 1890s is accompanied by an interesting transformation: while the body as the generator of moving pictures disappears, it simultaneously becomes their new subject. Indeed, one of the key themes of early films produced by Edison is a human body in motion: a man sneezing, a famous bodybuilder Sandow flexing his muscles, an athlete performing somersault, a woman dancing. Films of boxing matches play a key role in the commercial development of Kinetoscope. See Musser, **The Emergence of Cinema**, pp. 72-79; David Robinson, **From Peep Show to Palace: the Birth of American Film** (New York: Columbia University Press, 1996), pp. 44-48. 

[8] Robinson, **From Peep Show to Palace**, 12.

[9] This arrangement was previously used in magic lantern projections; it is described in the second edition of Athanasius Kircher's **Ars magna** (1671). See Musser, **The Emergence of Cinema**, pp. 21-22.

[10] Ceram, **Archeology of the Cinema**, p. 140.

[11] Musser, **The Emergence of Cinema**, p. 78.

[12] The extent of this lie is made clear by the films of Andy Warhol from the first part of the 1960s — perhaps the only real attempt to create cinema without a language.

[13] I have borrowed this definition of special effects from David Samuelson, **Motion Picture Camera Techniques** (London: Focal Press, 1978). 

[14] The following examples illustrate this disavowal of special effects; other examples can be easily found. The first example is from popular discourse on cinema. A section entitled "Making the Movies" in Kenneth W. Leish **Cinema** (New York: Newsweek Books, 1974) contains short stories from the history of the movie industry. The heroes of these stories are actors, directors and producers; special effects artists are mentioned only once. The second example is from an academic source: the authors of the authoritative **Aesthetics of Film** (1983) state that "the goal of our book is to summarize from a synthetic and didactic perspective the diverse theoretical attempts at examining these empirical notions [terms from the lexicon of film technicians], including ideas like frame vs. shot, terms from production crews' vocabularies, the notion of identification produced by critical vocabulary, etc." The fact that the text never mentions special effects techniques reflects the general lack of any historical or theoretical interest in the topic by film scholars. Bordwell and Thompson's **Film Art: An Introduction** which is used as a standard textbook in undergraduate film classes is a little better as it devotes three pages out of its five hundred pages to special effects. Finally, a relevant piece of statistics: a library of the University of California, San Diego contains 4273 titles cataloged under the subject "motion pictures" and only 16 tiles under "special effects cinematography". For the few important works addressing the larger cultural significance of special effects by film theoreticians see Vivian Sobchack and Scott Bukatman. Norman Klein is currently working on a history of special effects environments. 

Kenneth W. Leish **Cinema** (New York: Newsweek Books, 1974); Jacques Aumont, Alain Bergala, Michel Marie and Marc Vernet, **Aesthetics of Film**, trans. Richard Neupert (Austin: University of Texas Press, 1992), p. 7; David Bordwell and Kristin Thompson, **Film Art: an Introduction**, 4th ed. (New York: McGraw-Hill, Inc., 1993); Vivian Sobchack **Screening Space: The American Science Fiction Film**, 2nd ed. (New York: Ungar, 1987); Scott Bukatman, "The Artificial Infinite," in **Visual Display**, eds. Lynne Cooke and Peter Wollen (Seattle: Bay Press, 1995). 

[15] For a discussion of the subsumption of the photographic to the graphic, see Peter Lunenfeld, "Art Post-History: Digital Photography and Electronic Semiotics," which has been translated as "Die Kunst der Posthistorie: Digitale Fotographie und electroniche Semiotik," in the catalog, **Fotographie nacht der Fotographie** (Munich: Verlag der Kunst, 1996), pp. 93-99.

[16] For a complete list of people at ILM who worked on this film, see **SIGGRAPH '94 Visual Proceedings** (New York: ACM SIGGRAPH, 1994), p. 19.

[17] In this respect, 1995 can be called the last year of digital media. At 1995 National Association of Broadcasters convention Avid showed a working model of a digital video camera which records not on a video cassette but directly onto a hard drive. Once digital cameras become widely used, we will no longer have any reason to talk about digital media since the process of digitization will be eliminated.

[18] Here is another, even more radical definition: digital film = f (x, y, t). This definition would be greeted with joy by the proponents of abstract animation. Since computer breaks down every frame into pixels, a complete film can be defined as a function which, given horizontal, vertical and time location of each pixel, returns its color. This is actually how a computer represents a film, a representation which has a surprising affinity with a certain well-known the avant-garde vision of cinema! For a computer, a film is an abstract arrangement of colors changing in time, rather than something structured by "shots," "narrative," "actors" and so on.

[19] See Barbara Robertson, "Digital Magic: Appolo 13," **Computer Graphics World** (August 1995), p. 20.

[20] William J. Mitchell, **The Reconfigured Eye: Visual Truth in the Post-photographic Era** (Cambridge, Mass.: The MIT Press, 1992), p. 7.

[21] The full advantage of mapping time into 2-D space, already present in Edison's first cinema apparatus, is now realized: one can modify events in time by literally painting on a sequence of frames, treating them as a single image. 

[22] See Robinson, **From Peep Show to Palace**, p. 165. 

[23] See "Industrial Light & Magic alters history with MATADOR," promotion material by Parallax Software, SIGGRAPH 95 Conference, Los Angeles, August 1995. 

[24] The reader who followed my analysis of the new possibilities of digital cinema may wonder why I have stressed the parallels between digital cinema and the pre-cinematic techniques of the nineteenth century but did not mention twentieth-century avant-garde filmmaking. Did not the avant-garde filmmakers already explore many of these new possibilities? To take the notion of cinema as painting, Len Lye, one of the pioneers of abstract animation, was painting directly on film as early as 1935; he was followed by Norman McLaren and Stan Brakhage, the later extensively covering shot footage with dots, scratches, splattered paint, smears and lines in an attempt to turn his films into equivalents of Abstract Expressionist painting. More generally, one of the major impulses in all of avant-garde filmmaking, from Leger to Godard, was to combine the cinematic, the painterly and the graphic — by using live-action footage and animation within one film or even a single frame, by altering this footage in a variety of ways, or by juxtaposing printed texts and filmed images. 

I explore the notion that the avant-garde anticipated digital aesthetics in my **Engineering Vision: from Constructivism to the Computer** (The University of Texas Press, forthcoming); here I would like to bring up one point particularly relevant for this essay. When the avant-garde filmmakers collaged multiple images within a single frame, or painted and scratched film, or revolted against the indexical identity of cinema in other ways, they were working against "normal" filmmaking procedures and the intended uses of film technology. (Film stock was not designed to be painted on). Thus, they operated on the periphery of commercial cinema not only aesthetically but also technically. 

One general effect of the digital revolution is that avant-garde aesthetic strategies became embedded in the commands and interface metaphors of computer software. In short, **the avant-garde became materialized in a computer**. Digital cinema technology is a case in point. The avant-garde strategy of collage reemerged as a "cut and paste" command, the most basic operation one can perform on digital data. The idea of painting on film became embedded in paint functions of film editing software. The avant-garde move to combine animation, printed texts and live-action footage are repeated in the convergence of animation, title generation, paint, compositing and editing systems into single all-in-one packages. Finally, another moves to combine a number of film images together within one frame (for instance, in Leger's 1924 **Ballet Mechanique** or in Vertov's 1929 **A Man with a Movie Camera**) also become legitimized by technology, since all editing software, including Photoshop, Premiere, After Effects, Flame, and Cineon, by default assumes that a digital image consists of a number of separate image layers. All in all, what used to be exceptions for traditional cinema became the normal, intended techniques of digital filmmaking, embedded in technology design itself. 

For the experiments in painting on film by Lye, McLaren and Brakhage, see Robert Russett and Cecile Starr, **Experimental Animation** (New York: Van Nostrand Reinhold Company, 1976), pp. 65-71, 117-128; P. Adams Smith, **Visionary Film**, 2nd ed. (Oxford: Oxford University Press), pp. 230, 136-227. 

[25] Reporting in December 1995 issue of **Wired**, Paula Parise writes: "A decade ago, only an intrepid few, led by George Lucas's Industrial Light and Magic, were doing high-quality digital work. Now computer imaging is considered an indispensable production tool for all films, from the smallest drama to the largest visual extravaganza." (Parisi, "The New Hollywood Silicon Stars," p. 144.) 

[26] Therefore, one way in which the fantastic is justified in contemporary Hollywood cinema is through the introduction of various non-human characters such as aliens, mutants and robots. We never notice the pure arbitrariness of their colorful and mutating bodies, the beams of energy emulating from their eyes, the whirlpools of particles emulating from their wings, because they are made perceptually consistent with the set, i.e. they look like something which could have existed in a three-dimensional space and therefore could have been photographed. 

[27] Metz, "The Fiction Film and its Spectator: A Metaphychological Study". 

[28] This 28-minute film, made in 1962, is composed of still frames narrativized in time, and concludes with a very short live-action sequence. For documentation, see Chris Marker, **La Jetée: Ciné-roman** (New York: Zone Books, 1992).

[29] Thomas S. Kuhn, **The Structure of Scientific Revolutions** (2nd. ed. Chicago: University of Chicago Press, 1970).

[30] **Flora petrinsularis** is included in the compilation CD-ROM, **Artintact 1** (Karlsruhe, Germany: ZKM/Center for Art and Media, 1994). The Databank of the Everyday (concept and computer design — Natalie Bookchin; cinematography — Lev Manovich) is available from Natalie Bookchin [http://www.bookchin@jupiter.ucsd.edu](http://www.bookchin@jupiter.ucsd.edu).

[31] Steven Neale, **Cinema and Technology** (Bloomington: Indiana University Press, 1985), 52.

[32] It was Dziga Vertov who coined the term "kino-eye" in the 1920s to describe the cinematic apparatus's ability "to record and organize the individual characteristics of life's phenomena into a whole, an essence, a conclusion." For Vertov, it was the presentation of film "facts," based as they were on materialist evidence, that defined the very nature of the cinema. See **Kino-Eye: The Writings of Dziga Vertov**, ed. Annette Michelson, trans. Kevin O'Brien (Berkeley: University of California Press, 1984). The quotation above is from "Artistic Drama and Kino-Eye," originally published in 1924, pp.47-49, p. 47.

---

# To Lie and To Act: Cinema and Telecommunication

_author: Lev Manovich_
_year: 1995_

## To Lie and to Act: Potemkin's Villages, Cinema and Telepresence

In an opening sequence from the movie "Titanic" (James Cameron, 1997), we see an operator sitting at controls. The operator is wearing a head-mounted display. The display allows him to see an image transmitted from a remote location, thus making it possible to remotely control another vehicle, exploring the insides of the Titanic lying on the bottom of the ocean. In short, the operator becomes "telepresent".

With the rise of the Web, telepresence which until recently was restricted to few specialized industrial and military applications, became more of a familiar experience. The search on Yahoo! for "interesting devices connected to the Net" returns links to a variety of Net-based telepresence applications: coffee machines, robots, interactive model railroad, audio devices and, of course, the ever-popular webcams. [1] Some of these devices, for instance, most webcams, do not allow for true telepresence — you get images from a remote location but you can't perform any actions on it. Others, however, are true telepresence links, meaning that they do allow the user to perform remote actions.

This essay addresses the issues raised by the phenomenon of Internet telepresence and telerobotics by placing these recent technologies within the history of representational technologies. Before proceeding, I will make a conceptual substitution: rather than discussing technologies as tools for obtaining knowledge (the usual meaning of epistemology), I will discuss them in their opposite role: as tools of deception, i.e. as tools which allow their users to communicate lies rather truths.

Representational technologies have served two main functions throughout human history: to deceive the viewer and to enable action, i.e. to allow the viewer to manipulate reality through representations. [2] Fashion and make-up, paintings, dioramas, decoys and virtual reality fall into the first category. Maps, architectural drawings, x-rays, and telepresence fall into the second. To deceive the viewer or to enable action: these are the two axes which structure the history of visual representations.

What are the new possibilities for deception and action offered by computer-based technologies such as computer imaging and Internet-based telepresence in contrast to older technologies of architecture, cinema and video? If we are to construct a history which will connect all these technologies, where shall we locate key historical breaks? This essay will reflect on these questions. 

 \\#\# 1. To Lie

### Cinema

I will start with Potemkin's Villages. According to the historical myth, at the end of the eighteenth century, Russian ruler Catherine the Great decided to travel around Russia in order to observe first-hand how the peasants lived. The first minister and Catherine's lover, Potemkin, had ordered the construction of special fake villages along her projected route. Each village consisted of a row of pretty facades. The facades faced the road; at the same time, to conceal their artifice, they were positioned at a considerable distance. Since Catherine the Great never left her carriage, she returned from her journey convinced that all peasants lived in happiness and prosperity.

This extraordinary arrangement can be seen as a metaphor for life in the Soviet Union. There, the experience of all citizens was split between the ugly reality of their lives and the official shining facades of ideological pretense. However, The split took place not only on a metaphorical but also on a literal level, particularly in Moscow — the showcase Communist city. When prestigious foreign guests visited Moscow, they, like Catherine the Great, were taken around in limousines which always followed few special routes. Along these routes, every building was freshly painted, the shop windows displayed consumer goods, the drunks were removed, having been picked up by the militia early in the morning. The monochrome, rusty, half-broken, amorphous Soviet reality was carefully hidden from the view of the passengers.

In turning selected streets into fake facades, Soviet rulers adopted the eighteenth-century technique of creating fake reality. But, of course, the twentieth century brought with it a much more effective technology: cinema. By substituting a window of a carriage or a car with a screen showing projected images, cinema opened up new possibilities for deception.

Fictional cinema, as we know it, is based upon lying to a viewer. A perfect example is the construction of a cinematic space. Traditional fiction film transports us into space: a room, a house, a city. Usually, none of these exist in reality. What exists are the few fragments carefully constructed in a studio. Out of these disjointed fragments, a film synthesizes the illusion of a coherent space. 

The development of the techniques to accomplish this synthesis coincides with the shift in American cinema between approximately 1907 and 1917 from a so-called "primitive" to a "classical" film style. Before the classical period, the space of film theater and the screen space were clearly separated much like in theater or vaudeville. The viewers were free to interact, come and go, and maintain a psychological distance from the cinematic diegesis. Correspondingly, the early cinema's system of representation was **presentational**: actors played to the audience, and the style was strictly frontal. [3] The composition of the shots also emphasized frontality.

In contrast, classical Hollywood film positions each viewer inside the diegetic space. The viewer is asked to identify with the characters and to experience the story from their points of view. Accordingly, the space no longer acts as a theatrical backdrop. Instead, through new compositional principles, staging, set design, deep focus cinematography, lighting and camera movement, the viewer is situated at the optimum viewpoint of each shot. The viewer is "present" inside a space which does not really exist. A fake space.

In general, Hollywood cinema always carefully hides the artificial nature of its space, but there is one exception: rear screen projection shots. A typical shot shows actors sitting inside a stationary vehicle; a film of a moving landscape is projected on the screen behind car's windows. The artificiality of rear screen projection shots stands in striking contrast against the smooth fabric of Hollywood cinematic style in general.

The synthesis of a coherent space out of distinct fragments is only one example of how fictional cinema deceives a viewer. A film in general is comprised from separate image sequences. These sequences can come from different physical locations. Two consecutive shots of what looks like one room may correspond to two places inside one studio. They can also correspond to the locations in Moscow and Berlin, or Berlin and New York. The viewer will never know.

This is the key advantage of cinema over older fake reality technologies, be it eighteenth-century Potemkin's Villages or nineteenth-century Panoramas and Dioramas. Before cinema, the deception was limited to the construction of a fake space inside a real space visible to the viewer. Examples include theater decorations and military decoys. In the nineteenth century, Panorama offered a small improvement: by enclosing a viewer within a 360-degree view, the area of fake space was expanded. Louis-Jacques Daguerre introduced another innovation by having viewers move from one set to another in his London Diorama. As described by Paul Johnson, its "amphitheater, seating 200, pivoted through a 73-degree arc, from one "picture" to another. Each picture was seen through a 2,800-square-foot-window." [4] But, already in the eighteenth century, Potemkin had pushed this technique to its limit: he created a giant facade — a Diorama stretching for hundreds of miles — along which the viewer (Catherine the Great) passed. In cinema a viewer remains stationary: what is moving is the film itself. 

Therefore, if the older technologies were limited by the materiality of a viewer's body, existing at a particular point in space and time, film overcomes these spatial and temporal limitations. It achieves this by substituting recorded images for unmediated human sight and by editing these images together. Through editing, images that could have been shot in different geographic locations or in different times create an illusion of a contiguous space and time. 

Editing, or montage, is the key twentieth technology for creating fake realities. Theoreticians of cinema have distinguished between many kinds of montage but, for the purposes of sketching the archeology of the technologies of deception, I will distinguish between two basic techniques. The first is so montage within a shot: separate realities form contingent parts of a single image. (One example of this is a rear screen projection shot.) The second technique is the opposite of the first: separate realities form consecutive moments in time. This second technique of temporal montage is much more common; this is what we usually mean by montage in film.

In a fiction film temporal montage serves a number of functions. As already pointed out, it creates a sense of presence in a virtual space. It is also utilized to change the meanings of individual shots (recall Kuleshov's effect), or, rather, to construct a meaning from separate pieces of pro-filmic reality.

However, the use of temporal montage extends beyond the construction of an artistic fiction. Montage also becomes a key technology for ideological manipulation, through its employment in propaganda films, documentaries, news, commercials and so on.

The pioneer of this ideological montage is Russian documentary filmmaker Dziga Vertov. In 1923 Vertov analyzed how he put together episodes of his news program **Kino-Pravda** (Cinema-Truth) out of shots filmed at different locations and at different times. This is one example of his montage: "the bodies of people's heroes are being lowered into the graves (filmed in Astrakhan' in 1918); the graves are being covered with earth (Kronshtad, 1921); gun salute (Petrograd, 1920); eternal memory, people take down their hats (Moscow, 1922)." Here is another example: "montage of the greetings by the crowd and montage of the greetings by the machines to the comrade Lenin, filmed at different times." [5] As theorized by Vertov, through montage, film can overcome its indexical nature, presenting a viewer with objects which never existed in reality. 

### Video

Outside of cinema, montage within a shot becomes a standard technique of modern photography and design (photomontages of Alexander Rodchenko, El Lissitsky, Hannah Hoch, John Heartfield and countless other lesser-known twentieth-century designers). However, in the realm of a moving image, temporal montage dominates. Temporal montage is cinema's main means of creating fake realities. 

After World War II a gradual shift takes place from film-based to electronic image recording. This shift brings with it a new technique: keying. One of the most basic techniques used today in any video and television production, keying refers to combining two different image sources together. Any area of uniform color in one video image can be cut out and substituted with another source. Significantly, this new source can be a live video camera positioned somewhere, a pre-recorded tape, or computer-generated graphics. The possibilities for creating fake realities are multiplied once again.

With electronic keying becoming a part of a standard television practice in the 1970s, not just still but also time-based images finally begin to routinely rely on montage within a shot. In fact, rear projection and other special effects shots, which had occupied marginal presence in a classical film, became the norm: weather man in front of a weather map, an announcer in front of footage of a news event, a singer in front of an animation in a music video. 

An image created through keying presents a hybrid reality, composed of two different spaces. Television normally relates these spaces thematically, but not visually. To take a typical example, we may be shown an image of an announcer sitting in a studio; behind her, in a cutout, we see news footage of a city street. If classical cinematic montage creates an illusion of a coherent space and hides its own work, electronic montage openly presents the viewer with an apparent clash of different spaces.

What will happen if the two spaces seamlessly merge? This operation forms the basis of a remarkable video "Steps" directed by Polish-born filmmaker Zbigniew Rybczynski in 1987. "Steps" is shot on videotape and uses keying; it also utilizes film footage and makes an inadvertent reference to virtual reality. In this way, Rybczynski connects three generations of fake reality technologies: analog, electronic and digital. He also reminds us that it was the 1920s Soviet filmmakers who first fully realized the possibilities of montage which continue to be explored and expanded by electronic and digital media.

In the video, a group of American tourists is invited into a sophisticated video studio to participate in a kind of virtual reality/time machine experiment. The group is positioned in front of a blue screen. Next, the tourists find themselves literally inside the famous Odessa steps sequence from Eisenstein's **Potemkin**. Rybczynski skillfully keys the shots of the people in the studio into the shots from **Potemkin** creating a single coherent space. At the same time, he emphasizes the artificiality of this space by contrasting the color video images of the tourists with the original grainy black and white Eisenstein's footage. The tourists walk up and down the steps, snap pictures at the attacking soldiers, play with a baby in a crib. Gradually, the two realities begin to interact and mix together: some Americans fall down the steps after being shot by the soldiers from Eisenstein's sequence; a tourist drops an apple which is picked up by a soldier.

The Odessa steps sequence, already a famous example of cinematic montage, becomes just one element in a new ironic re-mix by Rybczynski. The original shots which were already edited by Eisenstein are now edited again with video images of the tourists, using both temporal montage and montage within a shot, the latter done through video keying. A "film look" is juxtaposed with "video look," color is juxtaposed with black and white, the "presentness" of video is juxtaposed with the "always already" of film. 

In "Steps" Eisenstein's sequence becomes a generator for numerous kinds of juxtapositions, super-impositions, mixes and re-mixes. But Rybczynski treats this sequence not only as a single element of his own montage but also as a singular, physically existing space. In other words, the Odessa steps sequence is read as a single shot corresponding to a real space, a space which could be visited like any other tourist attraction.

### Computer Imaging

The next generation in fake reality technologies is digital media. At first glance, digital media does not bring any conceptually new techniques. It simply expands the possibilities of joining together different image sources within one shot. Rather than **keying** together images from two video sources, we can now **composite** an unlimited number of image layers. A shot may consist of dozens or even hundreds of layers, all having different origins: film shot on location, computer-generated sets or actors, digital matte paintings, archival footage and so on. Most current Hollywood films contain such shots.

Historically, a digitally composed image, like an electronically keyed image, can be seen as a continuation of montage within a shot. But while electronic keying creates disjointed spaces reminding us of the avant-garde collages of Rodchenko or Moholy-Nagy from the 1920s, digital composing brings back the nineteenth-century techniques of creating smooth "combination prints" like those of Henry Peach Robinson and Oscar G. Reijlander. However, what in the nineteenth century was only a still image now can become a moving one. A moving nineteenth-century "combination print": this is the current state of the art in the technologies of visual deception.

But this historical continuity is deceiving. Computer imaging does represent a qualitatively new step in the history of visual deception since it allows the creation of moving images of non-existent worlds. Computer-generated characters can move within real landscapes; conversely, real actors can move and act within synthetic environments. In contrast to the nineteenth-century "combination prints" which emulated academic painting, digital composites fully simulate the established language of cinema and television. Regardless of the particular combination of live-action elements and computer-generated elements which are combined to create the scene, the camera can pan, zoom, and dolly through it. The interaction of parts of the virtual world over time along with the ability to look at it from different viewpoints becomes the guarantee of its authenticity.

Composing numerous elements to create a photo-realistic image is a time-consuming task. For instance, a 40-second sequence from "Titanic" in which the camera flies over the computer-generated ship populated by computer-generated characters took many months to produce and its total cost was 1.1 million dollars. In contrast, although the old technique of video keying could not create photorealistic images, it was possible to use it in real-time, combining two images on the fly.

Computer imaging brings new level of realism to keying. Virtual Sets technology which was first introduced in the early 1990s and is making its way into television studios around the world allows to composite video images and computer-generated three-dimensional elements in real-time. (Actually, because the generation of computer elements is computation intensive, the final image transmitted to the audience is few seconds behind the original image picked by television camera.) The typical application involves composing an image of an actor over computer-generated set. The computer reads the position of the video camera and uses this information to render the set in proper perspective. The illusion is made more convincing by generating shadows and/or reflections of the actor and integrating them into the composite. Because of the relatively low resolution of analog television, the resulting effect is quite convincing. A particularly interesting application of Virtual Sets is replacement and insertion of arena-tied advertising messages during live TV broadcasts of sports and entertainment events, offered by ORAD, a company based in Israel. The system can insert computer-synthesized advertising messages onto the playing field or other empty areas in the arena in the proper perspective, as though they are actually present in reality. [6]

Computer imaging represents a fundamental break with previous techniques for visual deception yet for another reason. Throughout the history of representation, artists focused on the problem of creating a convincing illusion within a single image, be it a painting, a film frame or a view seen by Catherine the Great through the window of her carriage. Set making, one-point perspective, chiaroscuro, trick photography and other cinematography techniques were all developed to solve this problem. Film montage introduces a new paradigm: creating an effect of presence in a virtual world by joining different images over time. As illustrated by digital composing for film and Virtual Sets applications for television, the computer era changes the paradigm once again. Having mastered the creation of a single convincing image, the artists now focus on how to join shamelessly a number of such images into one coherent whole. Whether it is composing a live video of a newscaster with a 3-D computer-generated set or composing thousands of elements to create a photo-realistic image of "Titanic," the main problem is no longer how to generate convincing-looking individual elements but how to blend them together. Consequently, what is important now is what happens on the edges where different images are joined. The borders where different realities come together is the new arena where Potemkins of our era try to outdo one another. 

## 2. To Act

### Telepresence

So far, I have considered the historical connections between some of the technologies of deception: fake architectural spaces, montage, video keying, digital composing. I will now consider the second axis which structures the history of visual representations: action.

If we look at the word itself, the meaning of the term **telepresence** is presence over distance. But presence where? Brenda Laurel defines telepresence as "a medium that allows you to take your body with you into some other environment ...you get to take some subset of your senses with you into another environment. And that environment may be a computer-generated environment, it may be a camera-originated environment, or it may be a combination of the two." [7] In this definition, telepresence encompasses two different situations: being "present" in a synthetic computer-generated environment (what is commonly referred as **virtual reality** ) and being "present" in a real remote physical location via a live video image. Scott Fisher, one of the developers of NASA Ames Virtual Environment Workstation, similarly does not distinguish between being "present" in a computer-generated or a real remote physical location. Describing the Ames system, he writes: "Virtual environments at the Ames system are synthesized with 3-D computer-generated imagery, or are remotely sensed by user-controlled, stereoscopic video camera configurations." [8] Fisher uses "virtual environments" as an all-encompassing term, reserving "telepresence" for the second situation: "presence" in a remote physical location. [9] I will follow his usage here.

Both popular media and the critics have downplayed the concept of telepresence in favor of virtual reality. The photographs of the Ames system, for instance, have been often featured to illustrate the idea of an escape from any physical space into a computer-generated world. The fact that a head-mounted display can also show a televised image of a remote physical location was hardly ever mentioned.

And yet, from the point of view of the history of the technologies of deception and action, telepresence is a much more radical technology than virtual reality, or computer simulations in general. Let us consider the difference between the two.

Like fake reality technologies which preceded it, virtual reality provides the subject with the illusion of being present in a simulated world. Virtual reality goes beyond this tradition by allowing the subject to actively change this world. In other words, the subject is given control over a fake reality. For instance, an architect can modify an architectural model, a chemist can try different molecule configurations, a tank driver can shoot at a model of a tank, and so on. But, what is modified in each case is nothing but data stored in a computer's memory! The user of any computer simulation has power over the virtual world which only exists inside a computer.

Telepresence allows the subject to control not just the simulation but reality itself. Telepresence provides the ability to **remotely manipulate physical reality in real-time through its image**. The body of a teleoperator is transmitted, in real-time, to another location where it can act on subject's behalf: repairing a space station, doing underwater excavation or bombing a military base in Baghdad or Yugoslavia. 

Thus, the essence of telepresence is that it is anti-presence. I don't have to be physically present in a location to affect reality at this location. A better term would be **teleaction**. Acting over distance. In real time.

Catherine the Great was fooled into mistaking painted facades for real villages. Today, from thousands of miles away (as it was demonstrated during the Gulf War) we can send missile equipped with a television camera close enough to tell the difference between a target and a decoy. We can direct the flight of the missile using the image transmitted back by its camera, we can carefully fly towards the target. And, using the same image, we blow the target away. All that is needed is to position the computer cursor over the right place in image and to press a button. 

### Image-Instruments

How new is this use of images? Does it originate with telepresence? Since we are accustomed to consider the history of visual representations in the West in terms of illusion, it may seem that to use images to enable action is a completely new phenomenon. However, French philosopher and sociologist Bruno Latour proposes that certain kinds of images have always functioned as instruments of control and power, power being defined as the ability to mobilize and manipulate resources across space and time.

One example of such image-instruments analyzed by Latour is perspectival images. Perspective establishes the precise and reciprocal relationship between objects and their signs. We can go from objects to signs (two-dimensional representations); but we can also go from such signs to three-dimensional objects. This reciprocal relationship allows us not only to represent reality but also to control it. [10] For instance, we cannot measure the sun in space directly, but we only need a small ruler to measure it on a photograph (the perspectival image par excellence). [11] And even if we could fly around the sun, we would still be better off studying the sun through its representations which we can bring back from the trip — because now we have unlimited **time** to measure, analyze, and catalog them. We can move objects from one place to another by simply moving their representations: "You can see a church in Rome, and carry it with you in London in such a way as to reconstruct it in London, or you can go back to Rome and amend the picture." Finally, we can also represent absent things and plan our movement through space by working on representations: "One cannot smell or hear or touch Sakhalin Island, but you can look at the map and determine at which bearing you will see the land when you send the next fleet." [12] All in all, perspective is more than just a sign system, reflecting reality — it makes possible the manipulation of reality through the manipulation of its signs. 

Perspective is only one example of image-instruments. Any representation which systematically captures some features of reality can be used as an instrument. In fact, most types of representations which do not fit into the history of illusionism — diagrams and charts, maps and x-rays, infrared and radar images — belong to the second history: that of representations as instruments for action. 

### Telecommunication

Given that images have always been used to affect reality, does telepresence bring anything new? A map, for instance, already allows for a kind of teleaction: it can be used to predict the future and therefore to change it. To quote Latour again, "one cannot smell or hear or touch Sakhalin Island, but you can look at the map and determine at which bearing you will see the land when you send the next fleet." 

In my view, there are two fundamental differences. Because telepresence involves electronic transmission of video images, the constructions of representations take place instantaneously. Making a perspectival drawing or a chart, taking a photograph, or shooting film takes time. Now I can use a remote video camera which capture images in real-time, sending these images back to me without any delay. This allows me to monitor any visible changes in a remote location (weather conditions, movements of troops, and so on), adjusting my actions accordingly.

The second difference is directly related to the first. The ability to receive visual information about a remote place in real-time allows us to manipulate physical reality in this place, also in real-time. If power, according to Latour, includes the ability to manipulate resources at a distance, then teleaction provides a new and unique kind of power: **real-time remote control**. I can drive a toy vehicle, repair a space station, do underwater excavation, operate on a patient, or kill — all from a distance.

What technology is responsible for this new power? Since teleoperator typically acts with the help of a live video image (for instance, remote operation of a moving vehicle such as in the opening sequence of "Titanic"), we may think at first that it is the technology of video, or, more precisely, of television. The original nineteenth-century meaning of television was "vision over distance." Only after the 1920s, when television was equated with broadcasting, does this meaning fade away. However, during the preceding half a century (television research begins in the 1870s), television engineers were mostly concerned with the problem of how to transmit consecutive images of a remote location to enable "remote seeing".

If images are transmitted at regular intervals, if these intervals are short enough, and if images have sufficient detail, the viewer will have enough reliable information about the remote location for teleaction. The early television systems used slow mechanical scanning and the resolution as low as thirty lines. In the case of modern television systems, the visible reality is being scanned at the resolution of a few hundred lines sixty times a second. This provides enough information for most telepresence tasks.

Now, consider the Telegarden project. [13] Instead of continuous scanning of video, it uses user-driven still images. The image shows the garden from the viewpoint of the video camera attached to the robotic arm. When the arm is moved to a new location, a new still image is transmitted. These still images provide enough information for the particular teleaction in this project — planting the seeds.

As this example indicates, it is possible to teleact without video. More generally, we can say that different kinds of teleaction require different temporal and spatial resolutions. If the operator needs immediate feedback on her actions (the example of remote operation of a vehicle is again appropriate here), frequent update of images is essential. But in the case of planting a garden using a remote robot arm, user-triggered still images are sufficient.

Now, consider another example of telepresence. Radar images are obtained by scanning reality once every few seconds. The visible is reduced to a single point. A radar image does not contain any indications about shapes, textures, or colors present in a video image — it only records the position of an object. Yet this information is quite sufficient for the most basic teleaction: to destroy an object.

In this extreme case of teleaction, the image is so minimal it hardly can be called an image at all. However, it is still sufficient for real-time remote action. What is crucial is that the information is transmitted instantaneously.

If we put the examples of typical telepresence which uses video cameras and radar telepresence together, the common denominator turns out to be not video but electronic transmission of signals, in other words, electronic telecommunication, itself made possible by two discoveries of the nineteenth century: electricity and electromagnetism. This is the technology which makes teleaction in real-time possible. It also allows for the new and unprecedented relationship between objects and their signs. **Electronic telecommunication makes instantaneous not only the process by which objects are turned into signs but also the reverse process — manipulation of objects through these signs**.

Umberto Eco once defined a sign as something which can be used to tell a lie. This definition correctly describes one function of visual representations — to deceive. But in the age of electronic telecommunication we need a new definition: a sign is something which can be used to teleact. 

## References:

 [1] [http://www.yahoo.com](http://www.yahoo.com), accessed March 27, 1999.

[2] The earlier versions of this essay were published in **Mythos Information — Welcome to the Wired World. Ars Electronica 95**, edited by Karl Gebel and Peter Weibel. Vienna and New York: Springer-Verlag, 1995; and in **Cinema Futures: Cain, Abel or Cable?**, edited by Thomas Elsaesser and Kay Hoffmann. Amsterdam: Amsterdam University Press, 1998. I am grateful to Thomas Elsaesser for a number of suggestions which I incorporated in this version.

[3] On presentational system of early cinema, see Charles Musser, **The Emergence of Cinema: The American Screen to 1907** (Berkeley: University of California Press, 1990), 3.

[4] Paul Johnson, **The Birth of the Modern: World Society 1815-1830** (London: Orion House, 1992), 156.

[5] Dziga Vertov, "Kinoki. Perevorot" (Kinoki. A revolution), **LEF** 3 (1923): 140.

[6] **IMadGibe. Virtual Advertising for Live Sport Events**. A promotional flyer by ORAD, P.O. Box 2177, Kfar Saba 44425, Israel, 1998. 

[7] Brenda Laurel, quoted in Rebecca Coyle, "The Genesis of Virtual Reality," in **Future Visions: New Technologies of the Screen**," edited by Philip Hayward and Tana Wollen (London: British Film Institute, 1993), 162.

[8] Fisher, 430. Emphasis mine — LM.

[9] Fisher defines telepresence as "a technology which would allow remotely situated operators to receive enough sensory feedback to feel like they are really at a remote location and are able to do different kinds of tasks." Scott Fisher, "Visual Interface Environments," in **The Art of Human-Computer Interface Design**, edited by Brenda Laurel (Reading, Mass.: Addison-Wesley Publishing Company, Inc., 1990), 427.

[10] Bruno Latour, "Visualization and Cognition: Thinking with Eyes and Hands," **Knowledge and Society: Studies in the Sociology of Culture Past and Present** 6 (1986): 1-40.

[11] Ibid., 22.

[12] Ibid., 8.

[13] [http://telegarden.aec.at](http://telegarden.aec.at), accessed March 27, 1999. 

---

# Archeology of a Computer Screen

_author: Lev Manovich_
_year: 1995_

## 1. A Screen

Contemporary human-computer interfaces appear to offer radical new possibilities for art and communication. [1] Virtual reality (VR) allows us to travel through non-existent three-dimensional spaces. A computer monitor connected to a network becomes a window through which we can be present in a place thousands of miles away. Finally, with the help of a mouse or a video camera, a computer is transformed into an intelligent being capable of engaging us in a dialogue.

VR, interactivity and telepresence are made possible by the recent technology of a digital computer. However, they are made real by a much, much older technology — the screen. It is by looking at a screen — a flat, rectangular surface positioned at some distance from the eyes — that the user experiences the illusion of navigating through virtual spaces, of being physically present somewhere else, or of being hailed by the computer itself. If computers have become a common presence in our culture only in the last decade, the screen, on the other hand, has been used to present visual information for centuries — from Renaissance painting to twentieth-century cinema.

Today, coupled with a computer, the screen is rapidly becoming the main means of accessing any kind of information, be it still images, moving images or text. We are already using it to read the daily newspaper, to watch movies, to communicate with coworkers, relatives and friends, and, most importantly, to work (the screens of airline agents, data entry clerks, secretaries, engineers, doctors, pilots, etc.; the screens of ATM machines, supermarket checkouts, automobile control panels, and, of course, the screens of computers.) We may debate whether our society is a society of spectacle or of simulation, but, undoubtedly, it is the society of a screen. What are the different stages of the screen's history? What are the relationships between the physical space where the viewer is located, his/her body, and the screen space? What are the ways in which computer displays both continue and challenge the tradition of a screen? 

## 2. A Screen's Genealogy

Let us start with the definition of a screen.

Visual culture of the modern period, from painting to cinema, is characterized by an intriguing phenomenon: the existence of another virtual space, another three-dimensional world enclosed by a frame and situated inside our normal space. The frame separates two absolutely different spaces that somehow coexist. This phenomenon is what defines the screen in the most general sense, or, as I will call it, the "classical screen."

What are the properties of a classical screen? It is a flat, rectangular surface. It is intended for frontal viewing (as opposed to, for instance, a panorama). It exists in our normal space, the space of our body, and acts as a window into another space. This other space, the space of representation, typically has a different scale from the scale of our normal space.

Defined in this way, a screen describes equally well a Renaissance painting (recall Alberti) and a modern computer display. Even though proportions have not changed in five centuries, they are similar for a typical fifteenth-century painting, a film screen and a computer screen. (In this respect it is not accidental that the very names of the two main formats of computer displays point to two genres of painting: a horizontal format is referred to as "landscape mode" while the vertical format is referred to as "portrait mode".)

---

A hundred years ago a new type of screen became popular, which I will call the "dynamic screen". This new type retains all the properties of a classical screen while adding something new: it can display an image changing over time. This is the screen of cinema, television, video.

The dynamic screen also brings with it a certain relationship between the image and the spectator — a certain viewing regime, so to speak. This relationship is already implicit in the classical screen but now it fully surfaces. A screen's image strives for complete illusion and visual plenitude while the viewer is asked to suspend disbelief and to identify with the image. Although the screen, in reality, is only a window of limited dimensions positioned inside the physical space of the viewer, the latter is supposed to completely concentrate on what is seen in this window, focusing attention on the representation and disregarding the physical space outside. This viewing regime is made possible by the fact that, be it a painting, movie screen, or television screen, the singular image completely fills the screen. This is why we are so annoyed in a movie theater when the projected image does not precisely coincide with the screen's boundaries: it disrupts the illusion, making us conscious of what exists outside the representation. [2]

Rather than being a neutral medium of presenting information, the screen is aggressive. It functions to filter, to screen out, to take over, rendering non-existent whatever is outside its frame. And although, of course, the degree of this filtering varies between cinema viewing (where the viewer is asked to completely merge with the screen's space) and television viewing (where the screen is smaller, lights are on, conversation between viewers is allowed, and the act of viewing is often integrated with other daily activities), overall, this viewing regime remains stable — until recently. This stability has been challenged by the arrival of the computer screen. On the one hand, rather than showing a single image, a computer screen typically displays a number of coexisting windows. (Indeed, the coexistence of a number of overlapping windows has become a fundamental principle of modern computer interface since the introduction of the first Macintosh computer in 1984.) No single window completely dominates the viewer's attention. In this sense, the possibility of simultaneously observing a few images which coexist within one screen can be compared with the phenomenon of zapping — the quick switching of television channels that allows the viewer to follow more than program. [3] In both instances, the viewer no longer concentrates on a single image. (Some television sets now enable a second channel to be watched within a smaller window positioned in a corner of the main screen. Perhaps future TV sets will adopt the window metaphor of a computer.) A window interface has more to do with modern graphic design, which treats a page as a collection of different but equally important blocks of data (text, images, graphic elements), than with cinema.

On the other hand, with VR, the screen disappears altogether. VR typically uses a head-mounted display whose images completely fill viewer's visual field. No longer is the viewer looking forward at a rectangular, flat surface located at a certain distance and which acts as a window into another space. Now, s/he is fully situated within this other space. Or, more precisely, we can say that the two spaces, the real, physical space and the virtual simulated space, coincide. The virtual space, previously confined to a painting or a movie screen, now completely encompasses the real space. Frontality, rectangular surface, difference in scale are all gone. The screen has vanished.

Both situations (window interface and VR) disrupt the viewing regime which characterizes the historical period of the dynamic screen. This regime, based on the identification of the viewer with a screen image, reaches its culmination in the cinema which goes to the extreme to enable this identification (the bigness of the screen, the darkness of the surrounding space) while still relying on a screen (a rectangular flat surface). 

Thus, as we celebrate a hundred years of cinema (the first paid public presentation of a film took place in December of 1895), we should also celebrate — and mourn — the era of the dynamic screen which began with cinema and is ending now. And it is this disappearance of the screen — it's splitting into many windows in window interface, its complete take over of the visual field in VR — that allows us today to recognize it as a cultural category and begin to trace its history.

---

The origins of the cinema's screen are well known. We can trace its emergence to the popular spectacles and entertainment of the eighteenth and nineteenth centuries: magic lantern shows, phantasmagoria, eidophusikon, panorama, diorama, zoopraxiscope shows, and so on. The public was ready for cinema and when it finally appeared it was a huge public event. Not by accident, the "invention" of cinema was claimed by at least a dozen of individuals from a half-dozen countries. [4]

The origin of the computer screen is a different story. It appears in the middle of this century but it does not become a public presence until much later; and its history has not yet been written. Both of these facts are related to the context in which it emerged: as with all the other elements of modern human-computer interface, the computer screen was developed by the military. Its history has to do not with public entertainment but with military surveillance.

The history of modern surveillance technologies begins at least with photography. From the advent of photography, there existed an interest in using it for aerial surveillance. Félix Tournachon Nadar, one of the most eminent photographers of the nineteenth century, succeeded in exposing a photographic plate at 262 feet over Bièvre, France in 1858. He was soon approached by the French Army to attempt photo reconnaissance but rejected the offer. In 1882, unmanned photo balloons were already in the air; a little later, they were joined by photo rockets both in France and in Germany. The only innovation of World War I was to combine aerial cameras with a superior flying platform — the airplane. [5]

Radar became the next major surveillance technology. Massively employed in World War II, it provided important advantages over photography. Previously, military commanders had to wait until the pilots returned from surveillance missions and the film was developed. The inevitable delay between the time of the surveillance and the delivery of the finished image limited its usefulness because, by the time the photograph was produced, enemy positions could have changed. However, with radar, as imaging became instantaneous, this delay was eliminated. The effectiveness of radar had to do with a new means of displaying an image — a new type of screen.

Consider the imaging technologies of photography and film. The photographic image is a permanent imprint corresponding to a single referent (whatever was in front of the lens when the photograph was taken) and to a limited time of observation (the time of exposure). Film is based on the same principle. A film sequence, composed of a number of still images, represents the sum of referents and the sum of exposure times of these individual images. In either case, the image is fixed once and for all. Therefore the screen can only show past events. 

With radar, we see for the first time the mass employment (television is founded on the same principle but its mass employment comes later) of a fundamentally new type of screen, the screen which gradually comes to dominate modern visual culture — video monitor, computer screen, instrument display. What is new about such a screen is that its image can change in real time, reflecting changes in the referent, be it the position of an object in space (radar), any alteration in visible reality (live video) or changing data in the computer's memory (computer screen). The image can be continually updated in real time. This is the third (after classic and dynamic) type of a screen — the screen of real time.

The radar screen changes, tracking the referent. But while it appears that the element of time delay, always present in the technologies of military surveillance, is eliminated, in fact, time enters the real-time screen in a new way. In older, photographic technologies, all parts of an image are exposed simultaneously. Whereas now the image is produced through sequential scanning: circular in the case of radar, horizontal in the case of television. Therefore, the different parts of the image correspond to different moments in time. In this respect, a radar image is more similar to an audio record since consecutive moments in time become circular tracks on a surface. [6]

What this means is that the image, in a traditional sense, no longer exists! And it is only by habit that we still refer to what we see on the real-time screen as "images". It is only because the scanning is fast enough and because, sometimes, the referent remains static, that we see what looks like a static image. Yet, such an image is no longer the norm, but the exception of a more general, new kind of representation for which we don't have a term yet.

---

The principles and technology of radar were worked out independently by scientists in the United States, England, France and Germany during the 1930s. But, after the beginning of the War only the U.S. had the necessary resources to continue radar development. In 1940, at MIT, a team of scientists was gathered to work in the Radiation Laboratory, or the "Rad Lab," as it came to be called. The purpose of the lab was radar research and production. By 1943, the "Rad Lab" occupied 115 acres of floor space; it had the largest telephone switchboard in Cambridge and employed 4,000 people. [7]

Next to photography, radar provided a superior way to gather information about enemy locations. In fact, it provided too much information, more information than one person could deal with. Historical footage from the early days of the war shows a central command room with a large, table-size map of Britain. [8] Small pieces of cardboard in the form of planes are positioned on the map to show the locations of actual German bombers. A few senior officers scrutinize the map. Meanwhile, women in army uniforms constantly change the location of the cardboard pieces by moving them with long sticks as information is transmitted from dozens of radar stations. [9]

Was there a more effective way to process and display information gathered by radar? The computer screen, as well as all of the other key principles and technologies of modern human-computer interface — interactive control, algorithms for 3-D wireframe graphics, bit-mapped graphics — were developed as a way of solving this problem.

The research again took place at MIT. The Radiation Laboratory was dismantled after the end of the War, but soon the Air Force created another secret laboratory in its place — Lincoln Laboratory. The purpose of Lincoln Laboratory was to work on human factors and new display technologies for SAGE — "Semi-Automatic Ground Environment," a command center to control the U.S. air defenses established in the mid-1950s. [10] Paul Edwards writes that SAGE's job "was to link together radar installations around the USA's perimeter, analyze and interpret their signals, and direct manned interceptor jets toward the incoming bee. It was to be a total system, one whose 'human components' were fully integrated into the mechanized circuit of detection, decision and response." [11]

Why was SAGE created and why did it require a computer screen? In the 1950s the American military thought that when the Soviet Union attacked the U.S., it would send a large number of bombers simultaneously. Therefore, it seemed necessary to create a center which could receive information from all U.S. radar stations, track a large number of enemy bombers and coordinate the counterattack. A computer screen and the other components of the modern human-computer interface owe their existence to this particular military doctrine. 

The earlier version of the center was called the Cape Cod network since it received information from the radars situated along the coast of New England. The center was operating right out of the Barta Building situated on the MIT campus.

Each of 82 Air Force officers monitored his own computer display which showed the outlines of the New England Coast and the locations of key radars. Whenever an officer noticed a dot indicating a moving plane, he would tell the computer to follow the plane. To do this the officer simply had to touch the dot with the special "light pen". [12]

Thus, the SAGE system contained all of the main elements of the modern human-computer interface. The light pen, designed in 1949, can be considered a precursor of the contemporary mouse. More importantly, at SAGE the screen came to be used not only to display information in real time (as in radar and television) but also to give commands to the computer. Rather than acting solely as a means to display an image of reality, the screen became the vehicle for directly affecting reality. Using the technology developed for SAGE, Lincoln researchers created a number of computer graphics programs that relied on the screen as a means to input and output information from a computer. They included programs to display brain waves (1957), simulate planet and gravitational activity (1960), as well as to create 2-D drawings (1958). [13] The single most well-known of these became a program called Sketchpad. Designed in 1962 by Ivan Sutherland, a graduate student supervised by Claude Shannon, it widely publicized the idea of interactive computer graphics. With Sketchpad, a human operator could create graphics directly on computer screen by touching the screen with a light pen. Sketchpad exemplified a new paradigm of interacting with computers: by changing something on the screen, the operator changed something in the computer's memory. The real-time screen became interactive.

---

This, in short, is the history of the birth of the computer screen. [14] But even before a computer screen became widely used, a new paradigm emerged — the simulation of an interactive three-dimensional environment without a screen. In 1966, Ivan Sutherland and his colleagues began research on the prototype of VR. The work was cosponsored by ARPA (Advanced Research Projects Agency) and the Office of Naval Research. [15]

The fundamental idea behind the three-dimensional display is to present the user with a perspective image which changes as he moves," wrote Sutherland in 1968. [16] The computer tracked the position of the viewer's head and adjusted the perspective of the computer graphic image accordingly. The display itself consisted of two six-inch-long monitors which were mounted next to the temples. They projected an image which appeared superimposed over viewer's field of vision.

The screen disappeared. It completely took over the visual field. 

## 3. The Screen and the Body

I have presented one possible genealogy of the modern computer screen. In my genealogy, the computer screen represents an interactive type, a subtype of the real-time type, which is a subtype of the dynamic type, which is a subtype of the classical type.

My discussion of these types relied on two ideas. First, the idea of temporality: the classical screen displays a static, permanent image; the dynamic screen displays a moving image of the past and finally, the real-time screen shows the present. Second, the relationship between the space of the viewer and the space of the representation (I defined the screen as a window into the space of representation which itself exists in our normal space).

Let us now look at the screen's history from another angle — the relationship between the screen and the body of the viewer.

---

This is how Roland Barthes described the screen in "Diderot, Brecht, Eisenstein," written in 1973:

Representation is not defined directly by imitation: even if one gets rid of notions of the "real," of the "vraisemblable," of the "copy," there will still be representation for as long as a subject (author, reader, spectator or voyeur) casts his gaze towards a horizon on which he cuts out a base of a triangle, his eye (or his mind) forming the apex. The "Organon of Representation" (which is today becoming possible to write because there are intimations of something else) will have as its dual foundation the sovereignty of the act of cutting out [découpage] and the unity of the subject of action... The scene, the picture, the shot, the cut-out rectangle, here we have the very condition that allows us to conceive theater, painting, cinema, literature, all those arts, that is, other than music and which could be called dioptric arts. [17] 

For Barthes, the screen becomes the all-encompassing concept which covers the functioning of even non-visual representation (literature), even though he does make an appeal to a particular visual model of linear perspective. At any rate, his concept encompasses all types of representational apparatuses I have discussed: painting, film, television, radar and computer display. In each of these, reality is cut by the rectangle of a screen: "a pure cut-out segment with clearly defined edges, irreversible and incorruptible; everything that surrounds it is banished into nothingness, remains unnamed, while everything that it admits within its field is promoted into essence, into light, into view." [18] This act of cutting reality into a sign and nothingness simultaneously doubles the viewing subject who now exists in two spaces: the familiar physical space of his/her real body and the virtual space of an image within the screen. This split comes to the surface with VR, but it already exists with painting and other dioptric arts.

What is the price the subject pays for the mastery of the world, focused and unified by the screen?

"The Draughtsman's Contract", a 1981 film by Peter Greenway, concerns an architectural draftsman hired to produce a set of drawings of a country house. The draughtsman employs a simple drawing tool consisting of a square grid. Throughout the film, we repeatedly see the draughtsman's face through the grid which looks like the prison bars. It is as if the subject who attempts to catch the world, to immobilize it, to fix it within the representational apparatus (here, perspectival drawing), is trapped by this apparatus himself. The subject is imprisoned.

I take this image as a metaphor for what appears to be a general tendency of the Western screen-based representational apparatus. In this tradition, the body must be fixed in space if the viewer is to see the image at all. From Renaissance monocular perspective to modern cinema, from Kepler's camera obscura to nineteenth-century camera lucida, the body had to remain still.

The imprisonment of the body takes place on both the conceptual and literal levels; both kinds of imprisonment already appear with the first screen apparatus, Alberti's perspectival window. According to many interpreters of linear perspective, it presents the world as seen by a singular eye, static, unblinking and fixated. As described by Norman Bryson, perspective "followed the logic of the Gaze rather than the Glance, thus producing a visual take that was eternalized, reduced to one "point of view" and "disembodied". [19] Bryson argues that "the gaze of the painter arrests the flux of phenomena, contemplates the visual field from a vantage-point outside the mobility of duration, in an eternal moment of disclosed presence." [20] Correspondingly, the world, as seen by this immobile, static and a temporal Gaze, which belongs more to a statue than to a living body, becomes equally immobile, reified, fixated, cold and dead. Writing about Dürer's famous print of a draftsman drawing a nude through a screen of perspectival threads, Martin Jay notes that "a reifying male look" turns "its targets into stone"; consequently, "the marmoreal nude is drained of its capacity to arouse desire." [21] Similarly, John Berger compares Alberti's window to "a safe let into a wall, a safe into which the visible has been deposited." [22] And in "The Draughtsman's Contract," time and again, the draughtsman tries to eliminate all motion, any sign of life from the scenes he is rendering. 

With perspectival machines, the imprisonment of the subject also happens in a literal sense. From the onset of the adaptation of perspective, artists and draftsmen have attempted to aid the laborious manual process of creating perspectival images and, between the sixteenth and nineteenth centuries, various "perspectival machines" were constructed. [23] By the first decades of the sixteenth century, Dürer described a number of such machines. [24] Many varieties were invented, but regardless of the type, the artist had to remain immobile throughout the process of drawing.

Along with perspectival machines, a whole range of optical apparatuses was in use, particularly for depicting landscapes and conducting topographical surveys. The most popular optical apparatus was camera obscura. [25] Camera obscura literally means "dark chamber". It was founded on the premise that if the rays of light from an object or a scene pass through a small aperture, they will cross and re-emerge on the other side to form an image on a screen. In order for the image to become visible, however, "it is necessary that the screen be placed in a chamber in which light levels are considerably lower than those around the object." [26] Thus, in one of the earliest depictions of the camera obscura, in Kircher's Ars magna Lucis et umbrae (Rome, 1649), we see the subject enjoying the image inside a tiny room, oblivious to the fact that he had to imprison himself inside this "dark chamber" in order to see the image on the screen.

Later, smaller tent-type camera obscura became popular — a movable prison, so to speak. It consisted of a small tent mounted on a tripod, with a revolving reflector and lens at its apex. Having positioned himself inside the tent which provided the necessary darkness, the draftsman would then spend hours meticulously tracing the image projected by the lens.

Early photography continued the trend toward the imprisonment of the subject and the object of representation. During photography's first decades, the exposure times were quite long. The daguerreotype process, for instance, required exposures of 4 to 7 minutes in the sun and from 12 to 60 minutes in diffused light. So, similar to the drawings produced with the help of camera obscura, which depicted reality as static and immobile, early photographs represented the world as stable, eternal, unshakable. And when photography ventured to represent the living, such as the human subject, s/he had to be immobilized. Thus, portrait studios universally employed various clamps to assure the steadiness of the sitter throughout the lengthy time of exposure. Reminiscent of the torture instruments, the iron clamps firmly held the subject in place, the subject who voluntarily became the prisoner of the machine in order to see her/his own image. [27]

Toward the end of the nineteenth century, the petrified world of the photographic image was shattered by the dynamic screen of the cinema. In "The Work of Art in the Age of Mechanical Reproduction," Walter Benjamin expressed his fascination with the new mobility of the visible:

Our taverns and our metropolitan streets, our offices and furnished rooms, our railroad stations and our factories appeared to have us locked up hopelessly. When came the film and burst this prison-world asunder by the dynamite of the tenth of a second, so that now, in the midst of its far-flung ruins and debris, we calmly and adventurously go traveling. [28]

The cinema screen enabled audiences to take a journey through different spaces without leaving their seats; in the words of Anne Friedberg, it created "a mobilized virtual gaze". [29] However, the cost of this virtual mobility was a new, institutionalized immobility of the spectator. All around the world large prisons were constructed which could hold hundreds of prisoners — movie houses. The prisoners could not neither talk to each other nor move from seat to seat. While they were taken on virtual journeys, their bodies had to remain still in the darkness of the collective camera obscuras.

The formation of this viewing regime took place in parallel with the shift from what film theorists call "primitive" to "classical" film language. [30] An important part of the shift, which took place in the 1910s, was the new functioning of the virtual space represented on the screen.

During the "primitive" period, the space of the film theater and the screen space was clearly separated much like those of theater or vaudeville. The viewers were free to interact, come and go, and maintain a psychological distance from the cinematic diegesis.

In contrast, classical film addressed each viewer as a separate individual and positioned her/him inside the diegetic space. As noted by a contemporary in 1913, "they [spectators] should be put in the position of being a "knot hole in the fence" at every stage in the play." [31] If "primitive cinema keeps the spectator looking across a void in a separate space," [32] now the spectator is placed at the best viewpoint of each shot, inside the virtual space.

This situation is usually conceptualized in terms of the spectator's identification with the camera eye. The body of the spectator remains in the seat while her/his eye is coupled with a mobile camera. However, it is also possible to conceptualize this differently. We can imagine that the camera does not, in fact, move at all, that it remains stationary, coinciding with the spectator's eyes. Instead, it is the virtual space as a whole that changes its position with each shot. Using the contemporary vocabulary of computer graphics, we can say that this virtual space is rotated, scaled and zoomed to always give the spectator the best viewpoint. Like a striptease, the space slowly disrobes itself, turning, presenting itself from different sides, teasing, stepping forward and retracting, always leaving something unrevealed, so the spectator will wait for the next shot ... the endless seductive dance. All spectator has to do is remain immobile.

Film theorists have taken this immobility to be the essential feature of the institution of cinema. Friedberg wrote: "As everyone from Baudry (who compares cinematic spectation to the prisoners in Plato's cave) to Musser points out, the cinema relies on the immobility of the spectator, seated in an auditorium." [33] Jean-Louis Baudry has probably more than anyone put the emphasis on immobility as the foundation of cinematic illusion. Baudry quoted Plato: "In this underground chamber they have been from childhood, chained by the leg and also by the neck, so that they cannot move and can only see what is in front of them, because the chains will not let them turn their heads." [34] This immobility and confinement, according to Baudry, enables prisoners/spectators to mistake representations for their perceptions, regressing back to childhood when the two were indistinguishable. Thus, rather than a historical accident, according to Baudry's psychoanalytic explanation, the immobility of the spectator is the essential condition of cinematic pleasure. 

---

Alberti's window, Dürer's perspectival machines, camera obscura, photography, cinema — in all of these screen-based apparatuses, the subject had to remain immobile. In fact, as Friedberg perceptively points out, the progressive mobilization of the image in modernity was accompanied by the progressive imprisonment of the viewer: "as the "mobility" of the gaze became more "virtual" — as techniques were developed to paint (and then to photograph) realistic images, as mobility was implied by changes in lighting (and then cinematography) — the observer became more immobile, passive, ready to receive the constructions of a virtual reality placed in front of his or her unmoving body." [35] What happens to this tradition with the arrival of a screen-less representational apparatus — VR?

On the one hand, VR does constitute a fundamental break with this tradition. It establishes a radically new type of relationship between the body of a viewer and an image. In contrast to cinema, where the mobile camera moves independent of the immobile spectator, now the spectator has to actually move around the physical space in order to experience the movement in virtual space. The effect is as though the camera is mounted on user's head. So, in order to look up in virtual space, one has to look up in physical space; in order to "virtually" step forward, one has to actually step forward and so on. [36] The spectator is no longer chained, immobilized, anesthetized by the apparatus which serves him the ready-made images; now s/he has to work, to speak, in order to see.

At the same time, VR imprisons the body to an unprecedented extent before. This can be seen clearly with the earliest VR system designed by Sutherland and his colleagues in the 1960s to which I have already referred. According to Howard Rheingold's history of VR, "Sutherland was the first to propose mounting small computer screens in binocular glasses — far from an easy hardware task in the early 1960s — and thus immerse the user's point of view inside the computer graphic world." [37] Rheingold further wrote:

In order to change the appearance of the computer-generated graphics when the user moves, some kind of gaze-tracking tool is needed. Because the direction of the user's gaze was most economically and accurately measured at that time by means of a mechanical apparatus, and because the HMD [head-mounted display] itself was so heavy, the users of Sutherland's early HMD systems found their heads locked into machinery suspended from the ceiling. The user puts his or her head into a metal contraption that was known as the "Sword of Damocles" display. [38]

A pair of tubes connected the display to tracks in the ceiling, "thus making the user a captive of the machine in a physical sense." [39] The user was able to turn around and rotate her/his head in any direction but s/he could not move away from the machine more than few steps. Like today's computer mouse, the body was tied to the computer. In fact, the body was reduced to nothing else — and nothing more — than a giant mouse, or more, precisely, a giant joystick. Instead of moving a mouse, the user had to turn her/his own body. Another comparison which comes to mind is the apparatus built in the late nineteenth century by Etienne-Jules Marey to measure the frequency of the wing movements of a bird. The bird was connected to the measuring equipment by wires which were long enough to enable it to flap its wings in midair but not fly anywhere. [40]

The paradox of VR that requires the viewer to physically move in order to see an image (as opposed to remaining immobile) and at the same time physically ties her/him to a machine is interestingly dramatized in a "cybersex" scene in Hollywood's "Lawnmower Man". In the scene, the heroes, a man and a woman, are situated in the same room, each fastened to a separate circular frame which allows the body to freely rotate 360 degrees in all directions. During "cybersex" the camera cuts back and forth between the virtual space (i.e., what the heroes see and experience) and the physical space. In the virtual world represented with psychedelic computer graphics, their bodies melt and morph together disregarding all the laws of physics, while in the real world each of them simply rotates within his/her own frame.

The paradox reaches its extreme in one of the most long-standing VR projects — the Super Cockpit developed by the U.S. Air Force in the 1980s. [41] Instead of using his own eyes to follow both the terrain outside of his plane and the dozens of instrument panels inside the cockpit, the pilot wears a head-mounted display that presents both kinds of information in a more efficient way. What follows is a description of the system from Air & Space magazine:

When he climbed into his F16C, the young fighter jock of 1998 simply plugged in his helmet and flipped down his visor to activate his Super Cockpit system. The virtual world he saw exactly mimicked the world outside. Salient terrain features were outlined and rendered in three dimensions by the two tiny cathode ray tubes focused at his personal viewing distance...His compass heading was displayed as a large band of numbers on the horizon line, his projected flight path a shimmering highway leading out toward infinity. [42]

If in most screen-based representations (painting, cinema, video) as well as in typical VR applications the physical and the virtual worlds have nothing to do with each other, here the virtual world is precisely synchronized to the physical one. The pilot positions himself in the virtual world in order to move through the physical one at a supersonic speed with his representational apparatus which is securely fastened to his body, more securely than ever before in the history of the screen.

---

In summary, on the one hand, VR continues the screen's tradition of viewer immobility by fastening the body to a machine, while on the other hand, it creates an unprecedented new condition, requiring the viewer to move. We may ask, in conclusion, whether this new condition is without a historical precedent or whether it fits within some other alternative tradition we so far have not noticed. In Ancient Greece, communication was understood as an oral dialogue between people. It was also assumed that physical movement stimulated dialogue and the process of thinking. Aristotle and his pupils walked around while discussing philosophical problems. In the Middle Ages, a shift occurred from a dialogue between subjects to communication between a subject and an information storage device, i.e., a book. A Medieval book chained to a table can be considered a precursor to the screen. 

The screen, as I defined it (a flat rectangle that acts as a window into the virtual world), makes its appearance in the Renaissance with modern painting. Previously, frescoes and mosaics were inseparable from the architecture. In contrast, a painting is essentially mobile. Separate from a wall, it can be moved anywhere.

But at the same time, an interesting reversal takes place. The interaction with a fresco or a mosaic, which can't be moved anywhere, does not assume immobility on the part of the spectator, while the mobile Renaissance painting does presuppose such immobility.

Do frescoes, mosaics and wall paintings, which are all part of the architecture, represent this alternative tradition I am searching for, the tradition which encourages the movement of the viewer? 

I began my discussion of the screen by emphasizing that a screen's frame separates two spaces, the physical and the virtual, which have different scales. Although this condition does not necessarily lead to the immobilization of the spectator, it does discourage any movement on her or his part: Why move when s/he can't enter the represented virtual space anyway? This was very well dramatized in "Alice in Wonderland" when Alice struggles to become just the right size in order to enter the other world.

The alternative tradition of which VR is a part can be found whenever the scale of a representation is the same as the scale of our human world so that the two spaces are continuous. This is the tradition of simulation rather than that of representation bound up to a screen. One example is mosaics, frescoes and wall paintings which create an illusionary space that starts behind the surface. The nineteenth century, with its obsession with naturalism, pushes this trend to the extreme with the wax museum and the dioramas of natural history museums. Another example is a sculpture on a human scale (for instance, Auguste Rodin's "The Burghers of Calais"). We think of such sculptures as part of post-Renaissance humanism which puts the human at the center of the universe, when in fact, they are aliens, black holes within our world into another parallel universe, the petrified universe of marble or stone, which exists in parallel to our own world.

VR continues this tradition of simulation. However, it introduces one important difference. Previously, the simulation depicted a fake space which was continuous with and extended from the normal space. For instance, a wall painting created a pseudo landscape which appeared to begin at the wall. In VR, either there is no connection between the two spaces (for instance, I am in a physical room while the virtual space is one of an underwater landscape) or, on the contrary, the two completely coincide (i.e., the Super Cockpit project). In either case, the actual physical reality is disregarded, dismissed, abandoned.

In this respect, the nineteenth-century panorama can be thought of as a transitional form from classical simulations (wall paintings, human-size sculptures, dioramas) toward VR. Like VR, panorama creates a 360-degree space. The viewers are situated in the center of this space and they are encouraged to move around the central viewing area in order to see different parts of the panorama. [43] But in contrast to wall paintings and mosaics which, after all, acted as decorations of a real space, the physical space of action, now this physical space is subordinate to the virtual space. In other words, the central viewing area is conceived as a continuation of fake space (rather than vice versa as before), and this is why it is empty. It is empty so that we can pretend that it continues the battlefield, or a view of Paris, or whatever else the panorama represents. From here we are one step away from VR where the physical space is totally disregarded and all the "real" actions take place in virtual space. The screen disappeared because what was behind it simply took over.

---

And what about the immobilization of the body in VR which connects it to the screen tradition? Dramatic as it is, this immobilization probably represents the last act in the long history of the body's imprisonment. All around us are the signs of increasing mobility and the miniaturization of communication devices — cellular telephones and modems, pagers and laptops. Eventually, VR apparatus will be reduced to a chip implanted in a retina and connected by cellular transmission to the Net. From that moment on, we will carry our prisons with us — not in order to blissfully confuse representations and perceptions (as in cinema), but to always "be in touch," always connected, always "plugged-in". The retina and the screen will merge.

This futuristic scenario may never become a reality. For now, we clearly live in the society of a screen. The screens are everywhere: the screens of airline agents, data entry clerks, secretaries, engineers, doctors, pilots, etc.; the screens of ATM machines, supermarket checkouts, automobile control panels, and, of course, the screens of computers. Dynamic, real-time and interactive, a screen is still a screen. Interactivity, simulation, and telepresence: like centuries ago, we are still looking at a flat rectangular surface, existing in the space of our body and acting as a window into another space. Whatever new era we may be entering today, we still have not left the era of a screen.

## References:

[1] The earlier versions of this essay have been presented at the "Generated Nature" symposium (Rotterdam, November 1994) and the "NewMediaLogia" symposium (Moscow, November 1994). I am grateful to the participants of both symposia as well as to the students in my "Visual Theory" seminar for their many very useful comments and suggestions.

[2] The degree to which a frame that acts as a boundary between the two spaces is emphasized seems to be proportional to the degree of identification expected from the viewer. Thus, in cinema, where the identification is most intense, the frame as a separate object does not exist at all — the screen simply ends at its boundaries — while both in painting and in television the framing is much more pronounced. 

[3] Here I agree with the parallel suggested by Anatoly Prokhorov between window interface and montage in cinema.

[4] For these origins, see, for instance, C.W. Ceram, _Archeology of the Cinema_(New York: Harcourt, Brace & World, Inc., 1965).

[5] Beaumont Newhall, _Airborne Camera_ (New York: Hastings House, Publishers, 1969). 

[6] This is more than a conceptual similarity. In the late 1920s, John H. Baird invented "phonovision," the first method for the recording and the playing back of a television signal. The signal was recorded on Edison's phonograph's record by a process very similar to making an audio recording. Baird named his recording machine "phonoscope". Albert Abramson, _Electronic Motion Pictures_ (University of California Press, 1955), 41-42. 

[7] _Echoes of War_ (Boston: WGBH Boston, n.d.), videotape.

[8] Ibid.

[9] Ibid.

[10] On SAGE, see Paul Edwards, "The Closed World. Systems discourse, military policy and post-World War II U.S. historical consciousness," in _Cyborg Worlds: The Military Information Society_, eds. Les Levidow and Kevin Robins (London: Free Association Books, 1989); Howard Rheingold, _Virtual Reality_ (New York: Simon & Schuster, Inc., 1991), 68-93.

[11] Edwards, 142.

[12] "Retrospectives II: The Early Years in Computer Graphics at MIT, Lincoln Lab, and Harvard," in _SIGGRAPH '89 Panel Proceedings_ (New York: The Association for Computing Machinery, 1989), 22-24.

[13] Ibid., 42-54.

[14] I will address important later developments such as bitmapped display and window interface in a future article.

[15] Rheingold, 105.

[16] Qtd. in Rheingold, 104.

[17] Roland Barthes, "Diderot, Brecht, Eisenstein," in _Images-Music-Text_, ed. Stephen Heath (New York: Farrar, Straus and Giroux, 1977), 69-70.

[18] Ibid.

1[9 ]As summarized by Martin Jay, "Scopic Regimes of Modernity," in _Vision and Visuality_, ed. Hal Foster (Seattle: Bay Press, 1988), 7. 

[20] Qtd. in Ibid, 7.

[21] Ibid, 8.

[22] Qtd. in Ibid., 9.

[23] For a survey of perspectival instruments, see Martin Kemp, _The Science of Art_ (New Haven: Yale University Press, 1990), 167-220.

[24] Ibid., 171-172.

[25] Ibid., 200.

[26] ]Ibid.

[27] Anesthesiology emerges approximately at the same time.

[28] Walter Benjamin, "The Work of Art in the Age of Mechanical Reproduction," in _Illuminations_, ed. Hannah Arendt (New York: Schochen Books, 1969), 238.

[29] Anne Friedberg, _Window Shopping: Cinema and the Postmodern_ (Berkeley: University of California Press, 1993), 2. 

[30] See, for instance, David Bordwell, Janet Steiger and Kristin Thompson, _The Classical Hollywood Cinema_ (New York: Columbia University Press, 1985).

[31] Qtd. in Ibid., 215.

[32] Ibid., 214.

[33] Friedberg, 134. She refers to Jean-Louis Baudry, "The Apparatus: Metapsychological Approaches to the Impression of Reality in the Cinema," in _Narrative, Apparatus, Ideology_, ed. Philip Rosen (New York: Columbia University Press, 1986) and Charles Musser, _The Emergence of Cinema: The American Screen to 1907_ (New York: Charles Scribner and Sons, 1990).

[34] Qtd. in Baudry, 303. 

[35] Friedberg, 28.

[36] A typical VR system adds other ways of moving around, for instance, the ability to move forward in a single direction by simply pressing a button on a joystick. However, to change the direction the user still has to change the position of his/her body.

[37] Rheingold, 104.

[38] Ibid., 105.

[39] Ibid., 109.

[40] Marta Braun, _Picturing Time: The Work of Etienne-Jules Marey (1830-1904)_ (Chicago: The University of Chicago Press, 1992), 34-35.

[41] Rheingold, 201-209.

[42] Qtd. in Ibid., 201.

[43] Here I disagree with Friedberg who writes, "Phantasmagorias, panoramas, dioramas — devices that concealed their machinery — were dependent on the relative immobility of their spectators." (23)

---
# Reading New Media Art

_author: Lev Manovich_
_year: 1995_

Consider the dichotomy: an art object in a gallery setting versus a software program on a computer. On entering an exhibition of media art, we encounter signs that tell us that we are in the realm of Art: the overall exhibition space is dark, each installation is positioned in a separate, carefully lit space, each accompanied by a label with an artist's name. We know well what to do in this situation: we are supposed to perceive, contemplate, and reflect. Yet these initial signs are misleading. An exhibition of media art points us to very different cultural settings such as a computer games hall or an entertainment park (in each of these one often has to wait in line before getting a chance to "try" a particular exhibit) and also to a different type of cultural object (and, correspondingly, a different set of behaviors) — a software program in a computer. In approaching a media artwork, we typically discover some elements of standard human-computer interface (a computer monitor, a mouse; arrows, buttons and so on); we have to read instructions which tell us how to use it; we then have to go through the process of learning its own unique navigational metaphors. All in all, the behaviors which are required of us are intellectual problem solving, systematic experimentation and the quick learning of new tasks. Is it possible to combine these with contemplation, perceptual enjoyment and emotional response? In other words, is it possible to experience the work aesthetically while simultaneously learning how to "use" it?

The works in the NEWFOUNDLAND II exhibition provided a variety of different solutions to this basic problem of media art. One solution is avoiding an interactive interface altogether, as in Tamas Waliczky's installation THE WAY. The installation shows the third part of his stunning 3-D computer animation trilogy (the first two parts are THE GARDEN and THE FOREST) which narrates Waliczky's journey from the East to the West using specially constructed perspectival systems. THE WAY presents a rather grim view of the West: the typical sterility of 3-D computer animation turns out to be a perfect metaphor for the sterility and regularity of the Western society; the inverted (as opposed to the central, as it is usually interpreted) perspective epitomizes the Western subject's self-sufficiency and isolation from his environment. 

A different solution is exemplified by Toshio Iwai's PIANO-AS AN IMAGE MEDIA. A viewer of his installation does not have to struggle with a new interface because Iwai uses an interface already familiar to everybody: that of a piano. The installation can be seen as a playful response to the whole modern tradition of image-sound synthesis and also as a commentary on various relationships between the physical and the virtual which characterize the end of the twentieth century. Iwai sets up a whole network of these relationships: the physical affects the virtual (pressing the trackball creates computer-generated images of sounds) which in turn affects the physical (as the images of sounds "hit" the piano keys they actually become depressed as though being played by an invisible hand) which in turn affects the virtual (piano keys generate another set of computer-generated images).

Another challenge faced by media art is how to integrate various media. By reducing everything to the same binary code, digital computer, at least in theory, gives the same importance to text, still images, video, and sounds. In reality, existing computer programs emphasize one type of media over others: DIRECTOR adopts the metaphor of a slide show, PREMIERE forces on its user the conventions of video editing, while World Wide Web documents are text-based. We are still waiting for a true digital Gesamtkunstwerk which will take full advantage of the ability to interweave the distinct languages of different media. Among ARTINACT works, Luc Courchesne's PORTRAIT ONE and Jean-Louis Boissier's FLORA PETRINSULARIS represent particularly successful solutions to this challenge. In Boissier's piece, we are presented with a white page, containing a list. A table of contents for a book? A list of chapters? Clicking on each item leads us to a pair of video loops, moving off-phase like waves; clicking on one of these takes us to yet another loop: a rhythmically vibrating water surface. The form of a loop which structures the work on a number of levels becomes a metaphor for human desire which can never achieve resolution. A loop, which gave birth to modern cinema (all pre-cinematic apparatuses were based on short loops consisting of a few images) and which was then banished to the low-art realm of cartoons, is resurrected by Boissier to become a fundamental element of a new multimedia language, an element capable of carrying rich and poetic meanings.

Courchesne's work elegantly combines the strengths of two visual traditions: modern graphic design and cinematic spectatorship. When a computer is waiting for our action, the black empty space between a silhouetted face of the character and sparsely positioned sentences becomes an active energy field — a negative space in the best tradition of modern design of still images. But as soon as a character begins to speak, we experience an intense cinematic identification which makes us mentally block the rest of the computer screen and even the rest of the room in which the computer is situated.

Another dichotomy which a number of works in NEWFOUNDLAND II begin to dissolve is between the traditions of collective and individualized viewing in screen-based arts. The first tradition span from magic lantern shows to twentieth-century cinemas. The second passes from the camera obscura, stereoscope and kinetoscope to head-mounted displays of VR. Both have their dangers. In the first tradition, individual's subjectivity can be dissolved in a mass-induced response. In the second, subjectivity is being defined through the interaction of an isolated subject with an object at the expense of intersubjective dialogue. In the case of viewers' interactions with ARTINTACT CD-ROMs, EVE and many of the installations in the show something quite new began to emerge: a combination of individualized and collective spectatorship. The interaction of one viewer with the work (via a joystick, a mouse, or a head mounted sensor) became in itself a new text for other viewers, situated within the work's arena, so to speak. This affects the behavior of this viewer who acts as a representative for the desires of others, and who is now oriented both to them and to the work.

EVE explores this situation most self-consciously. Its enclosed round shapes refer us back to the fundamental modern desire to construct a perfect self-sufficient utopia, whether visual (the nineteenth-century panorama) or social (after 1917 Russian Revolution G.I. Gidoni designed a monument to the Revolution in the form of a semi-transparent globe which could hold several thousand spectators). Yet, rather than being presented with a simulated world which has nothing to do with the real space of the viewer (as in typical VR), the visitors who enter EVE's enclosed space discover that EVE's apparatus shows the outside reality they just left. Moreover, instead of being fused in a single collective vision (Gesamtkunstwerk, cinema, mass society), the visitors are confronted with a subjective and partial view. The visitors only see what one person wearing a head-mounted sensor chooses to show them, i.e., they are literally limited by this person's point of view. In addition, instead of a 360o view, they see a small rectangular image — a mere sample of the world outside. This visitor wearing a sensor, and thus literally acting as an eye for the rest of the audience, occupies many positions at once — a master subject, a visionary who shows the audience what is worth seeing and at the same time just an object, an interface between them and outside reality, i.e., a tool for others; a projector, a light and a reflector all at once. Similarly, EVE summarizes the whole Western history of simulation, functioning as a kind of Plato's cave in reverse: visitors progress from the real world inside the space of simulation where instead of mere shadows they are presented with a technologically enhanced (via stereo) image, which looks more real than their normal perceptions. 

A viewer reading a work of media art is typically asked to utilize many distinct and opposing cultural codes at once. These include conventions for dealing with unrelated objects and settings (an artwork in a gallery versus a piece of software on a computer), opposing traditions of presentation (a rectangular frame versus a panoramic view; a movie screen versus a book page; a collective versus individual form of exhibition), and different mental processes and actions (perception and contemplation versus interaction and learning). This act of reading is always dangerous; like an acrobat on a tight rope, the viewer can lose his equilibrium and fall into the gap between the multitude of codes, interpretive conventions and cognitive skills required of him. Yet, by successfully meditating on what was previously thought of as distinct and unrelated a media artist can also discover new aesthetic possibilities. NEWFOUNDLAND II exhibition has given us many such discoveries.

---

# Virtual Worlds: Report from Los Angeles

_author: Lev Manovich_
_year: 1995_

Welcome to a Virtual World! Strap on your avatar! Don't have the programming skills or time to build your own? No problem. We provide a complete library of pre-assembled characters; one of them is bound to fit you perfectly. Join the community of like-minded users who agree that three-dimensional space is sexier! Yes, there is nothing more liberating than flying through a 3D scene, executing risky maneuvers and going for the kill. Mountains and valleys can represent files on a network, financial investments, the enemy troops, the body of a virtual sex partner — it does not really matter. Zoom! Roll! Pitch! Not enough visual realism? For just an extra $9.95 a month you can update your rendering speed to a blistering 490,000 polygons a second, increasing the quality of the experience by a staggering 27.4%! And, for another $4.95 you will get a chance to try a new virtual world every month, including a mall, a brothel, the Sistine Chapel, Paris during the Revolution of 1789, and even the fully navigable human brain. A 3D networked virtual world is waiting for you; all we need is your credit card number.

This advertisement is likely to appear on your computer screen quite soon, if it has not already. Ten years after William Gibson's fictional description of cyberspace and five years after the first theoretical conferences on the subject, cyberspace is finally becoming a reality. More than that, it promises to become a new standard in how we interact with computers — a new way to work, communicate and play.

## Virtual Worlds: History and Current Developments

(If such words as SIMNET, VRML, Quicktime VR and WorldChat are familiar to you, skip to the next section.)

Although a few networked multi-user graphical virtual environments were constructed already in the 1980s, they were specialized projects involving custom hardware and designed for particular groups of users. In Lucasfilm's Habitat, described by its designers as a "many-player online virtual environment," few dozen players used their home Commodore 64 computers to connect to a central computer running a simulation of a two-dimensional animated world. The players could interact with the objects in this world as well as with each other's graphical representations (avatars). Conceptually similar to Habitat but much more upscale in its graphics was SIMNET (Simulation Network) developed by DARPA (U.S. Defense Advanced Research Projects Agency). SIMNET was probably the first working cyberspace — the first collaborative THREE-DIMENSIONAL virtual environment. It consisted of a number of individual simulators linked by a high-speed network. Each simulator contained a copy of the same world database and the virtual representations of all the other simulators. In one of SIMNET's implementations, over two hundred M-1 tank crews, located in Germany, Washington D.C., Fort Knox, and other places around the world, were able to participate in the same virtual battle.

I remember attending a panel at a SIGGRAPH conference where a programmer who worked for Atari in the early 1980s argued that the military stole the idea of cyberspace from the games industry, modeling SIMNET after already existing civilian multi-participant games. With the end of the Cold War, the influences are running in the opposite way. Many companies that yesterday supplied very expensive simulators to the military are busy converting them into location-based entertainment systems (LBE). In fact, one of the first such systems which opened in Chicago in 1990 — BattleTech Center from Virtual World Entertainment, Inc. — was directly modeled on SIMNET. Like SIMNET, BattleTech Center comprised a networked collection of futuristic cockpit models with VR gear. For $7 each, a number of players could fight each other in a simulated 3D environment. By 1995, Virtual World was operating dozens of centers around the world that, also like SIMNET, depended on proprietary software and hardware.

In contrast to such custom-built and expensive location-based entertainment systems, the Internet provides a structure for 3D cyberspace that can simultaneously accommodate millions of users, which is inherently modifiable by them and which runs on practically every computer. A number of researchers and companies are already working to turn this possibility into reality.

Most important among the attempts to spatialize the Net is VRML (The Virtual Reality Modeling Language), conceived in the spring of 1994. According to the document defining Version 1.0 (May 26, 1995), VRML is "a language for describing multi-participant interactive simulations — virtual worlds networked via the global Internet and hyperlinked with the World Wide Web." Using VRML, Internet users can construct 3D scenes hyperlinked to other scenes and to regular Web documents. In other words, 

3D space becomes yet another media accessible via the Web, along with text, sounds, and moving images. But eventually, a VRML universe may subsume the rest of the Web inside itself. So while currently the Web is dominated by pages of text, with other media elements (including VRML 3D scenes) linked to it, future users may experience it as one gigantic 3D world which will contain all other media, including text, inside itself. This is certainly the vision of VRML designers who aim to "create a unified conceptualization of space spanning the entire Internet, a spatial equivalent of WWW." They see VRML as a natural stage in the evolution of the Net from an abstract data network toward a "perceptualized" Internet where the data has been sensualized," i.e., represented in three dimensions.

VRML 1.0 makes possible the creation of networked 3D worlds but it does not allow for the interaction between its users. Another direction in building cyberspace has been to add graphics to already popular Internet systems for interaction, such as chat lines and MUDs. Worlds Inc. which advertises itself as "a publisher of shared virtual environments" has created WorldChat, a 3D chat environment which has been available on the Internet since April 1995. Users first choose their avatars and then enter the virtual world (a space station) where they can interact with other avatars. The company imagines "the creation of 3-D worlds, such as sports bars, where people can come together and talk about or watch sporting events online, or shopping malls." Another company, Ubique, created technology called Virtual Places which also allows the users to see and communicate with other users' avatars and even take tours of the Web together.

Currently, the most ambitious full-scale 3D virtual world on the Internet is AlphaWorld, sponsored by Worlds Inc. At the time of this writing, it featured 200,000 buildings, trees and other objects, created by 4,000 Internet users. The world includes a bar, a store which provides prefabricated housing, and news kiosks which take you to other Web pages.

The movement toward spatialization of the Internet is not an accident. It is part of a larger trend in cyberculture — spatialization of ALL representations and experiences. This trend manifests itself in a variety of ways.

The designers of human-computer interfaces are moving from 2D toward 3D — from flat desktops to rooms, cities, and other spatial constructs. Web designers also often use pictures of buildings, aerial views of cities, and maps as front ends in their sites. Apple promotes Quicktime VR, a software-only system which allows the user of any personal computer to navigate a spatial environment and interact with 3D objects.

Another example is the emergence of a new field of scientific visualization devoted to spatialization of data sets and their relationships with the help of computer graphics. As the designers of human-computer interfaces, the scientists assume that spatialization of data makes working with it more efficient, regardless of what this data is.

Finally, in many computer games, from the original "Zork" to the best-selling CD-ROM "Myst," narrative and time itself are equated with the movement through space (i.e., going to new rooms or levels.) In contrast to modern literature, theater, and cinema which are built around the psychological tensions between characters, these computer games return us to the ancient forms of narrative where the plot is driven by the SPATIAL movement of the main hero, traveling through distant lands to save the princess, to find the treasure, to defeat the Dragon, and so on.

A similar spatialization of narrative has defined the field of computer animation throughout its history. Numerous computer animations are organized around a single, uninterrupted camera move through a complex and extensive set. A camera flies over mountain terrain, moves through a series of rooms, maneuvers past geometric shapes, zooms out into open space, and so on. In contrast to ancient myths and computer games, this journey has no goal, no purpose. It is an ultimate "road movie" where the navigation through the space is sufficient in itself.

## Aesthetics of Virtual Worlds?

The computerization of culture leads to the spatialization of all information, narrative, and even time. Unless this overall trend is to suddenly reverse, the spatialization of cyberspace is next. In the words of the scientists from Sony's The Virtual Society Project, "It is our belief that future online systems will be characterized by a high degree of interaction, support for multi-media, and most importantly the ability to support shared 3D spaces. In our vision, users will not simply access textual-based chat forums, but will enter into 3D worlds where they will be able to interact with the world and with other users in that world."

What will be the visual aesthetics of spatialized cyberspace? What would these 3D worlds look like? 

---

# Zeuxis meets RealityEngine -  Digital Realism and Virtual Worlds

_author: Lev Manovich_
_year: 1996_

## Prologue

How is the realism of a synthetic image different from the realism of the optical media? Is digital technology in the process of redefining our standards of realism as determined by our experience with photography and film? Do computer games, motion simulators and VR represent a new kind of realism which relies not only on visual illusion but also on the bodily, multi-sensory engagement of the user with a simulated world?

Some of my previous writings addressed these questions in relation to digital cinema, computer animation and digital photography. In this essay, I will discuss a number of characteristics which define visual realism in virtual worlds. By virtual worlds I mean 3D computer-generated interactive environments accessible to one or more users simultaneously. This definition fits a whole range of 3D computer environments already in existence: high-end VR works which feature head-mounted displays and photorealistic graphics generated by RealityEngines or similar expensive computers; arcade, CD-ROM and online multi-player computer games; low-end "desktop VR" systems such as QuickTime VR movies or VRML worlds, which increasingly populate the World Wide Web; graphical chat environments available on the Internet and most other major computer networks. More examples will be available in the near future; indeed, 3D environments represent a growing trend across computer culture, promising to become a new standard in human-computer interfaces and in computer networks. What follows then are a few tentative propositions on digital realism in virtual worlds.

## 1. Realism as Commodity

Digit in Latin means number. Digital media represents everything as numbers. 

This basic property of digital media has a profound effect on the nature of visual realism. In a digital representation, all dimensions that affect the reality effect — detail, tone, color, shape, movement — are quantified. As a consequence, the reality effect produced by the representation can itself be related to a set of numbers.

What are the dimensions which determine realism of a virtual world? First of all, it is determined by spatial and color resolution of images seen by the user, i.e., the number of pixels and the number of colors being used. For instance, given the same scene, a 640 x 480 image will contain more detail and therefore will produce a stronger reality effect than a 120 x 160 image. Second, since the world is modeled with 3D computer graphics, the number of geometric points each object is composed of, i.e. its 3D resolution, also affects the reality effect. 

Once the user begins to interact with a world, navigating through space or inspecting the objects in it, other dimensions come into play. One is temporal resolution — the number of frames a computer can generate in a second (the larger the number, the smoother the resulting motion). Another is the speed of the system's response: if the user clicks on an image of a door to open it or asks a virtual character a question, a delay in response breaks the illusion.

All these dimensions are quantifiable. The number of colors in an image, the temporal resolution the system is capable of and so on can be specified in exact numbers. For example, a particular VR system may be capable of displaying images limited to 256 colors at a resolution of 320 x 240 pixels.

These numbers also reflect something else: the cost involved. More bandwidth, higher resolution, faster processing result in a stronger reality effect — and cost more.

The bottom line: the reality effect of a digital representation can be measured in dollars. Realism has become a commodity. It can be bought and sold like anything else.

Not surprisingly, all these numbers are prominently displayed in the advertisements for graphics software and hardware. Even more importantly, those in the business of visual realism — the producers of special effects, military trainers, digital photographers, television designers — now have definite measures for what they are buying and selling. For instance, the Federal Aviation Administration which creates the standards for simulators to be used in pilot training specifies the required realism in terms of 3D resolution. In 1991 it required that for daylight, a simulator must be able to produce a minimum of 1000 surfaces or 4000 points.

Michael Baxandall describes how the price of a painting in the fourteenth century Italy reflected the quantities of expensive colors (such as gold and ultramarine) being used in it. By the end of the twentieth century, it became possible to delegate to a computer both the recipes for producing images as well as their pricing. The users can be billed for number of pixels and points, for CPU cycles, for bandwidth and so on.

It is likely that this situation will be explored by the designers of virtual worlds. If today users are charged for the connection time, in the future they can be charged for visual aesthetics and the quality of the overall experience: spatial resolution; number of colors; complexity of characters (both geometric and psychological); and so on. Since all these dimensions are specified in software, it becomes possible to automatically adjust the appearance of a virtual world on the fly, boosting it up if a customer is willing to pay more.

In this way, the logic of pornography will be extended to the culture at large. Peep shows and sex lines charge their customers by the minute, putting a precise cost on each bit of pleasure. In future virtual worlds, all dimensions of reality will be quantified and priced separately. 

Neal Stephenson's 1992 novel "Snow Crash" provides us with one possible scenario of such a future. Entering the Metaverse, the spatialized Net of the future, the hero sees "a liberal sprinkling of black-and-white people — persons who are accessing the Metaverse through cheap public terminals, and who are rendered in jerky, grainy black and white." He also encounters couples who can't afford custom avatars and have to buy off-the-shelf models, poorly rendered and capable of just a few standard facial expressions — virtual world equivalents of Barbie dolls. 

This scenario is gradually becoming a reality. A number of online stock photo services already provide their users with low-resolution photographs for a small cost, charging more for higher resolution copies. A company called Viewpoint Datalabs International is selling thousands of ready-to-use 3D geometric models widely used by computer animators and designers. Its catalog describes the models as follows: "VP4370: Man, Extra Low Resolution. VP4369: Man, Low Resolution. VP4752: Man, Muscular in Shorts and Tennis Shoe. VP5200. Man, w/Beard, Boxer Shorts..." For most popular models you can choose between different versions, with more detailed versions costing more than less detailed ones.

## 2. Romanticism and Photoshop Filters: From Creation to Selection

Viewpoint Datalabs' models exemplify another characteristic of virtual worlds: they are not created from scratch but assembled from ready-made parts. Put differently, in digital culture creation has been replaced by selection. 

E. H. Gombrich's concept of a representational schema and Roland Barthes' "death of the author" helped to sway us from the romantic ideal of the artist creating totally from scratch, pulling images directly from her imagination. As Barthes puts it, "The Text is a tissue of quotations drawn from the innumerable centers of culture." Yet, even though a modern artist may be only reproducing or, at best, combining in new ways preexisting texts and idioms, the actual material process of art making supports the romantic ideal. An artist operates like God creating the Universe — he or she starts with an empty canvas or a blank page. Gradually filling in the details, the artist brings a new world into existence.

Such a process of art making, manual and painstakingly slow, was appropriate for the age of pre-industrial artisan culture. In the twentieth century, as the rest of the culture moved to mass production and automation, literally becoming a "culture industry," art continued to insist on its artisan model. Only in the 1910s when some artists began to assemble collages and montages from already existing cultural "parts," was art introduced to the industrial method of production. 

In contrast, electronic art from its very beginning was based on a new principle: modification of an already existing signal. The first electronic instrument designed in 1920 by the legendary Russian scientist and musician Leon Theremin contained a generator producing a sine wave; the performer simply modified its frequency and amplitude. In the 1960s video artists began to build video synthesizers based on the same principle. The artist was no longer a romantic genius generating a new world purely out of her imagination; he became a technician turning a knob here, pressing switch there — an accessory to the machine.

Substitute a simple sine wave by a more complex signal (sounds, rhythms, melodies); add a whole bank of signal generators and you have arrived at a modern music synthesizer, the first instrument which embodies the logic of all new media: selection from a menu of choices. 

The first music synthesizers appeared in the 1950s, followed by video synthesizers in the 1960s, followed by DVE (Digital Video Effects) in the late 1970s — the banks of effects used by video editors; followed by computer software such as 1984 MacDraw that came with a repertoire of basic shapes. The process of art making has finally caught up with modern times. It has become synchronized with the rest of modern society where everything is assembled from ready-made parts; from objects to people's identities. The modern subject proceeds through life by selecting from numerous menus and catalogs of items — be it assembling an outfit, decorating the apartment, choosing dishes from a restaurant menu, choosing which interest groups to join. With electronic and digital media, art-making similarly entails choosing from ready-made elements: textures and icons supplied by a paint program; 3D models which come with a 3D modeling program; melodies and rhythms built into a music program. 

While previously the great text of culture from which the artist created her or his own unique "tissue of quotations" was bubbling and shimmering somewhere below the consciousness, now it has become externalized (and greatly reduced in the process) — 2D objects, 3D models, textures, transitions, effects which are available as soon as the artist turns on the computer. The World Wide Web takes this process to the next level: it encourages the creation of texts that completely consist of pointers to other texts that are already on the Web. One does not have to add any original writing; it is enough to select from what already exists. Put differently, now anybody can become a creator by simply providing a new menu, i.e. by making a new selection from the total corpus available. 

The same logic applies to much of interactive art and media. It is often claimed that a user of an interactive work becomes its co-author: by choosing a unique path through the elements of a work, she or he supposedly creates a new work. Yet, what the user is actually doing is only activating a part of the total work that already exists. If a complete work is a sum of all possible paths through its elements, then the user following a particular path only accesses a part of this whole. Just as with the Web example, rather than adding to a corpus, the user only selects from it. This is a new type of creativity which corresponds neither to pre-modern idea of providing minor modifications to the tradition nor to the modern idea of a creator-genius revolting against it; it does, however, fit perfectly with the age of mass culture, where almost every practical act involves choosing from some menu, catalog, or database. 

The shift from creation to selection also applies to 3D computer graphics — the main technique for building virtual worlds. The amount of labor involved in constructing three-dimensional reality from scratch in a computer makes it hard to resist the temptation to utilize pre-assembled, standardized objects, characters, and behaviors readily provided by software manufacturers — fractal landscapes, checkerboard floors, complete characters and so on. Every program comes with libraries of ready-to-use models, effects or even complete animations. For instance, a user of the Dynamation program (a part of the popular Wavefront 3D software) can access complete pre-assembled animations of moving hair, rain, a comet's tail or smoke, with a single mouse click. 

If even professional designers rely on ready-made objects and animations, the end users of virtual worlds on the Internet, who usually don't have graphic or programming skills, have no other choice. Not surprisingly, Web chat lines operators and virtual world providers encourage users to choose from the libraries of pictures, 3D objects, and avatars (graphic icons representing users in virtual worlds) they supply. Ubique's site features "Ubique Furniture Gallery" where one can choose images from such categories as "office furniture," "computers and electronics," and "people icons". VR-SIG from the U.K. provides VRML Object Supermarket while Aereal delivers the Virtual World Factory. The latter aims to make the creation of a custom virtual world particularly simple: "Create your personal world, without having to program! All you need to do is fill-in-the-blanks and out pops your world." Quite soon we will see a whole market for detailed virtual sets, characters with programmable behaviors, and even complete worlds (a bar with customers, a city square, a famous historical episode, etc.) from which a user can put together her or his own "unique" virtual world.

While a hundred years ago the user of a Kodak camera was asked just to push a button, she still had the freedom to point the camera at anything. Now, "you push the button, we do the rest" has become "you push the button, we create your world."

## 3. Brecht as Hardware

Another characteristic of virtual worlds lies in the peculiar temporal dynamic: constant, repetitive shifts between an illusion and its suspense. Virtual worlds keep reminding us about their artificiality, incompleteness, and constructedness. They present us with a perfect illusion only to reveal the underlying machinery next.

Web surfing circa 1996 provides a perfect example. A typical user may be spending equal time looking at a page and waiting for the next page to download. During waiting periods, the act of communication itself — bits traveling through the network — becomes the message. The user keeps checking whether the connection is being made, glancing back and forth between the animated icon and the status bar. Using Roman Jakobson's model of communication functions, we can say that communication comes to be dominated by contact, or phatic function — it is centered around the physical channel and the very act of connection between the addresser and the addressee.

Jakobson writes about verbal communication between two people who, in order to check whether the channel works, address each other: "Do you hear me?," "Do you understand me?" But in Web communication there is no human addresser, only a machine. So as the user keeps checking whether the information is coming, she actually addresses the machine itself. Or rather, the machine addresses the user. The machine reveals itself, it reminds the user of its existence — not only because the user is forced to wait but also because she is forced to witness how the message is being constructed over time. A page fills in part by part, top to bottom; text comes before images; images arrive in low resolution and are gradually refined. Finally, everything comes together in a smooth sleek image — the image which will be destroyed with the next click. 

Interaction with most 3D virtual worlds is characterized by the same temporal dynamic. Consider the technique called "distancing" or "level of detail," which for years has been used in VR simulations and is now being adapted to 3D games and VRML scenes. The idea is to render the models more crudely when the user is moving through virtual space; when the user stops, details gradually fill in. Another variation of the same technique involves creating a number of models of the same object, each with progressively less detail. When the virtual camera is close to an object, a highly detailed model is used; if the object is far away, a lesser detailed version is substituted to save unnecessary computation. 

A virtual world which incorporates these techniques has a fluid ontology that is affected by the actions of the user. As the user navigates through space the objects switch back and forth between pale blueprints and fully fleshed-out illusions. The immobility of a subject guarantees a complete illusion; the slightest movement destroys it. 

Navigating a QuickTime VR movie is characterized by a similar dynamic. In contrast to the nineteenth-century panorama that it closely emulates, QuickTime VR continuously deconstructs its own illusion. The moment you begin to pan through the scene, the image becomes jagged. And, if you try to zoom into the image, all you get are oversized pixels. The representational machine keeps hiding and revealing itself.

Compare this dynamic to traditional cinema or realist theater which aims at all costs to maintain the continuity of the illusion for the duration of the performance. In contrast to such totalizing realism, digital aesthetics have a surprising affinity to twentieth-century leftist avant-garde aesthetics. Bertold Brecht's strategy to reveal the conditions of an illusion's production, echoed by countless other leftist artists, has become embedded in hardware and software themselves. Similarly, Walter Benjamin's concept of "perception in the state of distraction" has found a perfect realization. The periodic reappearance of the machinery, the continuous presence of the communication channel in the message prevent the subject from falling into the dream world of illusion for very long, making her alternate between concentration and detachment. 

While virtual machinery itself already acts as an avant-garde director, the designers of interactive media (games, CD-ROM titles, interactive cinema, and interactive television programs) often consciously attempt to structure the subject's temporal experience as a series of periodic shifts. The subject is forced to oscillate between the roles of viewer and user, shifting between perceiving and acting, between following the story and actively participating in it. During one segment the computer screen presents the viewer with an engaging cinematic narrative. Suddenly the image freezes, menus and icons appear and the viewer is forced to act: make choices; click; push buttons. (Moscow media theorist Anatoly Prokhorov describes this process as the shift of the screen from being transparent to being opaque — from a window into a fictional 3D universe to a solid surface, full of menus, controls, text and icons. Three-dimensional space becomes surface; a photograph becomes a diagram; a character becomes an icon.) 

The effect of these shifts on the subject is hardly one of liberation and enlightenment. It is tempting to compare them to shot / counter-shot structure in cinema and to understand them as a new kind of suturing mechanism. By having periodically to complete the interactive text through active participation the subject is interpolated in it. 

Yet clearly we are dealing with something which goes beyond old realism. We can call this new realism meta-realism since it incorporates its own critique inside itself. Its emergence can be related to a larger cultural change. Old realism corresponded to the functioning of ideology during modernity: totalization of a semiotic field, "false consciousness," complete illusion. But today ideology functions differently: it continuously and skillfully deconstructs itself, presenting the subject with countless "scandals" and "investigations". Correspondingly, new meta-realism is based on oscillation between illusion and its destruction, between immersing a viewer in illusion and directly addressing her.

Can Brecht and Hollywood be married? Is it possible to create a new temporal aesthetic based on cyclical shifts between perception and action? So far, I can think of only one successful example — a military simulator, the only mature form of interactive media. It perfectly blends perception and action, cinematic realism and computer menus. The screen presents the subject with an illusionistic virtual world while periodically demanding quick actions: shooting at the enemy; changing the direction of a vehicle; and so on. In this art form, the roles of a viewer and an actant are blended perfectly — but there is a price to pay. The narrative is organized around a single and clearly defined goal: staying alive.  

## 4. Riegl, Panofsky, and Computer Graphics: Regression in Virtual Worlds

The last feature of virtual worlds that I will address can be summarized as follows: virtual spaces are not true spaces but collections of separate objects. Or: there is no space in cyberspace.

To explore this thesis further, we can borrow the categories developed by art historians early in this century. The founders of modern art history (Alois Riegl, Heinrich Wölfflin, and Erwin Panofsky) defined their field as the history of the representation of space. Working within the paradigms of cyclic cultural development and racial topology, they related the representation of space in art to the spirit of entire epochs, civilizations, and races. In his 1901 "Die Spätrömische Kunstindustrie," Riegl characterized humankind's cultural development as the oscillation between two extreme poles, two ways to understand space, which he called "haptic" and "optic". Haptic perception isolates the object in the field as a discrete entity, while optic perception unifies objects in a spatial continuum. Riegl's contemporary, Heinrich Wölfflin, similarly proposed that the temperament of a period or a nation expresses itself in a particular mode of seeing and representing space. Wölfflin's "Principles of Art History" (1913) plotted the differences between Renaissance and Baroque on five dimensions: linear — painterly; plane — recession; closed form — open form; multiplicity — unity; and clearness — unclearness. Finally, another founder of modern art history, Erwin Panofsky, contrasted the "aggregate" space of the Greeks with the "systematic" space of the Italian Renaissance in a famous essay "Perspective as a Symbolic Form" (1924-1925). Panofsky established a parallel between the history of spatial representation and the evolution of abstract thought. The former moves from the space of individual objects in antiquity to the representation of space as continuous and systematic in modernity; in Panofsky's neologisms, from "aggregate" space to "systematic" space. Correspondingly, the evolution of abstract thought progresses from ancient philosophy's view of the physical universe as discontinuous to the post-Renaissance understanding of space as infinite, ontologically primal in relation to bodies, homogeneous, and isotropic — in short, as "systematic".

We don't have to believe in grand evolutionary schemes but we can retain the categories themselves. What kind of space is a virtual space? At first glance, 3D computer graphics, the main technology of creating virtual spaces, exemplify Panofsky's concept of Renaissance "systematic" space which exists prior to the objects. Indeed, the Cartesian coordinate system is hardwired into computer graphics software and often into the hardware itself. When a designer launches a modeling program, she is typically presented with an empty space defined by a perspectival grid, the space that will be gradually filled by the objects she will create. If the built-in message of a music synthesizer is a sine wave, the built-in world of computer graphics is an empty Renaissance space, the coordinate system itself.

Yet computer-generated worlds are actually much more "haptic" and "aggregate" than "optic" and "systematic". The most commonly used 3D computer graphics technique to create 3D worlds is polygonal modeling. The virtual world created using this technique is a vacuum filled with separate objects defined by rigid boundaries. A perspective projection creates the illusion that these objects belong together but in fact, they have no connection to each other. What is missing is space in the sense of space-environment or space-medium: the environment between objects; an atmosphere which unites everything together; the effects of objects on each other. In short, computer space is the opposite of what Russian art historians call prostranstvennaya sreda, defined by Pavel Florensky who taught in Vkhutemas in the early 1920s as follows: "The space-medium is objects mapped onto space... We have seen the inseparability of Things and space, and the impossibility of representing Things and space by themselves." It is also the opposite of space as it is understood in much of modern art which, from Seurat to De Kooning, tried to eliminate the notions of a distinct object and empty space as such. Instead, it proposed a kind of dense field which sometimes harden into something which we can read as an object — an aesthetics which mainstream computer graphics has yet to discover.

Another basic technique used in creating virtual worlds — compositing (superimposing, keying)— also leads to an "aggregate" space. It involves superimposing animated characters, still images, QuickTime movies, and other graphical elements over a separate background. A typical scenario may involve an avatar animated in real time in response to the user's commands. The avatar is superimposed over a picture of a room. An avatar is controlled by the user; a picture of a room is provided by a virtual world operator. Because the elements come from different sources and are put together in real time, the result is a series of 2D planes rather than a real 3D environment.

In summary, although computer-generated virtual worlds are usually rendered in linear perspective, they are really collections of separate objects, unrelated to each other. In view of this, commonly expressed arguments that 3D computer graphics send us back to Renaissance perspectivalism and therefore, from the viewpoint of twentieth-century abstraction, should be considered regressive, turn out to be ungrounded. If we are to apply the evolutionary paradigm of Panofsky to the history of virtual computer space, it has not even achieved its Renaissance yet. It is still on the level of Ancient Greece which could not conceive of space as a totality.

And, if the World Wide Web and VRML 1.0 are any indications, we are not moving any closer toward systematic space; instead, we are embracing "aggregate" space as a new norm, both metaphorically and literally. The "space" of the Web in principle can't be thought of as a coherent totality: it is a collection of numerous files, hyperlinked but without any overall "perspective" to unite them. The same holds for actual 3D spaces on the Internet. A VRML file which describes a 3D scene is a list of separate objects which may exist anywhere on the Internet, each created by a different person or a different program. The objects have no connection to each other. And, since any user can add or delete objects, no one may even know the complete structure of the scene. 

The Web has already been compared to the American Wild West. The spatialized Web as envisioned by VRML (itself a product of California) even more closely reflects the treatment of space in American culture: the lack of attention to space which is not functionally used. The territories that exist between privately owned houses and businesses are left to decay. The VRML universe pushes these characteristics to the limit; it simply does not contain space as such — only objects which belong to different individuals. 

And what is an object in a virtual world? Something which can be acted upon: clicked; moved; opened — in short, used. It is tempting to interpret this as another case of regression — in this case, to the world view of an infant. A child does not think of the universe as existing separately from herself — it appears as a collection of unrelated objects with which he can enter in contact: touch; suck on; grab. Similarly, the user of a virtual world tries to click on whatever is in front of him; if the objects do not respond, he is disappointed. In the virtual universe, Descartes' maxim can be rewritten as follows: "I can be clicked on, therefore I exist." 

According to a well-known argument of Jean-Louis Baudry, immobility and confinement of cinema's viewers leads them to mistake representations for their perceptions; they regress back to childhood when the two were indistinguishable. Paradoxically, although interactive virtual worlds may appear to turn us into active adults, they actually reduce us once again to children — helplessly clicking on whatever is in front of us; participation becomes yet another kind of regression.

## Epilogue

Quantification of all visual and experiential dimensions; ready-made ontology; oscillation between illusion and its suspense; "aggregate" space — these are some of the features which distinguish reality as simulated by a computer. 

It should not be surprising that the same features characterize the "larger" reality beyond the computer screen. RealityEngine only caricatures, exaggerates, highlights the tendencies defining the experience of being alive in an advanced capitalist society, particularly in the U.S. The assumption that every aspect of experience can and should be quantified; the construction of one's identity from the menus of tastes, objects, affiliations; constant shifts between illusion and its suspense, be it commercial breaks on TV or endless "scandals" and "investigations" which disrupt the surface of an ideological field; the space lacking a unifying perspective, whether it is a space of an American city or the space of public discourse more and more fragmented by the competition of separate interest groups — all these experiences are transferred by computer designers into software and hardware they create. Rather than being a generator of new alternative reality, RealityEngine is a mirror of existing reality, a lens which focuses the culture around it. 

To the extent that Southern California, and particularly Los Angeles, brings to the extreme these tendencies of RL (real life) under American capitalism, we may expect L.A. to offer us a precise model of a virtual world, a physical equivalent to the fictions pumped out by the RealityEngines.

This is exactly the case. If you keep your visit to L.A. short and follow the standard tourist itinerary, you will discover a virtual world with all its features. There is no center, no hint of any kind of centralized organization, no traces of the hierarchy essential to traditional cities. One drives to particular locations defined strictly by their street addresses rather than by spatial landmarks. A trendy restaurant or club can be found in the middle of nowhere, among the miles of completely unremarkable buildings. The whole city feels like a set of particular points suspended in a vacuum, similar to a bookmark file of Web pages. You are immediately charged on arrival to any worthwhile location, again like on the Web (mandatory valet parking). There you discover the fashionable inhabitants (actors, singers, models, producers) who look like some new race, a result of successful mutation: unbelievably beautiful skin and faces; fixed smiles; and bodies whose perfect shapes surely can't be the result of human evolution. They probably come from the Viewpoint catalog of 3D models. These are not people but avatars: beautifully rendered with no polygons spared; shaped to the latest fashion; their faces switching between a limited number of expressions. Given the potential importance of any communicative contact, subtlety is not tolerated: avatars are designed to release stimuli the moment you notice them, before you have time to click to the next scene.

The best place to experience the whole gestalt is in one of the outdoor cafes on Sunset Plaza in West Hollywood. The avatars sip cappuccino amidst the illusion of 3D space. The space is clearly the result of a quick compositing job: billboards and airbrushed cafe interior in the foreground against a detailed matte painting of Los Angeles with the perspective exaggerated by haze. The avatars strike poses, waiting for their agents (yes, just like in virtual worlds) to bring valuable information. Older customers look even more computer generated, their faces bearing traces of extensive face-lifts. You can enjoy the scene while feeding the parking meter every twenty minutes.

A virtual world is waiting for you; all we need is your credit card number. RealityEngines are tirelessly cranking out the images, pushing pixels around to assure the smoothest possible experience. Enjoy RealityTM!

## References:

[1] For a detailed analysis of this story, see Stephen Bann, The True Vine. On Western Representation and the Western Tradition (Cambridge: Cambridge University Press, 1989).

[2] "What is Digital Cinema?" in The Digital Dialectics, edited by Peter Lunenfeld (Cambridge, Mass.: The MIT Press., forthcoming); "Archeology of a Computer Screen" (in German), in Kunstforum International 132 (November-January 1996): 124-135; "Paradoxes of Digital Photography" in Photography After Photography, edited by Hubertus von Amelunxen, Stefan Iglahut and Florian Rötzer (Berlin: Verlag der Kunst, 1995); "To Lie and to Act: Potemkin's Villages, Cinema and Telepresence," in Mythos Information — Welcome to the Wired World. Ars Electronica 95, edited by Karl Gebel and Peter Weibel (Vienna and New York: Springer-Verlag, 1995); "Assembling Reality: Myths of Computer Graphics," Afterimage 20, no. 2 (September 1992): 12-14; "Real" Wars: Esthetics and Professionalism in Computer Animation," Design Issues 6, no. 1 (Fall 1991): 18-25.

[3] Quicktime VR is a software-only system which allows the user of any Macintosh computer to navigate a spatial environment and interact with 3D objects. 

[4] VRML stands for The Virtual Reality Modeling Language. Using VRML, Internet users can construct 3D scenes and link them to other Web documents. For examples, see [http://www.worlds.net/info/aboutus.html](http://www.worlds.net/info/aboutus.html); [http://www.ubique.com](http://www.ubique.com); [http://www.thepalace.com](http://www.thepalace.com); [http://www.blacksun.com](http://www.blacksun.com); [http://www.worldsaway.ossi.com](http://www.worldsaway.ossi.com); [http://www.fpi.co.jp/Welcome.html](http://www.fpi.co.jp/Welcome.html); [http://www.wildpark.com](http://www.wildpark.com). 

[5] For instance, Silicon Graphics developed a 3D file system which was showcased in the movie Jurassic Park. The interface of Sony's MagicLink personal communicator is a picture of a room while Apple's E-World greets its users with a drawing of a city. Web designers often use pictures of buildings, aerial views of cities, and maps as front ends in their sites. In the words of the scientists from Sony's The Virtual Society Project [http://www.csl.sony.co.jp/project/VS/](http://www.csl.sony.co.jp/project/VS/), "It is our belief that future online systems will be characterized by a high degree of interaction, support for multi-media and most importantly the ability to support shared 3D spaces. In our vision, users will not simply access textual-based chat forums, but will enter into 3D worlds where they will be able to interact with the world and with other users in that world." 

[6] Barbara Robertson, "Those Amazing Flying Machines," Computer Graphics World (May 1992): 69.

[7] Michael Baxandall, Painting and Experience in Fifteenth-century Italy, 2nd ed. (Oxford and New York: Oxford University Press), 8.

[8] Neal Stephenson, Snow Crash (New York: Bantam Books, 1992), 43.

[9] Ibid., 37.

[10] [http://www.viewpoint.com](http://www.viewpoint.com)

[11] E.H. Gombrich, Art and Illusion (Princeton: Princeton University Press, 1960); Roland Barthes, "The Death of the Author," in Image, Music, Text, ed. Stephen Heath (New York: Farrar, Straus and Giroux, 1977).

[12] Barthes, 142.

[13] Bulat Galeyev, Soviet Faust. Lev Theremin — Pioneer Of Electronic Art (in Russian) (Kazan, 1995), 19.

[14] For a more detailed analysis of realism in 3D computer graphics, see Lev Manovich, "Assembling Reality: Myths of Computer Graphics," Afterimage 20, no. 2 (September 1992): 12-14.

[15] [http://www.ubique.com/places/gallery.html](http://www.ubique.com/places/gallery.html)

[16] [http://www.virtpark.com/factinfo.html](http://www.virtpark.com/factinfo.html)

[17] See Roman Jakobson, "Closing Statement: Linguistics and Poetics," in Style In Language, ed. Thomas Sebeok (Cambridge, Mass.: The MIT Press, 1960). 

[18] Walter Benjamin, "The Work of Art in the Age of Mechanical Reproduction," in Illuminations, ed. Hannah Arendt (New York: Schochen Books, 1969).

[19] Private communication, September 1995, St. Petersburg.

[20] On theories of suture in relation to cinema, see chapter 5 of Kaja Silverman, The Subject of Semiotics (New York: Oxford University Press, 1983).

[21] See Lev Manovich, "Mapping Space: Perspective, Radar and Computer Graphics," in SIGGRAPH '93 Visual Proceedings, ed. Thomas Linehan (New York: ACM, 1993.)

[22] Qtd. in Alla Efimova and Lev Manovich, "Object, Space, Culture: Introduction," in Tekstura: Russian Essays on Visual Culture, eds. Alla Efimova and Lev Manovich (Chicago: The University of Chicago Press, 1993), xxvi. 

[23] Jean-Louis Baudry, "The Apparatus: Metapsychological Approaches to the Impression of Reality in the Cinema," in Narrative, Apparatus, Ideology, ed. Philip Rosen (New York: Columbia University Press, 1986).

---
# On Totalitarian Interactivity

_author: Lev Manovich_
_year: 1996_

In "Art, Power, and Communication" (RHIZOME DIGEST: October 11, 1996. [http://www.rhizome.com](http://www.rhizome.com)) Alexei Shulgin writes:

Looking at very popular media art form such as "interactive installation" I always wonder how people (viewers) are excited about this new way of manipulation on them. It seems that manipulation is the only form of communication they know and can appreciate. They are happily following very few options given to them by artists: press left or right button, jump or sit. Their manipulator artists feel that and are using seduces of the newest technologies (future now!) to involve people in their pseudo-interactive games obviously based on banal will for power. But what nice words you can hear around it: interaction, interface for self-expression, artificial intelligence, communication even. So, emergence of media art is characterized by transition from representation to manipulation." 

Alexei Shulgin is right in analyzing the phenomenon of interactive art and media as a shift from representation to manipulation. Yes, interactive computer installations indeed represent an advanced form of audience manipulation, where the subject is put within a structure very similar to an experimental setup of a psychological laboratory or a high-tech torture chamber of CIA or KGB, the kind we saw frequently in spy films of the Cold War era. Yet — precisely because I — who was in Moscow and grew up there during Brezhnev’s era — I am so happy to agree with Shulgin's conclusions — I recognize the limitations of this analysis, or rather, its cultural specificity. It is only a post-communist subject who can see interactive art and media in these terms. (No surprisingly, in a conversation I had last year, another post-communist subject — art critic Boris Groys — analyzed interactive computer installations in a very similar way). 

The experiences of East and West structure how new media is seen in both places. For the West, interactivity is a perfect vehicle for the ideas of democracy and equality. For the East, it is another form of manipulation, in which the artist uses advanced technology to impose his / her totalitarian will on the people. (On modern artist as a totalitarian ruler see the works of Boris Groys.) Western media artists usually take technology absolutely seriously and despair when it does not work. Post-communist artists, on the other hand, recognize that the nature of technology is that it does not work, will always break down, will never work as it is supposed to... (For instance, Moscow conceptual artist and poet Dimity Prigov did an event during ISEA '94 in which he used business translation programs to translate a famous nineteenth Russian poem by Pushkin from Russian into Finnish and then from Finnish into English; he declared the mistakes in translation a new work of art.) A Western artist sees the Internet as a perfect tool to break down all hierarchies and bring the art to the people (while in reality more often than not using it as a super-media to promote his / her name). In contrast, as a post-communist subject, I cannot but see the Internet as a communal apartment of the Stalin era: no privacy, everybody spies on everybody else, always present line for common areas such as the toilet or the kitchen. Or I can think of it as a giant garbage site for the information society, with everybody dumping their used products of intellectual labor and nobody cleaning up. Or as a new, Mass Panopticon (which was already realized in communist societies) — complete transparency, everybody can track everybody else.

I apologize if I am making you mad. I promise to write on the blackboard, until the chalk runs out: Internet is good for the people, the Internet is good for the people, the Internet is good for the people, the Internet is good for the people. Down with the Museum, Down with the Museum, Down with the Museum, Down with the Museum. Workers of the World, Connect; Workers of the World, Connect; Workers of the World, Connect; Workers of the World, Connect. I promise to march in happy columns, screaming slogans, my face reflecting the shiny pixels of new version of Netscape browser. Ideology, history, class struggle are finally over, replaced by Microsoft vs. Netscape war and Java objects. Long Live Digital Revolution!

But before I give in, I would like to offer you one more thought, the last download from "the enemy of the people" — one more argument about interactivity as a totalitarian art form. All classical, and even more so modern art was already "interactive," requiring a viewer to fill in missing information (for instance, ellipses in literary narration; "missing" parts of objects in modernist painting) as well as to move his / her eyes (composition in painting and cinema) or the whole body (in experiencing sculpture and architecture). Computer interactive art takes "interaction" literally, equating it with strictly physical interaction between a user and an artwork (pressing a button), at the sake of psychological interaction. The psychological processes of filling-in, hypothesis forming, recall and identification — which are required for us to comprehend any text or image at all — are mistakenly identified strictly with an objectively existing structure of interactive links.

This literal quality can be seen as another example of a larger modern trend of externalization of mental life, the process in which new media technologies — photography, film, VR — have played a key role. On the one hand, we witness recurrent claims by the users and theorists of new media technologies, from Francis Galton (the inventor of composite photography in the 1870s) to Hugo Munsterberg, Sergei Eisenstein and, recently, Jaron Lanier, that these technologies externalize and objectify the mind. On the other hand, modern psychological theories of the mind, from Freud to cognitive psychology, also equate mental processes with external, technologically generated visual forms. Interactive computer media perfectly fits in this trend. Mental processes of reflection, problem solving, memory and association are externalized, equated with following a link, moving to a new image, choosing a new scene or a text. In fact, the very principle of new media — links — objectifies the process of human thinking which involves connecting ideas, images, memories. Now, with interactive media, instead of looking at a painting and mentally following our own private associations to other images, memories, ideas, we are asked to click on the image on the screen in order to go to another image on the screen, and so on. Thus we are asked to follow pre-programmed, objectively existing associations. In short, in what can be read as a new updated version of Althusser's "interpolation," we are asked to mistake the structure of somebody's else mind for our own.

This is a new kind of identification appropriate for the information age of cognitive labor. The cultural technologies of an industrial society — cinema and fashion — asked us to identify with somebody's bodily image. The interactive media asks us to identify with somebody's else mental structure.

P.S. I develop the arguments about modern media technologies and externalization of mental life in more detail in "From the Externalization of the Psyche to the Implantation of Technology." In _Mind Revolution: Interface Brain/Computer_, edited by Florian Rötzer, 90-100. M ünchen: Akademie Zum Dritten Jahrtausend, 1995.

---
# Automation of Sight: From Photography to Computer Vision

_author: Lev Manovich_
_year: 1997_

## Prologue

Nothing perhaps symbolizes mechanization as dramatically as the first assembly lines installed by Henry Ford in U.S. factories in 1913. It seemed that mechanical modernity was at its peak. Yet, in the same year the Spanish inventor Leonardo Torres y Quevedo had already advocated the industrial use of programmed machines. [1] He pointed out that although automatons existed before, they were never used to perform useful work: 

The ancient automatons ...imitate the appearance and movement of living beings, but this has not much practical interest, and what is wanted is a class of apparatus which leaves out the merely visible gestures of man and attempts to accomplish the results which a living person obtains, thus replacing a man by a machine. [2]

With mechanization, work is performed by a human but his or her physical labor is augmented by a machine. Automation takes mechanization one step further: the machine is programmed to replace the functions of human organs of observation, effort, and decision.

Mass automation was made possible by the development of digital computers during World War II and thus became synonymous with computerization. The term "automation" was coined in 1947; and in 1949 Ford began the construction of the first automated factories.

Barely a decade later, automation of imaging and of vision was well underway. By the early 1960s, construction of static and moving two-dimensional and perspectival images, correction of artifacts in photographs, the identification of objects from their images, and many other visual tasks were already handled by computers. A number of new disciplines were emerging as well — computer image processing, computer vision, computer graphics, computer-aided design.

What these new disciplines had all in common is that they employed perspectival images. In other words, automation of imaging and vision was first of all directed at perspectival sight.

The reasons for this are two-fold. On the one hand, by the time digital computers became available, modern society was already heavily invested in lens-based methods of image gathering (photography, film, television) which all produced perspectival images. Therefore, it is not surprising that it would want first of all to automate various uses of such images in order to obtain a new return from its investment. On the other hand, the automation of perspectival sight has already begun well before this century with the development of perspective machines, descriptive and perspective geometry and, of course, photography. Computers certainly proved to be very fast perspectival machines, but they were hardly the first.

## Perspective, Perspectival Machines, Photography

From the moment of adaptation of perspective, artists and draftsmen have attempted to aid the laborious manual process of creating perspectival images. [3] Between the sixteenth and the nineteenth century various "perspectival machines" were constructed. They were used to construct particularly challenging perspectival images, to illustrate the principles of perspective, to help students learn how to draw in perspective, to impress artists' clients, or to serve as intellectual toys. Already in the first decades of the sixteenth century, Dürer described a number of such machines. [4] One device is a net in the form of a rectangular grid, stretched between the artist and the subject. Another uses a string representing a line of sight. The string is fixed on one end, while the other end is moved successively to key points on the subject. The point where the string crosses the projection plane, defined by a wooden frame, is recorded by two crossed strings. For each position, a hinged board attached to the frame is moved and the point of intersection is marked on its surface. It is hard to claim that such a device, which gave rise to many variations, made the creation of perspectival images more efficient, however, the images it helped to produce had reassuring mechanical precision. Other major types of perspectival machines that appeared subsequently included the perspectograph, pantograph, physionotrace, and optigraph.

Why manually move the string imitating the ray of light from point to point? Along with perspectival machines a whole range of optical apparatuses was in use, particularly for depicting landscapes and conducting topographic surveys. They included versions of camera obscura from large tents to smaller, easily transportable boxes. After 1800, the artist's ammunition was strengthened by camera lucida, patented in 1806. [5] Camera lucida utilized a prism with two reflecting surfaces at 135˚. The draftsman carefully positioned his eye to see both the image and the drawing surface below and traced the outline of the image with a pencil.

Optical apparatuses came closer than previous perspectival devices to the automation of perspectival imaging. However, the images produced by camera obscura or camera lucida were only ephemeral and considerable effort was still required to fix these images. A draftsman had to meticulously trace the image to transform it into the permanent form of a drawing.

With photography, this time-consuming process was finally eliminated. The process of **imaging physical reality**, the creation of perspectival representations of real objects was now automated. Not surprisingly, photography was immediately employed in a variety of fields, from aerial photographic surveillance to criminal detection. Whenever the real had to be captured, identified, classified, stored, photography was put to work.

Photography automated one use of perspectival representation — but not others. According to Bruno Latour, the greatest advantage of perspective over other kinds of representations is that it establishes a "four-lane freeway" between physical reality and its representation. [6] We can combine real and imagined objects in a single geometric model and go back and forth between reality and the model. By the twentieth century, the creation of a geometric model of both existing and imagined reality still remained a time-consuming manual process, requiring the techniques of perspectival and analytical geometry, pencil, ruler, and eraser. Similarly, if one wanted to visualize the model in perspective, hours of drafting were required. And to view the model from another angle, one had to start all over again. The automation of geometrical modeling and display had to wait for the arrival of digital computers.

## 3-D Computer Graphics: Automation of Perspectival Imaging

Digital computers were developed towards the end of World War II. The automation of the process of constructing perspectival images of both existent and non-existent objects and scenes followed quite soon. [7] By the early 1960s Boeing designers already relied on 3-D computer graphics for the simulation of landings on the runway and of pilot movement in the cockpit. [8]

By automating perspectival imaging, digital computers completed the process which began in the Renaissance. This automation became possible because perspectival drawing has always been a step-by-step procedure, an algorithm involving a series of steps required to project coordinates of points in 3-D space onto a plane. Before computers the steps of the algorithm were executed by human draftsmen and artists. With a computer, these steps can be executed automatically and, therefore, much more efficiently.

The details of the actual perspective-generating algorithm which could be executed by a computer were published in 1963 by Lawrence G. Roberts, then a graduate student at MIT. [9] The perspective-generating algorithm constructs perspectival images in a manner quite similar to traditional perspectival techniques. In fact, Roberts had to refer to German textbooks on perspectival geometry from the early 1800s to get the mathematics of perspective. [10] The algorithm reduces reality to solid objects, and the objects are further reduced to planes defined by straight lines. The coordinates of the endpoint of each line are stored in a computer. Also stored are the parameters of a virtual camera — the coordinates of a point of view, the direction of sight, and the position of a projection plane. Given this information, the algorithm generates a perspectival image of an object, point by point.

The subsequent development of computer graphics can be seen as the struggle to automate other operations involved in producing perspectival stills and moving images. The computerization of perspectival construction made possible the automatic generation of a perspectival image of a geometric model as seen from an arbitrary point of view — a picture of a virtual world recorded by a virtual camera. But, just like with the early perspectival machines described by Dürer, early computer graphics systems did not really save much time over traditional methods. To produce a film of a simulated landing, Boeing had to supplement computer technology with manual labor. As in traditional animation, twenty-four plots were required for each second of film. These plots were computer-generated and consisted of simple lines. Each plot was then hand-colored by an artist. Finished plots were filmed, again manually, on an animation stand. [11] Gradually, throughout the 1970s and the 1980s, the coloring stage was automated as well. Many algorithms were developed to add the full set of depth cues to a synthetic image — hidden line and hidden surface removal, shading, texture, atmospheric perspective, shadows, reflections, and so on. [12]

At the same time, to achieve interactive perspectival display, special hardware was built. Each step in the process of 3-D image synthesis was delegated to a special electronic circuit: a clipping divider, a matrix multiplier, a vector generator. Later on, such circuits became specialized computer chips, connected together to achieve real-time, high resolution, photorealistic 3-D graphics. Silicon Graphics Inc., one of the major manufacturers of computer graphics hardware, labeled such a system "geometry engine".

The term appropriately symbolizes the second stage of the automation of perspectival imaging. At the first stage, the photographic camera, with perspective physically built into its lens, automated the process of creating perspectival images of existing objects. Now, with the perspectival algorithm and other necessary geometric operations embedded in silicon, it becomes possible to display and interactively manipulate models of non-existent objects as well.	

## Computer Vision: Automation of Sight

In his papers, published between 1963 and 1965, Roberts formalized the mathematics necessary for generating and modifying perspective views of geometric models on the computer. This, writes William J. Mitchell, was "an event as momentous, in its way, as Brunelleschi's perspective demonstration." [13] However, Roberts developed techniques of 3-D computer graphics having in mind not the automation of perspectival imaging but another, much more daring goal — "to have the machine recognize and understand photographs of three-dimensional objects." [14] Thus, the two fields were born simultaneously: 3-D computer graphics and computer vision, **automation of imaging and of sight**.

The field of computer vision can be seen as the culmination of at least two centuries-long histories. The first is the history of mechanical devices designed to aid human perception, such as Renaissance perspectival machines. This history reaches its final stage with computer vision, which aims to replace human sight altogether. The second is the history of automata, whose construction was especially popular in the seventeenth and eighteenth centuries. Yet, despite the similarity in appearance, there is a fundamental difference between Enlightenment automata which imitated human's or animal's bodily functions and the modern-day robots equipped with computer vision systems, artificial legs, arms, etc. As noted by Leonardo Torres, old automata, while successfully copying the appearance and movement of living beings, had no economic value. Indeed, such voice synthesis machines as Wolgang von Kempelen's 1778 device which directly imitated the functioning of the oral cavity, or Abbé Mical's **Têtes Parlantes** (1783) operated by a technician hiding offstage and pressing a key on a keyboard were used only for entertainment. [15] When in 1913 Torres called for automata that would "accomplish the results which a living person obtains, thus replacing a man by a machine" he was expressing a fundamentally new idea of using automata for productive labor. A few years later, the brother of the Czech writer Karel Capek coined the word **robot** from the Czech word **robota**, which means "forced labor". [16] Capek's play **R.U.R.** (1921) and Fritz Lang's **Metropolis** (1927) clearly demonstrate this new association of automata with physical industrial labor.

Therefore, it would be erroneous to conclude that, with computer vision, twentieth-century technology simply added the sense of sight to eighteenth-century mechanical statues. But even to see computer vision as the continuation of Torres', Capek's, or Lang's ideas about industrial automation which replaces manual labor would not be fully accurate. The idea of computer vision became possible and the economic means to realize this idea became available only with the shift from industrial to post-industrial society after World War II. The attention turned from the automation of the body to the automation of the mind, from physical to mental labor. This new concern with the automation of mental functions such as vision, hearing, reasoning, problem-solving is exemplified by the very names of the two new fields that emerged during the 1950s and 1960s — artificial intelligence and cognitive psychology. The latter gradually replacing behaviorism, the dominant psychology of the "Fordism" era. The emergence of the field of computer vision is a part of this cognitive revolution, a revolution which was financed by the military escalation of the Cold War. [17] This connection is solidified in the very term "artificial intelligence" which may refer simultaneously to two meanings of "intelligence": reason, the ability to learn or understand, and information concerning an enemy or a possible enemy or an area. Artificial intelligence: artificial reason to analyze collected information, collected intelligence.

In the 1950s, faced with the enormous task of gathering and analyzing written, photographic, and radar information about the enemy, the CIA and the NSA (National Security Agency) began to fund the first artificial intelligence projects. One of the earliest projects was a Program for Mechanical Translation, initiated in the early 1950s in the attempt to automate the monitoring of Soviet communications and media. [18] The work on mechanical translation was probably the major cause of many subsequent developments in modern linguistics, its move towards formalization; it can be discerned in Noam Chomsky's early theory which, by postulating the existence of language universals in the domain of grammar, implied that translation between arbitrary human languages could be automated. The same work on mechanical translation was also one of the catalysts in the development of the field of pattern recognition, the precursor to computer vision. Pattern recognition is concerned with automatically detecting and identifying predetermined patterns in the flow of information. A typical example is character recognition, the first stage in the process of automating translation. Pattern recognition was also used in the U.S. for the monitoring of Soviet radio and telephone communication. Instead of listening to every transmission, an operator would be alerted if computer picked up certain words in the conversation.

As a "logistics of perception" came to dominate modern warfare and surveillance and as the space race began, image processing became another major new field of research. [19] Image processing comprises techniques to improve images for human or computer interpretation. In 1964, the space program for the first time used image processing to correct distortions in the pictures of the Moon introduced by an onboard television camera of Ranger 7. [20] In 1961, the National Photographic Interpretation Center (NPIC) was created to produce photoanalysis for the rest of the U.S. intelligence community and, as Manual De Landa points out, by the end of the next decade computers "were routinely used to correct for distortions made by satellite's imaging sensors and by atmospheric effects, sharpen out-of-focus images, bring multicolored single images out of several pictures taken in different spectral bands, extract particular features while diminishing or eliminating their backgrounds altogether..." De Landa also notes that computer analysis of photographic imagery became the only way to deal with the pure volume of intelligence being gathered: "It became apparent during the 1970s that there is no hope of keeping up with the millions of images that poured into NPIC ... by simply looking at them the way they had been looked at in World War II. The computers therefore also had to be taught to compare new imagery of a given scene with old imagery, ignoring what had not changed and calling the interpreter's attention to what had." [21]

The techniques of image processing, which can automatically increase an image's contrast, remove the effects of blur, extract edges, record differences between two images, and so on, greatly eased the job of human photoanalysts. And the combining of image processing with pattern recognition made it possible in some cases to delegate the analysis of photographs to a computer. For instance, the technique of pattern matching used to recognize printed characters can also be used to recognize objects in a satellite photograph. In both cases, the image is treated as consisting of two-dimensional forms. The contours of the forms are extracted from the image are then compared to templates stored in computer memory. If a contour found in the image matches a particular template, the computer signals that a corresponding object is present in a photograph.

A general-purpose computer vision program has to be able to recognize not just two-dimensional but three-dimensional objects in a scene taken from an arbitrary angle. [22] Only then it can be used to recognize an enemy's tank, to guide an automatic missile towards its target, or to control a robotic arm on the factory floor. The problem with using simple template matching is that "a two-dimensional representation of a two-dimensional object is substantially like the object, but a two-dimensional representation of a three-dimensional object introduces a perspective projection that makes the representation ambiguous with respect to the object." [23] While pattern recognition was working for images of two-dimensional objects, such as letters or chromosomes, a different approach was required to "see" in 3-D.

Roberts' 1965 paper "Machine Recognition of Three-dimensional Solids" is considered to be the first attempt at solving the general task of automatically recognizing three-dimensional objects. [24] His program was designed to understand the artificial world composed solely of polyhedral blocks — a reduction of reality to geometry that would have pleased Cézanne. Using image processing techniques, a photograph of a scene was first converted into a line drawing. Next, the techniques of 3-D computer graphics were used:

Roberts' program had access to three-dimensional models of objects: a cube, a rectangular solid, a wedge, and a hexagonal prism. They were represented by coordinates (x, y, z) of their vertices. A program recognized these objects in a line drawing of the scene. A candidate model was selected on the basis of simple features such as a number of vertices. Then the selected model was rotated, scaled, projected, and matched with the input line drawing. If the match was good, the object was recognized, as were its position and size. Roberts' program could handle even a composite object made of multiple primitive shapes; it subtracted parts of a line drawing from the drawing as they were recognized, and the remaining portions were analyzed further. [25]

Was this enough to completely automate human vision? This depends upon how we define vision. The chapter on computer vision in **The Handbook of Artificial Intelligence** (1982) opens with the following definition: "Vision is the information-processing task of understanding a scene from its projected images." [26] But what does "understanding a scene" mean? With computer vision research financed by the military-industrial complex, the definition of understanding becomes highly pragmatic. In the best tradition of the pragmatism of James and Pierce, cognition is equated with action. The computer can be said to "understand" a scene if it can act on it — move objects, assemble details, destroy targets. Thus, in the field of computer vision "understanding a scene" implies two goals. First, it means the identification of various objects represented in an image. Second, it means reconstruction of three-dimensional space from the image. A robot, for instance, need not only to recognize particular objects, but it has to construct a representation of the surrounding environment to plan its movements. Similarly, a missile not only has to identify a target but also to determine the position of this target in three-dimensional space. It can be seen that Roberts' program simultaneously fulfilled both goals. His program exemplified the approach taken by most computer vision researchers in the following two decades. A typical program first reconstructs the three-dimensional scene from the input image and then matches the reconstructed objects to the models stored in memory. If the match is good, the program can be said to recognize the object, while simultaneously recording its position. A computer vision program thus acts like a blind person who "sees" objects (i.e., identifies them) by reading their shapes through touch. As for a blind person, understanding the world and the recognition of shapes are locked together; they cannot be accomplished independently of one another.

In summary, early computer vision was limited to recognition of two-dimensional forms. Later, researchers began to tackle the task of recognizing 3-D objects which involves reconstruction of shapes from their perspectival representations (a photograph or a video image). From this point on, the subsequent history of computer vision research can be seen as a struggle against perspective inherent to the photographic optics.

## The Retreat of Perspective

With the emergence of the field of computer vision, perspectival sight reaches its apotheosis and at the same time begins its retreat. At first computer vision researchers believed that they could invert the perspective and reconstruct the represented scene from a single perspectival image. Eventually, they realized that it is often easier to bypass perspectival images altogether and use other means as a source of three-dimensional information.

Latour points out that with the invention of perspective it became possible to represent absent things and plan our movement through space by working on representations. To quote him again, "one cannot smell or hear or touch Sakhalin island, but you can look at the map and determine at which bearing you will see the land when you send the next fleet." [27] Roberts' program extended these abilities even further. Now the computer could acquire full knowledge of the three-dimensional world from a single perspectival image! And because the program determined the exact position and orientation of objects in a scene, it became possible to see the reconstructed scene from another viewpoint. It also became possible to predict how the scene would look from an arbitrary viewpoint. [28] Finally, it also became possible to guide automatically the movement of a robot through the scene. 

Roberts' program worked using only a single photograph — but only because it was presented with a highly artificial scene and because it "knew" what it could expect to see. Roberts limited the world which his program could recognize to simple polyhedral blocks. The shapes of possible blocks were stored in a computer. Others simplified the task even further by painting all objects in a scene the same color.

However, given an arbitrary scene, composed of arbitrary surfaces of arbitrary color and lighted in an arbitrary way, it is very difficult to reconstruct the scene correctly from a single perspectival image. The image is "underdetermined". First, numerous spatial layouts can give rise to the same two-dimensional image. Second, "the appearance of an object is influenced by its surface material, the atmospheric conditions, the angle of the light source, the ambient light, the camera angle and characteristics, and so on," and all of these different factors are collapsed together in the image. [29] Third, perspective, like any other type of projection, does not preserve many geometric properties of a scene. Parallel lines turn into convergent lines; all angles change; equal lines appear unequal. All this makes it very difficult for a computer to determine which lines belong to a single object.

Thus, perspective, which until now served as a model of visual automation, becomes the drawback which needs to be overcome. Perspective, this first step towards the rationalization of sight (Ivins) has eventually become a limit to its total rationalization — the development of computer vision.

The realization of the ambiguities inherent in a perspectival image itself came after years of vision research. In trying to compensate for these ambiguities, laboratories began to scrutinize the formal structure of a perspectival image with a degree of attention unprecedented in the history of perspective. For instance, in 1968 Adolpho Guzman classified the types of junctions that appear in line representations after he realized that a junction type can be used to deduce whether regions of either side of a junction line were part of the same object. [30] In 1986 David Lowe presented a method to calculate the probability that a particular regularity in an image (for instance, parallel lines) reflects the physical layout of the scene rather than being an accident due to a particular viewpoint. [31] All other sources of depth information such as shading, shadows or texture gradients were also systematically studied and described mathematically.

Despite these advances, a single perspectival image remained too ambiguous a source of information for practical computer vision systems. An alternative has been to use more than one image at a time. Computer stereo systems employ two cameras which, like human eyes, are positioned a distance apart. If the common feature can be identified in both images, then the position of an object can be simply determined through geometric calculations. Other systems use a series of continuous images recorded by a video camera.

But why struggle with the ambiguity of perspectival images at all? Instead of inferring three-dimensional structure from a two-dimensional representation, it is possible to measure depth directly by employing various remote sensing technologies. In addition to video cameras, modern vision systems also utilize a whole range of different range finders such as lasers or ultrasound. [32] Range finders are devices which can directly produce a three-dimensional map of an object. The same basic principle employed in radar is used: the time required for an electro-magnetic wave to reach an object and reflect back is proportional to the distance to the object. But while radar reduces an object to a single point and in fact is blind to close-by objects, a range finder operates at small distances. By systematically scanning the surface of an object, it directly produces a "depth map," a record of an object's shape which can be then matched to geometric models stored in computer memory thus bypassing the perspectival image altogether. 

## Epilogue

The Renaissance's adaptation of perspective represented the first step in the automation of sight. While other cultures used sophisticated methods of space mapping, the importance of perspective lies not in its representational superiority but in its algorithmic character. This algorithmic character enabled the gradual development of visual languages of perspective and descriptive geometry and, in parallel, of perspectival machines and technologies, from a simple net described by Dürer to photography and radar. And when digital computers made possible mass automation in general, automation of perspectival vision and imaging followed soon.

The use of computers allowed to extend perspective, utilizing to the extreme its inherent qualities such as the algorithmic character and the reciprocal relationship it establishes between reality and representation. The perspective algorithm, a foundation of both computer graphics and computer vision, is used to generate perspectival views given a geometric model and to deduce the model given a perspectival view. Yet, while giving rise to new technologies of "geometric vision," perspective also becomes a limit to the final automation of sight — recognition of objects by a computer. Finally, it is displaced from its privileged role, becoming just one among other techniques of space mapping and visualization. 	

## References:

[1] Charles Eames and Ray Eames, **A Computer Perspective: Background to the Computer Age** (Cambridge: Harvard University Press, 1990), 65-67.

[2] Qtd. in ibid., 67.

[3] For a survey of perspectival instruments, see Martin Kemp, **The Science of Art** (New Haven: Yale University Press, 1990), 167-220.

[4] Ibid., 171-172.

[5] Ibid., 200.

[6] See Bruno Latour, "Visualization and Cognition: Thinking with Eyes and Hands," **Knowledge and Society: Studies in the Sociology of Culture Past and Present** 6 (1986): 1-40.

[7] For a comprehensive account of 3-D computer graphics techniques, see J. William Mitchell, **The Reconfigured Eye: Visual Truth in the Post-Photographic Era** (Cambridge, The MIT Press, 1992), 117-162.

[8] Jasia Reichardt, **The Computer in Art** (London and New York: Studio Vista and Van Nostrand Reinhold Company, 1971), 15.

[9] L.G. Roberts, **Machine Perception of Three-Dimensional Solids**, MIT Lincoln Laboratory TR 315, 1963; L.G. Roberts, **Homogeneous Matrix Representations and Manipulation of N-Dimensional Constructs**, MIT Lincoln Laboratory MS 1405, 1965.

[10] "Retrospectives II: The Early Years in Computer Graphics at MIT, Lincoln Lab, and Harvard," in **SIGGRAPH '89 Panel Proceedings** (New York: The Association for Computing Machinery, 1989), 72.

[11] This mixture of automated and pre-industrial labor is characteristic of the early uses of computers for the production of images. In 1955 the psychologist Attneave was the first to construct an image which was to become one of the icons of the age of digital visuality — random squares pattern. A pattern consisted of a grid made from small squares colored black or white. A computer-generated table of random numbers has been used to determine the colors of the square — odd number for one color, even number for another. Using this procedure, two research assistants manually filled in 19,600 squares of the pattern. Paul Vitz and Arnold B. Glimcher, **Modern Art and Modern Science** (New York: Praeger Publishers, 1984), 234. Later, many artists, such as Harold Cohen, used computers to generate line drawings which they then colored by hand, transferred to canvas to serve as a foundation for painting, etc.

[12] For further discussion of the problem of realism in 3-D computer graphics, see Lev Manovich, "Real" Wars: Esthetics and Professionalism in Computer Animation," **Design Issues** 6, no. 1 (Fall 1991): 18-25; Lev Manovich, "Assembling Reality: Myths of Computer Graphics," **Afterimage** 20, no. 2 (September 1992): 12-14. 

[13] Mitchell, **The Reconfigured Eye**, 118.

[14] "Retrospectives II: The Early Years in Computer Graphics at MIT, Lincoln Lab, and Harvard," 57.

[15] Remko Scha, "Virtual Voices," **MediaMatic** 7, no. 1 (1992): 33. Scha describes two fundamental approaches taken by the developers of voice imitating machines: the genetic method which imitates the physiological processes that generate speech sounds in the human body and the gennematic method which is based on the analysis and reconstruction of speech sounds themselves without considering the way in which the human body produces them. While the field of computer vision, and other fields of artificial intelligence, first clearly followed gennematic method, in the 1980s, with the growing popularity of neural networks, there was a shift towards the genetic method — direct imitation of the physiology of the visual system. In a number of laboratories, scientists begin to build artificial eyes which move, focus, and analyze information exactly like human eyes.

[16] Eames and Eames, **A Computer Perspective**, 100.

[17] Manuel De Landa, "Policing the Spectrum," in **War in the Age of Intelligent Machines** (New York: Zone Books, 1991), 194-203.

[18] Ibid., 214.

[19] The first paper on image processing was published in 1955. L.S.G. Kovasznay, and H.M. Joseph, "Image Processing," **Proceedings of IRE** 43 (1955): 560-570.

[20] Rafael C. Gonzalez, and Paul Wintz, **Digital Image Processing** (Reading, Mass.: Addison-Wesley Publishing Company, 1977), 2.

[21] Qtd. in De Landa, "Policing the Spectrum," 200.

[22] Within the field of computer vision, a scene is defined as a collection of three-dimensional objects depicted in an input picture. David McArthur, "Computer Vision and Perceptual Psychology," **Psychological Bulletin** 92, no. 2 (1982): 284.

[23] Paul R. Cohen and Edward A. Feigenbaum, eds., **The Handbook of Artificial Intelligence** (Los Altos, CA: William Kaufmann, Inc., 1982), 3: 139.

[24] L.G. Roberts, "Machine perception of three-dimensional solids," **Optical and Electro Optical Information Processing**, ed. J.T. Tippett (Cambridge: The MIT Press, 1965).

[25] Cohen and Feigenbaum, **The Handbook of Artificial Intelligence**, 3: 129.

[26] Ibid., 127.

[27] Latour, "Visualisation and Cognition," 8.

[28] Cohen and Feigenbaum, The Handbook of Artificial Intelligence, 3: 141.

[29] Ibid., 128.

[30] Ibid., 131.

[31] David Lowe, **Three-Dimensional Object Recognition from Single Two-Dimensional Images**, Robotics Report 62 (New York: Courant Institute of Mathematical Sciences, New York University, 1986).

[32] Cohen and Feigenbaum, **The Handbook of Artificial Intelligence**, 3: 254-259.	 

---
# Behind the Screen / Russian New Media

_author: Lev Manovich_
_year: 1997_

Should we be surprised that as the new computer-based media expand throughout the world, intellectual horizons and aesthetic possibilities seem to be narrowing? If one scans Internet-based discussion groups and journals from London to Budapest, New York to Berlin, and Los Angeles to Tokyo, certain themes are obsessively repeated, like mantras: copyright; online identity; cyborgs; interactivity; the future of the Internet. This follows from the Microsofting of the planet, which has cast uniform digital aesthetics over national visual cultures, accelerating the globalization already begun by Hollywood, MTV, and consumer packaging: hyperlinks and cute icons, animated fly-throughs, rainbow color palettes, and Phong-shaded spheres are ubiquitous and apparently inescapable.

So, given its intellectual traditions, totalitarian experience, distinct twentieth-century visuality (a particular mixture of the Northern and the Communist, the gray and the bleak), and finally, its continuing preoccupation with the brilliant avant-garde experimentation in the 1910s and 1920s, can we expect a different response to new media on the part of Russian artists and intellectuals? What will — or could — result from the juxtaposition of the Netscape Navigator web browser's frames with Eisenstein's theories of montage? It would be dangerous to reduce heterogeneous engagements to a single common denominator, some kind of unique "Russian New Media" meme. Yet a number of common threads do exist. These provide a useful alternative to the West's default thematics, while articulating distinctive visual poetics of new media.

One of these threads is the attitude of suspicion and irony. Moscow's Alexei Shulgin writes of the excitement generated by interactive installations: "It seems that manipulation is the only form of communication they know and can appreciate. They are happily following very few options given to them by artists: press left or right button, jump or sit." He views artists as manipulators employing the seductions of the newest technologies "to involve people in their pseudo-interactive games obviously based on [the] banal will for power… [The] emergence of media art is characterized by transition from representation to manipulation." [1]

Shulgin views interactive art and media as creating structures that are frighteningly similar to the psychological laboratories the CIA and the KGB operated during the Cold War era. I was born in Moscow and grew up there during Brezhnev's era, so I find his thoughts not only logical but enthralling. Yet my investment in his conclusions doesn't blind me to the limitations of his analysis, or rather, its cultural specificity: it takes a post-communist subject to frame interactive art and media in such stark terms.

For a Western artist, that is, interactivity is a perfect vehicle both to represent and promulgate ideals of democracy and equality; for a post-communist, it is yet another form of manipulation, in which artists use advanced technology to impose their totalitarian wills on the people. Further, Western media artists usually take technology absolutely seriously, despairing when it does not work; post-communist artists, on the other hand, recognize that the nature of technology is that it does not work, that it will necessarily break down. Having grown up in a society where truth and lie, reality and propaganda always go hand in hand, the post-communist artist is ready to accept the basic truisms of life in an information society (spelled out in Claude Shannon's mathematical theory of communication): that every signal always contains some noise; that signal and noise are qualitatively the same; and that what is noise in one situation can be signal in another.

In this spirit, Moscow conceptual artist and poet Dmitry Prigov organized a performance during the International Symposium on Electronic Art in Helsinki (1994) in which he used business traveller's software on one of Aleksander Pushkin's nineteenth-century poems, translating it from Russian into Finnish, and then from Finnish into English. For Prigov, the final product was not a miserably misbegotten translation, twice removed from the source, but a new poem, its originality indebted — however ironically — to the operations of the lowest level of artificial intelligence.

Like Prigov's performance, Shulgin's own new media projects can be described as meta-art. In contrast to many of his western colleagues who feel that they have to colonize and appropriate the Web through a distinct category of "artists" web projects," Shulgin proceeds from the assumption that the Web "is an open space where the difference between "art" and "not art" has become blurred as never before in the 20th century." In this spirit, he established the WWWArt Medal [http://www.cs.msu.su/wwwart/award](http://www.cs.msu.su/wwwart/award) to be awarded to "web pages that were created not as artworks but gave us definite "art" feeling". Visitors check links to a variety of "found" Web pages (importantly, not a single one of them is an "artists" web project"), which have been singled out for "flashing," "moderation" and "valiant psychedelics," among other categories. Like Prigov's poem, another of Shulgin's sites, "Remedy for Information Disease" [http://www.desk.nl/~you/remedy](http://www.desk.nl/~you/remedy), functions as a noise generator, implying that the cure for data overload is to shift from receiving to broadcasting.

Prigov and Shulgin exemplify how the conceptualism which has recently dominated the Moscow art scene offers a valuable strategy for approaching new media. Another strategy positions Russian new media within a larger historical tradition of "screen culture". For Russian thinkers, the meaning of the screen expands far beyond its function as a surface displaying an image originating from elsewhere: it is also a bridge across two spaces, one physical, one imaginary; a link between a human subject and an audio-visual stream; and a rectangular window which opens onto alternative (virtual) reality. So understood, the "screen" is that which unites old and new media, still and moving images, analog and digital culture.

The emphasis on the screen as a space that opens onto an alternative reality is echoed in much modern Russian art which remains firmly committed to the tradition of easel painting. In contrast to the West, where artists gave up on illusionistic pictorial space in favor of the notion of a painting as a self-sufficient material object, many Russian artists, both representational and abstract, continue to conceive of a painting ("kartina") as a parallel reality which begins at the picture frame and extends towards infinity. Thus, Eric Bulatov has described his paintings as windows onto another, spiritual universe, while Ilya Kabakov conceptualizes his installations as a logical expansion of pictorial traditions into the third dimension — a materialization of reality models previously presented by painting. [2]

Young Russian media artists are using the computer as an excuse to re-think basic categories and mechanisms of screen culture, such as frame, montage, and illusionistic space. Thus, rather than representing a radical break with the past, the computer screen becomes, for them, a re-articulation of the models which have defined screen consciousness for centuries. "My boyfriend came back from war!" is a Web-based work by the young Muscovite Olga Lialina [http://www.heise.de/tp/sa/3040/fhome.htm](http://www.heise.de/tp/sa/3040/fhome.htm). Using the web browser's capability to create frames within frames, Lialina leads us through a series of pages which begin with an undivided screen and become progressively divided into more and more frames as we follow different links. Throughout, an image of a human couple and of a constantly blinking window remain on the left part of screen. These two images enter into new combinations with texts and images themselves engendered by the user's interaction with the site. In this way, Lialina creatively bridges principles of traditional parallel montage, as it existed in the cinema, and the evolving possibilities of interactive hypertext.

St. Petersburg-based Olga Tobreluts uses a computer to expand the possibilities of cinematic montage in a different way. In "Gore ot Uma" (1994), a video work based on a famous play written by an early nineteenth-century writer Aleksandr Griboedov and directed by Olga Komarova, Tobreluts seamlessly composes images representing radically different realities on the windows and walls of various interior spaces. In one scene, two characters converse in front of a window which opens up onto a shock of soaring birds taken from Alfred Hitchcock's "The Birds"; in another, a delicate computer-rendered design fades in onto a wall behind a dancing couple. Because Tobreluts bends composited images to follow the same perspective as the rest of the shots, the two realities appear to inhabit the same physical space. The result is a different kind of montage for digital cinema. [3] Which is to say, if the 1920s avant-garde, and MTV in its wake, juxtaposed radically different realities within a single image, and if Hollywood digital artists use computer compositing to glue different images into a seamless illusionistic space (for instance, synthetic dinosaurs composited against filmed landscape in "Jurassic Park"), Tobreluts explores the creative space between these two extremes.

Lialina and Tobreluts' projects offer a vision of how Russian new media artists can negotiate between the extreme materialism of Western computer art practice and the historicism and conceptualism characteristic of their country's art. The question remains, however, will Russia be able to stop the march of Bill Gates' aesthetic imperialism, the way she previously froze out the armies of Napoleon?

## References:

[1] Rhizome Digest: October 11, 1996, [http://www.rhizome.com](http://www.rhizome.com).

[2] Eric Bulatov, conversation with the author, 1980; Ilya Kabakov, On the "Total" Installation (Bonn: Cantz Verlag, 1995).

[3] I explore digital compositing in relation to the history of cinema in more depth in "To Lie and to Act: Potemkin's Villages, Cinema and Telepresence," in Mythos Information — Welcome to the Wired World. Ars Electronica 95, edited by Karl Gebel and Peter Weibel, (Vienna and New York: Springler-Verlag, 1995): 343-353.

---
# Detour to the East

_author: Lev Manovich_
_year: 1997_

After spending a week in Linz, Austria at Ars Electronica festival where incredible food displays at parties, sponsored by the city of Linz and the Austrian TV station ORF, together with very stylishly designed exhibition spaces and even more funky and futuristic looking computer terminals overshadowed whatever art was displayed on these terminals, my eyes are tired from glitter. I crave monotone landscapes, old surfaces, a world of textures, rather than information. So, I decided to take a detour in my trip and head East.

And now, on a sunny and cold September day I enter an old building in an Eastern European city I will choose not to name here — for a similar experience could have taken place in a dozen of other cities and countries. In my mind I am trying to compile a list of essential traits which, for me, sum up the uniqueness of the Eastern European cityscape. Men in sport cloves wearing gold chains behind the wheels of foreign-made cars, or talking in groups of two or three in front of private business. Middle-aged couples, also in sport cloves, shopping for food or carefully studying the awkwardly displayed merchandise in shop windows. (Why sport cloves are the fashionable attire in the East, something to wear for any occasion, from shopping to a Sunday walk in the park? Is it because people feel that life here, with all the difficulties, absurdities, the need for constant tactics and simply inhuman stamina, is best compared to some challenging sport, a marathon with obstacles? Or is it simply to protect themselves from the ever-present dirt?) But to continue with my traits: ever present displays of fine woman lingerie in shop windows (woman lingerie serving as the ultimate symbol of liberation, but also, like sport cloves, serving as a protective layer between a post-communist body and a dangerous, chaotic, primordial, unkempt, decayed and irrational post-communist environment; a way for a woman to retrieve into the privacy of her body, to claim it for herself, to create a border between her body and the environment outside — just as, in general, for decades people responded to the always present threat and aggression of politicized, uncontrollable and dirty public spaces of Communist life by retrieving into their apartments where they were finally in control, the apartments with parquet floors and carefully selected wallpaper, with numerous heavy volumes of classics on the bookshelves, with the shinning imported plastic seat in the bathroom — the pride of apartment's owners — with pictures of semi-naked girls carefully cut out from foreign magazines and attached to the bathroom door — a particular version of Communist comfort, suffered over, fought for, painstakingly assembled object by object over the course of many years). To continue with my traits, which, as it is becoming clear, all have to do with the contrast between private individuals and their environment, the contrast which does not exist in the same way in the West, or simply is not of the same magnitude: we also should include such typical sight as some woman or a couple, very stylish, impeccably dressed, waiting for a bus or a tram against the backdrop of completely decay, dirty, degenerated cityscape, with buildings looking as though they were not painted for at least 300 years or more; more correctly, looking like they were painstakingly painted by some set designers to appear as though were they not cared for many centuries (although Communist regime only lasted few decades). Indeed, the whole cityscape can be compared to one giant Brothers Quay set, only it is even more decayed, the degree of decay underlined by the elegance of city dwellers who try to lead their lives, have children, affairs, careers and so on in the middle of this surreal and cruel environment. And another trait: Coca-Cola signs everywhere, on every corner, covering whole kiosks or even whole buildings. Apparently, it is quicker to signal the beginning of new post-communist life by putting up such signs than to rename every Lenin street or Revolution Square. And another: a particular expression on people's faces and a way of behaving: very light melancholy, acceptance of fate, an air of uncertainty, certain shyness. Of course, men in sport cloves and gold chains are an acceptance — they do not look shy. And a few more traits which are not as crucial.

On entering the old building which houses the media department of an art school, I follow through a number of rooms, which, for my Western gaze, accustomed to clean geometry and bright colors, look like they were recently bombed. Boxes of papers lying around here and where, rusted pipes and all kinds of junk (not Western junk, rich in its heterogeneity and color schemes, but East European junk, heavy and monotonous like the environment itself); old television sets from the 1960s and some other hard to identify pieces of electronic equipment are piled up on the shelves in the short corridor leading to the bathrooms with no doors (I immediately recall how a friend of mine leaving in California in response to the inevitable question of why she left Russia responded that there was only one reason: she no longer could face Russian public bathrooms). But these rooms were not bombed; they are, I am explained, the working studios of the media department.

Finally, we enter the computer lab which looks somewhat less rusty, with a couple of Silicon Graphics computer workstations proudly blinking in the center. A serious-looking young man — a post-graduate student — demonstrates to me his Internet project. I experience a momentary jolt, a sudden and rather painful cut from one world to another — from the old, decayed, melancholic, gloomy but at the same time warm, comfortable and very human environment (and indeed, what can be more human than an East European interior space, populated by semi-broken objects which have witnessed generations of human dramas and, through the decades of use, have become almost human, like pets — an environment which welcomes and protects people, enveloping them in a particular psychological mist, like the background of a 17th-century Dutch painting displaying an interior scene) — into a universal electronic space: rows of directory listings; aggressively lined up icons with no national trait whatsoever; perfect tidiness of a desktop assaulting and challenging my gaze. I feel as though a warm cup of tea was suddenly taken from my lips; as though somebody blew off a candle and switched on a bare electric bulb instead; or as though I entered a small room of some airport hotel, discovering an anonymous space, naked metal surfaces, which, in contrast to the empty surfaces of on old Dutch still-lives which would bath me, caress me, warm me and comfort me — instead, these surfaces only reflect my body, through it back at me, leaving me alone with myself; they force me to be an anonymous passenger, reminding that I have to leave soon, leave this room, this building, this directory, this server — always in transit, always moving, never arriving; force me to become nothing but a solid body which, for a certain time only, will occupy an assigned seat on a plane (conforming a philosophical definition that an object is something which occupies a volume in space); or a packet of data traveling between servers, nothing more. Clean surfaces, of hotel rooms, airport lounges or of computer screens; in either case, forcing me always to leave, never to stay for a long time.

What is this computer space? It is the space where all the signs can be exchanged between each other, a space where everything is only a sign — in short, a space where marketplace became an ontology, where everything consists only from transactions (messages querying servers; records querying their databases; modems negotiating communication speed and the type of protocol to be used) and translations (between different media types, between different computer languages, between user input and the operating system), and where there is no place for atmosphere, mood or inefficiency. Now, as this computer space is becoming our space, the space of our lives, the space of our daily habitation — how can we bring into it at least a bit of an old Dutch painting into it, a bit of air, a bit of deliberation, of lingering on, the background of a painting whose sole function is to provide breathing space for the depicted objects, giving them dignity — but also to warm us, to give refuge to our eyes tired after taking in an elaborate cut of a lemon peel and shiny reflections on a silver glass; to be a mirror for our breath, an equivalent for these spaces in a conversation between good friends where no words are exchanged yet meaning is created?

I feel a little angry inside for being transferred to this cold computer space — but, out of politeness and solidarity, I carefully listen to student's explanations, sometimes interrupting him with questions. His English is not very good; I don't understand the details of the project, but I decide not to press it. The project has something to do with visualization of the users who log on remotely to the project's site; their Internet addresses are mapped into points making up a virtual object. Or something like that. I politely ask more questions and try to encourage the student.

What, I think to myself, this project has to do with the room we are both in, its particular post-socialist atmosphere of decay and neglect, its particular East European feeling of fog and mist, the decay which warms me like the background of a painting I was looking at in museum earlier this day? What does this project has to do with the city lying outside the window of a room we are in, the city which is desperately trying to join the West by dressing itself into Coca-Cola signs, by welcoming tourists and Mafia from every country, by keeping its bars and restaurants open all night so a tourist or a Mafia man will never feel thirsty or hungry? What does this project have to do with the city's past, its boulevards and neighborhoods, its Renaissance and Baroque facades, its golden squares and monuments, the castle high over the river? [1] What does this project, finally, has to do with the life of its young creator, his experience of growing up during the last decade of a Communist rule, the short excitement during the first years after its end, and then slowly growing disappointment that the new, like Coca-Cola signs amid the decayed city, remains surrounded by the old, the old which slowly eats it, poisons it, dissolves it, covers it with a patina of corruption, of broken friendships, destroyed hopes and dreams.

Why? Why this project could have been created in hundreds of other places on Earth? Only later did I understand. It is simple, and it has to do with a cultural function of the Internet outside of the U.S. and a few other highly developed regions. For the rest, the Internet function as an agent of modernization, just as other means of communication did before it — railroad, post, telephone, motor car, air travel, radio. Internet is a way for people to enter into a singular socio-linguistic space, defined by a certain Euro-English vocabulary and the names of stars, by a computer competency, by pop music. It is a way for people in different places to enter modernity — the space of homogeneity, of currency exchange shops, of Coca-Cola signs, or raves and club cloves, or CDs, of constant youth, itself the best symbol for movement and constant change, the symbol for leaving your roots and traditions behind — the space where everything can be converted into money signs, just like a computer can convert everything can into bits. 

And this is why we, in the West, should not expect culturally-specific Internet art, should not wait for Internet dialects, for some national schools of Net art. And this is why I can't possibly expect that this young man's project will reflect something of his unique identity, of his history, of his culture. This simply would be a contradiction in terms. To expect different countries to create their own national schools of Net art is the same as to expect them to create their own customized brands of Coca-Cola. The sole meaning of Coca-Cola, its sole function is that it is the same everywhere.

Net is an agent of modernization as well as a perfect metaphor for it. It is a post, a telephone, a motor car, plane travel, taken to the extreme. Thus, we should not be surprised that a typical Net art project, whether it is done in Seattle or Bucharest, in Berlin or in Odessa, is about communication itself, is about the Internet. Net art projects are materializations of social networks. These projects make the networks visible and create them at the same time — just as raves, party cloves and body piercing, CDs and names of music groups, of transnational companies, of products. It is a way for young people in Oslo and Warsaw, in Belgrade and Glasgow to enter modernity and to become its agents for the rest of a society. And just as it would be naive (although of course we can immediately recall or imagine some serious museum show on the image of a gas station in modern landscape painting, and even thick art historical or anthropological monographs on the subject) to take seriously "the art of a gas station," the category of "Net art" is a logical mistake. So-called Net art projects are simply visible manifestations of social, linguistic and psychological networks being created or at least made visible by these very projects, of people entering the space of modernity, the space where old cities pay the price for entering the global economy by Disney-fying themselves, where everybody is paying some price: exchanging person-to-person communication for virtual communication (telephone, fax, Internet); exchanging close groups for distributed virtual communities, which more often than not are like train stations, with everybody constantly coming and leaving, rather than the cozy cafes of the old avant-garde; exchanging decayed but warm interiors for shiny, bright but cold surfaces. In short, exchanging the light of a candle for a light of an electric bulb, with all the consequences this exchange involves.

I thank the student and leave the building. Outside, a couple tries to push a dead car; elaborate woman lingerie is displayed in a window of a small shop; few taxis wait for non-existent passengers. It is getting cold and I button my overcoat.

The following day I depart the city. The memory which remains with me is that of this young couple, both in stylish (according to local fashion) matching sport closes, pushing the dead car in the middle of a street on a Sunday morning. Besides the couple, the street is empty: the air is cold; and the edges of buildings are bathing in a crisp September light. The car does not roll; the couple laughs. Their faces display a mixture of determination and content, melancholy and shyness.

Two weeks later I am in another East European country, being driven to the city from the airport. As we enter city's outskirts, we pass a couple in sport cloves, standing next to the car with an open hood. Before they disappear out of site, I have time to notice the crooked lines of car's body, the dirty engine, maybe one or two details about woman's face. Is this the same couple I remember from the previous city? Did they moved here, hoping for better luck (according to the English newspaper I pick up on the plane, this country is doing somewhat better: Communists are implementing market reforms; Western tourism is on the rise; a new BMW dealership just opened in the old castle, while MacDonald is scheduled to open a flagship shop in the Opera building, which will certainly become city's magnet, the central gathering place for its youth; and there is even an ad for an Internet provider offering an ISDN line which only costs an equivalent of 2-year average salary). [2] But it can't be the same couple; they look much older, and so does their car. And this couple no longer laughs, their faces looking more content, more desperate. This couple will probably never use the Internet, but their children will — in the hope of escaping the decay, escaping becoming a middle age couple reclining over a hood of car which was not painted in a decade and looks like it was driven through Sahara desert.

Train, telephone, radio, music videos, party cloves, CDs and now the Internet — these are the tools of modernization, the tickets to entry the world with clean corners, the world consisting from uniform bright surfaces, the world as pristine and sharp as the individual pixels which make its images, the world where the dead cars are simply left to rot somewhere out of site, the world there only athletes wear sport cloves, and only during competitions. 

## Notes:

[1] The city where a visitor, had he an X-ray vision, would see in an early hour of a morning thousands of couples embracing in their beds, before they have to leave for work, thousands of woman's arms, thin and thick, white and occasionally tanned, all embracing the necks of men in a similar gesture; men cloves accurately hanging on a chair nearby or maybe inside a chest, a fork and a knife lying on a table past a loaf of bread gradually getting hard; thousands of couples which are of course similar to thousands of other couples in any other city of that size anywhere; yet also unique, for only here, in this city, women arms fold around the necks on their lovers in this particular way, and only here a knife and a fork lie on a table next to a loaf of bread at this particular angle.

[2] Later I learn that the city just decided to rename one of its central squares "Coca-Cola Classic" in recognition of the historical role which the Coca-Cola factory played during the civil war, when this factory protected by its workers was the only place of order in an otherwise chaotic city with no government. At that time, city leaders were given refuge inside the factory where, helped by a free supply of Coca-Cola, generously donated by factory management, they wrote a historical proclamation of independence and coordinated military action which eventually lead to city's liberation. [3]

[3] Unfortunately, the brave decision to rename the square which won the major much-needed support of the youth and pro-Western citizens could not save him from a resignation two weeks later after a local newspaper printed a rather revealing photograph depicting him in the middle of a sex orgy with 8 prostitutes. What angered the citizens was less the fact of an orgy — for, after all, it definitely was a proof of his health and virility and thus the ability to govern — but that the prostitutes were imported from a neighboring country. Don't we have enough of our own prostitutes, was angrily asking the editorial which accompanied the photograph? Rather than employing our own local woman (and he would not have to stop at 8), why is he spending limited city resources on import? After this argument, the resignation was inevitable. 

---

# Jump over Proust

_author: Lev Manovich_
_year: 1997_

One of the greatest achievements of literary modernism was new ways to represent our mental life in art. Montage, multiple viewpoints and narrators, stream-of-consciousness and other techniques allowed to render human mind with new fidelity. Given that computer makes it possible to combine written word with audio, stills, digital video and even three-dimensional spaces, how can we take advantage of these new abilities to go beyond the achievements of modernism? Put differently, how to allow the user not just to be simply a "co-author" (which is what the ideologists of interactivity naively aim at) but rather to take him/her "inside" the mental space of a text, inside the thinking process of another subject? In short, how can we use new representational capacities of a computer to represent mental life — and, more generally, human subjectivity — in new ways? [1]

In thinking about these questions I was inspired by certain filmmakers who appear to be obsessed not simply with using cinema as a medium to convey ideas and arguments (which is what conventional documentaries are supposed to be doing) but rather as a medium capable of presenting the very process of thinking. Among these filmmakers I would single out Eisenstein, Marker and Godard. 

The first tried to formulate the notion of intellectual montage and planned to film Marx's Capital. Already at the end of the 1920s, he was predicting that in the future philosophy and history will be presented as films: 

"The proclamation that I'm going to make a movie of Marx's Das Kapital is not a publicity stunt. I believe that the films of the future will be found going in this direction (or else they'll be filming things like The Idea of Christianity from the bourgeois point of view!) In any case, they will have to do with philosophy ... the field is absolutely untouched. Tabula rasa." [2] 

Marker showed that cinema can be used to construct intellectual essays, the essays which associatively move from idea to image, and an image to another idea (for instance, in his recent "Level 5"). And Godard has already explored computer multimedia's new language in his films from the 1960s onward by systematically juxtaposing moving images, non-realistic sound and graphically presented texts; more recently he created a true multimedia masterpiece, an essay which takes us along his thinking process while using every multimedia code available: pages of text, still images, moving images, voice and sound ("JLG by JLG. Self-Portrait in November").

Not only to convey complex ideas through multimedia, but to take the reader along the process of thinking — this is the challenge of multimedia writing. The use of a computer as writer's tool can only be justified if we can evolve more nuanced, more complex and more precise ways of conveying what it means to think, of how it feels to move from one association to the next, from one memory to another, from one insight to the next. Only when we will give justice to the common view of a computer, which accompanied it from its very beginnings half a century ago, as a model of a human brain. A machine which has memory, which can store words and images, which can search and match, which, most importantly, can link, i.e. associate — even if it is not a human mind, it has most of the functions we, humans, perform when we think. Therefore, we should be able to use a computer to portray human thinking in a more precise and engaging way than literature and cinema have already done. To do this is the challenge of multimedia writing.

Earlier in this century, Proust, Nabokov, Joyce and other modernist giants came up with new techniques to represent our mental life: thinking and remembering, forgetting and repressing, formulating concepts, moving between the sensorial world outside and the mental world inside. Literature became the best mirror for the modern psyche, achieving the highest fidelity in relation to our inner world. But other arts did not match its achievement. Cinema, multimedia's main precursor, is particularly disappointing in this respect. By and large, its language followed 19th-century novel, rather than trying to match — and go beyond — the microscopic view of human inner experience recorded by Joyce, Nabokov and other moderns. This is particularly surprising, given that the first theoretical text on cinema — Hugo Munsterberg's The Film: A Psychological Study (1916) — proclaimed that the essence of the new medium lies in its ability to reproduce, or "objectify" various mental functions on the screen. According to Munsterberg, "The photoplay obeys the laws of the mind rather than those of the outer world." [3] In a provocative analysis, Munsterberg correlated the main cinematic techniques to different mental functions such as attention and memory, one-to-one. For example, in the close-up, "everything which our mind wants to disregard has been suddenly banished from our sight and has disappeared," analogous to how our attention selects a particular object from the environment. Similarly, the "cut-back" technique objectifies the mental function of memory. 

Yet, despite this promising analysis made by Munsterberg early on, cinema hardly took up the challenge of being a mirror of mind's operations. In my view, the only real systematic attempt in cinema to do this has been Godard's recent work, such as already mentioned "JLG by JLG" and also the majestic and monumental "Histories of Cinema". In the latter, Godard uses new cinematic techniques of his own invention in order to portray thinking process more accurately. For instance, he often superimposes 2, 3 or more images which gradually fade in and out, but never disappear completely, staying on the screen for a few minutes at a time. It is as though these are ideas or mental images floating around in our minds, coming in and out of mental focus. Another technique involves replacing one image by another not through a cut or a dissolve but through a repeated oscillation, with two images flickering back and forth over and over, until the second image finally replaces the first. This technique can be interpreted as an attempt to represent mind's movement from one concept, mental image or memory to another — the attempt, in other words, to represent what, according to Locke and other associationist philosophers, is the basis of our mental life — forming associations.

Yet, along with these innovative techniques which would certainly please Munsterberg (who, accidentally, was a professor of Psychology at Harvard University) by being visual equivalents of mental operations (or shall we say video-temporal equivalents, since time obviously plays a crucial role in Godard's techniques) Godard often "capitulates" to cinema's more conventional way of narration: showing reality (here, a person thinking) from a third person point of view, i.e. from the outside. In "Histories of Cinema," we repeatedly see close-ups of a book page, or Godard himself, standing next to a bookshelf, getting a book, reading a sentence or two; or, finally, Godard sitting at a table and typing or writing. Perhaps these can be thought of as being equivalents of "establishing shots" in traditional cinematic narration: Godard's shows us his location (i.e., his mind) from the outside, so to speak, before taking us inside. Perhaps these images can be also interpreted as challenges to the conventional cinema and its extension, computer multimedia — let's focus on intellectual life, on human mind rather than external actions and stories.

So far a computer, despite its persistent association with a human mind, has served as even worse artistic mirror for our mind than cinema. This is strange given the fact that while only Munsterberg and few others made a connection between human mind and cinema language, in the case of a computer making similar connections became the research focus of a number of new fields, enormously successful, fields which keep expanding more and more: artificial intelligence, cognitive psychology, neuroscience — in short, a whole set of disciplines grouped together under the name cognitive science, whose ultimate purpose is to map the language of the mind and the language of a computer one into another. While the attempts of artificial intelligence to simulate human mind have met with some limited success in such areas as parsing human speech, understanding stories, planning actions and interpreting images, the reverse problem — the cultural problem — using a computer to represent human mind in all its complexity and specificity (i.e., modeling not just the rational-computational part, as in artificial intelligence, but the phenomenological whole), pushing beyond what arts have accomplished so far — was hardly even raised. Obviously, current language of multimedia — presenting a user with a page containing a small number of links leading to other pages — is hardly an adequate mirror of our mental life, or how we think, remember, plan, make connections. 

At present, software tools themselves are more revolutionary than multimedia applications they are used to design. They are better artistic visions of our inner life. Relational databases; pointers; control structures ("if... than," "case," etc.); object-oriented programming — these and other programming concepts point towards potentially complex, dynamic and rich cultural representations of human mind. Even such seemingly trivial concept as a hierarchical file system is already more suggestive than the typical pages with hyperlinks which are being served to us in the 1990s under the slogan of "new media". Whatever it may involve, human thinking is certainly more like a computer program under execution (which involves translating between a hierarchy of computer languages, writing and reading data, keeping track of a current place in a program, clearing space in memory for new data and so on) than a set of pages linked by hyperlinks. 

To bring this new level of complexity, already achieved in software design, into the realm of cultural representation — this is the challenge of multimedia writing. To do this, we need to be looking both at the best cultural achievements in "mind modeling" — Proust and Nabokov, Joyce and Godard — and at the concepts of computer science, at the structure of computer hardware and software. Only when our multimedia texts will do justice both to the complexity of the machines used to compose and distribute these texts — computers — as well as to the complexity of what it feels to be a human being today: to think, to reflect, to carry the burden of human cultural history and of never before available amount of information and news from around the world, to interact with artificial minds of computers and with minds of other humans — and also, as always, still to respond to the sunlight shining through a window shade, the green of grass, the movement of wind in the trees. In short, to be human, to reflect and to exist, to be inside and to outside at the same time. To represent this uniquely human, embodied thinking — this is the challenge of new media art.

## References:

[1] In chapter 2 of my Ph.D. Dissertation, I explore many modern attempts to come with the techniques to represent ideas and logical arguments and to think through images. Lev Manovich, "The Engineering of Vision from Constructivism to Virtual Reality" (University of Rochester, 1993, unpublished). The historical material used in this text is drawn from this chapter. 

[2] Quoted in Annette Michelson, "Reading Eisenstein Reading "Capital", October 2 (1976): 28.

Einstein's theory was not an isolated development. Many in the artistic left of the 1920s shared a similar belief in the cognitive power of new visual forms such as montage. In the late 1920s, Alexander Rodchenko promoted the use of montage sequences in graphic design and, like Eisenstein, he saw montage as being equivalent to "dialectical" reasoning. In this formulation, an individual image corresponded to a single concept, and thinking was thought to be provoked when a number of images were juxtaposed in a series. Walter Benjamin's notion of "dialectical seeing," central in his unfinished Passagen-Werk project, also depends on montage, but within a single frame, so to speak. "Dialectical seeing" was conceived by Benjamin as a way to grasp the forms of the present by looking to the past and to the future in the same instant, juxtaposing them in the same mental image.

[3] Hugo Munsterberg, The Photoplay: A Psychological Study (New York: D. Appleton & Co., 1916), 41.

[4] See Lev Manovich, "The Labor of Perception: Electronic Art in Post-Industrial Society". In ISEA '94. Fifth International Symposium on Electronic Art Proceedings (Helsinki: University of Art and Design, 1994). Also published as "The Engineering of Vision and the Aesthetics of Computer Art," in Computer Graphics 28, no. 4 (1994): 259-263. 

---
# Cinema as a Cultural Interface

_author: Lev Manovich_
_year: 1997_

## The Most Popular Moving Image Sequence of All Times

Don't you wish that somebody, in 1895, 1897, or at least in 1903, realized the fundamental significance of cinema's emergence and produced a comprehensive record of new medium's emergence? [1] Interviews with the audiences; a systematic account of the narrative strategies, scenography and camera positions as they developed year by year; an analysis of the connections between the emerging language of cinema and different forms of popular entertainment which coexisted with it, would have been invaluable. But, of course, these records do not exist. Instead, we are left with newspaper reports, diaries of cinema's inventors, programs of film showings and other bits and pieces — a set of random and unevenly distributed historical samples.

Today we are living in the midst of an emerging new medium — the metamedium of the digital computer. All information becomes encoded in one code; all cultural objects become computer programs, something which is not only seen, heard or read, but first of all stored and transmitted, compiled and executed. In contrast to a hundred years ago, when cinema was coming into being, we are fully aware of the significance of this new media revolution. And yet I am afraid that future theorists and historians of computer media will be left with not much more than the equivalents of newspaper reviews and random bits of evidence similar to cinema's first decades. They will find that the analytical texts from our era are fully aware of the significance of computer's takeover of culture yet, by and large, they mostly contain speculations about the future rather than a record and a theory of the present. Future researchers will wonder why the theoreticians, who already had plenty of experience analyzing older cultural forms, did not try to describe computer media's semiotic codes, modes of address, and audience reception patterns. If, for instance, they painstakingly reconstructed how cinema emerged out of preceding cultural forms (panorama, optical toys, peep shows), why didn't they attempt to construct a similar genealogy for the language of computer media at the moment when it was just coming into being, while the elements of previous cultural forms going into its making are still clearly visible, still recognizable before melting into a new unity. Where there the theoreticians at the moment when the icons and the buttons of multimedia interfaces were like a wet paint on a just completed painting, before they became a universal convention and thus slipped into invisibility? Or, at the moment when the designers of Myst were debugging their code, converting graphics to 8-bit and massaging QuickTime clips? Or, at the historical moment when a young 20-something programmer at Netscape took the chewing gum out of his mouth, sipped warm Coke out of the can — he was at a computer for 16 hours straight, trying to meet a marketing deadline — and, finally satisfied with its small file size, saved a short animation of stars moving across the night sky, the animation which was to appear in the upper right corner of Netscape Navigator, thus becoming the most widely seen moving image sequence ever — until the next release.

The following is an attempt at both a record and a theory — of the present. Just as film historians traced the development of film language during cinema's first decades, I want to describe and understand the logic driving the development of the language of computer media. It is tempting to extend this parallel a little further and to speculate whether today this new language is already getting closer to acquiring its final and stable form, just as film language acquired its "classical" form during the 1910s. Or are the 1990s more like the 1890s, because future computer media language will be entirely different than the one used today? [2] In either case, by trying to understand which cultural forces are shaping the development of this language, we may be in a better position both to predict its future course as well as to offer different alternatives. For just as avant-garde filmmakers throughout cinema's existence offered alternatives to its particular narrative audio-visual regime, the task of an avant-garde computer artist today is to offer alternatives to the existing language of computer media. This can be better accomplished if we have a theory of how "mainstream" language is currently structured.

Does it make sense to theorize the present when it seems to be changing so fast? It is a gamble. If subsequent developments prove the theoretical projections of this text to be correct, I win. But, if the language of computer media develops in a different direction than the one suggested by the present analysis, this does not mean that I automatically lose. Rather, the analysis presented here will become a record of possibilities which were heretofore not realized, of the horizon which was visible to us today but later became unimaginable.

We no longer think of the history of cinema as a linear march towards only one possible language, or as a progression towards more and more accurate verisimilitude. Rather, we have come to see its history as a succession of distinct and equally expressive languages, each with its own aesthetic variables, each new language closing off some of the possibilities of the previous one — a cultural logic not dissimilar to Kuhn's analysis of scientific paradigms. [3] Similarly, every stage in the history of computer media offers its own aesthetic opportunities, as well as its own imagination of the future — in short, its own "research paradigm". This paradigm is modified or even abandoned at the next stage. In this paper I want to record the "research paradigm" of new media during its first decade before it slips into invisibility.

## Cultural Interfaces

During the 1990s, the cultural role of a digital computer has changed from a tool to a medium. In the beginning of the decade, a computer was still largely thought of as a simulation of a typewriter, a paintbrush or a drafting ruler — in other words, as a tool used to produce cultural content which, once created, will be stored and distributed in its appropriate media: printed page, film, photographic print, electronic recording. By the end of the decade, the computer's public image has begun to shift to one of a universal machine, used not only to author, but also to store, distribute and access all media. All culture, past and present, is beginning to be filtered through a computer, with its particular human-computer interface.

The term human-computer interface (HCI) describes the ways in which the user interacts with a computer. HCI includes physical input and output devices such a monitor, a keyboard, and a mouse. It also consists of metaphors used to conceptualize the organization of computer data. For instance, the Macintosh interface introduced by Apple in 1984 uses the metaphor of files and folders arranged on a desktop. Finally, HCI also includes ways of manipulating this data, i.e., a grammar of meaningful actions which the user can perform on it. An example of this grammar is the commands used in a command-line interface such as DOS and UNIX: copy file, delete file, set date, open port, list directory, and so on.

As the role of a computer is shifting from being a tool to a universal media machine, we are increasingly "interfacing" to predominantly cultural data: texts, photographs, films, music, virtual environments. In short, we are no longer interfacing to a computer but to culture encoded in digital form. I would like to introduce the term "cultural interfaces" to describe evolving interfaces used by the designers of Web sites, CD-ROM and DVD-ROM titles, multimedia encyclopedias, online museums, computer games and other digital cultural objects.

If you need to remind yourself what a typical cultural interface looked like in 1997, go back in time and click to a random Web page. You are likely to see something which graphically resembles a magazine layout from the same decade. The page is dominated by text: headlines, hyperlinks, blocks of copy. Within this text are few media elements: graphics, photographs, perhaps a QuickTime movie and a VRML scene. The page also includes radio buttons and a pull-down menu which allows you to choose an item from the list. Finally, there is a search engine: type a word or a phrase, hit the search button and the computer will scan through a file or a database trying to match your entry.

For another example of a prototypical cultural interface of the 1990s, you may load (assuming it would still run on your computer) the most well-known CD-ROM of the 1990s - Myst (Broderbund, 1993). Its opening clearly recalls a movie: credits slowly scroll across the screen, accompanied by a movie-like soundtrack to set the mood. Next, the computer screen shows a book open in the middle, waiting for your mouse click. Next, an element of a familiar Macintosh interface makes an appearance, reminding you that along with being a new movie/book hybrid, Myst is also a computer application: you can adjust sound volume and graphics quality by selecting from a usual Macintosh-style menu in the upper top part of the screen. Finally, you are taken inside the game, where the interplay between the printed word and cinema continues. A virtual camera frames images of an island which dissolve between each other. At the same time, you keep encountering books and letters, which take over the screen, providing with you with clues on how to progress in the game.

Given that computer media is simply a set of characters and numbers stored in a computer, there are numerous ways in which it could be presented to a user. Yet, as it always happens with cultural languages, only a few of these possibilities actually appear viable in a given historical moment. Just as early fifteenth-century Italian painters could only conceive of painting in a very particular way - quite different from, say, sixteenth-century Dutch painters — today's digital designers and artists use a small set of action grammars and metaphors out of a much larger set of all possibilities.

Why do cultural interfaces — web pages, CD-ROM titles, computer games — look the way they do? Why do designers organize computer data in certain ways and not in others? Why do they employ some interface metaphors and not others?

My theory is that there are three key cultural forms which are shaping cultural interfaces in the 1990s. What are these forms? The answer to this puzzle can be found in the opening sequence of Myst which activates them before our eyes, one by one. The first form is cinema. The second form is the printed word. The third form is a general-purpose human-computer interface (HCI).

At the time of this writing (1997), it appears that out of the three, the influence of cinema is becoming more and more important. So, despite frequent pronouncements that cinema is dead, it is actually on its own way to becoming a general purpose cultural interface, a set of techniques and tools which can be used to interact with any cultural data. Accordingly, I will devote the largest section of this article to the discussion of the ways in which cinematic techniques structure cultural interfaces.

As it should become clear from the following, I use words "cinema" and "printed word" as shortcuts. They stand not for particular objects, such as a film or a novel, but rather for larger cultural traditions (we can also use such words as cultural forms, mechanisms, languages or media). "Cinema" thus includes mobile camera, representation of space, editing techniques, narrative conventions, activity of a spectator — in short, different elements of cinematic perception, language and reception. Their presence is not limited to the twentieth-century institution of fiction films, they can be already found in panoramas, magic lantern slides, theater and other nineteenth-century cultural forms; similarly, since the middle of the twentieth century, they are present not only in films but also in television and video programs. In the case of the "printed word" I am also referring to a set of conventions which have developed over many centuries (some even before the invention of print) and which today are shared by numerous forms of printed matter, from magazines to instruction manuals: a rectangular page containing one or more columns of text; illustrations or other graphics framed by the text; pages which follow each sequentially; a table of contents and index.

Modern human-computer interface has a much shorter history than the printed word or cinema — but it is still a history. Its principles such as direct manipulation of objects on the screen, overlapping windows, iconic representation, and dynamic menus were gradually developed over a few decades, from the early 1950s to the early 1980s, when they finally appeared in commercial systems such as Xerox Star (1981), the Apple Lisa (1982), and most importantly the Apple Macintosh (1984). [4] Since then, they have become an accepted convention for operating a computer, and a cultural language in their own right.

Cinema, the printed word and human-computer interface: each of these traditions has developed its own unique ways of how information is organized, how it is presented to the user, how space and time are correlated with each other, how human experience is being structured in the process of accessing information. Pages of text and a table of contents; 3-D spaces framed by a rectangular frame which can be navigated using a mobile point of view; hierarchical menus, variables, parameters, copy/paste and search/replace operations — these and other elements of these three traditions are shaping cultural interfaces today. Cinema, the printed word and HCI: they are the three main reservoirs of metaphors and strategies for organizing information which feed cultural interfaces.

Bringing cinema, the printed word and HCI interface together and treating them as occupying the same conceptual plane has an additional advantage — a theoretical bonus. It is only natural to think of them as belonging to two different kind of cultural species, so to speak. If HCI is a general-purpose tool which can be used to manipulate any kind of data, both the printed word and cinema are less general: they offer ways to organize particular types of data: text in the case of print, audio-visual narrative taking place in a 3-D space in the case of cinema. HCI is a system of controls to operate a machine; the printed word and cinema are cultural traditions, distinct ways to record human memory and human experience, mechanisms for cultural and social exchange of information. Bringing HCI, the printed word and cinema together allows us to see that the three have more in common than we may anticipate at first. On the one hand, being a part of our culture now for half a century, HCI already represents a powerful cultural tradition, a cultural language offering its own ways to represent human memory and human experience. This language speaks in the form of discrete objects organized in hierarchies (hierarchical file system), or as catalogs (databases), or as objects linked together through hyperlinks (hypermedia). On the other hand, we begin to see that the printed word and cinema also can be thought of as interfaces, even though historically they have been tied to particular kinds of data. Each has its own grammar of actions, each comes with its own metaphors, each offers a particular physical interface. A book or a magazine is a solid object consisting of separate pages; the actions include going from page to page linearly, marking individual pages and using table of contents. In the case of cinema, its physical interface is a particular architectural arrangement of a movie theater; its metaphor is a window opening up into a virtual 3-D space.

Today, as media is being "liberated" from its traditional physical storage media — paper, film, stone, glass, magnetic tape — the elements of printed word interface and cinema interface, which previously were hardwired to the content, become "liberated" as well. A digital designer can freely mix pages and virtual cameras, table of contents and screens, bookmarks and points of view. No longer embedded within particular texts and films, these organizational strategies are now free floating in our culture, available for use in new contexts. In this respect, printed word and cinema have indeed become interfaces — rich sets of metaphors, ways of navigating through content, ways of accessing and storing data. For a user, both conceptually and psychologically, their elements exist on the same plane as radio buttons, pull-down menus, command line calls and other elements of standard human-computer interface.

Let us now discuss some of the elements of these three cultural traditions — cinema, the printed word and HCI — to see how they are shaping the language of cultural interfaces.

## I. Printed Word

In the 1980s, as PC's and word processing software became commonplace, text became the first cultural media to be subjected to digitization in a massive way. But already in the 1960s, two and a half decades before the concept of digital media was born, researchers were thinking about having the sum total of human written production — books, encyclopedias, technical articles, works of fiction and so on — available online (Ted Nelson's Xanadu project [5]).

Text is unique among other media types. It plays a privileged role in computer culture. On the one hand, it is one media type among others. But, on the other hand, it is a meta-language of digital media, a code in which all other media are represented: coordinates of 3-D objects, pixel values of digital images, the formatting of a page in HTML. It is also the primary means of communication between a computer and a user: one types single line commands or runs computer programs written in a subset of English; the other responds by displaying error codes or text messages. [6]

If a computer uses text as its meta-language, cultural interfaces in their turn inherit the principles of text organization developed by human civilization throughout its existence. One of these is a page: a rectangular surface containing a limited amount of information, designed to be accessed in some order, and having a particular relationship to other pages. In its modern form, the page is born in the first centuries of the Christian era when the clay tablets and papyrus rolls are replaced by a codex — the collection of written pages stitched together on one side.

Cultural interfaces rely on our familiarity with the "page interface" while also trying to stretch its definition to include new concepts made possible by a computer. In 1984, Apple introduced a graphical user interface which presented information in overlapping windows stacked behind one another — essentially, a set of book pages. The user was given the ability to go back and forth between these pages, as well as to scroll through individual pages. In this way, a traditional page was redefined as a virtual page, a surface which can be much larger than the limited surface of a computer screen. In 1987, Apple shipped popular Hypercard program which extended the page concept in new ways. Now the users were able to include multimedia elements within the pages, as well as to establish links between pages regardless of their ordering. A few years later, designers of HTML stretched the concept of a page even more by enabling the creation of distributed documents, where different parts of a document are located on different computers connected through the network. With this development, a long process of gradual "virtualization" of the page reached a new stage. Messages written on clay tablets, which were almost indestructible, were replaced by ink on paper. Ink, in its turn, was replaced by bits of computer memory, making characters on an electronic screen. Finally, with HTML, which allows parts of a single page to be located on different computers, the page became even more fluid and unstable.

The conceptual development of the page in digital media can also be read in a different way — not as further development of a codex form, but as a return to earlier forms such as the papyrus roll of ancient Egypt, Greece and Rome. Scrolling through the contents of a computer window or a World Wide Web page has more in common with unrolling than turning the pages of a modern book. In the case of the Web of the 1990s, the similarity with a roll is even stronger because the information is not available all at once, but arrives sequentially, top to bottom, as though the roll is being unrolled.

A good example of how cultural interfaces stretch the definition of a page while mixing together its different historical forms is the Web page designed in 1997 by the British design collective antirom for HotWired RGB Gallery. [7] The designers have created a large surface containing rectangular blocks of texts in different font sizes, arranged without any apparent order. The user is invited to skip from one block to another moving in any direction. Here, the different directions of reading used in different cultures are combined together in a single page.

By the mid-1990s, Web pages included a variety of media types — but they are still essentially pages. Different media elements — graphics, photographs, digital video, sound and 3-D worlds — were embedded within rectangular surfaces containing text. VRML evangelists wanted to overturn this hierarchy by imaging the future in which the World Wide Web is rendered as a giant 3-D space, with all the other media types, including text, existing within it. [8] Given that the history of a page stretches for thousands of years, I think it is unlikely that it would disappear so quickly.

While the 1990s cultural interfaces have retained the modern page format, they also have come to rely on a new way of organizing and accessing texts which have little precedent within book tradition — hyperlinking. We may be tempted to trace hyperlinking to earlier forms and practices of non-sequential text organization, such as the Torah's interpretations and footnotes, but it is actually fundamentally different from them. Both the Torah's interpretations and footnotes imply a master-slave relationship between one text and another. But in the case of hyperlinking, no such relationship of hierarchy is assumed. The two sources connected through hyperlinking have equal weight; they exist on the same level of importance. Thus the acceptance of hyperlinking in the 1980s can be read as a perfect reflection of contemporary culture with its suspicion of all hierarchies, and its aesthetics of collage where radically different sources are brought together within the singular cultural object ("post-modernism").

Traditionally, texts encoded human knowledge and memory, instructed, inspired, and seduced their readers to adopt new ideas, new ways of interpreting the world, new ideologies. In short, the word was always linked to the art of rhetoric. While it is probably possible to invent a new rhetoric of hypermedia, which will use hyperlinking not to distract the reader from the argument (as it is often the case today), but instead to further convince him/her of argument's validity, the sheer existence and popularity of hyperlinking exemplifies the continuing decline of the field of rhetoric in the modern era. Ancient and Medieval scholars have classified hundreds of different rhetorical figures. In the middle of the twentieth century Roman Jakobson, under the influence of computer's binary logic, information theory and cybernetics to which he was exposed at MIT, radically reduced rhetoric to just two figures: metaphor and metonymy. [9] Finally, in the 1990s, the World Wide Web hyperlinking privileged the single figure of metonymy at the expense of all others. [10] The hypertext of the World Wide Web leads the reader from one text to another, ad infinitum. Contrary to the popular image, in which digital media collapses all human culture into a single giant library (which implies the existence of some ordering system), or a single giant book (which implies a narrative progression), it maybe more accurate to think of the resulting object as an infinite flat surface composed from individual texts in no particular order — the antirom design for HotWired. Expanding this comparison further, we can note that Random Access Memory, the concept behind the group's name, also implies the lack of any hierarchy: any RAM location can be accessed as quickly as any other. In contrast to the older storage media of book, film, and magnetic tape, where data is organized sequentially and linearly, thus suggesting the presence of a narrative or a rhetorical trajectory, RAM "flattens" the data. Rather than seducing the user through the careful arrangement of arguments and examples, points and counterpoints, changing rhythms of presentation (i.e., the rate of data streaming, to use contemporary language), simulated false paths and orchestrated breakthroughs, cultural interfaces, like RAM itself, bombards the users with all the data at once. [11]

In the 1980s many critics described one of key's effects of "post-modernism" as that of spatialization: privileging space over time, flattening historical time, refusing grand narratives. Digital media, which has evolved during the same decade, accomplished this spatialization quite literally. It replaced sequential storage with random-access storage; hierarchical organization of information with a flattened hypertext; psychological movement of narrative in novel and cinema with physical movement through space, as witnessed by endless computer animated fly-throughs or computer games such as Myst and countless others. In short, time becomes a flat image or a landscape, something to look at or navigate through. If there is a new rhetoric or aesthetic which is possible here, it may have less to do with the ordering of time by a writer or an orator, and more with spatial wandering. The hypertext reader is like Robinson Crusoe, walking through the sand and water, picking up a navigation journal, a rotten fruit, an instrument whose purpose he does not know; leaving imprints in the sand, which, like computer hyperlinks, follow from one found object to another.

## II. Cinema

Printed word tradition which has initially dominated the language of cultural interfaces, is becoming less important, while the part played by cinematic elements is getting progressively stronger. This is consistent with a general trend in modern society towards presenting more and more information in the form of time-based audio-visual moving image sequences, rather than as text. As new generations of both computer users and computer designers are growing up in a media-rich environment dominated by television rather than by printed texts, it is not surprising that they favor cinematic language over the language of print.

A hundred years after cinema's birth, cinematic ways of seeing the world, of structuring time, of narrating a story, of linking one experience to the next, are being extended to become the basic ways in which computer users access and interact with all cultural data. In this way, the computer fulfills the promise of cinema as a visual Esperanto which pre-occupied many film artists and critics in the 1920s, from Griffith to Vertov. Indeed, millions of computer users communicate with each other through the same computer interface. And, in contrast to cinema where most of its "users" were able to "understand" cinematic language but not "speak" it (i.e., make films), all computer users can "speak" the language of the interface. They are active users of the interface, employing it to perform many tasks: send an email, run basic applications, organize files and so on.

The original Esperanto never became truly popular. But cultural interfaces are widely used and are easily learned. We have an unprecedented situation in the history of cultural languages: something which is designed by a rather small group of people is immediately adopted by millions of computer users. How is it possible that people around the world adopt today something which a 20-something programmer in Northern California has hacked together just the night before? Shall we conclude that we are somehow biologically "wired" to the interface language, the way we are "wired," according to the original hypothesis of Noam Chomsky, to different natural languages?

The answer is of course no. Users are able to "acquire" new cultural languages, be it cinema a hundred years ago, or cultural interfaces today, because these languages are based on previous and already familiar cultural forms. In the case of cinema, it was theater, magic lantern shows and other nineteenth-century forms of public entertainment. Cultural interfaces in their turn draw on older cultural forms such as the printed word and cinema. I have already discussed some ways in which the printed word tradition structures interface language; now it is cinema's turn.

I will begin with probably the most important case of cinema's influence on cultural interfaces — the mobile camera. Originally developed as part of 3-D computer graphics technology for such applications as computer-aided design, flight simulators and computer movie making, during the 1980s and 1990s the camera model became as much of an interface convention as scrollable windows or cut and paste function. It became an accepted way for interacting with any data which is represented in three dimensions — which, in a computer culture, means literally anything and everything: the results of a physical simulation, an architectural site, design of a new molecule, financial data, the structure of a computer network and so on. As computer culture is gradually spatializing all representations and experiences, they become subjected to the camera's particular grammar of data access. Zoom, tilt, pan and track: we now use these operations to interact with data spaces, models, objects and bodies.

Abstracted from its historical temporary "imprisonment" within the physical body of a movie camera directed at physical reality, a virtualized camera also becomes an interface to all types of media besides 3-D space. As an example, consider GUI (Graphical User Interface) of the leading computer animation software — PowerAnimator from Alias/Wavefront. [12] In this interface, each window, regardless of whether it displays a 3-D model, a graph or even plain text, contains Dolly, Track and Zoom buttons. In this way, the model of a virtual camera is extended to apply to navigation through any kind of information, not only the one which was spatialized. It is particularly important that the user is expected to dolly and pan over text as if it was a 3-D scene. Cinematic vision triumphed over the print tradition, with the camera subsuming the page. The Guttenberg galaxy turned out to be just a subset of the Lumières' universe.

Another feature of cinematic perception which persists in cultural interfaces is a rectangular framing of represented reality. [13] Cinema itself inherited this framing from Western painting. Since the Renaissance, the frame acted as a window onto a larger space which was assumed to extend beyond the frame. This space was cut by the frame's rectangle into two parts: "onscreen space," the part which is inside the frame, and the part which is outside. In the famous formulation of Leon-Battista Alberti, the frame acted as a window onto the world. Or, in a more recent formulation of Jacques Aumont and his co-authors, "The onscreen space is habitually perceived as included within a more vast scenographic space. Even though the onscreen space is the only visible part, this larger scenographic part is nonetheless considered to exist around it." [14]

Just as a rectangular frame of painting and photography presents a part of a larger space outside it, a window in HCI presents a partial view of a larger document. But if in painting (and later in photography), the framing chosen by an artist was final, computer interface benefits from a new invention introduced by cinema: the mobility of the frame. As a kino-eye moves around the space revealing its different regions, so can a computer user scroll through a window's contents.

It is not surprising to see that screen-based interactive 3-D environments, such as VRML words, also use cinema's rectangular framing since they rely on other elements of cinematic vision, specifically a mobile virtual camera. It may be more surprising to realize that Virtual Reality (VR) interface, often promoted as the most "natural" interface of all, utilizes the same framing. [15] As in cinema, the world presented to a VR user is cut by a rectangular frame. As in cinema, this frame presents a partial view of a larger space. [16] As in cinema, the virtual camera moves around to reveal different parts of this space.

Of course, the camera is now controlled by the user and in fact is identified with his/her own sight. Yet, it is crucial that in VR one is seeing the virtual world through a rectangular frame, and that this frame always presents only a part of a larger whole. This frame creates a distinct subjective experience which is much closer to cinematic perception than to unmediated sight.

Interactive virtual worlds, whether accessed through a screen-based or a VR interface, are often discussed as the logical successor to cinema, as potentially the key cultural form of the twenty-first century, just as cinema was the key cultural form of the twentieth century. These discussions usually focus on the issues of interaction and narrative. So, the typical scenario for twenty-first-century cinema involves a user represented as an avatar existing literally "inside" the narrative space, rendered with photorealistic 3-D computer graphics, interacting with virtual characters and perhaps other users, and affecting the course of narrative events.

It is an open question whether this and similar scenarios commonly invoked in new media discussions of the 1990s, indeed represent an extension of cinema or if they rather should be thought of as a continuation of some theatrical traditions, such as improvisational or avant-garde theater. But what undoubtedly can be observed in the 1990s is how virtual technology's dependence on cinema's mode of seeing and language is becoming progressively stronger. This coincides with the move from proprietary and expensive VR systems to more widely available and standardized technologies, such as VRML (Virtual Reality Modeling Language). [17]

The creator of a VRML world can define a number of viewpoints which are loaded with the world. [18] These viewpoints automatically appear in a special menu in a VRML browser which allows the user to step through them, one by one. Just as in cinema, ontology is coupled with epistemology: the world is designed to be viewed from particular points of view. The designer of a virtual world is thus a cinematographer as well as an architect. The user can wander around the world or she can save time by assuming the familiar position of a cinema viewer for whom the cinematographer has already chosen the best viewpoints.

Equally interesting is another option which controls how a VRML browser moves from one viewpoint to the next. By default, the virtual camera smoothly travels through space from the current viewpoint to the next as though on a dolly, its movement automatically calculated by the software. Selecting the "jump cuts" option makes it cut from one view to the next. Both modes are obviously derived from cinema. Both are more efficient than trying to explore the world on their own.

With a VRML interface, nature is firmly subsumed under culture. The eye is subordinated to the kino-eye. The body is subordinated to a virtual body of a virtual camera. While the user can investigate the world on her own, freely selecting trajectories and viewpoints, the interface privileges cinematic perception — cuts, pre-computed dolly-like smooth motions of a virtual camera, and pre-selected viewpoints.

The area of computer culture where cinematic interface is being transformed into a cultural interface most aggressively is computer games. By the 1990s, game designers had moved from two to three dimensions and had begun to incorporate cinematic language in an increasingly systematic fashion. Games started featuring lavish opening cinematic sequences (called in the game business "cinematics") to set the mood, establish the setting and introduce the narrative. Frequently, the whole game would be structured as an oscillation between interactive fragments requiring user's input and non-interactive cinematic sequences, i.e. "cinematics". [19] As the decade progressed, game designers were creating increasingly complex — and increasingly cinematic — interactive virtual worlds. Regardless of a game's genre — action/adventure, fighting, flight simulator, first-person action, racing or simulation — they came to rely on cinematography techniques borrowed from traditional cinema, including the expressive use of camera angles and depth of field, and dramatic lighting of 3-D sets to create mood and atmosphere. In the beginning of the decade, games used digital video of actors superimposed over 2-D or 3-D backgrounds, but by its end they switched to fully synthetic characters. [20] This switch also made virtual words more cinematic, as the characters could be better visually integrated with their environments. [21]

A particularly important example of how computer games use — and extend — cinematic language, is their implementation of a dynamic point of view. In driving and flying simulators and in combat games, such as Tekken 2 (Namco, 1994 -), after a certain event takes place (car crashes, a fighter being knocked down), it is automatically replayed from a different point of view. Other games such as the Doom series (Id Software, 1993 -) and Dungeon Keeper (Bullfrog Productions, 1997) allow the user to switch between the point of view of the hero and a top-down "bird's eye" view. Finally, Nintendo went even further by dedicating four buttons on their N64 joypad to controlling the view of the action. While playing Nintendo games such as Super Mario 64 (Nintendo, 1996) the user can continuously adjust the position of the camera. Some Sony Playstation games such as Tomb Rider (Eidos, 1996) also use the buttons on the Playstation joypad for changing point of view.

The incorporation of virtual camera controls into the very hardware of a game consoles is truly a historical event. Directing the virtual camera becomes as important as controlling the hero's actions. This is admitted by the game industry itself. For instance, a package for Dungeon Keeper lists four key features of the game, out of which the first two concern control over the camera: "switch your perspective," "rotate your view," "take on your friend," "unveil hidden levels". In games such as this one, cinematic perception functions as the subject in its own right. [22] Here, the computer games are returning to "The New Vision" movement of the 1920s (Moholy-Nagy, Rodchenko, Vertov and others), which foregrounded new mobility of a photo and film camera, and made unconventional points of view the key part of their poetics.

The fact that computer games continue to encode, step by step, the grammar of a kino-eye in software and in hardware is not an accident. This encoding is consistent with the overall trajectory driving the computerization of culture since the 1940s, that being the automation of all cultural operations. This automation gradually moves from basic to more complex operations: from image processing and spell checking to software-generated characters, 3-D worlds, and Web Sites. The side effect of this automation is that once particular cultural codes are implemented in low-level software and hardware, they are no longer seen as choices but as unquestionable defaults. To take the automation of imaging as an example, in the early 1960s the newly emerging field of computer graphics incorporated a linear one-point perspective in 3-D software, and later directly in hardware. [23] As a result, linear perspective became the default mode of vision in digital culture, be it computer animation, computer games, visualization or VRML worlds. Now we are witnessing the next stage of this process: the translation of cinematic grammar of points of view into software and hardware. As Hollywood cinematography is translated into algorithms and computer chips, its convention becomes the default method of interacting with any data subjected to spatialization, with a narrative, and with other human beings. (At SIGGRAPH '97 in Los Angeles, one of the presenters called for the incorporation of Hollywood-style editing in multi-user virtual worlds software. In such implementation, user interaction with other avatar(s) will be automatically rendered using classical Hollywood conventions for filming dialog. [24]) Element by element, cinema is being poured into a computer: first one-point linear perspective; next the mobile camera and a rectangular window; next cinematography and editing conventions, and, of course, digital personas also based on acting conventions borrowed from cinema, to be followed by make-up, set design, and, of course, the narrative structures themselves. From one cultural language among others, cinema is becoming the cultural interface, a toolbox for all cultural communication, overtaking the printed word.

But, in one sense, all computer software already has been based on a particular cinematic logic. Consider the key feature shared by all modern human-computer interfaces — overlapping windows. [25] All modern interfaces display information in overlapping and resizable windows arranged in a stack, similar to a pile of papers on a desk. As a result, the computer screen can present the user with practically an unlimited amount of information despite its limited surface.

Overlapping windows of HCI can be understood as a synthesis of two basic techniques of twentieth-century cinema: temporal montage and montage within a shot. In temporal montage, images of different realities follow each other in time, while in montage within the shot, these different realities co-exist within the screen. The first technique defines the cinematic language as we know it; the second is used more rarely. An example of this technique is the dream sequence in The Life of an American Fireman by Edward Porter in 1903, in which an image of a dream appears over a man's sleeping head. Other examples include the split screens beginning in 1908 which show the different interlocutors of a telephone conversation; superimpositions of a few images and multiple screens used by the avant-garde filmmakers in the 1920s; and the use of deep focus and a particular compositional strategy (for instance, a character looking through a window, such as in Citizen Kane, Ivan the Terrible and Rear Window) to juxtapose close and far away scenes. [26]

As testified by its popularity, temporal montage works. However, it is not a very efficient method of communication: the display of each additional piece of information takes time to watch, thus slowing communication. It is not accidental that the European avant-garde of the 1920s was inspired by the engineering ideal of efficiency, experiments with various alternatives, trying to load the screen with as much information at one time as possible. [27] In his 1927 Napoleon Abel Gance uses a multiscreen system which shows three images side by side. Two years later, in A Man with a Movie Camera (1929) we watch Dziga Vertov speeding up the temporal montage of individual shots, more and more, until he seems to realize: why not simply superimpose them in one frame? Vertov overlaps the shots together, achieving temporal efficiency — but he also pushes the limits of a viewer's cognitive capacities. His superimposed images are hard to read — information becomes noise. Here cinema reaches one of its limits imposed on it by human psychology; from that moment on, cinema retreats, relying on temporal montage or deep focus, and reserving superimpositions for infrequent cross-dissolves.

In window interface, the two opposites — temporal montage and montage within the shot — finally come together. The user is confronted with a montage within the shot — a number of windows present at once, each window opening up into its own reality. This, however, does not lead to the cognitive confusion of Vertov's superimpositions because the windows are opaque rather than transparent, so the user is only dealing with one of them at a time. In the process of working with a computer, the user repeatedly switches from one window to another, i.e., the user herself becomes the editor accomplishing montage between different shots. In this way, window interface synthesizes two different techniques of presenting information within a rectangular screen developed by cinema.

This last example shows once again the extent to which human-computer interfaces — and the cultural interfaces which follow them — are cinematic, inheriting cinema's particular ways of organizing perception, attention and memory. Yet it also demonstrates the cognitive distance between cinema and the computer age. For the viewers of the 1920s, the temporal replacement of one image by another, as well as superimposition of two images together were an aesthetic and perceptual event, a truly modern and unfamiliar experience. The cut from one image to another was a meaningful, even stressful event, because audiences had to assimilate a sequence in a different fashion than they were previously used to in other cultural forms. [28] Film directors exploited the novelty of this strategy as an effective way of creating meaning. At the end of the century, however, anesthetized first by cinema and then by television channel flipping, we feel at home with a number of overlapping windows on a computer screen. We switch back and forth between different applications, processes, tasks. Not only are we no longer shocked, but in fact, we feel angry when a computer occasionally crashes because we opened too many windows at once.

Cinema, the major cultural form of the twentieth century, has found a new life as the toolbox of a computer user. Cinematic means of perception, of connecting space and time, of representing human memory, thinking, and emotions become a way of work and a way of life for millions in the computer age. Cinema's aesthetic strategies have become basic organizational principles of computer software. The window in a fictional world of a cinematic narrative has become a window in a datascape. In short, what was cinema has become human-computer interface.

I will conclude this section by discussing a few artistic projects which, in different ways, offer alternatives to this trajectory. To summarize it once again, the trajectory involves gradual translation of elements and techniques of cinematic perception and language into a decontextualized set of tools to be used as an interface to any data. In the process of this translation, cinematic perception is divorced from its original material embodiment (camera, film stock), as well as from the historical contexts of its formation. If in cinema the camera functioned as a material object, co-existing, spatially and temporally, with the world it was showing us, it has now become a set of abstract operations. The art projects described below refuse this separation of cinematic vision from the material world. They reunite perception and material reality by making the camera and what it records a part of a virtual world's ontology. They also refuse the universalization of cinematic vision by computer culture, which (just as post-modern visual culture in general) treats cinema as a toolbox, a set of "filters" which can be used to process any input. In contrast, each of these projects employs a unique cinematic strategy which has a specific relation to the particular virtual world it reveals to the user.

In my own project Reality Generator (1996 - ongoing) I directly make points of view a part of the ontology of a virtual world. The world is described as a set of objects and a set of viewpoints attached to different points in space. Some viewpoints are simply XYZ coordinates which do not correspond to anything in particular. Other viewpoints are attached to particular objects: a leaf, a bottle on the ground, a cloud. In this way, every object also becomes the subject, the focalizer of the narrative. [29] Everything can be seen from any position. Modernist techniques of switching between narrators in different parts of the story and re-telling the same events from different points of view are combined with computer's combinatory logic.

In The Invisible Shape of Things Past Joachim Sauter and Dirk Lusenbrink of the Berlin-based Art+Com collective created a truly innovative cultural interface for accessing historical data about Berlin's history. [30] The interface de-virtualizes cinema, so to speak, by placing the records of cinematic vision back into their historical and material context. As the user navigates through a 3-D model of Berlin, he or she comes across elongated shapes lying on city streets. These shapes, which the authors call "filmobjects", correspond to documentary footage recorded at the corresponding points in the city. To create each shape the original footage is digitized, and the frames are stacked one after another in depth, with the original camera parameters determining the exact shape. The user can view the footage by clicking on the first frame. As the frames are displayed one after another, the shape is getting correspondingly thinner.

In following with the already noted general trend of computer culture towards spatialization of every cultural experience, this cultural interface spatializes time, representing it as a shape in a 3-D space. This shape can be thought of as a book, with individual frames stacked one after another as book pages. The trajectory through time and space were taken by a camera becomes a book to be read, page by page. The records of camera's vision become material objects, sharing the space with the material reality which gave rise to this vision. Cinema is solidified. This project, then, can be also understood as a virtual monument to cinema. The (virtual) shapes situated around the (virtual) city, remind us of the era when cinema was the defining form of cultural expression — as opposed to a toolbox for data retrieval and use, as it is becoming today in a computer.

Hungarian-born artist Tamás Waliczky openly refuses the default mode of vision imposed by computer software, that of the one-point linear perspective. Each of his computer animated films The Garden (1992), The Forest (1993) and The Way (1994) utilizes a particular perspectival system: a water-drop perspective in The Garden, a cylindrical perspective in The Forest and a reverse perspective in The Way. Working with computer programmers, the artist created custom-made 3-D software to implement these perspectival systems. Each of the systems has an inherent relationship to the subject of a film in which it is used. In The Garden, its subject is the perspective of a small child, for whom the world does not yet have an objective existence. In The Forest, the mental trauma of emigration is transformed into the endless roaming of a camera through the forest which is actually just a set of transparent cylinders. Finally, in The Way, the self-sufficiency and isolation of a Western subject from his/her environment are conveyed by the use of a reverse perspective.

In Waliczky's films the camera and the world are made into a single whole, whereas in The Invisible Shape of Things Past the records of the camera are placed back into the world. Rather than simply subjecting his virtual worlds to different types of perspectival projection, Waliczky modified the spatial structure of the worlds themselves. In The Garden, a child playing in a garden becomes the center of the world; as he moves around, the actual geometry of all the objects around him is transformed, with objects getting bigger as he gets close to him. To create The Forest, a number of cylinders were placed inside each other, each cylinder mapped with a picture of a tree, repeated a number of times. In the film, we see a camera moving through this endless static forest in a complex spatial trajectory — but this is an illusion. In reality, the camera does move, but the architecture of the world is constantly changing as well, because each cylinder is rotating at its own speed. As a result, the world and its perception are fused together.

## III. Human-Computer Interface

The development of human-computer interfaces, until recently, had little to do with cultural applications. Following some of the main applications from the 1940s until the early 1980s, when the current generation of GUI (Graphic User Interface) was developed and reached the mass market together with the rise of a PC (personal computer), we can list the most significant: real-time control of weapons and weapon systems; scientific simulation; computer-aided design; finally, office work with a secretary as a prototypical computer user, filing documents in a folder, emptying a trash can, creating and editing documents ("word processing"). Today, as the computer is starting to host very different applications for access and manipulation of cultural data and cultural experiences, their interfaces still rely on old metaphors and action grammars. Thus, cultural interfaces predictably use elements of a general-purpose HCI such as scrollable windows containing text and other data types, hierarchical menus, dialogue boxes, and command-line input. For instance, a typical "art collection" CD-ROM may try to recreate "the museum experience" by presenting a navigable 3-D rendering of a museum space, while still resorting to hierarchical menus to allow the user to switch between different museum collections. Even in the case of The Invisible Shape of Things Past which uses a unique interface solution of "filmobjects" which is not directly traceable to either old cultural forms or general-purpose HCI, the designers are still relying on HCI convention in one case - the use of a pull-down menu to switch between different maps of Berlin.

In general, cultural interfaces of the 1990s try to walk an uneasy path between the richness of control provided in general-purpose HCI and an "immersive" experience of traditional cultural objects such as books and movies. Modern general-purpose HCI, be it MAC OS, Windows or Unix, allow their users to perform complex and detailed actions on the digital data: get information about an object, copy it, move it to another location, change the way data is displayed, etc. In contrast, a conventional book or a film positions the user inside the imaginary universe whose structure is fixed by the author. Cultural interfaces attempt to mediate between these two fundamentally different and ultimately non-compatible approaches.

As an example, consider how cultural interfaces conceptualize the computer screen. If a general-purpose HCI clearly identifies to the user that certain objects can be acted on while others cannot (icons of files but not the desktop itself), cultural interfaces typically hide the hyperlinks within a continuous representational field. (This technique was already so widely accepted by the 1990s that the designers of HTML offered it early on to their users by implementing the "imagemap" feature). The field can be a two-dimensional collage of different images, a mixture of representational elements and abstract textures, or a single image of a space such as a city street or a landscape. By trial and error, clicking all over the field, the user discovers that some parts of this field are links. This concept of a screen combines two distinct pictorial conventions: the older Western tradition of pictorial illusionism in which a screen functions as a window into a virtual space, something for the viewer to look into but not to act upon; and the more recent convention of graphical human-computer interfaces which, by dividing the computer screen into a set of controls with clearly delineated functions, essentially treats it as a virtual instrument panel. As a result, the computer screen becomes a battlefield for a number of incompatible definitions: depth and surface, opaqueness and transparency, image as an illusionary space and image as an instrument for action. [31]

Here is another example of how cultural interfaces try to find a middle ground between the conventions of general-purpose HCI and the conventions of traditional cultural forms. Again, we encounter tension and struggle — in this case, between standardization and originality. One of the main principles of modern HCI is consistency principle. It dictates that menus, icons, dialogue boxes and other interface elements should be the same in different applications. The user knows that every application will contain a "file" menu, or that if he/she encounters an icon which looks like a magnifying glass it can be used to zoom on documents. In contrast, modern culture (including its "post-modern" stage) stresses originality: every cultural object is supposed to be different from the rest, and if it is quoting other objects, these quotes have to be contextualized. Cultural interfaces try to accommodate both the demand for consistency and the demand for originality. Most of them contain the same set of interface elements with standard semantics, such as "home," "forward" and "backward" icons. But because every Web site and CD-ROM is striving to have its own distinct design, these elements are always designed differently from one product to the next. For instance, many games such as War Craft II (Blizzard Entertainment, 1996) and Dungeon Keeper give their icons a "historical" look consistent with the mood of an imaginary universe portrayed in the game.

The language of cultural interfaces is a hybrid. It is a strange, often awkward mix between the conventions of traditional artistic forms and the conventions of HCI — between an immersive environment and a set of controls; between standardization and originality. Cultural interfaces try to balance the concept of a surface in painting, photography, cinema, and the printed page as something to be looked at, glanced at, read, but always from some distance, without interfering with it, with the concept of the surface in a computer interface as a virtual control panel, similar to the control panel on a car, plane or any other complex machine. [32] Finally, on yet another level, the traditions of the printed word and of cinema also compete between themselves. One pulls the computer screen towards being dense and flat information surface, while another wants it to become a window into a virtual space.

To see that this hybrid language of the cultural interfaces of the 1990s represents only one historical possibility, consider a very different scenario. Potentially, cultural interfaces could completely rely on already existing metaphors and action grammars of a standard HCI, or, at least, rely on them much more than they actually do. They don't have to "dress up" HCI with custom icons and buttons, or hide links within images, or organize the information as a series of pages or a 3-D environment. For instance, texts can be presented simply as files inside a directory, rather than as a set of pages connected by custom-designed icons. This strategy of using standard HCI to present cultural objects is encountered quite rarely. In fact, I am aware of only one project which uses it quite successfully: a CD-ROM by Gerald Van Der Kaap entitled BlindRom V.0.9. (Netherlands, 1993). The CD-ROM includes a standard-looking folder named "Blind Letter". Inside the folder there are a large number of text files. You don't have to learn yet another cultural interface, search for hyperlinks hidden in images or navigate through a 3-D environment. Reading these files required simply opening them in standard Macintosh SimpleText, one by one. The effect of this simple technique is remarkable. Rather than distracting the user from experiencing the work, the computer interface becomes part and parcel of the work. Opening these files, I felt that I was in the presence of a new literary form for a new medium, perhaps the real medium of a computer — its interface.

As the examples analyzed here illustrate, cultural interfaces try to create their own language rather than simply using general-purpose HCI. In doing so, these interfaces try to negotiate between metaphors and ways of controlling a computer developed in HCI, and the conventions of more traditional cultural forms. Indeed, neither extreme is ultimately satisfactory by itself. It is one thing to use a computer to control a weapon or to analyze statistical data, and it is another to use it to represent cultural memories, values and experiences. The interfaces developed for a computer in its functions of a calculator, control mechanism or a communication device are not necessarily suitable for a computer playing the role of a cultural machine. Conversely, if we simply mimic the existing conventions of older cultural forms such as the printed word and cinema, we will not take advantage of all the new capacities offered by a computer: its flexibility in displaying and manipulating data, interactive control by the user, and the ability to run simulations, etc.

Today the language of cultural interfaces is in its early stage, as was the language of cinema a hundred years ago. We don't know what the final result will be, or even if it will ever completely stabilize. Both the printed word and cinema eventually achieved stable forms which underwent little changes for long periods of time, in part because of the material investments in their means of production and distribution. Given that computer language is implemented in software, potentially it can keep on changing forever. But there is one thing we can be sure of. We are witnessing the emergence of a new cultural code, something which will be at least as significant as the printed word and cinema before it. We must try to understand its logic while we are in the midst of its natal stage.

## References:

[1] I am very grateful to Laura Nix for her help with editing this paper and many valuable suggestions.

[2] For an analysis of the parallels between the language of the nineteenth-century moving image presentations and the language of computer multimedia during the first half of the 1990s, see my "What is Digital Cinema?", in The Digital Dialectics, edited by Peter Lunenfeld (Cambridge, Mass.: The MIT Press, 1988).

[3] Thomas S. Kuhn, The Structure of Scientific Revolutions (2nd. ed. Chicago: University of Chicago Press, 1970).

[4] Brad. A. Myers, "A Brief History of Human-Computer Interaction Technology," technical report CMU-CS-96-163 and Human-Computer Interaction Institute Technical Report CMU-HCII-96-103 (Pittsburgh, Pennsylvania: Carnegie Mellon University, Human-Computer Interaction Institute, 1996).

[5] [http://www.xanadu.net/the.project](http://www.xanadu.net/the.project), accessed December 1, 1997.

[6] XML which is supposed to replace HTML on the World Wide Web will enable any user to create his/her customized markup language. Thus, the next stage in digital media culture will involve authoring not simply new documents but new languages. For more information on XML, see [http://www.ucc.ie/xml](http://www.ucc.ie/xml)., accessed December 1, 1997.

[7] [http://www.hotwired.com/rgb/antirom/index2.html](http://www.hotwired.com/rgb/antirom/index2.html), accessed December 1, 1997.

[8] See, for instance, Mark Pesce, "Ontos, Eros, Noos, Logos," keynote address for International Symposium on Electronic Arts 1995, [http://www.xs4all.nl/~mpesce/iseakey.html](http://www.xs4all.nl/~mpesce/iseakey.html), accessed December 1, 1997.

[9] Roman Jakobson, "Deux aspects du langage et deux types d'aphasie", in Temps Modernes, no. 188 (January 1962).

[10] XLM promises to diversify types of links available to include bi-directional links, multi-way links and links to a span of text rather than a simple point. See [http://www.ucc.ie/xml](http://www.ucc.ie/xml).

[11] This may imply that new digital rhetoric may have less to do with arranging information in a particular order and more to do simply with selecting what is included and what is not included in the total corpus being presented.

[12] See [http://www.aw.sgi.com/pages/home/pages/products/pages/poweranimator\_film\_sgi/indx.html](http://www.aw.sgi.com/pages/home/pages/products/pages/poweranimator_film_sgi/indx.html), accessed December 1, 1997.

[13] In The Address of the Eye Vivian Sobchack discusses the three metaphors of frame, window and mirror which underlie modern film theory. The metaphor of a frame comes from modern painting and is central to formalist theory which is concerned with signification;. The metaphor of window underlies realist film theory (Bazin) which stresses the act of perception. Realist theory follows Alberti in conceptualizing the cinema screen as a transparent window onto the world. Finally, the metaphor of a mirror is central to psychoanalytic film theory. In terms of these distinctions, my discussion here is concerned with the window metaphor. The distinctions themselves, however, open up a very productive space for thinking further about the relationships between cinema and computer media, in particular, the cinema screen and the computer window. Vivian Sobchack, The Address of the Eye: a Phenomenology of Film Experience (Princeton: Princeton University Press, 1992).

[14] Jacques Aumont et al., Aesthetics of Film (Austin: Texas University Press, 1992), 13.

[15] By VR interface I mean the common forms of a head-mounted or head-coupled directed display employed in VR systems. For a popular review of such displays written when the popularity of VR was at its peak, see Steve Aukstakalnis and David Blatner, Silicon Mirage: The Art and Science of Virtual Reality (Berkeley: CA: Peachpit Press, 1992), pp. 80-98. For a more technical treatment, see Dean Kocian and Lee Task, "Visually Coupled Systems Hardware and the Human Interface" in Virtual Environments and Advanced Interface Design, edited by Woodrow Barfield and Thomas Furness III (New York and Oxford: Oxford University Press, 1995), 175-257.

[16] See Kocian and Task for details on field of view of various VR displays. Although it varies widely between different systems, the typical size of the field of view in commercial head-mounted displays (HMD) available in the first part of the 1990s was 30-50[o].

[17] The following examples refer to a particular VRML browser — WebSpace Navigator 1.1 from Silicon Graphics, Inc. Other browsers have similar features. [http://webspace.sgi.com/WebSpace/Help/1.1/index.html](http://webspace.sgi.com/WebSpace/Help/1.1/index.html), accessed December 1, 1997.

[18] See John Hartman and Josie Wernecke, The VRML 2.0 Handbook: Building Moving Worlds on the Web (Reading, Mass.: Addison-Wesley Publishing Company, 1996), 363.

[19] For a more detailed analysis of this narrative structure, see my article, "The Aesthetics of Virtual Worlds: Report from Los Angeles," in CTHEORY [www.ctheory.com](www.ctheory.com).

[20] Examples of an earlier trend are Return to Zork (Activision, 1993) and The 7th Guest (Trilobyte/Virgin Games, 1993). Examples of the later trend are Soulblade (Namco, 1997) and Tomb Raider (Eidos, 1996).

[21] Critical literature on computer games, and in particular on their language, remains very slim. Useful facts on history of computer games, descriptions of different genres and the interviews with the designers can be found in Chris McGowan and Jim McCullaugh, Entertainment in the Cyber Zone (New York: Random House, 1995). Another useful source is J.C. Herz, Joystick Nation: How Videogames Ate Our Quarters, Won Our Hearts, and Rewired Our Minds (Boston: Little, Brown and Company, 1997).

[22] Dungeon Keeper, MS-DOS/Windows 95 CD-ROM (Bullfrog Productions, 1997).

[23] For a more detailed discussion of the history of computer imaging as gradual automation, see my articles "Mapping Space: Perspective, Radar and Computer Graphics," in SIGGRAPH '93 Visual Proceedings, edited by Thomas Linehan, 143-147 (New York: ACM, 1993); and "Automation of Sight from Photography to Computer Vision," in Electronic Culture: Technology and Visual Representation, edited by Timothy Druckery and Michael Sand (New York: Aperture, 1996).

[24] Moses Ma's presentation, panel on "Putting a Human Face on Cyberspace: Designing Avatars and the Virtual Worlds They Live In," SIGGRAPH '97, August 7, 1997.

[25] Overlapping windows were first proposed by Alan Kay in 1969.

[26] The examples of Citizen Kane and Ivan the Terrible are from Aumont et al., Aesthetics of Film, 41.

[27] On the ideal of engineering efficiency in relation to the avant-garde and digital media, see my article "The Engineering of Vision and the Aesthetics of Computer Art," Computer Graphics 28, no. 4 (November 1984): 259-263.

[28] The same novelty made possible surrealism.

[29] On the concept of focalization in narrative theory, see Mieke Bal, Narratology: Introduction to the Theory of Narrative (Toronto: University of Toronto Press, 1985).

[30] See [http://www.artcom.de/projects/invisible\_shape/welcome.en](http://www.artcom.de/projects/invisible_shape/welcome.en), accessed December 1, 1997.

[31] The computer screen also functions both as a window into an illusionary space and as a flat surface carrying text labels and graphical icons. We can relate this to a similar understanding of a pictorial surface in the Dutch art of the seventeenth century, as analyzed by Svetlana Alpers in her The Art of Describing. In the chapter entitled "Mapping Impulse" she discusses how a Dutch painting of this period functioned as a combined map/picture, combining different kinds of information and knowledge of the world. See Svetlana Alpers, The Art of Describing: Dutch Art in the Seventeenth Century (Chicago: University of Chicago Press, 1983).

[32] This historical connection is illustrated by popular flight simulator games where the computer screen is used to simulate the control panel of a plane, i.e. the very type of object from which computer interfaces have developed. The conceptual origin of modern GUI in a traditional instrument panel can be seen even more clearly in the first graphical computer interfaces of the late 1960s and early 1970s which used tiled windows. The first tiled window interface was demonstrated by Douglas Engelbart in 1968.

---
# The Genealogy of the Interface

_author: Lev Manovich_
_year: 1997_ 

## Paper Abstract

In the 1990s the role of the computer has begun to shift from functioning as a particular technology (a calculator, a symbol processor, an image manipulator) to being a filter to all culture. As a window of a Web browser comes to replace cinema screen, a wall in an art gallery, a building and a book, a new situation manifests itself: all culture, past and present, is filtered by a computer, mediated by its interface.

But where does this human-computer interface come from? What is its genealogy? What is its relationship to earlier cultural forms? In my paper, I postulate the connection between the metaphors and tools which define human-computer interaction at the end of this century and the artistic avant-garde practices in its beginning.

My thesis is that avant-garde aesthetic strategies became embedded in the commands and interface metaphors of computer software. In short, the avant-garde vision became materialized in a computer. Constructivist design, New Typography, avant-garde cinematography and film editing, photo-montage, and other strategies developed to awaken the audiences from the dream-existence of the bourgeois society, to built a new post-bourgeois consciousness, to shock and to insult the traditional taste have come to define the basic routine of a post-industrial society: the interaction with a computer.

As an example, consider the fate of the "New Vision" of the 1920s. Putting into practice Schklovsky's notion of "defamiliarization," advanced originally in relation to literature, a number of photographers, such as Moholy-Nagy and Rodchenko, began to use unorthodox viewpoints in their photographs: aerial and "worm's-eye" views, diagonal positions of the camera, elimination of the horizon line, extreme close-ups.

Computer interfaces adopt and advance the avant-garde's strategy of "de-familiarizing" points of view through interactive 3-D computer graphics. 3-D graphics allows a computer user to observe any object of a scene from an arbitrary viewpoint, thus helping to understand the object's structure, and to uncover the relations between visualized data. From chemistry and physics to architectural and product design, from financial analysis to pilot training, the mobile virtual camera is an essential tool of post-industrial labor. "De-familiarization" now involves simply a movement of a computer mouse.

Similarly, all of the "New Vision" photographic strategies became standard software tools for the visual analysis of data: "zoom in, "zoom out," "magnify," "negate the colors" and so on. What was radical aesthetic vision has become a standard computer technology. The techniques which were harnessed to help the viewer to uncover the social structure behind the visible surfaces, to unveil the underlying struggle between the old and the new, to prepare for rebuilding a society from the ground up, have become the elemental work procedures of the computer age.

In the paper, I will analyze these and other examples of the transformations of radical aesthetic strategies developed by the avant-garde in the 1920s into the fundamental components of human-computer interfaces including windows, "cut and paste," hyperlinks, database and the pixel. 

---
# Navigable space

_author: Lev Manovich_
_year: 1998_

## Doom and Myst

Looking at the first decade of new media — the 1990s — one can point at a number of objects which exemplify new media’s potential to give rise to genuinely original and historically unprecedented aesthetic forms. Among them, two stand out. Both are computer games. Both were published in the same year, 1993. Each became a phenomenon whose popularity has extended beyond the hard-core gaming community, spilling into sequels, books, TV, films, fashion and design. Together, they defined the new field and its limits. These games are Doom (id Software, 1993) and Myst (Cyan, 1993). 

In a number of ways, Doom and Myst are completely different. Doom is fast paced; Myst is slow. In Doom the player runs through the corridors trying to complete each level as soon as possible and then moves to the next one. In Myst, the player is moving through the world literally one step at a time, unraveling the narrative along the way. Doom is populated with numerous demons lurking around every corner, waiting to attack; Myst is completely empty. The world of Doom follows the convention of computer games: it consists of a few dozen levels. Although Myst also contains four separate worlds, each is more like a self-contained universe than a traditional computer game level. While the usual levels are quite similar to each other in structure and the look, the worlds of Myst are distinctly different. 

Another difference lies in the aesthetics of navigation. In Doom’s world, defined by rectangular volumes, the player is moving in straight lines, abruptly turning at right angles to enter a new corridor. In Myst, the navigation is more free-form. The player, or more precisely, the visitor, is slowly exploring the environment: she may look around for a while, go in circles, return to the same place over and over, as though performing an elaborate dance. 

Finally, the two objects exemplify two different types of cultural economy. With Doom, id software pioneered the new economy which the critic of computer games J.C. Herz summarizes as follows: "It was an idea whose time has come. Release a free, stripped-down version through shareware channels, the Internet, and online services. Follow with a spruced-up, registered retail version of the software." 15 million copies of the original Doom game were downloaded around the world. [1] By releasing detailed descriptions of game files formats and a game editor, id software also encouraged the players to expand the game, creating new levels. Thus, hacking and adding to the game became its essential part, with new levels widely available on the Internet for anybody to download. Here was a new cultural economy which transcended the usual relationship between producers and consumers or between "strategies" and "tactics" (de Certeau): the producers define the basic structure of an object, and release few examples and the tools to allow the consumers to build their own versions, shared with other consumers. In contrast, the creators of Myst followed an older model of cultural economy. Thus, Myst is more similar to a traditional artwork than to a piece of software: something to behold and admire, rather than to take apart and modify. To use the terms of the software industry, it is a closed, or proprietary system, something which only the original creators can modify or add to.

Despite all these differences in cosmogony, gameplay, and the underlying economic model, the two games are similar in one key respect. Both are spatial journeys. The navigation through 3-D space is an essential, if not the key component, of the gameplay. Doom and Myst present the user with a space to be traversed, to be mapped out by moving through it. Both begin by dropping the player somewhere in this space. Before reaching the end of the game narrative, the player must visit most of it, uncovering its geometry and topology, learning its logic and its secrets. In Doom and Myst — and in a great many other computer games — narrative and time itself are equated with the movement through 3-D space, the progression through rooms, levels, or words. In contrast to modern literature, theater, and cinema which are built around the psychological tensions between the characters and the movement in psychological space, these computer games return us to the ancient forms of narrative where the plot is driven by the spatial movement of the main hero, traveling through distant lands to save the princess, to find the treasure, to defeat the Dragon, and so on. As J.C. Herz writes about the experience of playing a classical text-based adventure game Zork, "you gradually unlocked a world in which the story took place, and the receeding edge of this world carried you through to the story's conclusion." [2] Stripping away the representation of inner life, psychology and other modernist nineteenth-century inventions, these are the narratives in the original Ancient Greek sense, for, as Michel de Certau reminds us, "In Greek, narration is called "diegesis": it establishes an itinerary (it guides) and it passes through (it "transgresses"). [3]

The central role of navigation through space is acknowledged by the games’ designers themselves. Robyn Miller, one of the two co-designers of Myst pointed out that "We are creating environments to just wonder around inside of. People have been calling it a game for lack of anything better, and we've called it a game at times. But that's not what it really is; it's a world." [4] Richard Garriott, the designer of classical RPG Ultima series, contrasts game design and fiction writing: "A lot of them [fiction writers] develop their individual characters in detail, and they say what is their problem in the beginning, and what they are going to grow to learn in the end. That's not the method I've used... I have the world. I have the message. And then the characters are there to support the world and the message." [5]

Structuring the game as a navigation through space is common to games across all the game genres. This includes adventure games (for instance, Zork, 7th Level, The Journeyman Project, Tomb Raider, Myst), strategy games (Command and Conquer) role-playing games (Diablo, Final Fantasy), flying, driving, and other simulators (Microsoft Flight Simulator), action games (Hexen, Mario), and, of course, first person shooters which have followed in Doom’s steps (Quake, Unreal). These genres follow different conventions. In adventure games, the user is exploring a universe, gathering resources. In strategy games, the user is engaged in allocating and moving resources and in risk management. In RPGs (role-playing games), the user is building a character, acquiring skills; the narrative is one of self-improvement. The genre conventions by themselves do not make it necessary for these games to employ a navigable space interface. Therefore, the fact that they all consistently do use it suggests to me that navigable space represents a larger cultural form. In other words, it is something which transcends computer games, and in fact, as we will see later, computer culture as well. Just like a database, navigable space is a form which already exists before computers; however, the computer becomes its perfect medium.

Indeed, the use of navigable space is common in all areas of new media. During the 1980s, numerous 3-D computer animations were organized around a single, uninterrupted camera move through a complex and extensive set. In a typical animation, a camera would fly over mountain terrain, or move through a series of rooms, or maneuver past geometric shapes. In contrast to both ancient myths and computer games, this journey had no goal, no purpose. In short, there was no narrative. Here was the ultimate "road movie" where the navigation through the space was sufficient in itself.

In the 1990s, these 3-D fly-throughs have come to constitute the new genre of post-computer cinema and location-based entertainment — the motion simulator. [6] By using the first person point of view and by synchronizing the movement of the platform housing the audience with the movement of a virtual camera, motion simulators recreate the experience of traveling in a vehicle. Thinking about the historical precedents of a motion simulator, we begin to uncover some places where the form of navigable space already manifested itself. They include _Hale's Tours and Scenes of the World_, a popular film-based attraction which debuted at the St. Louis Fare in 1904; roller-coaster rides; flight, vehicle and military simulators, which used a moving base since the early 1930s; and the fly-through sequences in _2001: A Space Odyssey_ (Kubrick, 1968) and _Star Wars_ (Lucas, 1977). Among these, _A Space Odyssey_ plays particularly important role; Douglas Trumbull, who since the late 1980s produced some of the most well-known motion simulator attractions and was the key person behind the rise of the whole motion simulator phenomenon began his career by creating ride sequences for this film.

Along with providing a key foundation for new media aesthetics, navigable space also became a new tool of labor. It is now a common way to visualize and work with any data. From scientific visualization to walk-throughs of architectural designs, from models of a stock market performance to statistical datasets, the 3-D virtual space combined with a camera model is the accepted way to visualize all information (see the section "The Language of Cultural Interfaces"). It is as accepted in computer culture as charts and graphs were in a print culture.

Since navigable space can be used to represent both physical spaces and abstract information spaces, it is only logical that it also emerged as an important paradigm in human-computer interfaces. Indeed, on one level HCI can be seen as a particular case of data visualization, the data being computer files rather than molecules, architectural models or stock market figures. The examples of 3-D navigable space interfaces are the Information Visualizer (Xerox Parc) which replaces a flat desktop with 3-D rooms and planes rendered in perspective; [7] T\_Vision (ART+COM) which uses a navigable 3-D representation of the earth as its interface; [8] and The Information Landscape (Silicon Graphics) in which the user flies over a plane populated by data objects. [9]

The original (i.e. the 1980s) vision of cyberspace called for a 3-D space of information to be traversed by a human user, or, to use the term of William Gibson, a "data cowboy". [10] Even before Gibson's fictional descriptions of cyberspace were published, cyberspace was visualized in the film _Tron_ (Disney, 1982). Although _Tron_ takes place inside a single computer rather than a network, its vision of users zapping through the immaterial space defined by lines of light is remarkably similar to the one articulated by Gibson in his novels. In an article which appeared in the 1991 anthology Cyberspace: First Steps Marcos Novak still defined cyberspace as "a completely spatialized visualization of all information in global information processing systems." [11] In the first part of the 1990s, this vision survived among the original designers of VRML (The Virtual Reality Modeling Language). In designing the language, they aimed to "create a unified conceptualization of space spanning the entire Internet, a spatial equivalent of WWW." [12] They saw VRML as a natural stage in the evolution of the Net from an abstract data network toward a "perceptualized" Internet where the data has been sensualized," i.e., represented in three dimensions. [13]

The term cyberspace itself is derived from another term — cybernetics. In his 1947 book, Cybernetics mathematician Norbert Wiener defined it as "the science of control and communications in the animal and machine." Wiener conceived of cybernetics during World War II when he was working on problems concerning gunfire control and automatic missile guidance. He derived the term cybernetics from the ancient Greek word _kybernetikos_ which refers to the art of the steersman and can be translated as "good at steering." Thus, the idea of navigable space lies at the very origins of computer era. The steersman navigating the ship and the missile traversing space on its way to the target have given rise to a whole number of new figures: the heroes of William Gibson, the "data cowboys" moving through the vast terrains of cyberspace; the "driver" of a motion simulator; a computer user, navigating through the scientific data sets and computer data structures, molecules and genes, earth's atmosphere and the human body; and last but not least, the player of Doom, Myst and their endless imitations.

From one point of view, navigable space can be legitimately seen as a particular kind of an interface to a database, and thus something which does not deserve a special focus. I would like, however, to also think of it as a cultural form of its own, not only because of its prominence across the new media landscape and, as we will see later, its persistence in new media history, but also because, more so than a database, it is a new form which may be unique to new media. Of course, both the organization of space and its use to represent or visualize something else have always been a fundamental part of human culture. Architecture and ancient mnemonics, city planning and diagramming, geometry and topology are just some of the disciples and techniques which were developed to harness space's symbolic and economic capital. [14] Spatial constructions in new media draw on all these existing traditions — but they are also fundamentally different in one key respect. For the first time, space becomes a media type. Just as other media types — audio, video, stills, and text — it can be now instantly transmitted, stored and retrieved, compressed, reformatted, streamed, filtered, computed, programmed and interacted with. In other words, all operations which are possible with media as a result of its conversion to computer data can also now apply to representations of 3-D space.

Recent cultural theory has paid increasing attention to the category of space. The examples are Henri Lefebvre's work on the politics and anthropology of everyday space; Michel Foucault's analysis of the Panopticon's topology as a model of modern subjectivity; and the writings of Frederick Jameson, David Harvey, and Edward Soja on the post-modern space of global capitalism. [15] At the same time, new media theoreticians and practitioners have come with many formulations of how cyberspace should be structured and how computer-based spatial representations can be used in new ways. [16] What received little attention, however, both in cultural theory and in new media theory, is a particular category of navigation through space. And yet, this category characterizes new media as it actually exists; in other words, new media spaces are always spaces of navigation. At the same time, as we will see later in this section, this category also fits a number of developments in other cultural fields such as anthropology and architecture. 

To summarize, along with a database, navigable space is another key form of new media. It is already an accepted way for interacting with any type of data; an interface of computer games and motion simulators and, potentially, of any computer in general. Why does computer culture spatialize all representations and experiences (the library is replaced by cyberspace; narrative is equated with traveling through space; all kinds of data are rendered in three dimensions through computer visualization)? Shall we try to oppose this spatialization (i.e., what about time in new media?) And, finally, what are the aesthetics of navigation through virtual space?

 \\#\# Computer Space

The very first coin-op arcade game was called Computer Space. The game simulated the dogfight between a spaceship and a flying saucer. Released in 1971, it was a remake of the first computer game Spacewar programmed on PDP-1 at MIT in 1962. [17] Both of these legendary games included the word space in their titles; and appropriately, space was one of the main characters in each of them. In the original Spacewar, the player was navigating two spaceships around the screen while shooting torpedoes at one another. The player also had to be careful in maneuvering the ships to make sure they would not get too close to the star in the center of the screen which pulled them towards it. Thus, along with the spaceships, the player also had to interact with space itself. And although, in contrast to such films as _2001_, _Star Wars_, or\_Tron\_, the space of Spacewar and Computer Space was not navigable — one could not move through it — the simulation of gravity made it truly an active presence. Just as the player had to engage with the spaceships, he had to engage with the space itself.

This active treatment of space is an exception rather than the rule in new media. Although new media objects favor the use of space for representations of all kinds, most often virtual spaces are not true spaces but collections of separate objects. Or, to put this in a slogan: there is no space in cyberspace.

To explore this thesis further, we can borrow the categories developed by art historians early in this century. Alois Riegl, Heinrich Wölfflin, and Erwin Panofsky, the founders of modern art history, defined their field as the history of the representation of space. Working within the paradigm of cyclic cultural development, they related the representation of space in art to the spirit of entire epochs, civilizations, and races. In his 1901 Die Spätrömische Kunstindustrie, (The late-Roman art industry) Riegl characterized mankinds cultural development as the oscillation between two ways of understanding space, which he called haptic and optic. Haptic perception isolates the object in the field as a discrete entity, while optic perception unifies objects in a spatial continuum. Riegl’s contemporary, Heinrich Wölfflin, similarly proposed that the temperament of a period or a nation expresses itself in a particular mode of seeing and representing space. Wölfflin’s Principles of Art History (1913) plotted the differences between Renaissance and Baroque styles along five axes: linear/painterly; plane/recession; closed form/open form; multiplicity/unity; and clearness/unclearness. [18] Erwin Panofsky, another founder of modern art history, contrasted the "aggregate" space of the Greeks with the "systematic" space of the Italian Renaissance in his famous essay Perspective as Symbolic Form (1924-25). [19] Panofsky established a parallel between the history of spatial representation and the evolution of abstract thought. The former moves from the space of individual objects in antiquity to the representation of space as continuous and systematic in modernity. Correspondingly, the evolution of abstract thought progresses from ancient philosophy’s view of the physical universe as discontinuous and "aggregate", to the post-Renaissance understanding of space as infinite, homogeneous, isotropic, and with ontological primacy in relation to objects — in short, as systematic.

We don’t have to believe in grand evolutionary schemes in order to usefully retain such categories. What kind of space is virtual space? At first glance, the technology of 3-D computer graphics exemplifies Panofsky’s concept of systematic space, which exists prior to the objects in it. Indeed, the Cartesian coordinate system is built into computer graphics software and often into the hardware itself. [20] A designer launching a modeling program is typically presented with an empty space defined by a perspectival grid; the space will be gradually filled by the objects created. If the built-in message of a music synthesizer is a sine wave, the built-in world of computer graphics is an empty Renaissance space: the coordinate system itself.

Yet computer-generated worlds are actually much more haptic and aggregate than optic and systematic. The most commonly used computer-graphics technique of creating 3-D worlds is polygonal modeling. The virtual world created with this technique is a vacuum containing separate objects defined by rigid boundaries. What is missing from computer space is space in the sense of medium: the environment in which objects are embedded and the effect of these objects on each other. This is what Russian writers and artists call prostranstvennaya sreda. Pavel Florensky, a legendary Russian philosopher and art historian described it in the following way in the early 1920s: "The space-medium is objects mapped onto space... We have seen the inseparability of Things and space, and the impossibility of representing Things and space by themselves." [21] This understanding of space also characterizes a particular tradition of modern painting which stretches from Seurat to Giacometti and De Kooning. These painters tried to eliminate the notions of a distinct object and an empty space as such. Instead, they depicted a dense field that occasionally hardens into something which we can read as an object. Following the example of Gilles Deleuze’s analysis of cinema as activity of articulating new concepts, akin to philosophy, [22] it can be said that modern painters which belong to this tradition worked to articulate the particular philosophical concept in their painting — that of space-medium. This concept is something mainstream computer graphics still has to discover.

Another basic technique used in creating virtual worlds also leads to aggregate space. It involves superimposing animated characters, still images, digital movies, and other elements over a separate background. Traditionally this technique was used in video and computer games. Responding to the limitations of the available computers, the designers of early games would limit animation to a small part of a screen. 2-D animated objects and characters called sprites were drawn over a static background. For example, in Space Invaders the abstract shapes representing the invaders would fly over a blank background, while in Pong the tiny character moved across the picture of a maze. The sprites were essentially animated 2-D cutouts thrown over the background image at game time, so no real interaction between them and the background took place. In the second half of the 1990s, much faster processors and 3-D graphics cards made it possible for games to switch to real-time 3-D rendering. This allowed for modeling of visual interactions between the objects and the space they are in, such as reflections and shadows. Consequently, the game space became more of a coherent, true 3-D space, rather than a set of 2-D planes unrelated to each other. However, the limitations of earlier decades returned in another area of new media — online virtual worlds. Because of the limited bandwidth of the 1990s Internet, virtual world designers have to deal with constraints similar to and sometimes even more severe than the games designers two decades earlier. In online virtual worlds, a typical scenario may involve an avatar — a 2-D or 3-D graphic representing the user — animated in real time in response to the user’s commands. The avatar is superimposed on a picture of a room, in the same way as in video games the sprites were superimposed over the background. The avatar is controlled by the user; the picture of the room is provided by a virtual-world operator. Because the elements come from different sources and are put together in real time, the result is a series of 2-D planes rather than a real 3-D environment. Although the image depicts characters in a 3-D space, it is an illusion since the background and the characters do not "know" about each other, and no interaction between them is possible.

Historically, we can connect the technique of superimposing animated sprites over the background to traditional cell animation. In order to save labor, animators similarly divide the image between a static background and animated characters. In fact, the sprites of computer games can be thought of as reincarnated animation characters. Yet the use of this technique did not prevent Fleischer and Disney animators from thinking of space as space-medium (to use Floresky's term), although they created this space-medium in a different way than the modern painters. (Thus while the masses run away from the serious and "difficult" abstract art to enjoy the funny and figurative images of cartoons, what they saw was not that different from Giacometti's and de Kooning’s canvases.) Although all objects in cartoons have hard edges, the total anthropomorphism of the cartoon universe breaks the distinctions both between subjects and objects and objects and space. Everything is subjected to the same laws of stretch and squash, everything moves and twists in the same way, everything is alive to the same extent. It is as though everything — the character’s body, chairs, walls, plates, food, cars and so on — is made from the same bio-material. This monism of the cartoon worlds stands in opposition to the binary ontology of computer worlds in which the space and the sprites ¾ characters appear to be made from two fundamentally different substances.

In summary, although 3-D computer-generated virtual worlds are usually rendered in linear perspective, they are really collections of separate objects, unrelated to each other. In view of this, the common argument that 3-D computer simulations return us to the Renaissance perspective and therefore, from the viewpoint of twentieth-century abstraction, should be considered regressive, turns out to be ungrounded. If we are to apply the evolutionary paradigm of Panofsky to the history of virtual computer space, we must conclude that it has not reached its Renaissance stage yet. It is still at the level of ancient Greece, which could not conceive of space as a totality.

Computer space is also aggregate yet in another sense. As I already noted using the example of Doom, traditionally the world of a computer game is not a continuous space but a set of discrete levels. In addition, each level is also discrete — it is a sum of rooms, corridors, and arenas built by the designers. Thus, rather conceiving space as a totality, one is dealing with a set of separate places. The convention of levels is remarkably stable, persisting across genres and numerous computer platforms.

If the World Wide Web and VRML are any indications, we are not moving any closer toward systematic space; instead, we are embracing aggregate space as a new norm, both metaphorically and literally. The space of the Web in principle can’t be thought of as a coherent totality: it is a collection of numerous files, hyperlinked but without any overall perspective to unite them. The same holds for actual 3-D spaces on the Internet. A 3-D scene as defined by a VRML file is a list of separate objects that may exist anywhere on the Internet, each created by a different person or a different program. A user can easily add or delete objects without taking into account the overall structure of the scene. [23] Just as, in the case of a database, the narrative is replaced by a list of items, here a coherent 3-D scene becomes a list of separate objects.

With its metaphors of navigation and home steading, The Web has been compared to the American Wild West. The spatialized Web envisioned by VRML (itself a product of California) reflects the treatment of space in American culture generally, in its lack of attention to any zone not functionally used. The marginal areas that exist between privately owned houses, businesses and parks are left to decay. The VRML universe, as defined by software standards and the default settings of software tools, pushes this tendency to the limit: it does not contain space as such but only objects that belong to different individuals. Obviously, the users can modify the default settings and use the tools to create the opposite of what the default values suggest. In fact, the actual multi-user spaces built on the Web can be seen precisely as the reaction against the anti-communal and discrete nature of American society, the attempt to substitute for the much discussed disappearance of traditional community by creating virtual ones. (Of course, if we are to follow the nineteenth-century sociologist Ferdinand Tönnies, the shift from traditional close-knit scale community to modern impersonal society already took place in the nineteenth century and is an inevitable side-effect as well as a prerequisite for modernization. [24] However, it is important that the ontology of virtual space as defined by the software itself is fundamentally aggregate, a set of objects without a unifying point of view.

If art historians, literary and film scholars have traditionally analyzed the structure of cultural objects as reflecting larger cultural patterns (for instance, Panofsky's reading of perspective), in the case of new media we should look not only at the finished objects but first of all at the software tools, their organization and default settings. [25] This is particularly important because in new media the relation between the production tools and the products is one of continuity; in fact, it is often hard to establish the boundary between them. Thus, we may connect the American ideology of democracy with its paranoid fear of hierarchy and centralized control with the flat structure of the Web, where every page exists on the same level of importance as any other and where any two sources connected through hyperlinking have equal weight. Similarly, in the case of virtual 3-D spaces on the Web, the lack of a unifying perspective in U.S. culture, whether in the space of an American city, or in the space of an increasingly fragmented public discourse, can be correlated with the design of VRML, which substitutes a collection of objects for a unified space. 

## The Poetics of Navigation

In order to analyze the computer representations of 3-D space, I have used theories from early art history; but it would not be hard to find other theories which can work as well. However, navigation through space is a different matter. While art history, geography, anthropology, sociology and other disciplines have came up with many approaches to analyze space as a static, objectively existing structure, we don’t have the same wealth of concepts to help us think about the poetics of navigation through space. And yet, if I am right to claim that the key feature of computer space is that it is navigable, we need to be able to address this feature theoretically. 

As a way to begin, we may take a look at some of the classical navigable computer spaces. The 1978 project Aspen Movie Map, designed at the MIT Architecture Machine Group, headed by Nicholas Negroponte (which later expanded into MIT Media Laboratory) is acknowledged as the first publicly shown interactive virtual navigable space, and also as the first hypermedia program. The program allowed the user to "drive" through the city of Aspen, Colorado. At each intersection, the user was able to select a new direction using a joystick. To construct this program, the MIT team drove through Aspen in a car taking pictures every three meters. The pictures were then stored on a set of videodiscs. Responding to the information from the joystick, the appropriate picture or sequence of pictures was displayed on the screen. Inspired by a mockup of an airport used by the Israeli commandos to train for the Entebbe hostage-freeing raid of 1973, Aspen Movie Map was a simulator and therefore its navigation modeled the real-life experience of moving in a car, with all its limitations. [26] Yet its realism also opened a new set of aesthetic possibilities which unfortunately later designers of navigable spaces did not explore further. Al of them relied on interactive 3-D computer graphics to construct their spaces. In contrast, Aspen Movie Map utilized a set of photographic images; in addition, because the images were taken every three meters, this resulted in an interesting sampling of three-dimensional space. Although in the 1990s Apple’s QuickTime VR technology made this technique itself quite accessible, the idea of constructing a large-scale virtual space from photographs or a video of a real space was never tried out systematically again, although it opens up unique aesthetic possibilities not available with 3-D computer graphics.

Jeffrey Shaw's Legible City (1988-1991), another well-known and influential computer navigable space, is also based on the exiting city. [27] As in Aspen Movie Map, the navigation also simulates a real physical situation, in this case driving a bicycle. Its virtual space, however, is not tied to the simulation of physical reality: it is an imaginary city made from 3-D letters. In contrast to most navigable spaces whose parameters are chosen arbitrarily, in Legible City (Amsterdam and Karlsruhe versions) every value of its virtual space is derived from the actual existing physical space it replaces. Each 3-D letter in the virtual city corresponds to an actual building in a physical city; the letter’s proportions, color and location are derived from the building it replaces. By navigating through the space, the user reads the texts composed by the letters; these texts are drawn from the archive documents describing the city history. Through this mapping Jeffrey Shaw foregrounds, or, more precisely, "stages," one of the fundamental problematics of new media and the computer age as a whole: the relation between the virtual and the real. In his other works Shaw systematically "staged" other key aspects of new media such as the interactive relation between the viewer and the image, or the discrete quality of all computer-based representations. In the case of Legible City, it functions not only as a unique navigable virtual space of its own, but also as a comment on all the other navigable spaces. It suggests that instead of creating virtual spaces which have nothing to do with actual physical spaces, or the spaces which are closely modeled after existing physical structures, such as towns or shopping malls, (this holds for most commercial virtual worlds and VR works), we may take a middle road. In Legible City, the memory of the real city is carefully preserved without succumbing to illusionism; the virtual representation encodes the city’s genetic code, its deep structure rather than its surface. Through this mapping, Shaw proposes an ethics of the virtual. Shaw suggests that the virtual can at least preserve the memory of the real it replaces, encoding its structure, if not aura, in a new form.

While Legible City was a landmark work in that it presented a symbolic rather than illusionistic space, its visual appearance reflected the default real-time graphics capability of SGI workstations on which it was running: flat-shaded shapes attenuated by a fog. Char Davies and her development team at SoftImage have consciously addressed the goal of creating a different, more painterly aesthetic for the navigable space in their interactive VR installation Osmose (1994-1995). [28] From the point of view of history of modern art the result hardly represented an advancement. Osmose simply replaced the usual hard-edge polygonal Cézanne-like look of 3-D computer graphics look with a softer, more atmospheric, Renoir or late Monet-like environment made of translucent textures and flowing particles. Yet in the context of other 3-D virtual worlds it was an important advance. The "soft" aesthetic of Osmose is further supported through the use of slow cinematic dissolves between its dozen or so worlds. Like in Aspen Movie Map and in Legible City, the navigation in Osmose is modeled on a real-life experience, in this case, of scuba diving. The "immersant" is controlling navigation by breathing: breathing in sends the body upward, while breathing out makes it fall. The resulting experience, according to the designers, is one of floating, rather than flying or driving, typical of virtual worlds. Another interesting aspect of Osmose's navigation is its collective character. While only one person can be "immersed" at a time, the audience can witness her or his journey through the virtual worlds as it unfolds on a large projection screen. At the same size, another translucent screen enables the audience to observe the body gestures of the "immersant" as a shadow-silhouette. The "immersant" thus becomes a kind of ship captain, taking the audience along on a journey; like the captain, she occupies a visible and symbolically marked position, being responsible for the audience's aesthetic experience.

Tamás Waliczky’s The Forest (1993) liberated the virtual camera from its typical enslavement to the simulation of humanly possible navigation, be it walking, driving a car, pedaling a bicycle or scuba diving. In The Forest, the camera slides through the endless black and white forest in a series of complex and melancholic moves. If modern visual culture exemplified by MTV can be thought of as a Mannerist stage of cinema, its perfected techniques of cinematography, mise-en-scene and editing self-consciously displayed and paraded for its own sake, Waliczky's film presents an alternative response to cinema’s classical age, which is now behind us. In this meta-film, the camera, part of cinema’s apparatus, becomes the main character (in this we may connect The Forest to another meta-film, A Man with a Movie Camera). At first glance, the logic of camera movements can be identified as the quest of a human being trying to escape from the forest (which, in reality, is just a single picture of a tree repeated over and over). Yet, just as in some of the Brothers Quay animated films such as The Street of Crocodiles, the virtual camera of The Forest neither simulates natural perception nor does it follow the standard grammar of cinema’s camera; instead, it establishes a distinct system of its own. If in The Street of Crocodiles the camera suddenly takes off, moving in a straight line as though mounted on some robotic arm, and just as suddenly stops to frame a new corner of the space, in The Forest it never stops at all, the whole film being one uninterrupted camera trajectory. The camera system of The Forest can be read as a comment on a fundamentally ambiguous nature of computer space. On the one hand, not indexically tied up to physical reality or human body, computer space is isotropic. In contrast to human space, in which the verticality of the body and the direction of the horizon are two dominant directions, computer space does not privilege any particular axis. In this way it is similar to the space of El Lissitzky's Prouns and Kazimir Malevich's suprematist compositions — an abstract cosmos, unencumbered by either Earth’s gravity or the weight of a human body. (Thus the game Spacewar with its simulated gravity got it wrong!) William Gibson’s term "matrix" which he used in his novels to refer to cyberspace, captures well this isotropic quality. But, on the other hand, computer space is also a space of a human dweller, something which is used and traversed by a user, who brings her own anthropological framework of horizontality and verticality. The camera system of The Forest foregrounds this double character of computer space. While no human figures or avatars appear in the film and we never get to see either the ground or the sky, it is centered around the stand-in for the human subject — a tree. The constant movements of the camera along the vertical dimension throughout the film — sometimes getting closer to where we imagine the ground plane is located, sometimes moving towards (but again, never actually showing) the sky — can be interpreted as an attempt to negotiate between isotropic space and the space of human anthropology, with its horizontality of the ground plane and the horizontal and vertical dimension of human bodies. The navigable space of The Forest thus mediates between human subjectivity and the very different and ultimately alien logic of a computer — the ultimate and omnipresent Other of our age.

The computer spaces just discussed, from Aspen Movie Map to The Forest, each establishes a distinct aesthetic of their own. However, the majority of navigable virtual spaces mimic existing physical reality without proposing any coherent aesthetic programs. What artistic and theoretical traditions can the designers of navigable spaces draw upon to make them more interesting? One obvious candidate is modern architecture. From Melnikov, Le Corbusier and Frank Lloyd Wright to Arhigram and Bernard Tschumi, modern architects elaborated a variety of schemes for structuring and conceptualizing space to be navigated by users. Using a few examples from these architects, we can look at the 1925 USSR Pavilion (Melnikov,), Villa Savoye (Le Corbusier), Walking City (Arhigram), and Parc de la Villette (Tschumi). [29] Even more relevant is the tradition of "paper architecture" — the designs which were not intended to be built and whose authors, therefore, felt unencumbered by the limitations of materials, gravity and budgets. [30] Another highly relevant tradition is film architecture. [31] As discussed in the "Theory of Cultural Interfaces" section, the standard interface to computer space is the virtual camera modeled after a film camera, rather than a simulation of unaided human sight. After all, film architecture is The architecture designed for navigation and exploration by a film camera.

Along with different architectural traditions, designers of navigable spaces can find a wealth of relevant ideas in modern art. They may consider, for instance, the works of modern artists which exist between art and architecture and which, like projects of paper architects, display spatial imagination not tied up to the questions of utility and economy: warped worlds of Jean Dubuffet, mobiles by Alexander Calder, earth works by Robert Smithson, moving text spaces by Jenny Holzer. While many modern artists felt compelled to create 3-D structures in real spaces, others were satisfied with painting their virtual worlds: think, for, instance, of melancholic cityscapes by Giorgio de Chirico, biomorphic worlds by Yves Tanguy, economical wireframe structures by Alberto Giacometti, existential landscapes by Anselm Kiefer. Besides providing us with many examples of imaginative spaces, both abstract and figurative, modern painting is relevant to the design of virtual navigable spaces in two additional ways. First, since new media is most often experienced, like painting, via a rectangular frame (see "The Screen and the User"), virtual architects can study how painters organized their spaces within the constraints of a rectangle. Second, modern painters who belong to what I call the "space-medium" tradition elaborated the concept of space as a homogeneous dense field, where everything is made from the same "stuff" — in contrast to architects which always have to work with a basic dichotomy between the building structure and the empty space. And although virtual spaces realized until now, with the possible exception of Osmose, follow the same dichotomy between rigid objects and a void between them, on the level of material organization they are intrinsically related to the monistic ontology of modern painters such as Matta, Giacometti, or Pollock, for everything in them is also made from the same material — pixels, on the level of surface; polygons or voxels, on the level of 3-D representation). Thus virtual computer space is structurally closer to modern painting than to architecture.

Along with painting, a genre of modern art which has a particular relevance to the design of navigable virtual spaces is installation. Seen in the context of new media, many installations can be thought of as dense multimedia information spaces. They combine images, video, texts, graphics and 3-D elements within a spatial layout. While most installations leave it up to the viewer to determine the order of "information access" to their elements, one of the most well-known installation artists, Ilya Kabakov, elaborated a system of strategies to structure the viewer's navigation through his spaces. [32] According to Kabakov, in most installations "the viewer is completely free because the space surrounding her and the installation remain completely indifferent to the installation it encloses." [33] In contrast, by creating a separate enclosed space with carefully chosen proportions, colors and lighting within the larger space of a museum or a gallery, Kabakov aims to completely "immerse" the viewer inside his installation. He calls this installation type a "total installation." 

For Kabakov, "total" installation has a double identity. On the one hand, it belongs to plastic arts designed to be viewed by an immobile spectator — painting, sculpture, architecture. On the other hand, it also belongs to time-based arts such as theater and cinema. We can say the same about virtual navigable spaces. Another concept of Kabakov’s theory which is directly applicable to virtual space design is his distinction between the spatial structure of an installation and its dramaturgy, i.e. the time-space structure created by the movement of a viewer through an installation. [34] Kabakov’s strategies of dramaturgy include dividing the total space of an installation into two or more connected spaces; creating a well-defined path through the space which does not preclude the viewer from wandering on her own, yet prevents her from feeling being lost and being bored. To make such a path, Kabakov constructs corridors and abrupt openings between objects, he also places objects in strange places to obstruct passages where one expects to discover a clear pathway. Another strategy of "total installation" is the choice of particular kinds of narratives which lead themselves to spatialization. These are the narratives which take place around the main event which becomes the center of an installation: "the beginning [of the installation] leads to the main event [of the narrative] while the last part exists after the event took place." Yet another strategy involves the positioning of text within the space of an installation as a way to orchestrate the attention and navigation of the viewer. For instance, placing two to three pages of texts at a particular point in the space creates a rhythmic stop in the navigation rhythm. [35] Finally, Kabakov "directs" the viewer to keep alternating between focusing her attention on particular details and the installation as a whole. He describes these two kinds of spatial attention (which we can also correlate with haptic and optic perception as theorized by Riegl and others) as follows: "wandering, total ("summarnaia") orientation in space — and active, well-aimed "taking in" of partial, small, the unexpected." [36]

All these strategies can be directly applied to the design of virtual navigable spaces (and interactive multimedia in general). In particular, Kabakov is very successful in making the viewers of his installations carefully read significant amounts of text included in them — something which represents a constant challenge for new media designers. His constant emphasis on always thinking about the viewer's attention and reaction to what she will encounter — "the reaction of the viewer during her movement through the installation is the main concern of her designer… The loss of the viewer's attention is the end of the installation" [37] — is also an important lesson to new media designers who often forgot that what they are designing is not an object in itself but a viewer's experience in time and space.

I have used the word "strategy" to refer to Kabakov’s techniques on purpose. To evoke the terminology of The Practice of Everyday Life by French writer Michel de Certeau, Kabakov uses strategies to impose a particular matrix of space, time, experience and meaning on his viewers; they, in their turn, use "tactics" to create their own trajectories (this is a term actually used by de Certeau) within this matrix. If Kabakov is perhaps the most accomplished architect of navigable spaces, de Certeau can very well be their best theoretician. Like Kabakov, he never dealt with computer media directly, and yet his The Practice of Everyday Life has a multitude of ideas directly applicable to new media. His general notion of how a user's "tactics" which create their own trajectories through the spaces defined by others (both metaphorically, and, in the case of spatial tactics, literally) is a good model to think about computer users navigating through computer spaces they did not design:

Although they are composed of the vocabularies of established languages (those of television, newspapers, supermarkets of established sequences) and although they remain subordinated to prescribed syntactical forms (temporal modes of schedules, paradigmatic orders of spaces, etc.), the trajectories trace out the rules of other interests and desires that are neither determined, nor captured by, the system in which they develop. [38]

## The Flâneur and The Explorer

Why is navigable space such a popular construct in new media? What are the historical origins and precedents of this form?

In his famous 1863 essay "The Painter of Modern Life", Charles Baudelaire documented the new modern male urban subject — the flâneur. [39] An anonymous observer, the flâneur navigates through the space of a Parisian crowd, recording and immediately erasing the faces and the figures of the passers-by in his memory. From time to time, his gaze meets the gaze of a passing woman, engaging her in a split-second virtual affair, only to be unfaithful to her with the next female passer-by. The flâneur is only truly at home in one place — moving through the crowd. Baudelaire writes: "To the perfect spectator, the impassioned observer, it is an immense joy to make his domicile amongst numbers, amidst fluctuation and movement, amidst the fugitive and infinite… To be away from home, and yet to feel at home; to behold the world, to be in the midst of the world and yet to remain hidden from the world." There is a theory of navigable virtual spaces hidden here, and we can turn to Walter Benjamin to help us in articulating it. According to Benjamin, the flâneur’s navigation transforms the space of the city: "The Crowd is the veil through which the familiar city lures the flâneur like a phantasmargonia. In it, the city is now a landscape, now a room." [40] The navigable space thus is a subjective space, its architecture responding to the subject’s movement and emotion. In the case of the flâneur moving through the physical city, this transformation of course only happens in the flâneur’s perception, but in the case of navigation through a virtual space, the space can literally change, becoming a mirror of the user’s subjectivity. The virtual spaces built on this principle can be found in such films as Waliczky's The Garden and The Dark City (Alex Proyas, 1998).

Following the European tradition, the subjectivity of the flâneur is determined by his interaction with a group — even though it is a group of strangers. In place of a close-knit community of a small-scale traditional society (Gemeinschaft) we now have an anonymous association of a modern society (Gesellschaft). [41] We can interpret the flâneur’s behavior as a response to this historical shift. It is as though he is trying to compensate for the loss of a close relationship with his group by inserting himself into the anonymous crowd. He thus exemplifies the historical shift from Gemeinschaft to Gesellschaft, and the fact that he only feels at home in the crowd of strangers shows the psychological price paid for modernization. Still, the subjectivity of the flâneur is, in its essence, intersubjectivity: the exchange of glances between him and the other human beings. 

A very different image of a navigation through space — and of subjectivity — is presented in the novels of nineteenth-century American writers such as James Fenimore Cooper (1789-1851) or Mark Twain (1835-1910). The main character of Cooper's novels, the wilderness scout Natty Bumppo, alias Leatherstocking, navigates through spaces of nature rather than culture. Similarly, in Twain's Huckleberry Finn, the narrative is organized around the voyage of the two boy heroes down the Mississippi River. Instead of the thickness of the urban human crowd which is the milieu of a Parisian flâneur, the heroes of these American novels are most at home in the wilderness, away from the city. They navigate forests and rivers, overcoming obstacles and fighting enemies. The subjectivity is constructed through the conflicts between the subject and nature, and between the subject and his enemies, rather than through interpersonal relations within a group. This structure finds its ultimate expression in the unique American form, the Western, and its hero, the cowboy — a lonely explorer who only occasionally shows up in town to get a drink at the bar. Rather than providing the home for the cowboy, as it does for the flâneur, the town is a hostile place, full of conflict, which eventually erupts into the inevitable showdown. 

Both the flâneur and the explorer find their expression in different subject positions, or phenotypes, of new media users. Media theoretician and activist Geert Lovink describes the figure of the present-day media user and Net surfer whom he calls the Data Dandy. Although Lovink's reference is Oscar Wilde rather than Baudelaire, his Data Dandy exhibits the behaviors which also qualify him to be called a Data Flâneur. "The Net is to the electronic dandy what the metropolitan street was for the historical dandy." [42] A perfect aesthete, the Data Dandy loves to display his private and totally irrelevant collection of data to other Net users. "Wrapped in the finest facts and the most senseless gadgets, the new dandy deregulates the time economy of the info = money managers ...if the anonymous crowd in the streets was the audience of the Boulevard dandy, the logged-in Net-users are that of the data dandy." [43] While displaying his dandyism, the data dandy does not want to be above the crowd; like Baudelaire's flâneur, he wants to lose himself in its mass, to be moved by the semantic vectors of mass media icons, themes and trends. As Lovink points out, a data dandy "can only play with the rules of the Net as a non-identity. What is exclusivity in the age of differentiation? ...Data dandyism is born of an aversion of being exiled into a subculture of one's own." [44] Although Lovink positions Data Dandy exclusively in data space ("Cologne and pink stockings have been replaced by precious Intel"), the Data Dandy does have a dress code of his own. This look is popular with new media artists of the 1990s: no labels, no distinct design, no bright colors or extravagant shapes — a non-identity which is nevertheless paraded as style and which in fact is carefully constructed (as I learned while shopping in Berlin in 1997 with Russian net artist Alexei Shulgin.) The designers who exemplified this style in the 1990s are Hugo Boss and Prada, whose restrained no-style style contrasts with the opulence of Versace and Gucci, the stars of the 1980s. The new style of non-identity perfectly corresponds to the rise of the Net, where endless mailing lists, newsgroups, and sites delude any single topic, image or idea — "On the Net, the only thing which appears as a mass is information itself... Today's new theme is tomorrow's 23 newsgroups." [45]

If the Net surfer, who keeps posting to mailing lists and newsgroups and accumulating endless data, is a reincarnation of Baudelaire's flâneur, the user navigating a virtual space assumes the position of the nineteenth-century explorer, a character from Cooper and Twain. This is particularly true for the navigable spaces of computer games. The dominance of spatial exploration in games exemplifies the classical American mythology in which the individual discovers his identity and builds character by moving through space. Correspondingly, in many American novels and short stories (O’Henry, Hemingway) narrative is driven by the character’s movements in the outside space. In contrast, in the 19th century European novels, there is not much movement in physical space, because the action takes place in a psychological space. From this perspective, most computer games follow the logic of American rather than European narrative. Their heroes are not developed and their psychology is not represented. But, as these heroes move through space, defeating enemies, acquiring resources and, more importantly, skill, they are "building character". This is particularly true for Role Playing Games (RPG) whose narrative is one of self-improvement. But it also holds for other game genres (action, adventure, simulators) which put the user in command of a character (Doom, Mario, Tomb Raider). As the character progresses through the game, the user herself or himself acquires new skills and knowledge. She learns how to outwit the mutants lurking in Doom levels, how to defeat the enemies with just a few kicks in Tomb Rider, how to solve the secrets of the playful world in Mario, and so on. [46]

While movement through space as a means of building character is one theme of American frontier mythology, another is exploring and "culturing" unknown space. This theme is also reflected in computer games’ structure. A typical game begins at some point in a large unknown space; in the course of the game, the player has to explore this space, mapping out its geography and unraveling its secrets. In the case of games organized into discrete levels such as Doom, the player has to systematically investigate all the spaces of a given level before he can move to the next level. In other game which takes place over one large territory, the gameplay gradually involves larger and larger parts of this territory (Adventure, WarCraft). 

This is one possible theory, one historical trajectory: from flâneur to Net surfer; from nineteenth-century American explorer to the explorer of navigable virtual space. It is also possible to construct a different trajectory which will lead from the Parisian flanerie to navigable computer spaces. In Window Shopping film historian Anne Friedberg presents archeology of a mode of perception which, according to her, characterizes modern cinematic, televisual, and cyber cultures and which she calls a "mobilized virtual gaze". [47] This mode combines two conditions: "a received perception mediated through representation" and a travel "in an imaginary flanerie through an imaginary elsewhere and an imaginary elsewhen." [48] According to Friedberg’s archeology, this mode emerged when a new nineteenth-century technology of virtual representation — photography — merged with the mobilized gaze of tourism, urban shopping and flanerie. [49] As can be seen, Friedberg connects Baudelaire's flâneur with a range of other modern practices: "The same impulses which send flâneurs through the arcades, traversing the pavement and wearing thin their shoe leather, sent shoppers into the department stores, tourists to exhibitions, spectators into the panorama, diorama, wax museum, and cinema." [50] The flâneur occupies the privileged position among these practices because he embodied most strongly the desire to combine perception with motion through space. All that remained in order to arrive at a "mobilized virtual gaze" was to virtualize this perception — something which cinema accomplished in the last decade of the nineteenth century. 

While Friedberg's account ends with television and does consider new media, the form of navigable virtual space fits well in her historical trajectory. Navigation through a virtual space, whether in a computer game, a motion simulator, data visualizations, or a 3-D human-computer interface, follows the logic of a "virtual mobile gaze". Instead of Parisian streets, shopping windows and the faces of the passers-by, the virtual flâneur travels through virtual streets, highways and planes of data; the eroticism of a split-second virtual affair with a passer-by of the opposite sex is replaced with the excitement of locating and opening a particular file or zooming into the virtual object. Just as the original flâneur of Baudelaire, the virtual flâneur is happiest on the move, clicking from one object to another, traversing room after room, level after level, data volume after data volume. Thus, just as a database form can be seen as an expression of "database complex," an irrational desire to preserve and store everything, navigable space is not just a purely functional interface. It is also an expression and gratification of psychological desire; a state of being; a subject position — or rather, a subject’s trajectory. If the subject of modern society was looking for refuge from the chaos of the real world in the stability and balance of the static composition of a painting, and later in cinema’s image, the subject of the information society finds peace in the knowledge that she can slide over endless fields of data, locating any morsel of information with the click of a button, zooming through file systems and networks. She is comforted not by the equilibrium of shapes and colors, but by the variety of data manipulation operations under her control. 

Does this mean that we have reached the end of the trajectory described by Friedberg? While still enjoying a privileged place in computer culture, flanerie now shows its age. Here we can make an analogy with the history of GUI (Graphical User Interface). Developed at Xerox Parc in the 1970s and commercialized by Apple in the early 1980s, it was appropriate when a typical user’s hard drive contained dozens or even hundreds of files. But for the next stage of Net-based computing in which the user is accessing millions of files it is no longer sufficient. [51] Bypassing the ability to display and navigate the files graphically, the user resorts to a text-based search engine. Similarly, while a "mobilized virtual gaze," described by Friedberg, was a significant advancement over earlier more static methods of data organization and access (static image, text, catalog, library), in the information age its "bandwidth" is too limited. Moreover, a simple simulation of movement through a physical space defeats a computer’s new capabilities of data access and manipulation. Thus, for a virtual flâneur such operations as search, segmentation, hyperlinking and visualization and data mining are more satisfying than just navigating through a simulation of a physical space. 

In the 1920s Dziga Vertov already understood this very well. A Man with a Movie Camera is an important point in the trajectory which leads from Baudelaire's flanerie to Aspen Movie Map, Doom and VRML worlds not simply because Vertov’s film is structured around the camera’s active exploration of city spaces, and not only because it fetishizes the camera’s mobility. Vertov wanted to overcome the limits of human vision and human movement through space to arrive at more efficient ways of data access. However, the data he worked with is raw visible reality — not reality digitized and stored in computer’s memory as numbers. Similarly, his interface was a film camera, i.e. an anthropomorphic simulation of human vision — not computer algorithms. Thus Vertov stands halfway between Baudelaire's flâneur and computer user: no longer just a pedestrian walking through a street, but not yet Gibson’s data cowboy who zooms through pure data armed with data mining algorithms.

In his research on what can be called "kino-eye interface," Vertov systematically tried different ways to overcome what he thought were the limits of human vision. He mounted cameras on the roof of a building and a moving automobile; he slowed and speed up film speed; he superimposed a number of images together in time and space (temporal montage and montage within a shot). A Man with a Movie Camera is not only a database of city life in the 1920s, a database of film techniques, and a database of new operations of visual epistemology, but it is also a database of new interface operations which together aim to go beyond a simple human navigation through a physical space. 

Along with A Man with a Movie Camera, another key point in the trajectory, from the navigable space of a nineteenth-century city to the virtual navigable computer space, is flight simulators. At the same time when Vertov was working on his film, young American engineer E.A. Link, Jr. developed the first commercial flight simulator. Significantly, Link’s patent for his simulator filed in 1930 refers to it as a "Combination Training Device for Student Aviators and Entertainment Apparatus." [52] Thus, rather than being an after-thought, the adaptation of flight simulator technology to consumer entertainment which took place in the 1990s was already envisioned by its inventor. Link’s design was a simulation of a pilot’s cockpit with all the controls, but, in contrast to a modern simulator, it had no visuals. In short, it was a motion ride without a movie. In the 1960s, visuals were added by using new video technology. A video camera was mounted on a movable arm positioned over a room-size model of an airport. The movement of the camera was synchronized with the simulator controls; its image was transmitted to a video monitor in the cockpit. While useful, this approach was limited because it was based on physical reality of an actual model set. As we saw in the "Compositing" section, a filmed and edited image is a better simulation technology than a physical construction; and a virtual image controlled by a computer is better still. Not surprisingly, soon after interactive 3-D computer graphics technology was developed, it was applied to produce visuals for the simulators by one of his developers. In 1968, Ivan Sutherland, who already pioneered interactive computer-aided design ("Sketchpad," 1962) and virtual reality (1967), formed a company to produce computer-based simulators. In the 1970s and 1980s simulators were one of the main applications of real-time 3-D computer graphics technology, thus determining to a significant degree the way this technology was developed (see "Synthetic Realism as Bricolage".) For instance, simulation of particular landscape features which are typically seen by a pilot, such as flat and mountain terrain, sky with clouds, and fog, all became important research problems. [53] The application of interactive graphics for simulators has also shaped the imagination of researchers regarding how this technology can be used. It naturalized a particular idiom: flying through a simulated spatial environment. 

Thus, one of the most common forms of navigation used today in computer culture — flying through spatialized data — can be traced back to the 1970s military simulators. From Baudelaire's flâneur strolling through physical streets, we move to Vertov's camera mounted on a moving car and then to the virtual camera of a simulator which represents the viewpoint of a military pilot. Although it was not an exclusive factor, the end of the Cold War played an important role in the extension of this military mode of perception into general culture. Until 1990, such companies as Evans and Sutherland, Boeing and Lockheed were busy developing multi-million simulators. As the military orders dried up, they had to look for consumer applications of their technology. During the 1990s, these and other companies converted their expensive simulators into arcade games, motion rides and other forms of location-based entertainment. By the end of the decade, Evans and Sutherland’s list of products included image generators for use in military and aviation simulators; a virtual set technology for use in television production; Cyber Fighter, a system of networked game stations modeled after networked military simulators; and Virtual Glider, an immersive location-based entertainment station. [54]As the military budgets continued to diminish and entertainment budgets soared, entertainment and military often came to share the same technologies and to employ the same visual forms. Probably the most graphic example of the ongoing circular transfer of technology and imagination between the military and the civilian sector in new media is the case of Doom. Originally developed and released over the Internet as a consumer game in 1993 by id software, it was soon picked by the U.S. Marine Corps who customized it into a military simulator for group combat training. [55] Instead of using multi-million dollar simulators, the Army could now train soldiers on a $50 game. The Marines, who were involved in the modifications, then went on to form their own company in order to market the customized Doom as a commercial game. 

The discussion of the military origins of navigable space form would be incomplete without acknowledging the pioneering work of Paul Virilio. In his brilliant 1984 book War and Cinema Virilio documented numerous parallels between military and film cultures of the twentieth century, including the use of a mobile camera moving through space in film in military aerial surveillance and cinematography. [56] Virilio went on to suggest that while space was the main category of the nineteenth century, the main category of the twentieth century was time. As already discussed in "Teleaction," for Virilio, telecommunication technology eliminates the category of space altogether as it makes every point on Earth as accessible as any other — at least in theory. This technology also leads to real-time politics, which require instant reactions to the events transmitted at the speed of light, and ultimately can only be handled efficiently by computers responding to each other without human intervention. From a post-Cold War perspective, Virilio’s theory can be seen as another example of the imagination transfer from the military to civilian sector. In this case, techno-politics of the Cold War nuclear arms equilibrium between the two superpowers, which at any moment were able to strike each other at any point on Earth, came to be seen by Virilio as a fundamentally new stage of culture, where real-time triumphs over space. 

Although Virilio did not write on a computer interface, the logic of his books suggests that the ideal computer interface for a culture of real-time politics would be the War Room in Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (Stanley Kubrick, 1964) with its direct lines of communication between the generals and the pilots; or DOS command lines with their military economy of command and response, rather than the more spectacular but inefficient VRML worlds. Yet, uneconomical and inefficient as it may be, the navigable space interface is thriving across all areas of new media. How can we explain its popularity? Is it simply a result of cultural inertia? A left-over from the nineteenth century? A way to make the ultimately Alien space of a computer compatible with humans by anthropomorphizing it, superimposing a simulation of a Parisian flanerie over abstract data? A relic of Cold War culture?

While all these answers make sense, it would be unsatisfactory to see navigable space as only the end of a historical trajectory, rather than as a new beginning. The few computer spaces discussed here point toward some of the aesthetic possibilities of this form; more possibilities are contained in the works of modern painters, installation artists and architects. Theoretically as well, navigable space represents a new challenge. Rather than only considering topology, geometry and logic of a static space, we need to take into account the new way in which space functions in computer culture: as something traversed by a subject, as a trajectory rather than an area. But computer culture is not the only field where the use of the category of navigable space makes sense. I will conclude this section by looking at two other fields — anthropology and architecture — where we find more examples of navigable space imagination.

In his book Non-places. Introduction to an Anthropology of Supermodernity French anthropologist Marc Auge advances the hypothesis that "supermodernity produces non-places, meaning spaces which are not themselves anthropological places and which, unlike Baudelairean modernity, do not integrate with earlier places." [57] Place is what anthropologists have studied traditionally; it is characterized by stability, and it supports stable identity, relations and history. [58] Auge's main source for his distinction between place and space, or non-place, is Michel de Certeau: "Space, for him, is a "frequent place," "an intersection of moving bodies": it is the pedestrians who transform a street (geometrically defined as a place by town planners) into a space"; it is an animation of a place by the motion of a moving body. [59] Thus, from one perspective we can understand place as a product of cultural producers, while non-places are created by users; in other words, non-place is an individual trajectory through a place. From another perspective, in supermodernity, traditional places are replaced by equally institutionalized non-places, a new architecture of transit and impermanence: hotel chains and squats, holiday clubs and refugee camps, supermarkets, airports and highways. Non-place becomes the new norm, the new way of existence.

It is interesting that as the subject who exemplifies the condition of supermodernity, Auge picks up the counterpart to the pilot or a user of a flight simulator — an airline passenger. "Alone, but one of many, the user of a non-place has contractual relations with it." This contract relieves the person of his usual determinants. "He becomes no more than what he does or experiences in the role of passenger, customer or driver." [60] Auge concludes that "as anthropological places create the organically social, so non-places create solitary contractuality," something which he sees as the very opposite of a traditional object of sociology: "Try to imagine a Durkheimian analysis of a transit lounge at Roissy!" [61]

Architecture by its very definition stands on the side of order, society and rules; it is thus a counterpart of sociology as it deals with regularities, norms and "strategies" (to use de Certeau’s term). Yet the very awareness of these assumptions underlying architecture led many contemporary architects to focus their attention on the activities of users who through their "speech acts" "reappropriate the space organized by the techniques of sociocultural production" (de Certeau). [62] Architects come to accept that the structures they design will be modified by users’ activities, and that these modifications represent an essential part of architecture. They also took up the challenge of "a Durkheimian analysis of a transit lounge at Roissy," putting their energy and imagination into design of non-places such as an airport (Kansai International Airport in Osaka by Renzo Piano), a train terminal (Waterloo International Terminal in London by Nicholas Grimshaw) or a highway control station (Steel Cloud or Los Angeles West Coast Gateway by Asymptote Architecture group). [63] Probably the ultimate in non-place architecture has been one million square meter Euralille project which redefined the existing city of Lille, France as the transit zone between the Continent and London. The project attracted some of the most interesting contemporary architects: Rem Koolhaas designed the masterplan while Jean Nouvel built Centre Euralille containing a shopping center, a school, a hotel, and apartments next to the train terminal. Centered around the entrance to the Chunnel, the underground tunnel for cars which connects the Continent and England, and the terminal for the high-speed train which travels between Lille, London, Brussels and Paris, Euralille is a space of navigation par excellence; a mega-non-place. Like the network players of Doom, Euralille users emerge from trains and cars to temporarily inhabit a zone defined through their trajectories; an environment "to just wander around inside of" (Robyn Miller); "an intersection of moving bodies" (de Certeau).

## EVE and Place

We have come a long way since Spacewar (1962) and Computer Space (1971) — at least, in terms of graphics. The images of these early computer games seem to have more in common with abstract paintings of Malevich and Mondrian than with the photorealistic renderings of Quake (1996) and Unreal (1997). But whether this graphics evolution was also accompanied by a conceptual evolution is another matter. Given the richness of modern concepts of space developed by artists, architects, filmmakers, art historians and anthropologists, our computer spaces have a long way to go.

Often the way to go forward is to go back. As this article suggested, the designers of virtual spaces may find a wealth of relevant ideas by looking at twentieth-century art, architecture, film and other arts. Similarly, as I pointed out, some of the earliest computer spaces, such as Spacewar and Aspen Movie Map, contained aesthetic possibilities which are still waiting to be explored. As a conclusion, I will discuss two more works by Jeffrey Shaw who draws upon rich cultural traditions of space construction and representation probably more systematically more than any other new media artist.

While Friedberg’s concept of virtual mobile gaze is useful in allowing us to see the connections between a number of technologies and practices of spatial navigation, such as Panorama, cinema and shopping, it can also make us blind to the important differences between them. In contrast, Shaw’s EVE (1993 — ) and Place: A User’s Manual (1995) emphasize both similarities and differences between various technologies of navigation. [64] In these works, Shaw evokes the navigation methods of Panorama, cinema, video and VR. But rather than collapsing different technologies into one, Shaw "layers" them side by side. That is, he literally encloses the interface of one technology within the interface of another. For instance, in the case of EVE the visitors find themselves inside a large semi-sphere reminiscent of the 19th-century Panorama. The projectors located in the middle of the sphere throw a rectangular image on the inside surface of the semi-sphere. In this way, the interface of cinema (an image enclosed by a rectangular frame) is placed inside the interface of Panorama (a semi-spherical enclosed space). In Place: A User’s Manual a different "layering" takes place: Panorama interface is placed inside a typical computer space interface. The user navigates a virtual landscape using first-person perspective characteristic of VR, computer games and navigable computer spaces in general. Inside this landscape are eleven cylinders with photographs mapped on them. Once the user moves inside one of these cylinders, she switches to a mode of perception typical of Panorama tradition. 

By placing interfaces of different technologies next to each other within a single work, Shaw foregrounds the unique logic of seeing, spatial access and user’s behavior characteristic of each technology. The tradition of the framed image, i.e. a representation which exists within the larger physical space which contains the viewer (painting, cinema, computer screen), meets the tradition of the "total" simulation, i.e. a simulated space which encloses the viewer (Panorama, VR). of 

Another historical dichotomy staged for us by Shaw is between the traditions of collective and individualized viewing in screen-based arts. The first tradition span from magic lantern shows to twentieth-century cinema. The second passes from the camera obscura, stereoscope and kinetoscope to head-mounted displays of VR. Both have their dangers. In the first tradition, individual's subjectivity can be dissolved in a mass-induced response. In the second, subjectivity is being defined through the interaction of an isolated subject with an object at the expense of intersubjective dialogue. In the case of viewers' interactions with computer installations, as I already noted when talking about Osmose, something quite new begins to emerge: a combination of individualized and collective spectatorship. The interaction of one viewer with the work (via a joystick, a mouse, or a head-mounted sensor) becomes in itself a new text for other viewers, situated within the work's arena, so to speak. This affects the behavior of this viewer who acts as a representative for the desires of others, and who is now oriented both to them and to the work.

EVE rehearses the whole Western history of simulation, functioning as a kind of Plato's cave in reverse: visitors progress from the real world inside the space of simulation where instead of mere shadows they are presented with technologically enhanced (via stereo) images, which look more real than their normal perceptions. [65] At the same time, EVE's enclosed round shape refers us back to the fundamental modern desire to construct a perfect self-sufficient utopia, whether visual (the nineteenth-century panorama) or social. (For instance, after 1917 Russian Revolution architect G.I. Gidoni designed a monument to the Revolution in the form of a semi-transparent globe which could hold several thousand spectators.) Yet, rather than being presented with a simulated world which has nothing to do with the real space of the viewer (as in typical VR), the visitors who enter EVE's enclosed space discover that EVE's apparatus shows the outside reality they just left. Moreover, instead of being fused in a single collective vision (Gesamtkunstwerk, cinema, mass society) the visitors are confronted with a subjective and partial view. The visitors only see what one person wearing a head-mounted sensor chooses to show them, i.e. they are literally limited by this person's point of view. In addition, instead of a 360o view, they see a small rectangular image — a mere sample of the world outside. The one visitor wearing a sensor, and thus literally acting as an eye for the rest of the audience, occupies many positions at once — a master subject, a visionary who shows the audience what is worth seeing, and at the same time just an object, an interface between them and outside reality, i.e., a tool for others; a projector, a light and a reflector all at once. 

## References:

[1] J.C.Hertz, Joystick Nation (Boston: Little, Brown and Company, 1997), 90, 84.

[2] Ibid., 150.

[3] Michel de Certeau, The Practice of Everyday Life, trans. Steven Rendall (Berkely, University of California Press, 1984), 129.

[4] Chris McGowan and Jim McCullaugh, Entertainment in the Cyber Zone (New York: Random House, 1995), 120.

[5] Qtd. in J.C.Hertz, Joystick Nation, 155-156.

[6] For critical anaylsis of motion simulator phenomenon, see Erkki Huhtamo, "Phantom Train to Technopia," in Minna Tarkka, ed., ISEA '94. The 5fth International Symposium on Electronic Art Catalogue (Helsinki: University of Art and Design, 1994); "Encapsulated Bodies in Motion: Simulators and the Quest for Total Immersion," in Simon Penny, ed., Critical Issues in Electronic Media (State University of New York Press, 1995).

[7] Stuart Card, George Robertson, Jock Mackingly, "The Information Visualizer, an Information Workplace," in CHI'91: Human Factors in Computing Systems Conference Proceedings (New York: ACM, 1991) , 181-186; available online at [http://www.acm.org/pubs/articles/proceedings/chi/108844/p181-card/p181-card.pdf](http://www.acm.org/pubs/articles/proceedings/chi/108844/p181-card/p181-card.pdf), accessed June 18, 1999.

[8] [http://www.artcom.de/projects/t\_vision/](http://www.artcom.de/projects/t\_vision/), accessed Dec. 26, 1998.

[9] [http://www.acm.org/sigchi/chi95/proceedings/panels/km\_bdy.htm](http://www.acm.org/sigchi/chi95/proceedings/panels/km\_bdy.htm), accessed Dec. 26, 1998.

[10] William Gibson, Neuromancer (New York: Ace Books, 1984).

[11] Marcos Novak, "Liquid Architecture in Cyberspace," in Michael Benedict, ed., Cyberspace: First Steps (Cambridge, Mass.: The MIT Press, 1991).

[12] Mark Pesce, Peter Kennard and Anthony Parisi, "Cyberspace," 1994. [http://www.hyperreal.org/~mpesce/www.html](http://www.hyperreal.org/~mpesce/www.html), accessed June 17, 1999.

[13] Ibid. 

[14] Michael Benedict explores the relevance of some of these disciplines to the concept of cyberspace in the introduction to his groundbreaking anthology Cyberspace: First Steps, which remains one of the best books on the topic of cyberspace. Michael Benedict, ed., Cyberspace: First Steps (Cambridge, Mass.: The MIT Press, 1991).

[15] Henri Lefebvre, The Production of Space (Oxford: Blackwell Publishers, 1991); Michel Foucault, Discipline and Punish: the Birth of the Prison (New York: Pantheon Books, 1977); Fredric Jameson, The Geopolitical Aesthetic: Cinema and Space in the World System (Bloomington: Indiana University Press, 1992); David Harvey, The Condition of Postmodernity (Oxford, England: Blackwell, 1989); Edward Soja, Postmodern Geographies: the Reassertion of Space in Critical Social Theory (London: Verso, 1989).

[16] See, for instance, Benedict, Cyberspace: First Steps; the articles of Marcos Novak [http://www.aud.ucla.edu/~marcos](http://www.aud.ucla.edu/~marcos).

[17] [http://icwhen.com/the70s/1971.html](http://icwhen.com/the70s/1971.html,), accessed November 21, 1998.

[18] Heinrich Wölfflin, Principles of Art History, translated by M. D. Hottinger (New York, Dover Publications, 1950).

[19] Erwin Panofsky, Perspective as Symbolic Form, translated by Christopher S. Wood (New York: Zone Books, 1991).

[20] Lev Manovich, "Mapping Space: Perspective, Radar and Computer Graphics," in SIGGRAPH "93 Visual Proceedings, ed. Thomas Linehan (New York: ACM, 1993.)

[21] Quoted in Alla Efimova and Lev Manovich, "Object, Space, Culture: Introduction," in Tekstura: Russian Essays on Visual Culture, eds. Alla Efimova and Lev Manovich (Chicago: University of Chicago Press, 1993), xxvi.

[22] Gilles Deleuse, Cinema (Minneapolis: University of Minnesota Press, 1986-1989).

[23] Jed Hatman and Josie Werneke, The VRML 2.0 Handbook (Reading, Mass.: Addison-Wesley Publishing Company, 1996).

[24] See Ferdinand Tönnies, Community and Society, trans. Charles P. Loomis (East Lansing, Michigan State University Press, 1957).

[25] One important exception was the Apparatus theory developed by film theoreticians in the 1970s. 

[26] Stewart Brand, The Media Lab (New York: Penguin Books, 1988), 141.

[27] Manuela Abel, ed., Jeffrey Shaw — a User's Manual (Karlsruhe, Germany: ZKM, 1997), 127-129. Three different versions of Legible City were created based on the ground plans of Manhattan, Amsterdam and Karlsruhe, Germany.

[28] [http://www.softimage.com/Projects/Osmose/](http://www.softimage.com/Projects/Osmose/)

[29] For a discussion of Archigram group in the context of computer-based virtual spaces, see Hans-Peter Schwarz, Media-Art-History. Media Museum (Munich: Prestel-Verlag, 1997), 74-76.

[30] See, for instance, Visionary Architects: Boullee, Ledoux, Lequeu (Houston: University of St. Thomas, 1968); Heinrich Klotz, ed., Paper architecture: New Projects from the Soviet Union (Frankfurt: Deutsches Architekturmuseum, 1988).

[31] See, for instance, Dietrich Neumann, ed., Film architecture: Set Designs from Metropolis to Blade Runner (Munich: Prestel, 1996). 

[32] Ilya Kabakov, On the "Total Installation" (Bonn: Cantz Verlag, 1995).

[33] Ibid., 125. This and the following translations from Russian text of Kabakov are mine — L.M.

[34] Ibid., 200.

[35] Ibid., 200-208.

[36] Ibid., 162.

[37] Ibid., 162.

[38] de Certeau, The Practice of Everyday Life, xviii.

[39] Charles Baudelaire, "The Painter of Modern Life," in My Heart Laid Bare and Other Prose Writings (London: Soho Book Company, 1986).

[40] Walter Benjamin, "Paris, Capital of the Nineteenth Century," in Reflections (New York: Schocken Books, 1986), 156.

[41] The distinction between Gemeinschaft and Gesellschaft was developed by Tönnies in Community and Society.

[42] Adilkno, The Media Archive (Brooklyn, New York: 1988), 99.

[43] Ibid., 100.

[44] Ibid.

[45] Ibid. 

[46] This narrative of maturation can be also seen as a particular case of an initiation ceremony, something which traditionally was a part of every human society.

[47] Anne Friedberg, Window Shopping: Cinema and the Post-modern (Berkeley: University of California Press, 1993), 2.

[48] Ibid.

[49] Ibid., 184.

[50] Ibid., 94.

[51] See Don Gentner and Jakob Nielson, "The Anti-Mac Interface," Communications of the ACM 39, no. 8 (August 1996), 70-82. Available online at [http://www.acm.org/cacm/AUG96/antimac.htm](http://www.acm.org/cacm/AUG96/antimac.htm).

[52] Benjamin Wooley, Virtual Worlds (Oxford, UK and Cambridge, USA: Blackwell, 1992), 39, 43.

[53] For more on the history of 3-D computer graphics, see my article "Mapping Space: Perspective, Radar and Computer Graphics," SIGGRAPH '93 Visual Proceedings, edited by Thomas Linehan, 143-147. New York: ACM, 1993. 

[54] [http://www.es.com/product\_index.html](http://www.es.com/product\_index.html), accessed January 27, 1999.

[55] Elizabeth Sikorovsky, "Training spells Doom for Marines," Federal Computer Week, July 15, 1996, available online at [http://www.fcm.com/pubs/fcw/0715/guide.htm](http://www.fcm.com/pubs/fcw/0715/guide.htm).

[56] Paul Virilio, War and Cinema (London and New York: Verso, 1989).

[57] Marc Auge, Non-places. Introduction to an Anthropology of Supermodernity, translated by John Howe (London and New York: Verso, 1995), 78.

[58] Ibid., 53-53.

[59] Ibid., 79-80.

[60] Ibid., 101, 103.

[61] Ibid., 94.

[62] De Certeau, The Practice of Everyday Life, XIV. 

[63] Jean-Claude Dubost and Jean-Francois Gonthier, eds., Architecture for the Future (Paris: Éditions Pierre Terrail, 1996), 171.

[64] Abel, Jeffrey Shaw, 138-139; 142-145.

[65] Here I am describing the particular application of EVE which I saw at "Multimediale 4" exhibition, Karlsruhe, Germany, May 1995.

---

# The Language of New Media (article)

_author: Lev Manovich_
_year: 1998_

## 1. Categories

New media requires a new critical language — to describe it, analyze it, and teach it. Where shall this language come from? We can't go on simply using technical terms such as "a website" to refer to works radically different from each other in intention and form. At the same time, traditional cultural concepts and forms prove to be inadequate as well. Image and viewer, narrative and montage, illusion and representation, space and time — everything needs to be re-defined again.

To articulate the critical language of new media we need to correlate older cultural/theoretical concepts and the concepts which describe the organization/operation of a digital computer. As an example of this approach, consider the following four categories: **interface**, **database**, **navigation** and **spatialization**. Each of these categories provides a different lens through which to inquire about the emerging logic, grammar, and poetics of new media; each brings with it a set of different questions.

**Database**. After the novel and later cinema privileged narrative as the key form of cultural expression of the modern age, the computer age brings with it a new form — database. What are the origins, ideology, and possible aesthetics of a database? How can we negotiate between a narrative and a database? Why is database imagination taking over at the end of the 20th century?

**Interface**. In contrast to a film which is projected upon a blank screen and a painting which begins with a white surface, new media objects always exist within a larger context of a human-computer interface. How does a user's familiarity with the computer's interface structure the reception of new media art? Where does interface end and the "content" begin?

**Spatialization**. The overall trend of computer culture is to spatialize all representations and experiences. The library is replaced by cyberspace; narrative is equated with traveling through space ("Myst"); all kinds of data are rendered in three dimensions through computer visualization. Why is space being privileged? Shall we try to oppose this spatialization (i.e., what about time in new media)? What are the different kinds of spaces possible in new media?

**Navigation**. We no longer only look at images or read texts; instead, we navigate through new media spaces. How can we relate the concept of navigation to more traditional categories such as viewing, reading, and identifying? In what ways do current popular navigation strategies reflect military origins of computer imaging technology? How do we de-militarize our interaction with a computer? How can we describe the person doing the navigation beyond the familiar metaphors of "user" and "flaneur"?

## 2. Genres

The next step in articulating the critical language of new media involves defining genres, forms, and figures which persist in spite of constantly changing hardware and software, using the categories as building blocks. For example, consider two key genres of computer culture: a database and navigable space. (That is, creating works in new media can be understood as either constructing the right interface to a multimedia database or as defining a navigation method through spatialized representations.)

Why does computer culture privilege these genres over other possibilities? We may associate the first genre with work (post-industrial labor of information processing) and the second with leisure and fun (computer games), yet this very distinction is no longer valid in computer culture. Increasingly, the same metaphors and interfaces are used at work and at home, for business and for entertainment. For instance, the user navigates through a virtual space both to work and to play, whether analyzing financial data or killing enemies in "Doom".

 \\#\# 3. Application

New media theory also should trace the historical formation of these categories and genres. Here are examples of such an analysis.

Exhibit 1: Dziga Vertov, "Man with a Movie Camera". USSR, 1928.

Vertov's avant-garde masterpiece anticipates every trend of new media of the 1990s. Of particular relevance are its DATABASE structure and its focus on the camera's NAVIGATION through space.

Computer culture appears to favor a database ("collection," "catalog" and "library" are also appropriate here) over a narrative form. Most Web sites and CD-ROMs, from individual artistic works to multimedia encyclopedias, are collections of individual items, grouped together using some organizing principle. Web sites, which continuously grow with new links being added to already existent material, are particularly good examples of this logic. In the case of many artists' CD-ROMs, the tendency is to fill all the available storage space with different materials: documentation, related texts, previous works and so on. In this case, the identity of a CD-ROM (or of a DVD-ROM ) as a storage media is projected onto a higher plane, becoming a cultural form of its own.

Vertov's film reconciles narrative and a database by creating narrative out of a database. Records drawn from a database and arranged in a particular order become a picture of modern life — and simultaneously an interpretation of this life. "A Man with a Movie Camera" is a machine for visual epistemology. The film also fetishizes the camera's mobility, its ability to investigate the world beyond the limits of human vision. In structuring the film around the camera's active exp.

Exhibit 2: Evans & Sutherland, Real-time Computer Graphics for Military Simulators. The USA, the early 1990s. 

Military and flight simulators have been one of the main applications of real-time 3-D photorealistic computer graphics technology in the 1970s and the 1980s, thus determining to a significant degree the way this technology developed. One of the most common forms of NAVIGATION used today in computer culture — flying through spatialized data — can be traced back to simulators representing the world through the viewpoint of a military pilot. Thus, from Vertov's mobile camera we move to the virtual camera of a simulator, which, with the end of the Cold War, became an accepted way to interact with any and all data, the default way of encountering the world in computer culture.

Exhibit 3: Peter Greenaway, "Prospero's Books". 1991.

One of the few directors of his generation and stature to enthusiastically embrace new media, Greenaway tries to re-invent cinema's visual language by adopting computer's INTERFACE conventions. In "Prospero's Books," cinematic screen frequently emulates a computer screen, with two or more images appearing in separate windows. Greenaway also anticipates the aesthetics of later computer multimedia by treating images and text as equals.

Like Vertov, Greenaway can be also thought of a DATABASE filmmaker, working on a problem of how to reconcile database and narrative forms. Many of his films progress forward by recounting a list of items, a catalog which does not have an inherent order (for example, different books in "Prospero's Books").

Exhibit 4: Tamás Waliczky, "The Garden" (1992), "The Forest" (1993), "The Way" (1994). Hungary / Germany. Joachim Sauter & Dirk Lüsenbrink (Art+Com), The Invisible Shape of Things Past. Berlin, 1997.

Tamás Waliczky openly refuses the default mode of SPATIALIZATION imposed by computer software, that of the one-point linear perspective. Each of his computer animated films "The Garden," The Forest" and "The Way" utilizes a particular perspective system: a water-drop perspective in "The Garden," a cylindrical perspective in The Forest, and a reverse perspective in "The Way". Working with computer programmers, the artist created custom-made 3-D software to implement these perspective systems.

In "The Invisible Shape of Things Past" Joachim Sauter and Dirk Lüsenbrink created an original INTERFACE for accessing historical data about Berlin. The interface de-virtualizes cinema, so to speak, by placing the records of cinematic vision back into their historical and material context. As the user navigates through a 3-D model of Berlin, he or she comes across elongated shapes lying on city streets. These shapes, which the authors call "film objects", correspond to documentary footage recorded at the corresponding points in the city. To create each shape the original footage is digitized and the frames are stacked one after another in-depth, with the original camera parameters determining the exact shape.

Exhibit 5. Computer Games. The 1990s.

Today computer games represent the most advanced area of new media, combining the latest in real-time photorealistic 3-D graphics, virtual actors, artificial intelligence, artificial life and simulation. They also illustrate the general trend of computer culture towards the SPATIALIZATION of every cultural experience. In many games, narrative and time itself are equated with the movement through space (i.e., going to new rooms, levels, or words.) In contrast to modern literature, theater, and cinema which are built around the psychological tensions between characters, these computer games return us to the ancient forms of narrative where the plot is driven by the spatial movement of the main hero, traveling through distant lands to save the princess, to find the treasure, or to defeat the Dragon.

## Notes:

This text is based on the program of the symposium "Computing Culture: Defining New Media Genres" which I and my colleagues organized in the Spring of 1988 at the Center for Research in Computing and the Arts, University of California, San Diego. See http://jupiter.ucsd.edu/~culture/symposium.html.

---

# Filters, Plug-ins, and Menus - from Creation to Selection

_author: Lev Manovich_
_year: 1998_

How can we creatively transform and extend the languages of older cultural forms in order to create new aesthetic models for digital media? Given that the digital tools which we use are themselves modeled after the production tools of the previous cultural era, do these tools actually work against us, pulling us into the past? What are the mechanisms by which the existing cultural norms and the older cultural languages are being encoded in the design of digital tools, becoming new cultural defaults? What are the ways in which these tools promote standardization, overriding our own creative intentions to impose their own ideology?

In my paper, I will propose some answers to these questions. My examples will come from a variety of areas, including the Web, virtual worlds, interactive multimedia, and interface design. My focus will be on the ways in which a particular cultural language of cinema is being encoded in the design of digital tools, including their defaults and interfaces. 

 \\#\# The Logic of the Menu

One example of the new cultural logic brought about by the digital tools is the switch from creation to selection. In digital culture authentic creation has been replaced by selection from a menu. I will analyze the historical origins of this new cultural logic and sketch a theoretical framework to understand the particular dynamics of standardization and creativity created by it.

E. H. Gombrich’s concept of a representational schema and Roland Barthes’ "death of the author" helped to sway us from the romantic ideal of the artist pulling images directly from the imagination. As Barthes puts it, "The Text is a tissue of quotations drawn from the innumerable centers of culture." Yet, even though a modern artist may be only reproducing or recombining preexisting texts and idioms, the actual material process of art-making nevertheless supports the romantic ideal. An artist operates like God creating the universe, starting with an empty canvas or a blank page and gradually filling in the details until finally bringing a new world into existence.

Such a process, manual and painstakingly slow, was appropriate for a preindustrial artisan culture. In the twentieth century, as mass production and automation gave rise to a "culture industry," art at first continued to insist on its artisanal model. Only in the 1910s, when artists began to assemble collages and montages from preexisting materials, was art introduced to the industrial mode of production.

In contrast, electronic art from its very beginning was based on a new principle: modification of an already existing signal. The first electronic musical instrument, designed in 1920 by the legendary Russian scientist and musician Leon Theremin, contained a generator producing a sine wave; the performer simply modified its frequency and amplitude. In the 1960s video artists began to build video synthesizers based on the same principle. No longer was the artist a romantic genius generating a new world out of his imagination. Turning a knob here, pressing a switch there, he became instead a technician, an accessory to the machine.

Replace the simple sine wave with a more complex signal, add a bank of signal generators, and you have the modern synthesizer, the first musical instrument to embody the logic of all new media: selection from a menu of choices. The first music synthesizers appeared in the 1950s, followed by video synthesizers in the 1960s, digital effects generators in the late 1970s, and in the 1980s computer software, such as MacDraw, that came with a repertoire of basic shapes. The process of art making has now become synchronized with the rest of modern society: everything is assembled from ready-made parts, from art objects to consumer products to people’s identities. The modern subject proceeds through life by selecting from menus and catalogs, whether assembling a wardrobe, decorating an apartment, choosing dishes at a restaurant, or joining an interest group. With electronic and digital media the creative act similarly entails selection from ready-made elements: textures and icons supplied by a paint program, 3-D models chosen from a modeling program, and melodies and rhythms built into a music program.

While previously the great text of culture from which the artist created a unique "tissue of quotations" was bubbling and simmering somewhere below consciousness, now it has become externalized and reduced to geometric objects, 3-D models, ready-made textures, and effects that are available as soon as the artist turns on the computer. The Web takes this process to the next level: it encourages the creation of texts that consist completely of pointers to other texts that are already on the Web. One does not have to add any original writing; it is enough to select from and rearrange what already exists.

The same logic applies to interactive art and media. It is often claimed that the user of an interactive work, by choosing a unique path through its elements, becomes its co-author, creating a new work. Yet if the complete work is the sum of all possible paths, what the user is actually doing is simply activating a preexisting part of the whole. As with the Web example, the user assembles a new menu, making an original selection from the total corpus available, rather than adding to it. This is a new type of creativity, which corresponds neither to the premodern idea of providing a minor modification to the tradition nor to the modern idea of a creator-genius revolting against it. It does, however, fit perfectly with the age of mass culture, where almost every practical act involves a process of selection from the given options.

The shift from creation to selection also applies to 3-D computer graphics, the main technique for building virtual sets and virtual worlds. The immense labor involved in originally constructing three-dimensional representations in a computer makes it hard to resist the temptation to utilize the preassembled, standardized objects, characters, and behaviors provided by software manufacturers — fractal landscapes, checkerboard floors, ready-made characters, and so on.10 Every program comes with libraries of ready-to-use models, effects, or even complete animations. For instance, a user of the typical high-end animation software can access preassembled animations of moving hair, rain, a comet’s tail, or smoke, with a single mouse click.

If even professional designers rely on ready-made objects and animations, the end users of virtual worlds on the Internet, who usually don’t have graphic or programming skills, have no other choice. Not surprisingly, Web virtual world providers from the beginning encouraged users to choose from the pictures, 3-D objects, and avatars that they supply. Quite soon we will see a market for detailed virtual sets, characters with programmable behaviors, and even complete scenarios (a bar with customers, a city square, a famous historical episode, etc.) from which a user can put together her or his own "unique" virtual world.

When the Kodak camera user was told, "You push the button, we do the rest," the freedom still existed to point the camera at anything. On the computer screen the slogan has become, "You push the button, we create your world." Before, the corporate imagination controlled the method of picturing reality; now, it prepackages the reality itself.

## Cinema as an Interface

"The logic of the menu" is a dramatic example of how digital tools bring to their logical conclusion certain tendencies which were already present in modern culture. But what was before a set of social practices, rituals and conventions now became encoded in the software itself. The result is a new form of control, soft but powerful. Although software does not directly prevent the users from creating from scratch, its design on every level makes it "natural" to follow a different logic: that of selection.

Along with encoding existing cultural norms in their design, digital tools also turn older cultural forms and languages into their defaults, making it difficult for the users to imagine the alternatives. In the rest of this paper, I will discuss this process by focusing on a particular cultural language — that of cinema. What are the ways in which the language of cinema is being encoded in digital tools and the very human-computer interface we are using? Rather than simply accepting this language as it, how can we creatively extend it to make the new media really new?

As new generations of both computer users and computer designers are growing up in a media-rich environment dominated by television rather than by printed texts, it is not surprising that they favor cinematic language over the language of print. As a result, cinematic elements are playing more and more central role in digital media. A hundred years after cinema's birth, cinematic ways of seeing the world, of structuring time, of narrating a story, of linking one experience to the next, are being extended to become the basic ways in which computer users access and interact with all data. In this way, the computer fulfills the promise of cinema as a visual Esperanto which pre-occupied many film artists and critics in the 1920s, from Griffith to Vertov. Indeed, millions of computer users communicate with each other through the same computer interface. And, in contrast to cinema where most of its "users" were able to "understand" cinematic language but not "speak" it (i.e., make films), all computer users can "speak" the language of the interface. They are active users of the interface, employing it to perform many tasks: send an email, run basic applications, organize files, and so on.

The original Esperanto never became truly popular. But software interfaces are widely used and are easily learned. We have a truly unprecedented situation in the history of cultural languages: something which is designed by a rather small group of people is immediately adopted by millions of computer users. How is it possible that people around the world adopt today something which a 20-something programmer in Northern California has hacked together just the night before? Shall we conclude that we are somehow biologically "wired" to the interface language, the way we are "wired," according to the original hypothesis of Noam Chomsky, to different natural languages?

Interestingly, the speed with which the language of software interfaces is formulated at the end of the twentieth century is comparable to the speed with which cinematic language was formulated exactly a hundred years ago. In both cases, the ease with which the users "acquired" these languages was to a large extent due to the fact that these languages drew on previous and already well-acquired cultural forms. In the case of cinema, it was theater, magic lantern shows, and other nineteenth-century forms of public entertainment. Software interfaces in their turn draw on older cultural forms such as cinema.

## Camera

I will begin with probably the most important case of cinema's influence on software interfaces — the mobile camera. Originally developed as part of 3-D computer graphics technology for such applications as computer-aided design, flight simulators, and computer movie making, during the 1980s and 1990s the camera model became as much of an interface convention as scrollable windows or cut and paste function. It became an accepted way for interacting with any data which is represented in three dimensions — which, in a computer culture, means literally anything and everything: the results of a physical simulation, an architectural site, design of a new molecule, financial data, the structure of a computer network and so on. As computer culture is gradually spatializing all representations and experiences, they become subjected to the camera's particular grammar of data access. Zoom, tilt, pan, and track: we now use these operations to interact with data spaces, models, objects, and bodies.

Abstracted from its historical temporary "imprisonment" within the physical body of a movie camera directed at physical reality, a virtualized camera also becomes an interface to all types of media besides 3-D space. As an example, consider the GUI (Graphical User Interface) of the leading computer animation software — PowerAnimator from Alias/Wavefront. In this interface, each window, regardless of whether it displays a 3-D model, a graph, or even plain text, contains Dolly, Track, and Zoom buttons. In this way, the model of a virtual camera is extended to apply to navigation through any kind of information, not only the one which was spatialized. It is particularly important that the user is expected to dolly and pan over text as though it is a 3-D scene. Cinematic vision triumphed over the print tradition, with the camera subsuming the page. The Guttenberg galaxy turned out to be just a subset of the Lumieres' universe.

## Frame

Another feature of cinematic perception which persists in software interfaces is a rectangular framing of represented reality. Cinema itself inherited this framing from Western painting. Since the Renaissance, the frame acted as a window into a larger space assumed to extend beyond the frame. This space was cut by the frame's rectangle into two parts: "onscreen space," the part which is inside the frame, and the part which is outside. In the famous formulation of Leon-Battista Alberti, the frame acted as a window onto the world. Or, in a more recent formulation of Jacques Aumont and his co-authors, "The onscreen space is habitually perceived as included within a more vast scenographic space. Even though the onscreen space is the only visible part, this larger scenographic part is nonetheless considered to exist around it."

Just as a rectangular frame of painting and photography presents a part of a larger space outside it, a window in HCI presents a partial view of a larger document. But if in painting (and later in photography), the framing chosen by an artist was final, computer interface benefits from a new invention introduced by cinema: the mobility of the frame. As a kino-eye moves around the space revealing its different regions, so can a computer user scroll through a window's contents.

It is not surprising to see that screen-based interactive 3-D environments, such as VRML words, also use cinema's rectangular framing since they rely on other elements of cinematic vision, specifically a mobile virtual camera. It may be more surprising to realize that Virtual Reality (VR) interface, often promoted as the most "natural" interface of all, utilizes the same framing. As in cinema, the world presented to a VR user is cut by a rectangular frame. As in cinema, this frame presents a partial view of a larger space. As in cinema, the virtual camera moves around to reveal different parts of this space.

Of course, the camera is now controlled by the user and, in fact, is identified with his/her own sight. Yet, it is crucial that in VR one is seeing the virtual world through a rectangular frame, and that this frame always presents only a part of a larger whole. This frame creates a distinct subjective experience which is much more close to cinematic perception than to unmediated sight.

Interactive virtual worlds, whether accessed through a screen-based or a VR interface, are often discussed as the logical successor to cinema, as potentially the key cultural form of the twenty-first century, just as cinema was the key cultural form of the twentieth century. These discussions usually focus on the issues of interaction and narrative. So, the typical scenario for twenty-first-century cinema involves a user represented as an avatar existing literally "inside" the narrative space, rendered with photorealistic 3-D computer graphics, interacting with virtual characters and perhaps other users, and affecting the course of narrative events.

It is an open question whether this and similar scenarios commonly invoked in new media discussions of the 1990s, indeed represent an extension of cinema or if they rather should be thought of as a continuation of some theatrical traditions, such as improvisational or avant-garde theater. But what undoubtedly can be observed in the 1990s is how virtual technology's dependence on cinema's mode of seeing and language is becoming progressively stronger. This coincides with the move from proprietary and expensive VR systems to more widely available and standardized technologies, such as VRML (Virtual Reality Modeling Language).

The creator of a VRML world can define a number of viewpoints which are loaded with the world. These viewpoints automatically appear in a special menu in a VRML browser which allows the user to step through them, one by one. Just as in cinema, ontology is coupled with epistemology: the world is designed to be viewed from particular points of view. The designer of a virtual world is thus a cinematographer as well as an architect. The user can wander around the world or she can save time by assuming the familiar position of a cinema viewer for whom the cinematographer has already chosen the best viewpoints.

Equally interesting is another option which controls how a VRML browser moves from one viewpoint to the next. By default, the virtual camera smoothly travels through space from the current viewpoint to the next as though on a dolly, its movement automatically calculated by the software. Selecting the "jump cuts" option makes it cut from one view to the next. Both modes are obviously derived from cinema. Both are more efficient than trying to explore the world on their own.

With a VRML interface, nature is firmly subsumed under culture. The eye is subordinated to the kino-eye. The body is subordinated to a virtual body of a virtual camera. While the user can investigate the world on her own, freely selecting trajectories and viewpoints, the interface privileges cinematic perception — cuts, pre-computed dolly-like smooth motions of a virtual camera, and pre-selected viewpoints.

## Point of View

The area of computer culture where cinema is being transformed into an interface most aggressively is computer games. By the 1990s, game designers have moved from two to three dimensions and had begun to incorporate cinematic language in an increasingly systematic fashion. Games started featuring lavish opening cinematic sequences (called in the game business "cinematics") to set the mood, establish the setting and introduce the narrative. Frequently, the whole game would be structured as an oscillation between interactive fragments requiring user's input and non-interactive cinematic sequences, i.e. "cinematics". As the decade progressed, game designers were creating increasingly complex — and increasingly cinematic — interactive virtual worlds. Regardless of a game's genre — action/adventure, fighting, flight simulator, first-person action, racing, or simulation — they came to rely on cinematography techniques borrowed from traditional cinema, including the expressive use of camera angles and depth of field, and dramatic lighting of 3-D sets to create mood and atmosphere. At the beginning of the decade, games used digital video of actors superimposed over 2-D or 3-D backgrounds, but by its end, they switched to fully synthetic characters. This switch also made virtual words more cinematic, as the characters could be better visually integrated with their environments.

A particularly important example of how computer games use — and extend — cinematic language, is their implementation of a dynamic point of view. In driving and flying simulators and in combat games, such as Tekken 2 (Namco, 1994 -), after a certain event takes place (car crashes, a fighter being knocked down), it is automatically replayed from a different point of view. Other games such as the Doom series (Id Software, 1993 -) and Dungeon Keeper (Bullfrog Productions, 1997) allow the user to switch between the point of view of the hero and a top-down "bird's eye" view. Finally, Nintendo went even further by dedicating four buttons on their N64 joypad to controlling the view of the action. While playing Nintendo games such as Super Mario 64 (Nintendo, 1996) the user can continuously adjust the position of the camera. Some Sony Playstation games such as Tomb Raider (Eidos, 1996) also use the buttons on the Playstation joypad for changing point of view.

The incorporation of virtual camera controls into the very hardware of game consoles is truly a historical event. Directing the virtual camera becomes as important as controlling the hero's actions. This is admitted by the game industry itself. For instance, a package for Dungeon Keeper lists four key features of the game, out of which the first two concern control over the camera: "switch your perspective," "rotate your view," "take on your friend," "unveil hidden levels". In games such as this one, cinematic perception functions as the subject in its own right. Here, the computer games are returning to "The New Vision" movement of the 1920s (Moholy-Nagy, Rodchenko, Vertov, and others), which foregrounded new mobility of a photo and film camera, and made unconventional points of view the key part of their poetics. 

## Automating Culture

The fact that computer games continue to encode, step by step, the grammar of a kino-eye in software and in hardware is not an accident. This encoding is consistent with the overall trajectory driving the computerization of culture since the 1940s, that being the automation of all cultural operations. This automation gradually moves from basic to more complex operations: from image processing and spell checking to software-generated characters, 3-D worlds, and Web Sites. The side effect of this automation is that once particular cultural codes are implemented in low-level software and hardware, they are no longer seen as choices but as unquestionable defaults. To take the automation of imaging as an example, in the early 1960s the newly emerging field of computer graphics incorporated a linear one-point perspective in 3-D software, and later directly in hardware. As a result, linear perspective became the default mode of vision in digital culture, be it computer animation, computer games, visualization, or VRML worlds. Now we are witnessing the next stage of this process: the translation of cinematic grammar of points of view into software and hardware. As Hollywood cinematography is translated into algorithms and computer chips, its convention becomes the default method of interacting with any data subjected to spatialization, with a narrative, and with other human beings. (At SIGGRAPH '97 in Los Angeles, one of the presenters called for the incorporation of Hollywood-style editing in multi-user virtual worlds software. In such implementation, user interaction with other avatar(s) will be automatically rendered using classical Hollywood conventions for filming dialog.) Element by element, cinema is being poured into a computer: first one-point linear perspective; next the mobile camera and a rectangular window; next cinematography and editing conventions, and, of course, digital personas also based on acting conventions borrowed from cinema, to be followed by make-up, set design, and, of course, the narrative structures themselves. From one cultural language among others, cinema is becoming the cultural interface, a toolbox for all cultural communication, overtaking the printed word.

Cinema, the major cultural form of the twentieth century, has found a new life as the toolbox of a computer user. What was an individual artistic vision — of Griffith, Eisenstein, Gance, Vertov — has become a way of work and a way of life for millions in the computer age. Cinema's aesthetic strategies have become basic organizational principles of computer software. The window in a fictional world of a cinematic narrative has become a window in a datascape. In short, what was cinema has become human-computer interface.

## Resisting the Defaults

I will conclude my paper by discussing a few artistic projects which, in different ways, offer alternatives to this trajectory. To summarize it once again, the trajectory involves gradual translation of elements and techniques of cinematic perception and language into a decontextualized set of tools to be used as an interface to any data. In the process of this translation, cinematic perception is divorced from its original material embodiment (camera, film stock), as well as from the historical contexts of its formation. If in cinema the camera functioned as a material object, co-existing, spatially and temporally, with the world it was showing us, it has now become a set of abstract operations. The art projects described below refuse this separation of cinematic vision from the material world. They reunite perception and material reality by making the camera and what it records a part of a virtual world's ontology. They also refuse the universalization of cinematic vision by computer culture, which (just as post-modern visual culture in general) treats cinema as a toolbox, a set of "filters" which can be used to process any input. In contrast, each of these projects employs a unique cinematic strategy which has a specific relation to the particular virtual world it reveals to the user.

In my own project Reality Generator (1996 — ongoing) I directly make points of view a part of the ontology of a virtual world. The world is described as a set of objects and a set of viewpoints attached to different points in space. Some viewpoints are simply XYZ coordinates which do not correspond to anything in particular. Other viewpoints are attached to particular objects: a leaf, a bottle in the ground, a cloud. In this way, every object also becomes the subject, the focalizer of the narrative. Everything can be seen from any position. Modernist techniques of switching between narrators in different parts of the story and re-telling the same events from different points of view are combined with computer's combinatory logic.

In The Invisible Shape of Things Past Joachim Sauter and Dirk Lüsenbrink of the Berlin-based Art+Com collective created a truly innovative cultural interface for accessing historical data about Berlin's history. The interface de-virtualizes cinema, so to speak, by placing the records of cinematic vision back into their historical and material context. As the user navigates through a 3-D model of Berlin, he or she comes across elongated shapes lying on city streets. These shapes, which the authors call "film objects", correspond to documentary footage recorded at the corresponding points in the city. To create each shape the original footage is digitized and the frames are stacked one after another in-depth, with the original camera parameters determining the exact shape. The user can view the footage by clicking on the first frame. As the frames are displayed one after another, the shape is getting correspondingly thinner.

In following with the already noted general trend of computer culture towards spatialization of every cultural experience, this cultural interface spatializes time, representing it as a shape in a 3-D space. This shape can be thought of as a book, with individual frames stacked one after another as book pages. The trajectory through time and space taken by a camera becomes a book to be read, page by page. The records of camera's vision become material objects, sharing the space with the material reality which gave rise to this vision. Cinema is solidified. This project, then, can be also understood as a virtual monument to cinema. The (virtual) shapes situated around the (virtual) city, remind us about the era when cinema was the defining form of cultural expression — as opposed to a toolbox for data retrieval and use, as it is becoming today in a computer. 

Hungarian-born artist Tamás Waliczky openly refuses the default mode of vision imposed by computer software, that of the one-point linear perspective. Each of his computer animated films The Garden (1992), The Forest (1993), and The Way (1994) utilizes a particular perspectival system: a water-drop perspective in The Garden, a cylindrical perspective in The Forest, and a reverse perspective in The Way. Working with computer programmers, the artist created custom-made 3-D software to implement these perspectival systems. Each of the systems has an inherent relationship to the subject of a film in which it is used. In The Garden, its subject is the perspective of a small child, for whom the world does not yet have an objective existence. In The Forest, the mental trauma of emigration is transformed into the endless roaming of a camera through the forest which is actually just a set of transparent cylinders. Finally, in The Way, the self-sufficiency and isolation of a Western subject from his/her environment are conveyed by the use of a reverse perspective.

In Waliczky's films, the camera and the world are made into a single whole, whereas in The Invisible Shape of Things Past the records of the camera are placed back into the world. Rather than simply subjecting his virtual worlds to different types of perspectival projection, Waliczky modified the spatial structure of the worlds themselves. In The Garden, a child playing in a garden becomes the center of the world; as he moves around, the actual geometry of all the objects around him is transformed, with objects getting bigger as he gets close to him. To create The Forest, a number of cylinders were placed inside each other, each cylinder mapped with a picture of a tree, repeated a number of times. In the film, we see a camera moving through this endless static forest in a complex spatial trajectory — but this is an illusion. In reality, the camera does move, but the architecture of the world is constantly changing as well, because each cylinder is rotating at its own speed. As a result, the world and its perception are fused together.

The work of artists such as Waliczky demonstrates that it possible to transcend the defaults of digital tools and to transform older cultural forms into a new language. But this can only be done if we are aware of these defaults and if we understand mechanisms by which particular cultural norms and preferences become encoded in the design of the tools. I hope that this paper can contribute to this.

## References:

[1] E.H. Gombrich, Art and Illusion (Princeton: Princeton University Press, 1960); Roland Barthes, "The Death of the Author," in Image, Music, Text, ed. Stephen Heath (New York: Farrar, Straus and Giroux, 1977).

[2] Barthes, 142. 

[3] Bulat Galeyev, Soviet Faust. Lev Theremin — Pioneer Of Electronic Art (in Russian) (Kazan, 1995), 19.

[4] See [http://www.aw.sgi.com/pages/home/pages/products/pages/poweranimator\_film\_sgi/index.html](http://www.aw.sgi.com/pages/home/pages/products/pages/poweranimator_film_sgi/index.html), accessed December 1, 1997.

[5] Jacques Aumont et al., Aesthetics of Film (Austin: Texas University Press, 1992), 13. 

[6] By VR interface I mean the common forms of a head-mounted or head-coupled directed display employed in VR systems. For a popular review of such displays written when the popularity of VR was at its peak, see Steve Aukstakalnis and David Blatner, Silicon Mirage: The Art and Science of Virtual Reality (Berkeley: CA: Peachpit Press, 1992), pp. 80-98. For a more technical treatment, see Dean Kocian and Lee Task, "Visually Coupled Systems Hardware and the Human Interface" in Virtual Environments and Advanced Interface Design, edited by Woodrow Barfield and Thomas Furness III (New York and Oxford: Oxford University Press, 1995), 175-257. 

[7] See Kocian and Task for details on field of view of various VR displays. Although it varies widely between different systems, the typical size of the field of view in commercial head-mounted displays (HMD) available in the first part of the 1990s was 30-50o.

[8] The following examples refer to a particular VRML browser — WebSpace Navigator 1.1 from Silicon Graphics, Inc. Other browsers have similar features. [http://www.webspace.sgi.com/WebSpace/Help/1.1/index.html](http://www.webspace.sgi.com/WebSpace/Help/1.1/index.html), accessed December 1, 1997.

[9] See John Hartman and Josie Wernecke, The VRML 2.0 Handbook: Building Moving Worlds on the Web (Reading, Mass.: Addison-Wesley Publishing Company, 1996), 363.

[10] For a more detailed analysis of this narrative structure, see my article, "The Aesthetics of Virtual Worlds: Report from Los Angeles," in CTHEORY [http://www.ctheory.com](http://www.ctheory.com).

[11] Examples of an earlier trend are Return to Zork (Activision, 1993) and The 7th Guest (Trilobyte/Virgin Games, 1993). Examples of the later trend are Soulblade (Namco, 1997) and Tomb Raider (Eidos, 1996). 

[12] Critical literature on computer games, and in particular on their language, remains very slim. Useful facts on history of computer games, descriptions of different genres, and the interviews with the designers can be found in Chris McGowan and Jim McCullaugh, Entertainment in the Cyber Zone (New York: Random House, 1995). Another useful source is J.C. Herz, Joystick Nation: How Videogames Ate Our Quarters, Won Our Hearts, and Rewired Our Minds (Boston: Little, Brown and Company, 1997).

[13] Dungeon Keeper, MS-DOS/Windows 95 CD-ROM (Bullfrog Productions, 1997). 

[14] For a more detailed discussion of the history of computer imaging as gradual automation, see my articles "Mapping Space: Perspective, Radar and Computer Graphics," in SIGGRAPH '93 Visual Proceedings, edited by Thomas Linehan, 143-147 (New York: ACM, 1993); and "Automation of Sight from Photography to Computer Vision," in Electronic Culture: Technology and Visual Representation, edited by Timothy Druckery and Michael Sand (New York: Aperture, 1996).

[15] Moses Ma's presentation, panel on "Putting a Human Face on Cyberspace: Designing Avatars and the Virtual Worlds They Live In," SIGGRAPH '97, August 7, 1997. 

[16] On the concept of focalization in narrative theory, see Mieke Bal, Narratology: Introduction to the Theory of Narrative (Toronto: University of Toronto Press, 1985). 

[17] See [http://www.artcom.de/projects/invisible\_shape/welcome.en](http://www.artcom.de/projects/invisible_shape/welcome.en), accessed December 1, 1997.

---

# Database as a Symbolic Form

_author: Lev Manovich_
_year: 1998_

## The Database Logic

After the novel, and subsequently cinema privileged narrative as the key form of cultural expression of the modern age, the computer age introduces its correlate — database. [1] Many new media objects do not tell stories; they don't have beginning or end; in fact, they don't have any development, thematically, formally, or otherwise which would organize their elements into a sequence. Instead, they are collections of individual items, where every item has the same significance as any other.

Why does new media favor database form over others? Can we explain its popularity by analyzing the specificity of the digital medium and of computer programming? What is the relationship between database and another form, which has traditionally dominated human culture — narrative? These are the questions I will address in this article.

Before proceeding, I need to comment on my use of the word database. In computer science database is defined as a structured collection of data. The data stored in a database is organized for fast search and retrieval by a computer and therefore it is anything but a simple collection of items. Different types of databases — hierarchical, network, relational, and object-oriented — use different models to organize data. For instance, the records in hierarchical databases are organized in a treelike structure. Object-oriented databases store complex data structures, called "objects," which are organized into hierarchical classes that may inherit properties from classes higher in the chain. [2] New media objects may or may not employ these highly structured database models; however, from the point of view of user's experience, a large proportion of them are databases in a more basic sense. They appear as collections of items on which the user can perform various operations: view, navigate, search. The user experience of such computerized collections is therefore quite distinct from reading a narrative or watching a film or navigating an architectural site. Similarly, literary or cinematic narrative, an architectural plan, and database each present a different model of what a world is like. It is this sense of database as a cultural form of its own which I want to address here. Following art historian Ervin Panofsky's analysis of linear perspective as a "symbolic form" of the modern age, we may even call database a new symbolic form of a computer age (or, as philosopher Jean-Francois Lyotard called it in his famous 1979 book Postmodern Condition, "computerized society"), [3] a new way to structure our experience of ourselves and of the world. Indeed, if after the death of God (Nietzche), the end of Grand Narratives of Enlightenment (Lyotard), and the arrival of the Web (Tim Berners-Lee) the world appears to us as an endless and unstructured collection of images, texts, and other data records, it is only appropriate that we will be moved to model it as a database. But it is also appropriate that we would want to develop poetics, aesthetics, and ethics of this database.

Let us begin by documenting the dominance of database form in new media. The most obvious examples of this are popular multimedia encyclopedias, which are collections by their very definition; as well as other commercial CD-ROM titles which are collections as well — of recipes, quotations, photographs, and so on. [4] The identity of a CD-ROM as a storage media is projected onto another plane, becoming a cultural form of its own. Multimedia works which have "cultural" content appear to particularly favor the database form. Consider, for instance, the "virtual museums" genre — CD-ROMs which take the user on a "tour" through a museum collection. A museum becomes a database of images representing its holdings, which can be accessed in different ways: chronologically, by country, or by artist. Although such CD-ROMs often simulate the traditional museum experience of moving from room to room in a continuous trajectory, this "narrative" method of access does not have any special status in comparison to other access methods offered by a CD-ROM. Thus the narrative becomes just one method of accessing data among others. Another example of a database form is a multimedia genre which does not has an equivalent in traditional media — CD-ROMs devoted to a single cultural figure such as a famous architect, film director, or writer. Instead of a narrative biography, we are presented with a database of images, sound recordings, video clips and/or texts which can be navigated in a variety of ways.

CD-ROMs and other digital storage media (floppies, and DVD-ROMs) proved to be particularly receptive to traditional genres which already had a database-like structure, such as a photo album; they also inspired new database genres, like a database biography. Where the database form really flourished, however, is on the Internet. As defined by original HTML, a Web page is a sequential list of separate elements: text blocks, images, digital video clips, and links to other pages. It is always possible to add a new element to the list — all you have to do is to open a file and add a new line. As a result, most Web pages are collections of separate elements: texts, images, links to other pages or sites. A home page is a collection of personal photographs. A site of a major search engine is a collection of numerous links to other sites (along with a search function, of course). A site of a Web-based TV or radio station offers collections of video or audio programs along with the option to listen to the current broadcast; but this current program is just one choice among many other programs stored on the site. Thus the traditional broadcasting experience, which consisted solely of a real-time transmission, becomes just one element in a collection of options. Similar to the CD-ROM medium, the Web offered fertile ground to already existing database genres (for instance, bibliography) and also inspired the creation of new ones such as the sites devoted to a person or a phenomenon (Madonna, Civil War, new media theory, etc.) which, even if they contain original material, inevitably center around the list of links to other Web pages on the same person or phenomenon.

The open nature of the Web as medium (Web pages are computer files which can always be edited) means that the Web sites never have to be complete; and they rarely are. The sites always grow. New links are being added to what is already there. It is as easy to add new elements to the end of list as it is to insert them anywhere in it. All this further contributes to the anti-narrative logic of the Web. If new elements are being added over time, the result is a collection, not a story. Indeed, how can one keep a coherent narrative or any other development trajectory through the material if it keeps changing?

## Data and Algorithm

Of course not all new media objects are explicitly databases. Computer games, for instance, are experienced by their players as narratives. In a game, the player is given a well-defined task — winning the match, being first in a race, reaching the last level, or reaching the highest score. It is this task which makes the player experience the game as a narrative. Everything which happens to her in a game, all the characters and objects she encounters either take her closer to achieving the goal or further away from it. Thus, in contrast to the CD-ROM and Web databases, which always appear arbitrary since the user knows that additional material could have been added without in any way modifying the logic of the database, in a game, from a user's point of view, all the elements are motivated ( i.e., their presence is justified). [5]

Often the narrative shell of a game ("you are the specially trained commando who has just landed on a Lunar base; your task is to make your way to the headquarters occupied by the mutant base personnel...") masks a simple algorithm well-familiar to the player: kill all the enemies on the current level, while collecting all treasures it contains; go to the next level and so on until you reach the last level. Other games have different algorithms. Here is an algorithm of the legendary "Tetris": when a new block appears, rotate it in such a way so it will complete the top layer of blocks on the bottom of the screen making this layer disappear. The similarity between the actions expected from the player and computer algorithms is too uncanny to be dismissed. While computer games do not follow database logic, they appear to be ruled by another logic — that of an algorithm. They demand that a player executes an algorithm in order to win.

An algorithm is the key to the game experience in a different sense as well. As the player proceeds through the game, she gradually discovers the rules which operate in the universe constructed by this game. She learns its hidden logic, in short, its algorithm. Therefore, in games where the gameplay departs from following an algorithm, the player is still engaged with an algorithm, albeit in another way: she is discovering the algorithm of the game itself. I mean this both metaphorically and literally: for instance, in a first person shooter, such as "Quake," the player may eventually notice that under such and such condition the enemies will appear from the left, i.e. she will literally reconstruct a part of the algorithm responsible for the gameplay. Or, in a different formulation of the legendary author of Sim games Will Wright, "Playing the game is a continuous loop between the user (viewing the outcomes and inputting decisions) and the computer (calculating outcomes and displaying them back to the user). The user is trying to build a mental model of the computer model." [6]

What we encountered here is an example of the general principle of new media: the projection of the ontology of a computer onto culture itself. If in physics the world is made of atoms and in genetics, it is made of genes, computer programming encapsulates the world according to its own logic. The world is reduced to two kinds of software objects which are complementary to each other: data structures and algorithms. Any process or task is reduced to an algorithm, a final sequence of simple operations which a computer can execute to accomplish a given task. And any object in the world — be it the population of a city, or the weather over the course of a century, a chair, a human brain — is modeled as a data structure, i.e. data organized in a particular way for efficient search and retrieval. [7] Examples of data structures are arrays, linked lists, and graphs. Algorithms and data structures have a symbiotic relationship. The more complex the data structure of a computer program, the simpler the algorithm needs to be, and vice versa. Together, data structures and algorithms are two halves of the ontology of the world according to a computer.

The computerization of culture involves the projection of these two fundamental parts of computer software — and of the computer's unique ontology — onto the cultural sphere. If CD-ROMs and Web databases are cultural manifestations of one half of this ontology — data structures, then computer games are manifestations of the second half — algorithms. Games (sports, chess, cards, etc.) are one cultural form which required algorithm-like behavior from the players; consequently, many traditional games were quickly simulated on computers. In parallel, new genres of computer games came into existence such as a first person shooter ("Doom," "Quake"). Thus, as it was the case with database genres, computer games both mimic already existing games and create new game genres.

It may appear at first sight that data is passive and algorithm is active — another example of passive-active binary categories so loved by human cultures. A program reads in data, executes an algorithm, and writes out new data. We may recall that before "computer science" and "software engineering" became established names for the computer field, it was called "data processing". This name remained in use for a few decades during which computers were mainly associated with performing calculations over data. However, the passive/active distinction is not quite accurate since data does not just exist — it has to be generated. Data creators have to collect data and organize it, or create it from scratch. Texts need to be written, photographs need to be taken, video and audio need to be recorded. Or they need to be digitized from already existing media. In the 1990s, when the new role of a computer as a Universal Media Machine became apparent, already computerized societies went into a digitizing craze. All existing books and videotapes, photographs, and audio recordings started to be fed into computers at an ever-increasing rate. Steven Spielberg created the Shoah Foundation which videotaped and then digitized numerous interviews with Holocaust survivors; it would take one person forty years to watch all the recorded material. The editors of Mediamatic journal, who devoted a whole issue to the topic of "the storage mania" (Summer 1994) wrote: "A growing number of organizations are embarking on ambitious projects. Everything is being collected: culture, asteroids, DNA patterns, credit records, telephone conversations; it doesn't matter." [8] Once it is digitized, the data has to be cleaned up, organized, indexed. The computer age brought with it a new cultural algorithm: reality-\> media-\>data-\>database. The rise of the Web, this gigantic and always changing data corpus, gave millions of people a new hobby or profession: data indexing. There is hardly a Web site which does not feature at least a dozen links to other sites, therefore every site is a type of database. And, with the rise of Internet commerce, most large-scale commercial sites have become real databases, or rather front-ends to company databases. For instance, in the Fall of 1998, Amazon.com, an online book store, had 3 million books in its database; and the maker of leading commercial database Oracle has offered Oracle 8i, fully integrated with the Internet and featuring unlimited database size, natural-language queries, and support for all multimedia data types. [9] Jorge Luis Borges's story about a map which was equal in size to the territory it represented became re-written as the story about indexes and the data they index. But now the map has become larger than the territory. Sometimes, much larger. Porno Web sites exposed the logic of the Web to its extreme by constantly re-using the same photographs from other porno Web sites. Only rare sites featured the original content. On any given date, the same few dozen images would appear on thousands of sites. Thus, the same data would give rise to more indexes than the number of data elements themselves.

## Database and Narrative

As a cultural form, database represents the world as a list of items and it refuses to order this list. In contrast, a narrative creates a cause-and-effect trajectory of seemingly unordered items (events). Therefore, database and narrative are natural enemies. Competing for the same territory of human culture, each claims an exclusive right to make meaning out of the world.

In contrast to most games, most narratives do not require algorithm-like behavior from their readers. However, narratives and games are similar in that the user, while proceeding through them, must uncover its underlying logic — its algorithm. Just like a game player, a reader of a novel gradually reconstructs an algorithm (here I use it metaphorically) which the writer used to create the settings, the characters, and the events. From this perspective, I can re-write my earlier equations between the two parts of the computer's ontology and its corresponding cultural forms. Data structures and algorithms drive different forms of computer culture. CD-ROMs, Web sites, and other new media objects which are organized as databases correspond to the data structure; while narratives, including computer games, correspond to the algorithms.

In computer programming, data structures and algorithms need each other; they are equally important for a program to work. What happens in a cultural sphere? Do databases and narratives have the same status in computer culture?

Some media objects explicitly follow database logic in their structure while others do not; but behind the surface practically all of them are databases. In general, creating a work in new media can be understood as the construction of an interface to a database. In the simplest case, the interface simply provides the access to the underlying database. For instance, an image database can be represented as a page of miniature images; clicking on a miniature will retrieve the corresponding record. If a database is too large to display all of its records at once, a search engine can be provided to allow the user to search for particular records. But the interface can also translate the underlying database into a very different user experience. The user may be navigating a virtual three-dimensional city composed of letters, as in Jeffrey Shaw's interactive installation "Legible City". [10] Or she may be traversing a black and white image of a naked body, activating pieces of text, audio, and video embedded in its skin (Harwood's CD-ROM "Rehearsal of Memory".) [11] Or she may be playing with virtual animals which come closer or run away depending upon her movements (Scott Fisher et al, VR installation, "Menagerie".) [12] Although each of these works engages the user in a set of behaviors and cognitive activities which are quite distinct from going through the records of a database, all of them are databases. "Legible City" is a database of three-dimensional letters which make up the city. "Rehearsal of Memory" is a database of texts and audio and video clips which are accessed through the interface of a body. And "Menagerie" is a database of virtual animals, including their shapes, movements, and behaviors.

Database becomes the center of the creative process in the computer age. Historically, the artist made a unique work within a particular medium. Therefore the interface and the work were the same; in other words, the level of an interface did not exist. With new media, the content of the work and the interface become separate. It is therefore possible to create different interfaces to the same material. These interfaces may present different versions of the same work, as in David Blair's WaxWeb. [13] Or they may be radically different from each other, as in Moscow WWWArt Centre. [14] This is one of the ways in which the already discussed principle of variability of new media manifests itself. But now we can give this principle a new formulation. The new media object consists of one or more interfaces to a database of multimedia material. If only one interface is constructed, the result will be similar to a traditional art object; but this is an exception rather than the norm.

This formulation places the opposition between database and narrative in a new light, thus redefining our concept of narrative. The "user" of a narrative is traversing a database, following links between its records as established by the database's creator. An interactive narrative (which can be also called "hyper-narrative" in an analogy with hypertext) can then be understood as the sum of multiple trajectories through a database. A traditional linear narrative is one, among many other possible trajectories; i.e. a particular choice made within a hyper-narrative. Just as a traditional cultural object can now be seen as a particular case of a new media object (i.e., a new media object which only has one interface), traditional linear narrative can be seen as a particular case of a hyper-narrative.

This "technical," or "material" change in the definition of narrative does not mean that an arbitrary sequence of database records is a narrative. To qualify as a narrative, a cultural object has to satisfy a number of criteria, which literary scholar Mieke Bal defines as follows: it should contain both an actor and a narrator; it also should contain three distinct levels consisting of the text, the story, and the fabula; and its "contents" should be "a series of connected events caused or experienced by actors." [15] Obviously, not all cultural objects are narratives. However, in the world of new media, the word "narrative" is often used as an all-inclusive term, to cover up the fact that we have not yet developed a language to describe these new strange objects. It is usually paired with another over-used word — interactive. Thus, a number of database records linked together, so that more than one trajectory is possible, are assumed to constitute "interactive narrative". But to just create these trajectories is of course not sufficient; the author also has to control the semantics of the elements and the logic of their connection so that the resulting object will meet the criteria of narrative as outlined above. Another erroneous assumption frequently made is that by creating her own path (i.e., choosing the records from a database in a particular order) the user constructs her own unique narrative. However, if the user simply accesses different elements, one after another, in a usually random order, there is no reason to assume that these elements will form a narrative at all. Indeed, why should an arbitrary sequence of database records, constructed by the user, result in "a series of connected events caused or experienced by actors"? 

In summary, database and narrative do not have the same status in computer culture. In the database/narrative pair, database is the unmarked term. [16] Regardless of whether new media objects present themselves as linear narratives, interactive narratives, databases, or something else, underneath, on the level of material organization, they are all databases. In new media, the database supports a range of cultural forms which range from direct translation (i.e., a database stays a database) to a form whose logic is the opposite of the logic of the material form itself — a narrative. More precisely, a database can support narrative, but there is nothing in the logic of the medium itself which would foster its generation. It is not surprising, then, that databases occupy a significant, if not the largest, territory of the new media landscape. What is more surprising is why the other end of the spectrum — narratives — still exist in new media.

## The Semiotics of Database

The dynamics which exist between database and narrative are not unique in new media. The relation between the structure of a digital image and the languages of contemporary visual culture is characterized by the same dynamics. As defined by all computer software, a digital image consists of a number of separate layers, each layer containing particular visual elements. Throughout the production process, artists and designers manipulate each layer separately; they also delete layers and add new ones. Keeping each element as a separate layer allows the content and the composition of an image to be changed at any point: deleting a background, substituting one person for another, moving two people closer together, blurring an object, and so on. What would a typical image look like if the layers were merged together? The elements contained on different layers will become juxtaposed resulting in a montage look. Montage is the default visual language of composite organization of an image. However, just as database supports both the database form and its opposite — narrative, a composite organization of an image on the material level supports two opposing visual languages. One is modernist-MTV montage — two-dimensional juxtaposition of visual elements designed to shock due to its impossibility in reality. The other is the representation of familiar reality as seen by a photo of film camera (or its computer simulation, in the case of 3-D graphics). During the 1980s and 1990s, all image making technologies became computer-based thus turning all images into composites. In parallel, a Renaissance of montage took place in visual culture, in print, in broadcast design, and new media. This is not unexpected — after all, this is the visual language dictated by the composite organization. What needs to be explained is why photorealist images continue to occupy such a significant space in our computer-based visual culture.

It would be surprising, of course, if photorealist images suddenly disappeared completely. The history of culture does not contain such sudden breaks. Similarly, we should not expect that new media would completely substitute narrative by database. New media does not radically break with the past; rather, it distributes weight differently between the categories which hold culture together, foregrounding what was in the background, and vice versa. As Frederick Jameson writes in his analysis of another shift, in this case from modernism to post-modernism: "Radical breaks between periods do not generally involve complete changes but rather the restructuration of a certain number of elements already given: features that in an earlier period of system were subordinate became dominant, and features that had been dominant again become secondary." [17] 

Database — narrative opposition is the case in point. To further understand how computer culture redistributes weight between the two terms of opposition in computer culture I will bring in a semiological theory of syntagm and paradigm. According to this model, originally formulated by Ferdinand de Saussure to describe natural languages such as English and later expanded by Roland Barthes and others to apply to other sign systems (narrative, fashion, food, etc.), the elements of a system can be related on two dimensions: syntagmatic and paradigmatic. [18] As defined by Barthes, "the syntagm is a combination of signs, which has space as a support." To use the example of natural language, the speaker produces an utterance by stringing together the elements, one after another, in a linear sequence. This is the syntagmatic dimension. Now, let's look at the paradigm. To continue with an example of a language user, each new element is chosen from a set of other related elements. For instance, all nouns form a set; all synonyms of a particular word form another set. In the original formulation of Saussure, "the units which have something in common are associated in theory and thus form groups within which various relationships can be found." [19] This is the paradigmatic dimension.

The elements on a syntagmatic dimension are related _in praesentia_, while the elements on a paradigmatic dimension are related _in absentia_. For instance, in the case of a written sentence, the words which comprise it materially exist on a piece of paper, while the paradigmatic sets to which these words belong only exist in writer's and reader's minds. Similarly, in the case of a fashion outfit, the elements which make it, such as a skirt, a blouse, and a jacket, are present in reality, while pieces of clothing which could have been present instead — different skirt, different blouse, different jacket — only exist in the viewer's imagination. Thus, syntagm is explicit and paradigm is implicit; one is real and the other is imagined.

Literary and cinematic narratives work in the same way. Particular words, sentences, shots, scenes which make up a narrative have a material existence; other elements which form an imaginary world of an author or a particular literary or cinematic style and which could have appeared instead exist only virtually. Put differently, the database of choices from which narrative is constructed (the paradigm) is implicit; while the actual narrative (the syntagm) is explicit.

New media reverses this relationship. Database (the paradigm) is given material existence, while narrative (the syntagm) is de-materialized. Paradigm is privileged, syntagm is downplayed. Paradigm is real, syntagm is virtual. To see this, consider the new media design process. The design of any new media object begins with assembling a database of possible elements to be used. (Macromedia Director calls this database "cast," Adobe Premiere calls it "project", ProTools calls it a "session," but the principle is the same.) This database is the center of the design process. It typically consists of a combination of original and stock material distributed such as buttons, images, video, and audio sequences; 3-D objects; behaviors, and so on. Throughout the design process, new elements are added to the database; existing elements are modified. The narrative is constructed by linking elements of this database in a particular order, i.e. designing a trajectory leading from one element to another. On the material level, a narrative is just a set of links; the elements themselves remain stored in the database. Thus the narrative is more virtual than the database itself. (Since all data is stored as electronic signals, the word "material" seem to be no longer appropriate. Instead, we should talk about different degrees of virtuality.)

The paradigm is privileged over syntagm in yet another way in interactive objects presenting the user with a number of choices at the same time — which is what typical interactive interfaces do. For instance, a screen may contain a few icons; clicking on each icon leads the user to a different screen. On the level of an individual screen, these choices form a paradigm of their own which is explicitly presented to the user. On the level of the whole object, the user is made aware that she is following one possible trajectory among many others. In other words, she is selecting one trajectory from the paradigm of all trajectories which are defined.

Other types of interactive interfaces make the paradigm even more explicit by presenting the user with an explicit menu of all available choices. In such interfaces, all of the categories are always available, just a mouse click away. The complete paradigm is present before the user, its elements neatly arranged in a menu. This is another example of how new media makes explicit the psychological processes involved in cultural communication. Other examples include the already discussed shift from creation to selection, which externalizes and codifies the database of cultural elements existing in the creator's mind; as well as the very phenomena of interactive links. New media takes "interaction" literally, equating it with a strictly physical interaction between a user and a screen (by pressing a button), at the sake of psychological interaction. The psychological processes of filling-in, hypothesis forming, recall and identification — which are required for us to comprehend any text or image at all — are erroneously equated with an objectively existing structure of interactive links.

Interactive interfaces foreground the paradigmatic dimension and often make explicit paradigmatic sets. Yet, they are still organized along the syntagmatic dimension. Although the user is making choices at each new screen, the end result is a linear sequence of screens which she follows. This is the classical syntagmatic experience. In fact, it can be compared to constructing a sentence in a natural language. Just as a language user constructs a sentence by choosing each successive word from a paradigm of other possible words, a new media user creates a sequence of screens by clicking on this or that icon at each screen. Obviously, there are many important differences between these two situations. For instance, in the case of a typical interactive interface, there is no grammar and paradigms are much smaller. Yet, the similarity of basic experience in both cases is quite interesting; in both cases, it unfolds along a syntagmatic dimension.

Why does new media insist on this language-like sequencing? My hypothesis is that it follows the dominant semiological order of the twentieth century — that of cinema. Cinema replaced all other modes of narration with a sequential narrative, an assembly line of shots which appear on the screen one at a time. For centuries, a spatialized narrative where all images appear simultaneously dominated European visual culture; then it was delegated to "minor" cultural forms as comics or technical illustrations. "Real" culture of the twentieth century came to speak in linear chains, aligning itself with the assembly line of an industrial society and the Turing machine of a post-industrial era. New media continues this mode, giving the user information one screen at a time. At least, this is the case when it tries to become "real" culture (interactive narratives, games); when it simply functions as an interface to information, it is not ashamed to present much more information on the screen at once, be it in the form of tables, normal or pull-down menus, or lists. In particular, the experience of a user filling in an online form can be compared to pre-cinematic spatialized narrative: in both cases, the user is following a sequence of elements which are presented simultaneously. 

## A Database Complex

To what extent is the database form intrinsic to modern storage media? For instance, a typical music CD is a collection of individual tracks grouped together. The database impulse also drives much of photography throughout its history, from William Henry Fox Talbot's "Pencil of Nature" to August Sander's monumental typology of modern German society "Face of Our Time," to the Bernd and Hilla Becher's equally obsessive cataloging of water towers. Yet, the connection between storage media and database forms is not universal. The prime exception is cinema. Here the storage media supports the narrative imagination. We may quote once again Christian Metz who wrote in the 1970s, "Most films shot today, good or bad, original or not, "commercial" or not, have as a common characteristic that they tell a story; in this measure, they all belong to one and the same genre, which is, rather, a sort of "super-genre" ["sur-genre"]." [20] Why then, in the case of photography storage media, does technology sustain database, while in the case of cinema it gives rise to a modern narrative form par excellence? Does this have to do with the method of media access? Shall we conclude that random access media, such as computer storage formats (hard drives, removable disks, CD-ROMs), favors database, while sequential access media, such as film, favors narrative? This does not hold either. For instance, a book, this perfect random-access medium, supports database forms, such as photo albums, and narrative forms, such as novels.

Rather than trying to correlate database and narrative forms with modern media and information technologies, or deduce them from these technologies, I prefer to think of them as two competing imaginations, two basic creative impulses, two essential responses to the world. Both have existed long before modern media. The ancient Greeks produced long narratives, such as Homer's epic poems The Iliad and The Odyssey; they also produced encyclopedias. The first fragments of a Greek encyclopedia to have survived were the work of Speusippus, a nephew of Plato. Diderot wrote novels — and also was in charge of monumental Encyclopédie, the largest publishing project of the 18th century. Competing to make meaning out of the world, database and narrative produce endless hybrids. It is hard to find a pure encyclopedia without any traces of a narrative in it and vice versa. For instance, until alphabetical organization became popular a few centuries ago, most encyclopedias were organized thematically, with topics covered in a particular order (typically, corresponding to seven liberal arts.) At the same time, many narratives, such as the novels by Cervantes and Swift, and even Homer's epic poems — the founding narratives of the Western tradition — traverse an imaginary encyclopedia.

Modern media is the new battlefield for the competition between database and narrative. It is tempting to read the history of this competition in dramatic terms. First the medium of visual recording — photography — privileges catalogs, taxonomies and lists. While the modern novel blossoms, and academicians continue to produce historical narrative paintings all through the nineteenth century, in the realm of the new techno-image of photography, database rules. The next visual recording medium — film — privileges narrative. Almost all fictional films are narratives, with few exceptions. Magnetic tape used in video does not bring any substantial changes. Next storage media — computer-controlled digital storage devices (hard drives, removable drives, CD-ROMs, DVD-ROMs) privilege database once again. Multimedia encyclopedias, virtual museums, pornography, artists' CD-ROMs, library databases, Web indexes, and, of course, the Web itself: database is more popular than ever before.

Digital computer turns out to be the perfect medium for the database form. Like a virus, databases infect CD-ROMs and hard drives, servers, and Web sites. Can we say that database is the cultural form most characteristic of a computer? In her 1978 article "Video: The Aesthetics of Narcissism," probably the single most well-known article on video art, art historian Rosalind Krauss argued that video is not a physical medium but a psychological one. In her analysis, "video's real medium is a psychological situation, the very terms of which are to withdraw attention from an external object — an Other — and invest it in the Self." [21] In short, video art is a support for the psychological condition of narcissism. Does new media similarly function to play out a particular psychological condition, something which can be called a database complex? In this respect, it is interesting that database imagination has accompanied computer art from its very beginning. In the 1960s, artists working with computers wrote programs to systematically explore the combinations of different visual elements. In part, they were following art world trends such as minimalism. Minimalist artists executed works of art according to pre-existent plans; they also created series of images or objects by systematically varying a single parameter. So, when minimalist artist Sol LeWitt spoke of an artist's idea as "the machine which makes the work," it was only logical to substitute the human executing the idea by a computer. [22] At the same time, since the only way to make pictures with a computer was by writing a computer program, the logic of computer programming itself pushed computer artists in the same directions. Thus, for artist Frieder Nake a computer was a "Universal Picture Generator," capable of producing every possible picture out of a combination of available picture elements and colors. [23] In 1967 he published a portfolio of 12 drawings which were obtained by successfully multiplying a square matrix by itself. Another early computer artist Manfred Mohr produced numerous images which recorded various transformations of a basic cube.

Even more remarkable works were filmed by John Witney, the pioneer of computer filmmaking. His films such as "Permutations" (1967), "Arabesque" (1975), and others systematically explored the transformations of geometric forms obtained by manipulating elementary mathematical functions. Thus they substituted successive accumulation of visual effects for narrative, figuration, or even formal development. Instead, they presented the viewer with databases of effects. This principle reaches its extreme in Witney's earlier film which was made using analog computer and was called "Catalog". In his Expanded Cinema (1970) critic Gene Youngblood writes about this remarkable film: "The elder Whitney actually never produced a complete, coherent movie on the analog computer because he was continually developing and refining the machine while using it for commercial work... However, Whitney did assemble a visual catalog of the effects he had perfected over the years. This film, simply titled Catalog, was completed in 1961 and proved to be of such overwhelming beauty that many persons still prefer Whitney's analog work over his digital computer films." [24] One is tempted to read "Catalog" as one of the founding moments of new media. Today all software for media creation arrives with endless "plug-ins" — the banks of effects which with a press of a button generate interesting images from any input whatsoever. In parallel, much of the aesthetics of computerized visual culture is effects driven, especially when a new techno-genre (computer animation, multimedia, Web sites) is just getting established. For instance, countless music videos are variations of Witney's "Catalog" — the only difference is that the effects are applied to the images of human performers. This is yet another example of how the logic of a computer — in this case, the ability of a computer to produce endless variations of elements and to act as a filter, transforming its input to yield a new output — becomes the logic of culture at large.

## Database Cinema: Greenaway and Vertov

Although database form may be inherent to new media, countless attempts to create "interactive narratives" testify to our dissatisfaction with the computer in the sole role of an encyclopedia or a catalog of effects. We want new media narratives, and we want these narratives to be different from the narratives we saw or read before. In fact, regardless of how often we repeat in public that the modernist notion of medium specificity ("every medium should develop its own unique language") is obsolete, we do expect computer narratives to showcase new aesthetic possibilities which did not exist before digital computers. In short, we want them to be new media specific. Given the dominance of database in computer software and the key role it plays in the computer-based design process, perhaps we can arrive at new kinds of narrative by focusing our attention on how narrative and database can work together. How can a narrative take into account the fact that its elements are organized in a database? How can our new abilities to store vast amounts of data, to automatically classify, index, link, search and instantly retrieve it lead to new kinds of narratives?

Peter Greenaway, one of the very few prominent film directors concerned with expanding cinema's language, complained that "the linear pursuit — one story at a time told chronologically — is the standard format of cinema." Pointing out that cinema lags behind modern literature in experimenting with narrative, he asked: "Could it not travel on the road where Joyce, Eliot, Borges and Perec have already arrived?" [25] While Greenaway is right to direct filmmakers to more innovative literary narratives, new media artists working on the database — narrative problem can learn from cinema "as it is". Cinema already exists right in the intersection between database and narrative. We can think of all the material accumulated during shooting forming a database, especially since the shooting schedule usually does not follow the narrative of the film but is determined by production logistics. During editing the editor constructs a film narrative out of this database, creating a unique trajectory through the conceptual space of all possible films which could have been constructed. From this perspective, every filmmaker engages with the database-narrative problem in every film, although only a few have done this self-consciously.

One exception is Greenaway himself. Throughout his career, he has been working on a problem of how to reconcile database and narrative forms. Many of his films progress forward by recounting a list of items, a catalog which does not have any inherent order (for example, different books in Prospero's Books). Working to undermine a linear narrative, Greenaway uses different systems to order his films. He wrote about this approach: "If a numerical, alphabetic color-coding system is employed, it is done deliberately as a device, a construct, to counteract, dilute, augment or complement the all-pervading obsessive cinema interest in plot, in narrative, in the "I'm now going to tell you a story school of film-making." [26] His favorite system is numbers. The sequence of numbers acts as a narrative shell which "convinces" the viewer that she is watching a narrative. In reality the scenes which follow one another are not connected in any logical way. By using numbers, Greenaway "wraps" a minimal narrative around a database. Although Greenaway's database logic was present already in his "avant-garde" films such as The Falls (1980), it has also structured his "commercial" films from the beginning. Draughtsman's Contract (1982) is centered around twelve drawings being made by the draftsman. They do not form any order; Greenaway emphasizes this by having draftsman to work on a few drawings at once. Eventually, Greenaway's desire to take "cinema out of cinema" led to his work on a series of installations and museum exhibitions in the 1990s. No longer having to conform to the linear medium of film, the elements of a database are spatialized within a museum or even the whole city. This move can be read as the desire to create a database at its most pure form: the set of elements not ordered in any way. If the elements exist in one dimension (time of a film, list on a page), they will be inevitably ordered. So the only way to create a pure database is to spatialize it, distributing the elements in space. This is exactly the path which Greenaway took. Situated in three-dimensional space which does not have an inherent narrative logic, a 1992 installation "100 Objects to Represent the World" in its very title proposes that the world should be understood through a catalog rather than a narrative. At the same time, Greenaway does not abandon narrative; he continues to investigate how database and narrative can work together. Having presented "100 Objects" as an installation, Greenaway next turned it into an opera set. In the opera, the narrator Thrope uses the objects to conduct Adam and Eve through the whole of human civilization, thus turning 100 objects into a sequential narrative. [27] In another installation "The Stairs-Munich-Projection" (1995) Greenaway put up a hundred screens — each for one year in the history of cinema — throughout Munich. Again, Greenaway presents us with a spatialized database — but also with a narrative. By walking from one screen to another, one follows cinema’s history. The project uses Greenaway's favorite principle of organization by numbers, pushing it to the extreme: the projections on the screens contain no figuration, just numbers. The screens are numbered from 1895 to 1995, one screen for each year of cinema's history. Along with numbers, Greenaway introduces another line of development. Each projection is slightly different in color. [28] The hundred colored squares form an abstract narrative of their own which runs in parallel to the linear narrative of cinema’s history. Finally, Greenaway superimposes yet a third narrative by dividing the history of cinema into five sections, each section staged in a different part of the city. The apparent triviality of the basic narrative of the project — one hundred numbers, standing for one hundred years of cinema’s history — "neutralizes" the narrative, forcing the viewer to focus on the phenomenon of the projected light itself, which is the actual subject of this project.

Along with Greenaway, Dziga Vertov can be thought of as a major "database filmmaker" of the twentieth century. His Man with a Movie Camera is perhaps the most important example of database imagination in modern media art. In one of the key shots repeated few times in the film we see an editing room with a number of shelves used to keep and organize the shot material. The shelves are marked "machines," "club," "the movement of a city," "physical exercise," "an illusionist," and so on. This is the database of the recorded material. The editor — Vertov's wife, Elizaveta Svilova — is shown working with this database: retrieving some reels, returning used reels, adding new ones.

Although I pointed out that film editing, in general, can be compared to creating a trajectory through a database, in the case of Man with a Movie Camera this comparison constitutes the very method of the film. Its subject is the filmmaker's struggle to reveal (social) structure among the multitude of observed phenomena. Its project is a brave attempt at an empirical epistemology which only has one tool — perception. The goal is to decode the world purely through the surfaces visible to the eye (of course, its natural sight enhanced by a movie camera). This is how the film's co-author Mikhail Kaufman describes it:

An ordinary person finds himself in some sort of environment, gets lost amidst the zillions of phenomena, and observes these phenomena from a bad vantage point. He registers one phenomenon very well, registers a second and a third, but has no idea of where they may lead... But the man with a movie camera is infused with the particular thought that he is actually seeing the world for other people. Do you understand? He joins these phenomena with others, from elsewhere, which may not even have been filmed by him. Like a kind of scholar, he is able to gather empirical observations in one place and then in another. And that is actually the way in which the world has come to be understood. [29]

Therefore, in contrast to standard film editing which consists of selection and ordering of previously shot material according to a pre-existent script, here the process of relating shots to each other, ordering and reordering them in order to discover the hidden order of the world constitutes the film's method. Man with a Movie Camera traverses its database in a particular order to construct an argument. Records drawn from a database and arranged in a particular order become a picture of modern life — but simultaneously an argument about this life, an interpretation of what these images, which we encounter every day, every second, actually mean. [30]

Was this brave attempt successful? The overall structure of the film is quite complex, and at the first glance has little to do with a database. Just as new media objects contain a hierarchy of levels (interface — content; operating system — application; web page — HTML code; high-level programming language — assembly language — machine language), Vertov's film consists of at least three levels. One level is the story of a cameraman filming material for the film. The second level is the shots of an audience watching the finished film in a movie theater. The third level is this film, which consists of footage recorded in Moscow, Kiev and Riga and is arranged according to a progression of one day: waking up — work — leisure activities. If this third level is a text, the other two can be thought of as its meta-texts. [31] Vertov goes back and forth between the three levels, shifting between the text and its meta-texts: between the production of the film, its reception, and the film itself. But if we focus on the film within the film (i.e., the level of the text) and disregard the special effects used to create many of the shots, we discover almost a linear printout, so to speak, of a database: a number of shots showing machines, followed by a number of shots showing work activities, followed by different shots of leisure, and so on. The paradigm is projected onto syntagm. The result is a banal, mechanical catalog of subjects which one can expect to find in the city of the 1920s: running trams, city beaches, movie theaters, factories... 

Of course, watching Man with a Movie Camera is anything but a banal experience. Even after the 1990s during which computer-based image and video-makers systematically exploited every avant-garde device, the original still looks striking. What makes its striking is not its subjects and the associations Vertov tries to establish between them to impose "the communist decoding of the world" but the most amazing catalog of the film techniques contained within it. Fades and superimpositions, freeze-frames, acceleration, split screens, various types of rhythm and intercutting — what film scholar Annette Michelson called "a summation of the resources and techniques of the silent cinema" [32] — and of course, a multitude of unusual, "constructivist" points of view are stringed together with such density that the film can't be simply labeled avant-garde. If a "normal" avant-garde film still proposes a coherent language different from the language of mainstream cinema, i.e. a small set of techniques which are repeated, Man with a Movie Camera never arrives at anything like a well-defined language. Rather, it proposes an untamed, and apparently endless unwinding of cinematic techniques, or, to use contemporary language, "effects," as cinema's new way of speaking. Why in the case of Witney's computer films and music videos are the effects just effects, while in the hands of Vertov they acquire meaning? Because in Vertov's film they are motivated by a particular argument, this being that the new techniques to obtain images and manipulate them, summed up by Vertov in his term "kino-eye," can be used to decode the world. As the film progresses, "straight" footage gives way to manipulated footage; newer techniques appear one after one, reaching a roller coaster intensity by the film's end, a true orgy of cinematography. It is as though Vertov re-stages his discovery of the kino-eye for us. Along with Vertov, we gradually realize the full range of possibilities offered by the camera. Vertov's goal is to seduce us into his way of seeing and thinking, to make us share his excitement, his gradual process of discovery of film's new language. This process of discovery is film's main narrative and it is told through a catalog of discoveries being made. Thus, in the hands of Vertov, a database, this normally static and "objective" form, becomes dynamic and subjective. More importantly, Vertov is able to achieve something which new media designers still have to learn — how to merge database and narrative merge into a new form.

## References:

[1] This article which later became the chapter in my The Language of New Media (The MIT Press, 2001) was written in the Fall of 1998. Rather than updating some details which have changed since that time — for instance, changing references to CD-ROM to DVD — I decided to leave the article as is. I think that the theoretical arguments advanced in this article still hold while particular historical details remain

[2] "database" Britannica Online. [http://www.eb.com:180/cgi-bin/g?DocF=micro/160/23.html](http://www.eb.com:180/cgi-bin/g?DocF=micro/160/23.html) [Accessed 27 November 1998].

[3] Jean-Francois Lyotard, The Postmodern Condition: A Report on Knowledge, trans. Geoff Bennington and Brian Massumi (Minneapolis: University of Minnesota Press, 1984), 3. 

[4] As early as 1985 Grolier, Inc. issued text-only "Academic American Encyclopedia" on CD-ROM. The first multimedia encyclopedia was "Compton's MultiMedia Encyclopedia" published in 1989. 

[5] David Bordwell and Kristin Thompson define motivation in cinema in the following way: "Because films are human constructs, we can expect that any one element in a film will have some justification for being there. This justification is the motivation for that element." Here are some examples of motivation: "When Tom jumps from the balloon to chase a cat, we motivate his action by appealing to notions of how dogs are likely to act when cats are around." "The movement of a character across a room may motivate the moving of the camera to follow the action and keep the character within a frame." David Bordwell and Kristin Thompson, Film Art: an Introduction. 5th Edition (New York: The McGraw-Hill Companies, Inc., 1997), 80. 

[6] Chris McGowan and Jim McCullaugh, Entertainment in the Cyber Zone (New York: Random House, 1995), 71.

[7] This is true for a procedural programming paradigm. In an object-oriented programming paradigm, represented by such computer languages as Java and C++, algorithms and data structures are modeled together as objects. 

[8] Mediamatic 8, no. 1 (Summer 1994), 1860. 

[9] [http://www.amazon.com/exec/obidos/subst/misc/company-info.html/](http://www.amazon.com/exec/obidos/subst/misc/company-info.html/), [http://www.oracle.com/database/oracle8i/](http://www.oracle.com/database/oracle8i/), accessed Nov. 28, 1998; 

[10] [http://artnetweb.com/guggenheim/mediascape/shaw.html](http://artnetweb.com/guggenheim/mediascape/shaw.html)

[11] Harwood, Rehearsal of Memory, CD-ROM (London: Artec and Bookworks, 1996.)

[12] [http://www.telepresence.com/MENAGERIE](http://www.telepresence.com/MENAGERIE), accessed October 22, 1998.

[13] [http://jefferson.village.virginia.edu/wax/](http://jefferson.village.virginia.edu/wax/), accessed September 12, 1998.

[14] [http://www.cs.msu.su/wwwart/](http://www.cs.msu.su/wwwart/), accessed October 22, 1998.

[15] Mieke Bal, Narratology: Introduction to the Theory of Narrative (Toronto: University of Toronto Press, 1985), 8. 

[16] The theory of markedness was first developed by linguists of the Prague School in relation to phonology but subsequently applied to all levels of linguistic analysis. For example, "bitch" is the marked term and "dog" is unmarked term. Whereas the "bitch" is used only in relation to females, "dog" is applicable to both males and females.

[17] Fredric Jameson, "Postmodernism and Consumer Society," in The Anti-Aesthetic. Essays on Postmodern Culture, ed. Hal Foster (Seattle: Bay Press, 1983), 123. 

[18] Roland Barthes, The Elements of Semiology (New York: Hill and Wang, 1968), 58. 

[19] Qtd. in ibid., 58.

[20] Christian Metz, "The Fiction Film and its Spectator: A Metapsychological Study," in Apparatus, edited by Theresa Hak Kyung Cha (New York: Tanam Press, 1980), p. 402.

[21] Rosalind Krauss, "Video: The Aesthetics of Narcissism," in John Hanhardt, ed., Video Culture (Rochester: Visual Studies Workshop, 1987), 184.

[22] Qtd. in Sam Hunter and John Jacobus, Modern Art: Painting, Sculpture and Architecture, 3rd ed. (New York: Abrams, 1992), 326. 

[23] Frank Dietrich, "Visual Intelligence: The First Decade of Computer Art (1965 — 1975)," IEEE Computer Graphics and Applications (July 1985), 39.

[24] Gene Youngblood, Expanded Cinema (New York: E.P. Dutton & Co,Inc., 1970), 210.

[25] Peter Greenaway, The Stairs — Munich — Projection 2 (London: Merrell Holberton Publishers, 1995), 21.

[26] Qtd. in David Pascoe, Peter Greenaway: Museums and Moving Images (London: Reaktion Books, 1997), 9-10.

[27] [http://www.tem-nanterre.com/greenaway-100objects/](http://www.tem-nanterre.com/greenaway-100objects/), accesed November 3, 1998. 

[28] Greenaway, The Stairs — Munich — Projection 2, 47-53.

[29] Mikhail Kaufman, "An Interview," October 11 (Winter 1979): 65.

[30] It can be used that Vertov uses "the Kuleshov's effect" to give the meaning to the database records by placing them in a particular order. 

[31] Linguistics, semiotics and philosophy uses the concept of metalanguage. Metalanguage is the language used for the analysis of object language. Thus, a metalanguage may be thought of as a language about another language. A metatext is a text in metalanguage about a text in object language. For instance, an article in a fashion magazine is a metatext about the text of cloves. Or, an HTML file is a metatext which describes the text of a Web page.

[32] Kaufman, "An Interview," 55.

---

# The Camera and the World

_author: Lev Manovich_
_year: 1998_

Tamás Waliczky is among the few artists who have been working with and thinking about the computer for many years, long before it became fashionable — and this depth of involvement can be clearly seen in his works. In the new pieces — "Landscape," "Sculptures" and "Focus" — the strategies which were already central to "The Garden" (1992), "The Forest" (1993) and "The Way" (1994) are further developed and the new ones are being deployed, yet, taken together, these six works look like different experiments undertaken within a single research paradigm. That is to say, all of Waliczky's works are the result of a single aesthetic investigation systematically being pursued by the artist.

Computer forces us to re-invent every one of the traditional aesthetic concepts, forms and techniques. What used to be a well-mapped territory now became one big white spot. Image and viewer, narrative and montage, illusion and representation, space and time — everything needs to be re-defined again. In his works, Waliczky systematically maps out an important part of the new post-computer aesthetic space. It is the part where new ways to structure the world and new ways to see it meet. The interactions between the virtual camera and the virtual world — this is the main subject of Waliczky's aesthetic research. Waliczky thus is neither a virtual filmmaker who works only with images nor a virtual architect who works only with space. Rather, he can be described as a maker of virtual documentaries. In every one of his works, he creates a world structured in a unique way; and then he documents it for us. In "Landscape," it is the world where the time was frozen. In "Sculptures," it is the world consisting of three-dimensional time-sculptures. In "Focus," it is the world whose ontology was derived from the basic quality of a digital image — its organization as a number of layers.

In his concern with ordering every world according to its principle, Waliczky can be also compared to ancient cosmologists. Each of his worlds establishes a cosmology of its own, a unique logical system which governs all of the world's elements. For instance, if in "The Forest" the world is like a mechanical clock or a system of planets, with all the elements continuously moving according to a complex set of rules, in "Focus" the world is immobile, the spatial relationships between all the elements being fixed once and for all. Therefore, although all of Waliczky's works are concerned with the same aesthetic problem, they are also fundamentally different from each other, because each world is organized according to its own unique principle.

One of the trajectories in computerization of culture involves gradual translation of elements and techniques of cinematic perception and language into a decontextualized set of tools to be used as an interface to any data. For instance, in the last decade, the camera model derived from cinema became as much of an interface convention as scrollable windows or cut and paste function. It became an accepted way for interacting with any data which is represented in three dimensions — which, in a computer culture, means literally anything and everything: the results of a physical simulation, an architectural site, design of a new molecule, financial data, the structure of a computer network and so on. As computer culture is gradually spatializing all representations and experiences, they become subjected to the camera's particular grammar of data access: zoom, tilt, pan and track.

In the process of this translation, cinematic perception is divorced from its original material embodiment (camera, film stock), as well as from the historical contexts of its formation. If in cinema the camera functioned as a material object, co-existing, spatially and temporally, with the world it was showing us, it has now become a set of abstract operations. Waliczky's works refuse this separation of cinematic vision from the material world. They reunite perception and material reality by treating the camera and the world as parts of a single system.

In Waliczky's earlier films, rather than simply subjecting the virtual worlds to different types of perspectival projection, the artist modified the spatial structure of the worlds themselves. In "The Garden," a child playing in a garden becomes the center of the world; as he moves around, the actual geometry of all the objects around him is transformed, with objects getting bigger as he gets close to him. To create "The Forest," a number of cylinders were placed inside each other, each cylinder mapped with a picture of a tree, repeated a number of times. In the film, we see a camera moving through this endless static forest in a complex spatial trajectory — but this is an illusion. In reality, the camera does move, but the architecture of the world is constantly changing as well, because each cylinder is rotating at its own speed. As a result, the world and its perception are fused together.

In each of the new works, the camera and the world similarly function as parts of a single gestalt, creating an effect which is larger than the sum of the individual parts. And even more than before, Walitczky's virtual camera operating not only as a tool of perception but also as a tool of epistemology, putting its author within a modern artistic tradition which includes such filmmakers as Sergei Eisenstein and Dziga Vertov. In fact, without the operations of the camera, the structure of the world would remain hidden from us. Thus, Walitczky's cosmologies are distinctly post-cinematic: their structure can only be revealed by actions of a virtual camera. In "Landscape," without camera's movement we would not be able to discover that when time is stopped, the result is not simply an interruption in the familiar structure of our world but a creation of a new one, distinctly different. In "Sculptures," the camera passes through time-sculptures at different speeds and angles, revealing new time and space relationships which otherwise would remain invisible. And finally, in "Focus," we ourselves are handed over camera's controls (focus and depth of field) to uncover the space whose topology corresponds to a network of human relations. In Walitczky's aesthetic universe, the camera and the world can't exist without each other, and their interactions always result in new and surprising discoveries.

---

# Cinema by Numbers

_author: Lev Manovich_
_year: 1999_

If the history of analog cinema officially begins in 1895 with the Lumières, the history of digital cinema, which yet is to be written, can start in the late 1930s with German Zuse. Starting in 1936, and continuing into the Second World War, German engineer Konrad Zuse had been building a computer in the living room of his parents' apartment in Berlin. Zuse's machine was the first working digital computer. One of his innovations was program control by punched tape. For the tape, Zuse used discarded 35mm movie film.

One of these surviving pieces of film shows binary code punched over the original frames of an interior shot. A typical movie scene — two people in a room involved in some action — becomes a support for a set of computer commands. Whatever meaning and emotion contained in this movie scene are wiped out by this new function as data carrier. The pretense of modern media to create simulation of sensible reality is similarly canceled: media is reduced to its original condition as information carrier, nothing else, nothing more. In a technological remake of the Oedipal complex, a son murders his father. The iconic code of cinema is discarded in favor of the more efficient binary one. Cinema becomes a slave to a computer.

But this is not yet the end of the story. Our story has a new twist — a happy one. Zuse's film with its strange superimposition of the binary over iconic anticipates the convergence that gets underway half a century later. Media and computer — Daguerre's daguerreotype and Babbage's Analytical Engine, the Lumière Cinématographie, and Hollerith's tabulator— merge into one. All existing media are translated into numerical data accessible to the computer. The result: graphics, moving images, sounds, shapes, spaces, and text become computable, that is, simply another set of computer data. In short, media becomes new media.

This meeting changes both the identity of media and of computer itself. No longer just a calculator, a control mechanism, or a communication device, a computer becomes a media processor and synthesizer. If before, a computer would read in a row of numbers and output a statistical result or a projectile's trajectory, now it can read in pixel values, blurring the image, adjusting its contrast, or checking whether it contains an outline of a gun. Building upon these lower-level operations, it can also perform more ambitious ones: searching image databases for images similar in composition or content to an input image; detecting shot changes in a movie; or synthesizing the movie shot itself, complete with setting and the actors.

The identity of media has been changed even more dramatically. For instance, old media involved a human creator who manually assembled textual, visual, or audio elements (or their combination) into a particular sequence. This sequence was stored in or on some material, its order determined once and for all. Numerous copies could be run off from the master, and, in perfect correspondence with the logic of an industrial society, they were all identical. New media, in contrast, is characterized by automation and variability. Many operations involved in media creation and manipulation are automated, thus removing human intentionality from the creative process, at least in part. For instance, many websites automatically generate pages from databases when the user reaches them; in Hollywood films, flocks of birds, ant colonies, and even crowds of people are automatically created by AL (artificial life) programs; word processing, page layout, and presentation software comes with "wizards" and "agents" that offer to automatically create the layout of a document; 3D software automatically renders photorealistic images given the scene description.

New media is also essentially variable (other terms that can be used to describe this quality might be "mutable" or "liquid"). [1] Stored digitally, rather than in some permanent material, media elements maintain their separate identity and can be assembled into numerous sequences under program control. At the same time, because the elements themselves are broken into discrete samples (for instance, an image is represented as an array of pixels), they can be also created and customized on the fly.

The logic of new media thus corresponds to the postindustrial logic of "production on demand" and "just-in-time" delivery, which themselves were made possible by the use of digital computers and computer networks in all stages of manufacturing and distribution. In this regard, the "culture industry" is actually ahead of the rest of the industry. The idea that a customer determines the exact features of her car at the showroom, the data is transmitted to the factory, and that hours later is delivered the new car remains a dream; but in the case of computer media, it is already a reality. Since the same machine (i.e. a computer) is used as a showroom and a factory, and since the media exist not as a material object but as data that can be sent through the wires at the speed of light, the response is immediate.

This is the new logic of new media, or at least some of its axioms; but how does this logic manifests itself on the level of language? In other words, given the new structure of media on the material level (discrete character on different levels; distributed — that is, network-based — representation), and the new kind of operations we can perform on it (copy and paste, sampling, digital compositing, image processing, and other algorithmic actions), do we create different-looking images? In particular, since filmmakers can now compose feature films entirely on a computer, do they make radically new kinds of films?

The answer to these questions so far have been definitely mixed. In the case of a moving image, the introduction of, first, electronic and, later, computer tools in video postproduction throughout the 1980s and the 1990s has led to the emergence of a new visual language of television: multilayered space, 2D combined with 3D, transparent planes, dynamic typography. So if you compare the look of television in the 1990s with that of the 1970s, the difference is dramatic. In the case of feature films, however, filmmakers are using basically the same technology as their TV counterparts — but the result is a much more traditional film language. 3D animation, digital compositing, mapping, paint retouching: in commercial cinema, these radical new techniques are mostly used to solve technical problems, while the old cinematic language is preserved unchanged. Frames are hand-painted to remove the wires that supported an actor during a shoot; a flock of birds is added to a landscape; a city street is filled with crowds of simulated extras. Although most Hollywood releases now involve digitally manipulated scenes, the use of computers is always carefully hidden. Commercial narrative cinema still continues to hold on to the classical realist style in which images function as unretouched photographic records of some events that took place in front of the camera.

How to make sense of this mixed evidence? If, historically, each cultural period (Renaissance, Baroque, and so on) brought with it a new expressive language, why is the computer age often satisfied with using the language of the previous period, in other words, that of the industrial age? The answer to this question is important because usually a new cultural language and new social-economic regime go together. Normally this thesis, especially beloved by Marxist critics, is used to move from the economic to the cultural, that is, a critic tries to see how a new economic order finds its reflection in culture. But we can also move in the opposite direction, from culture to economy. In other words, we can interpret radical shifts in culture as indicators of the changes in economic-social structure. From this perspective, if the new information age did not bring with it a revolution in aesthetic forms, perhaps this is because it has not come yet? Despite the pronouncements about the new net economy by Wired magazine, we may be still living in the same economic period that gave rise to "Human Comedy" and "Gone With the Wind". Net.capitalism is still capitalism. Cultural forms that were good enough for the age of the engine turned out to be also good for the age of the "geometry engine" and the "emotion engine". ("Geometry engine" is the name of a computer chip introduced in Silicon Graphics workstations a number of years ago perform real-time 3D graphics calculations; "emotion engine" is the name of the processor to be used in the forthcoming Playstation 2; it will allow real-time rendering of facial expressions). In short, as far as its cultural languages are concerned, new media is still old media.

When radically new cultural forms appropriate for the age of wireless telecommunication, multitasking operating systems, and information appliances will arrive, what will they look like? How would we even know they are here? Would future films look like a "data shower" from the movie "Matrix"? Does the famous fountain at Xerox PARC in which the strength of the water stream reflects the behavior of the stock market, with stock data arriving in real time over the Internet, represent the future of public sculpture?

We don't yet know the answers to these questions. However, what we as artists and critics can do now is point out the radically new nature of media by staging — as opposed to hiding — its new properties. And this is exactly what Vuk Cosic's ASCII films accomplish so well [http://www.vuk.org/ascii](http://www.vuk.org/ascii).

It is worthwhile to relate Cosic's fims to both Zuse's "found footage movies" from the 1930s and to the first all-digital commercial movie made sixty years later — Lucas's "Stars Wars: Episode 1, The Phantom Menace." Zuse superimposes digital code over the film images. Lucas follows the opposite logic: in his film, digital code lies under his images. That is, given that most images in the film were put together on computer workstations, during the postproduction process they were pure digital data. The frames were made up from numbers rather than bodies, faces, and landscapes. The Phantom Menace is, therefore, the first feature-length commercial abstract film: two hours worth of frames made up from matrix of numbers. But this is hidden from the audience.

What Lucas hides, Cosic reveals. His ASCII films "perform" the new status of media as digital data. The ASCII code that results when an image is digitized is displayed on the screen. The result is as satisfying poetically as it is conceptually — for what we get is a double image, a recognizable film image, and an abstract code together. Both are visible at once. Thus, rather than erasing the image in favor of the code as in Zuse's film, or hiding the code from us as in Lucas's film, here the code and the image coexist.

Like my own "little movies" series of Net films (1994 — present; [http://jupiter.ucsd.edu/~manovich/little-movies](http://jupiter.ucsd.edu/~manovich/little-movies)), Cosic uses well-known films as his material for "ASCII history of moving images." Both projects also rely on the same strategy of defamiliarizing ("otstranenie") familiar lens-based images through algorithmic operations. In my "Classic Cinema 1," I reduce a scene from Hitchcock's "Psycho" to a Mondriaan-like abstraction by applying standard "mosaic" filter in Adobe's "Premiere" video-editing software; in Cosic's "ASCII history," the scenes from classic films are running through a custom player application that converts moving images into an ASCII code [http://www.vuk.org/ascii/film](http://www.vuk.org/ascii/film). The result something looks as though it were weaved. These are the kind of movies which J. M. Jacquard could have produced on his programmable loom, which he invented around 1800 and which inspired Charles Babbage in his work on the Analytical Engine.

Like VinylVideo by Gebhard Sengmüller [http://www.onlineloop.com/pub/VinulVideo](http://www.onlineloop.com/pub/VinulVideo), Cosic's ASCII initiative [http://www.vuk.org/ascii/aae.html](http://www.vuk.org/ascii/aae.html) is a systematic program of translating media content from one obsolete format into another. These projects remind us that since at least the 1960s the operation of media translation has been at the core of our culture. Films transferred to video; video transferred from one video format to another; video transferred to digital data; digital data transferred from one format to another: from floppy disks to Jaz drives, from CD-ROMs to DVDs; and so on, indefinitely. The artists were first to notice this new functioning of culture: in the 1960s, Roy Lichtenstein and Andy Warhol already made media translation the basis of their art. Sengmuller and Cosic understand that the only way to fight media obsolescence is by resurrecting dead media. Sengmuller translates old TV programs into vinyl disks; Cosic translates old films into ASCII images. [2] 

Why do I call ASCII images an obsolete media format? Before the printers capable of outputting raster digital images became widely available toward the end of the 1980s, it was commonplace to make printouts of images on dot matrix printers by converting the images into ASCII code. I was surprised that in 1999 I still was able to find the appropriate program on my UNIX system. Called simply "toascii," the command, according to the UNIX system manual page for the program, "prints textual characters that represent the black and white image used as input."

The reference to the early days of computing is not unique to Cosic but shared by other net.artists. Jodi.org, the famous website, often evokes DOS commands and the characteristic green color of computer terminals from the 1980s [http://www.jodi.org](http://www.jodi.org); Alexei Shulgin, who collaborated with Cosic on "ASCII Music Videos" project, has performed music using old 386PC [http://www.easylife.org/386dx](http://www.easylife.org/386dx). But in the case of ASCII code, its use evokes not only a peculiar episode in the history of computer culture but a number of earlier forms of media and communication technologies as well.

ASCII is an abbreviation of the American Standard Code for Information Interchange. The code was originally developed for teleprinters and was only later adopted for computers in the 1960s. A teleprinter was a twentieth-century telegraph system that translated the input from a typewriter keyboard into a series of coded electric impulses, which were then sent transmitted over communications lines to a receiving system, which decoded the pulses and printed the message onto a paper tape or other medium. Teleprinters were introduced in the 1920s and were widely used until the 1980s (Telex being the most popular system), when they were gradually replaced by fax and computer networks. [3]

ASCII code was itself an extension of an earlier code invented by Jean-Maurice-Emile Baudot in 1874. In Baudot code, each letter of an alphabet is represented by a five-unit combination of current-on or current-off signals of equal duration. ASCII code extends Baudot code by using eight-unit combinations (that is, eight "bits" or one "byte") to represent 256 different symbols. Baudot code itself was an improvement over the Morse code invented for early electric telegraph systems in the 1830s. And so on.

The history of ASCII code compresses a number of technological and conceptual developments which lead to (but I am sure will not stop at) modern digital computers: cryptography, real-time communication, communication networks... By juxtaposing this code with the history of cinema, Cosic accomplishes what can be called an artistic compression: he brings together many key issues of computer culture and new media art together in one rich and elegant project.

## References:

 [1] Jon Ippolito discusses the notion of "variable media" in "The Museum of the Future: a Contradiction in Terms?" ArtByte 1 (no. 2, June-July 1988), 18-19. My usage of the term "variable" is similar to his, although I see variability as a fundamental condition of all computer media, rather than as something that applies only to digital art. 

[2] See also Bruce Sterling's Dead Media Project [http://www.eff.bilkent.edu.tr/pub/Net\_culture/Folklore/Dead\_Media\_Project/](http://www.eff.bilkent.edu.tr/pub/Net_culture/Folklore/Dead_Media_Project/).

[3] "teleprinter" Encyclopaedia Britannica Online [http://www.eb.com:180/bol/topic?thes\_id=378047](http://www.eb.com:180/bol/topic?thes_id=378047) Accessed May 27, 1999.

## Links:

[http://www.vuk.org/ascii](http://www.vuk.org/ascii)

[http://www.vuk.org/ascii/film](http://www.vuk.org/ascii/film)

[http://www.vuk.org/ascii/aae.html](http://www.vuk.org/ascii/aae.html)

[http://www.jupiter.ucsd.edu/~manovich/media\_db/zuse-film.jpg](http://www.jupiter.ucsd.edu/~manovich/media_db/zuse-film.jpg)

[http://www.jupiter.ucsd.edu/~manovich/little-movies](http://www.jupiter.ucsd.edu/~manovich/little-movies)

[http://www.onlineloop.com/pub/VinulVideo](http://www.onlineloop.com/pub/VinulVideo)

[http://www.starwars.com](http://www.starwars.com)

[http://www.jupiter.ucsd.edu/~manovich/star\_wars.html](http://www.jupiter.ucsd.edu/~manovich/star_wars.html)

[http://www.jodi.org](http://www.jodi.org)

[http://www.easylife.org/386d](http://www.easylife.org/386d)

---
# Review of Stars Wars: Episode 1

_author: Lev Manovich_
_year: 1999_

You would think that San Rafael, California would be the ideal place to watch the premiere of new Lucasfilm extravaganza, the first truly all-digital feature (%95 of all shots in the movie were computer generated or assembled), two years in pre-production and four years in post-production — the movie which displays the NATO unmatched rendering power and which for sure will get Ars Electronica top prize — Stars Wars: Episode 1 The Phantom Menace. 

It is there, at Industrial Light & Magic located in San Rafael that most of the movie was rendered; and indeed, I can see some traces of Northern California in the film. The humans wear tasteful all-natural cotton cloves; the older Jedi Knight looks like a CEO of some hot Internet start-up in the Silicon Valley who writes books about social dangers of computing on the side; Skywalker's mother features minimal make-up and understated but dignified manner, a Northern California type you see frequently in San Francisco expensive restaurants.

However, San Diego is a perfect location to see Stars Wars: Episode 1 as well, especially now. Let me explain. San Diego has the largest concentration of Airforce and Navy bases in the U.S. and until recently was known largely as a military town; it is thus a key place in the Federation's power grid. When I drive between San Diego and Los Angeles, I pass a long stretch of the base. Quite often one can see a few military helicopters in training, flying back and forth, sometimes quite close to the highway. The wall surrounding the base features a line in big letters written by an unknown Navy poet: "No Beach is Out of Reach". 

I am stationed not on the base but in the Art Department of the University of California, San Diego. When I first got here three years ago I went into the campus bookstore and asked if they have the best in Star Wars criticism — Paul Virilio's "War and Cinema". I was told that they had two copies in stock for a year but none was sold so they send it back to the distributor. The center of the bookstore meanwhile was occupied by numerous Java and C++ textbooks; it seems there were hundreds of different ones. The campus itself originally was a military base; a few years ago one of my colleagues still had his studio in a former military building which is still found here and there throughout the campus.

In the 1960s University of California, San Diego was one of hotbeds of student movement. Herbert Marcuse, one of the ideologists of this movement, was teaching here and Angela Davis (now herself a professor at the University of California, Santa Cruz) was one of the students. (When I was a pupil in the school in Moscow in the early 1970s I once had to draw a big poster which said "Free Angela Davies!" Later we were all marching under a cloudy Communist sky, as Star Destroyers were flying above us ...this was long, long ago, in the galaxy far away which later was liberated by the Federation.) The University of California did not know how to get rid of Marcuse; so they changed the age of mandatory retirement in the University system, and Marcuse had to retire. Today very few traces of all this are left. The students worry about the final exams, not the war in Yugoslavia. Yes, and they do worry about Star Wars.

May 1999. All across the USA long lines of young Star Wars fans lined up to catch the first show of the new (or rather, old, as it takes place before the original Star Wars trilogy) movie. They camped out before the movie theatres, sleeping in lawn chairs. Some were dressed up in self-made costumes of Star Wars characters: Luke Skywalker, Princess Lei and so on. (See [http://www.starwars.com](http://www.starwars.com) for details).

The US newspapers dropped or reduce the stories about the war in Yugoslavia in order to cover the much anticipated premiere. The attention of the whole nation was focused on the set of pixels ready to flicker on a movie screen for a couple of hours. The pixels simulating grass, the sky, metal and skin, arms and legs, humans and non-humans. The pixels were crafted by thousands of people during six years of movie's production. The pixels which make up Stars Wars: Episode 1 The Phantom Menace.

I waited a couple of days for the passion to die out and for the lines of fans to disappear, then walked to the box office and asked if I can buy the ticket. Surprisingly, it worked. Inside, I bought a Big Coke; found an empty seat; placed the Coke into the special round opening on the left of the seat, and prepared to be carried away by the Force.

The Hollywood industry is structured around the collective and corporate authorship and decisions by committee; the focus groups and marketing pie charts rule over the Romantic genius. Therefore it produces films which are characterized by a bricolage, post-modern, or, to use more contemporary language, plug-in structure. As noted by Jay David Bolter, in order to appeal to different market segments a single movie combines a number of genres and styles. Like Eisenstein's montage of attractions, a contemporary Hollywood product fires a sequence of unrelated stimuli into its audience, designed to hit whoever happens to be in the dark. A chase scene; a 70s reference; a love story sub-plot; a character borrowed from last year's hit; an early 80s reference; and so on. In short, Hollywood's strategy is blanket bombing, not laser-guided missiles. 

Star Wars: Episode 1 is no exception to plug-in architecture of Hollywood movies, although its segments seem to hold better than in a typical Hollywood product. Still, it is less a coherent building than a set of Photoshop filters. Despite the perfect digital composing, the human characters seem to exist in their world, separate from fully digital sets. The race on the Tatooine where young Skywalker first shows his stuff forms a self-contained mini-movie of its own. The computer-generated creatures add the comic gigs. Lovers of desert landscapes get the sands of Tatooine; the northerners can enjoy the forest of Naboo; while the dwellers of New York and Tokyo can enjoy the super density of Coruscant, this ultimate metropolis which tops whatever Rem Koolhaas can ever imagine.

What I saw was of course wonderfully crafted. It was truly epic both in its scale and the attention to detail. Indeed if our civilization has any equivalent to Medieval cathedrals, it is special effects Hollywood films. Assembled by thousands of highly skilled craftsmen over the course of years, each such movie is the ultimate display of collective craftsmanship we have today. But if Medieval masters left after themselves the material wonders of stone and glass inspired by religious faith, today our craftsmen leave just the pixel sets to be projected on movie theatre screens. A kind of immaterial cathedral made of light, with noise of film stock mixed in together with human labor during the movie projection. The religious references are still present, both in the story (for instance, Skywalker was conceived without a father) and in the virtual sets.

The virtual sets of Stars Wars: Episode 1 are splendid in their glory although sometimes quite vulgar. Endless waterfalls are stuck in too many shots, the particle systems obeying the masters of the Skywalker Ranch. Of course, we know that the law of digital aesthetics is "copy and paste" and that once you rendered a perfect waterfall, you are tempted to use it over and over. Of course, it is the same waterfalls which, scaled down, you will find in the courtyards of corporate buildings throughout California. As Fred Turner pointed out to me, in the civilization built in the desert (i.e., California), display of water signifies power and wealth. This explains the waterfalls of Star Wars' sets. These sets are ultimate in corporate campuses planning. You can imagine Coruscant housing some future Microsoft / Disney / Getty conglomerate or other mega-corporation. 

The overall visual aesthetics of the movie is a comic book painted by Veronese or Titian. The virtual architecture is rich and self-assured; in contrast, the cinematography is quite modest, even understated. Indeed, if you spend many months building a virtual set, and if, given even the massive computing power of NATO's rendering farm, it still takes a few hours to render every single frame, you want to show the result in all its ray-traced glory, without messing it up by shadows or camera moves.

Thus many shots of the movie look as though they came from some 3D computer animation textbook or from SIGGRAPH exhibition floor. A shining ship with a reflection map composed over a live plate of a landscape. Another live plate with thousands of Battle Droid — the same 3D object cloned over and over. And so on.

Many shots of the movie also reminded me of the kind of animations which were dreamed about by my undergraduate students a few years ago when I was teaching 3D computer animation in another University before being sent to San Diego. This was in the days before Alias and Wavefront merged together and before Softimage was bought by Microsoft. None of the students ever finished their animations because they did not have enough rendering time. But George Lucas has enough workstations to render any of his fantasies. The best force in all of the Federation. So now these boys can go to the movies to see the ultimate student 3D animation of all times.

If you take away the humans and the plot, what you are left with, on some basic level, is a pure display of computational resources. In a nutshell, Stars Wars: Episode 1 is a shameless advertisement for NATO, a showcase for Western technology. Millions of polygons and millions of particles making every frame. And every frame is dense with detail whose only motivation seems to be to show off human and computer labor which went into its making. The armies of modelers, animators, technical directors, programmers, and plain "paint monkeys" (the industry name for the low-level artists employed by special effects houses) being translated into the endless rows and columns of vehicles and architectural details filing every shot. The skies were dark from the vehicles crossing them back and forth. The endless fields of Battle Droids are shown from every angle; the endless flocks of various vehicles flying over Coruscant.

In all its rendered glory and with all its shots featuring the endless armies of Droids, the automated soldiers of the future, Stars Wars: Episode 1 is the ultimate military parade. It reminds me of the parade which took every year in Moscow's Red Square when I was growing up when one Empire was displaying its force for another on pre-arranged dates. Today only one of these Empires is left. And it is now putting on its own parade, both on the fields of Europe and on the movie screens around the world. Let's hope that Anakin Skywalker and his friends are on our side. 

---

# New Media: A User’s Guide

_author: Lev Manovich_
_year: 1999_

## How Media Became New

On August 19, 1839, the Palace of the Institute in Paris was completely full of curious Parisians who came to hear the formal description of the new reproduction process invented by Louis Daguerre. Daguerre, already well-known for his Diorama, called the new process daguerreotype. [1] According to a contemporary, "a few days later, opticians' shops were crowded with amateurs panting for daguerreotype apparatus, and everywhere cameras were trained on buildings. Everyone wanted to record the view from his window, and he was lucky who at first trial got a silhouette of rooftops against the sky." [2] The media frenzy has begun. Within five months more than thirty different descriptions of the techniques were published all around the world: Barcelona, Edinburg, Halle, Naples, Philadelphia, Saint Petersburg, Stockholm. At first, daguerreotypes of architecture and landscapes dominated the public's imagination; two years later, after various technical improvements to the process, portrait galleries were opened everywhere — and everybody rushed in to have their picture taken by a new media machine. [3]

In 1833 Charles Babbage started the design for a device he called the Analytical Engine. The Engine contained most of the key features of the modern digital computer. The punch cards were used to enter both data and instructions. This information was stored in the Engine's memory. A processing unit, which Babbage referred to as a "mill," performed operations on the data and wrote the results to memory; the final results were to be printed out on a printer. The Engine was designed to be capable of doing any mathematical operation; not only would it follow the program fed into it by cards, but it would also decide which instructions to execute next, based upon intermediate results. However, in contrast to the daguerreotype, not even a single copy of the Engine was completed. So while the invention of this modern media tool for the reproduction of reality impacted society right away, the impact of the computer was yet to be measured.

Interestingly, Babbage borrowed the idea of using punch cards to store information from an earlier programmed machine. Around 1800, J.M. Jacquard invented a loom which was automatically controlled by punched paper cards. The loom was used to weave intricate figurative images, including Jacquard's portrait. This specialized graphics computer, so to speak, inspired Babbage in his work on the Analytical Engine, a general computer for numerical calculations. As Ada Augusta, Babbage's supporter and the first computer programmer, put it, "the Analytical Engine weaves algebraical patterns just as the Jacquard loom weaves flowers and leaves." [4] Thus, a programmed machine was already synthesizing images even before it was put to process numbers. The connection between the Jacquard loom and the Analytical Engine is not something historians of computers make much of, since for the image synthesis and manipulation represent just one application of the modern digital computer among thousands of others; but for a historian of new media it is full of significance. 

We should not be surprised that both trajectories — the development of modern media, and the development of computers — begin around the same time. Both media machines and computing machines were absolutely necessary for the functioning of modern mass societies. The ability to disseminate the same texts, images, and sounds to millions of citizens thus assuring that they will have the same ideological beliefs was as essential as the ability to keep track of their birth records, employment records, medical records, and police records. Photography, film, the offset printing press, radio, and television made the former possible while computers made possible the latter. Mass media and data processing are the complementary technologies of a mass society.

For a long time the two trajectories developed in parallel without ever crossing paths. Throughout the nineteenth and the early twentieth century, numerous mechanical and electrical tabulators and calculators were developed; they were gradually getting faster and their use became more widespread. In parallel, we witness the rise of modern media which allows the storage of images, image sequences, sounds, and text in different material forms: a photographic plate, film stock, a gramophone record, etc.

Let us continue tracing this joint history. In the 1890s modern media took another step forward as still photographs were put in motion. In January of 1893, the first movie studio — Edison's "Black Maria" — started producing twenty seconds shorts which were shown in special Kinetoscope parlors. Two years later the Lumière brothers showed their new Cinématographie camera/projection hybrid first to a scientific audience, and, later, in December of 1895, to the paying public. Within a year, the audiences in Johannesburg, Bombay, Rio de Janeiro, Melbourne, Mexico City, and Osaka were subjected to the new media machine, and they found it irresistible. [5] Gradually the scenes grew longer, the staging of reality before the camera and the subsequent editing of its samples became more intricate, and the copies multiplied. They would be sent to Chicago and Calcutta, to London and St. Petersburg, to Tokyo and Berlin and thousands and thousands of smaller places. Film images would soothe movie audiences, who were too eager to escape the reality outside, the reality which no longer could be adequately handled by their own sampling and data processing systems (i.e., their brains). Periodic trips into the dark relaxation chambers of movie theatres became a routine survival technique for the subjects of modern society.

The 1890s was the crucial decade, not only for the development of media, but also for computing. If individuals' brains were overwhelmed by the amounts of information they had to process, the same was true of corporations and of government. In 1887, the U.S. Census office was still interpreting the figures from the 1880 census. For the next 1890 census, the Census Office adopted electric tabulating machines designed by Herman Hollerith. The data collected for every person was punched into cards; 46, 804 enumerators completed forms for a total population of 62,979,766. The Hollerith tabulator opened the door for the adoption of calculating machines by businesses; during the next decade, electric tabulators became standard equipment in insurance companies, public utilities companies, railroads and accounting departments. In 1911, Hollerith's Tabulating Machine Company was merged with three other companies to form the Computing-Tabulating-Recording Company; in 1914 Thomas J. Watson was chosen as its head. Ten years later its business tripled and Watson renamed the company the International Business Machines Corporation, or IBM. [6]

We are now in the new century. The year is 1936. This year the British mathematician Alan Turing wrote a seminal paper entitled "On Computable Numbers". In it, he provided a theoretical description of a general-purpose computer later named after its inventor the Universal Turing Machine. Even though it was only capable of four operations, the machine could perform any calculation which can be done by a human and could also imitate any other computing machine. The machine operated by reading and writing numbers on an endless tape. At every step the tape would be advanced to retrieve the next command, to read the data, or to write the result. Its diagram looks suspiciously like a film projector. Is this a coincidence?

If we believe the word cinematograph, which means "writing movement," the essence of cinema is recording and storing visible data in a material form. A film camera records data on film; a film projector reads it off. This cinematic apparatus is similar to a computer in one key respect: a computer's program and data also have to be stored in some medium. This is why the Universal Turing Machine looks like a film projector. It is a kind of film camera and film projector at once: reading instructions and data stored on endless tape and writing them in other locations on this tape. In fact, the development of a suitable storage medium and a method for coding data represent important parts of both cinema and computer pre-histories. As we know, the inventors of cinema eventually settled on using discrete images recorded on a strip of celluloid; the inventors of a computer — which needed a much greater speed of access as well as the ability to quickly read and write data — came to store it electronically in a binary code.

In the same year, 1936, the two trajectories came even closer together. Starting this year, and continuing into the Second World War, German engineer Konrad Zuse had been building a computer in the living room of his parents' apartment in Berlin. Zuse's computer was the first working digital computer. One of his innovations was program control by punched tape. The tape Zuse used was actually discarded 35 mm movie film. [7]

One of these surviving pieces of this film shows binary code punched over the original frames of an interior shot. In a typical movie scene — two people in a room involved in some action — becomes a support for a set of computer commands. Whatever meaning and emotion was contained in this movie scene has been wiped out by its new function as a data carrier. The pretense of modern media to create simulation of sensible reality is similarly canceled; media is reduced to its original condition as information carrier, nothing else, nothing more. In a technological remake of the Oedipal complex, a son murders his father. The iconic code of cinema is discarded in favor of the more efficient binary one. Cinema becomes a slave to the computer.

But this is not yet the end of the story. Our story has a new twist — a happy one. Zuse's film, with its strange superimposition of the binary code over the iconic code anticipates the convergence which gets underway half a century later. The two separate historical trajectories finally meet. Media and computer — Daguerre's daguerreotype and Babbage's Analytical Engine, the Lumière Cinématographie and Hollerith's tabulator — merge into one. All existing media are translated into numerical data accessible for the computers. The result: graphics, moving images, sounds, shapes, spaces and text become computable, i.e. simply another set of computer data. In short, media becomes new media.

This meeting changes both the identity of media and of the computer itself. No longer just a calculator, a control mechanism or a communication device, a computer becomes a media processor. Before the computer could read a row of numbers outputting a statistical result or a gun trajectory. Now it can read pixel values, blurring the image, adjusting its contrast, or checking whether it contains an outline of an object. Building upon these lower-level operations, it can also perform more ambitious ones: searching image databases for images similar in composition or content to an input image; detecting shot changes in a movie; or synthesizing the movie shot itself, complete with setting and the actors. In a historical loop, a computer returned to its origins. No longer just an Analytical Engine, suitable only to crunch numbers, the computer became Jacquard's loom — a media synthesizer and manipulator. 

## Principles of New Media

The identity of media has changed even more dramatically. In the following, I tried to summarize some of the key differences between old and new media. In compiling this list of differences I tried to arrange them in a logical order. That is, the principles 3 and 4 are dependent on the principles 1 and 2. This is not dissimilar to axiomatic logic where certain axioms are taken as starting points and further theorems are proved on their basis.

### 1. Discrete representation on different scales

This principle can be called "fractal structure of new media." Just as a fractal has the same structure on different scales, a new media object has the same discrete structure throughout. Media elements, be it images, sounds, or shapes, are represented as collections of discrete samples (pixels, polygons, voxels, characters). These elements are assembled into larger-scale objects but they continue to maintain their separate identity. Finally, the objects themselves can be combined into even larger objects — again, without losing their independence. For example, a multimedia "movie" authored in popular Macromedia Director software may consist from hundreds of images, QuickTime movies, buttons, text elements which are all stored separately and are loaded at run time. These "movies" can be assembled into a larger "movie," and so on.

We can also call this "modularity principle" using the analogy with structured computer programming. Structural computer programming involves writing small and self-sufficient modules (called in different computer languages routines, functions, or procedures) which are assembled into larger programs. Many new media objects are in fact computer programs which follow structural programming style. For example, an interactive multimedia application is typically programmed in Macromedia Director’s Lingo language. However, in the case of new media objects which are not computer programs, an analogy with structural programming still can be made because their parts can be accessed, modified, or substituted without affecting the overall structure.

### 2. Numerical representation  

Consequences:

2.1. Media can be described formally (mathematically). For instance, an image or a shape can be described using a mathematical function. 

2.2. Media becomes a subject to algorithmic manipulation. For instance, by applying appropriate algorithms, we can automatically remove "noise" from a photograph, alter its contrast, locate the edges of shapes, and so on. 

### 3. Automation

Discrete representation of information (1) and its numerical coding (2) allow to automate many operations involved in media creation, manipulation and access. Thus, humans intentionally can be removed from the creative process, at least in part.

The following are some of the examples of what can be called "low-level" automation of media creation, in which the computer modifies (i.e., formats) or creates from scratch a media object using templates or simple algorithms. These techniques are robust enough that they are included in most commercial software: image editing, 3-D graphics, word processing, graphic layout. Image editing programs such as Photoshop can automatically correct scanned images, improving contrast range and removing noise. They also come with filters which can automaticaly modify an image, from creating simple variations of color to changing the whole image as though it was painted by Van Gog, Seurat or other brand-name artist. Other computer programs can automatically generate 3-D objects such as trees, landscapes, human figures, and detailed ready-to-use animations of complex natural phenomena such as fire and waterfalls. In Hollywood films, flocks of birds, ant colonies, and even crowds of people are automatically created by AL (artificial life) programs. Word processing, page layout, presentation, and Web creation software comes with "agents" which offer the user to automatically create the layout of a document. Writing software helps the user to create literary narratives using formalized highly conventions genre convention. Finally, in what maybe the most familiar experience of automation of media generation to most computer users, many Web sites automatically generate Web pages on the fly when the user reaches the site. They assemble the information from the databases and format it using templates and scripts.

The researchers are also working on what can be called "high-level" automation of media creation which requires a computer to understand, to a certain degree, the meanings embedded in the objects being generated, i.e. their semantics. This research can be seen as a part of a larger initiative of artificial intelligence (AI). As it is well known, AI projects achieved only very limited success since their beginnings in the 1950s. Correspondingly, work on media generation which requires understanding of semantics is also in the research stage and is rarely included in commercial software. Beginning already in the 1970s, computers were often used to generate poetry and fiction. In the 1990s, the users of Internet chat rooms became familiar with bots — the computer programs which simulate human conversation. Meanwhile, the researchers at New York University showed the systems which allow the user to interact with a "virtual theatre" composed of a few "virtual actors" which adjust their behavior in real-time. [8] The researchers at MIT Media Lab demonstrated "smart camera" which can automatically follow the action and frame the shots given a script. [9] Another Media Lab project was ALIVE, a virtual environment where the user interacted with animated characters. [10] Finally, Media Lab also showed a number of versions of a new kind of human-computer interface where the computer presents itself to a user as an animated talking character. The character, generated by a computer in real-time, communicates with user using natural language; it also tries to guess user’s emotional state and to adjust the style of interaction accordingly. [11]

The areas of new media where the average computer user encountered AI in the 1990s was not, however, human-computer interface but computer games. Almost every commercial game includes a component called AI engine. It stands for part of the game’s computer code which controls its characters: car drivers in a car race simulation, the enemy forces in a strategy game such as Command and Conquer, the single enemies which keep attacking the user in first-person shooters such as Quake. AI engines use a variety of approaches to simulate intelligence, from rule-based systems to neural networks. The characters they create are not really too intelligent. Like AI expert systems, these computer-driven have expertise in some well-defined areas such as attacking the user. And because computer games are highly codified and rule-based and because they severally limit possible behaviors of the user, these characters function very effectively. To that extent, every computer game can be thought of as being another version of a competition between a human chess player and a computer opponent. For instance, in a martial arts fighting game, I can’t ask questions of my opponent, nor do I expect him to start a conversation with me. All I can do is to "attack" him by pressing a few buttons; and within this severally limited communication bandwidth the computer can "fight" me back very effectively. In short, computer characters can display intelligence and skills only because they put severe limits on our possible interactions with them. So, to use another example, once I was playing against both human and computer-controlled characters in a V R simulation of some non-existent sport game. All my opponents appeared as simple blobs covering a few pixels of my VR display; at this resolution, it made absolutely no difference who was human and who was not. The computers can pretend to be intelligent only by tricking us into using a very small part of who were are when we communicate with them. 

Along with "low-level" and "high-level" automation of media creation, another area of media use which is being subjected to increasing automation is media access. The switch to computers as a means to store and access enormous amount of media material, exemplified by the Internet’s "media assets" distributed across numerous Web sites, creates the need to find more efficient ways to classify and search media objects. Word processors and other text management software for a long time provided the abilities to search for specific strings of text and automatically index documents. In the 1990s software designers started to provide media users with similar abilities. Virage introduced Virage's VIR Image Engine which allows the user to search for visually similar image content among millions of images as well as a set of video search tools to allow indexing and searching video files. [12] By the end of the 1990s, the key Web search engines already included the options to search the Internet by specific media such as images, video and audio.

The Internet also crystallized the basic condition of the new information society: over-abundance of information of all kinds. One response was the popular idea of "agent" software. Some "Agents" are supposed to act as filters which deliver small amounts of information given user's criteria. Others are allowing users to tap into the expertise of other users, following their selections and choices. For example, MIT Software Agents Group developed such agents as BUZZwatch which "distills and tracks trends, themes, and topics within collections of texts across time" such as Internet discussions and Web pages; Letizia, "a user interface agent that assists a user browsing the World Wide Web by… scouting ahead from the user's current position to find Web pages of possible interest"; Footprints which "uses information left by other people to help you find your way around." [13]

At the end of the twentieth century, the problem was no longer how to create a new media object such as an image; the new problem was how to find the object which already exists somewhere. That is, if you want a particular image, chances are it already exists somewhere but it may be easier to create one from scratch when to find the one already stored. Historically, we first developed technologies which automated media construction: a photo camera, a film camera, a tape recorder, a video recorder, etc. These technologies allowed us, over the course of about one hundred and fifty years, to accumulate an unprecedented amount of media materials: photo archives, film libraries, audio archives…This then led to the next stage in media evolution: the need for technologies to store, organize and efficiently access these media. The computer provided a basis for these new technologies: digital archives of media; hyperlinking, hierarchical file system and other ways of indexing the digital material; and software for content-based search and retrieval. Thus automation of media access is the next logical stage of the process which was already put into motion when the first photograph was taken. 

### 4. Variability

A new media object (such as a Web site) is not something fixed once and for all but can exist in different (potentially infinite) versions. This is another consequence of discrete representation of information (1) and its numerical coding (2). [14]

Old media involved a human creator who manually assembled textual, visual, or audio elements (or their combination) into a particular sequence. This sequence was stored in some material, its order determined once and for all. Numerous copies could be run off from the master, and, in perfect correspondence with the logic of an industrial society, they were all identical. New media, in contrast, is characterized by variability. 

Stored digitally, rather than in some permanent material, media elements maintain their separate identity and can be assembled into numerous sequences under program control. At the same time, because the elements themselves are broken into discrete samples (for instance, an image is represented as an array of pixels), they can be also created and customized on the fly. 

The logic of new media thus corresponds to the post-industrial logic of "production on demand" and "just in time" delivery which themselves were made possible by the use of digital computers and computer networks in all stages of manufacturing and distribution. Here "culture industry" is actually ahead of the rest of the industry. The idea that a customer determines the exact features of her car at the showroom, the data is transmitted to the factory, and hours later the new car is delivered remains a dream, but in the case of computer media, it is reality. Since the same machine is used as a showroom and a factory, and since the media exists not as a material object but as data which can be sent through the wires with the speed of light, the response is immediate.

Here are some particular cases of the variability principle:

4.1. Media elements are stored in a media database; a variety of end-user objects which vary both in resolution, in form and in content can be generated, either beforehand, or on demand, from this database.

4.2. It becomes possible to separate the levels of "content" (data) and interface. A number of different interfaces can be created to the same data. A new media object can be defined as one or more interfaces to a multimedia database.

4.3. The information about the user can be used by a computer program to automatically customize the media composition as well as to create the elements themselves. Examples: Web sites use the information about the type of hardware and browser or user's network address to automatically customize the site which the user will see; interactive computer installations use information about the user's body movements to generate sounds, shapes, or control behaviors of artificial creatures. 

4.4 A particular case of 4.3 is branching-type interactivity. (It is also sometimes called menu-based interactivity.) The program presents the user with choice(s) and let her pick. In this case, the information used by a program is the output of user's cognitive process (rather than the network address or body position).

4.5. Hypermedia: the multimedia elements making a document are connected through hyperlinks. Thus the elements and the structure are separate rather than hard-wired as in traditional media. By following the links the user retrieves a particular version of a document. (World Wide Web is a particular implementation of hypermedia in which the elements are distributed throughout the network).

Out of these four principles, the principle of variability maybe is the most interesting. On the one hand, such popular new media forms as branching-type interactivity and hypermedia can be seen as particular instances of variability principle. On the other hand, this principle demonstrates how the changes in media technologies are closely tied up with changes in social organization. Just as the logic of old media corresponded to the logic of industrial mass society, the logic of the new media fits the logic of the post-industrial society of personal variability. In industrial mass society everybody was supposed to enjoy the same goods — and to have the same beliefs. This was also the logic of media technology. A media object was assembled in a media factory (such as a Hollywood studio). Millions of identical copies were produced from a master and distributed to all the citizens. Broadcasting, film distribution, print technologies all followed this logic.

In a post-industrial society, every citizen can construct her own custom lifestyle and "select" her ideology from a large (but not infinite) number of choices. Rather than pushing the same objects/information to a large group, marketing tries to target each individual separately, The logic of new media technology reflects this new condition perfectly. Every visitor to a Web site automatically gets her own custom version of the site created on the fly from a database. Every hypertext reader gets her own version of the text. Every viewer of an interactive installation gets her own version of the work. And so on. In this way, new media technology acts as the most perfect realization of the utopia of a perfect society composed from unique individuals. New media objects assure the users that their choices — and therefore, their underlying thoughts and desires — are unique, rather than pre-programmed and shared with others. As though trying to compensate for their earlier role in making us all the same, today descendants of the Jacquard's loom, The Hollerith tabulator, and Zuse's cinema-computer are now working to convince us that we are all different. 

## References:

[1] This is a revised and substantially expanded version of the article first published in NET.CONDITION (ZKM / Zentrum für Kunst und Medientechnologie Karlsruhe), 1999.

[2] Qtd. in Beaumont Newhall, _The History of Photography from 1839 to the Present Day. Revised and Enlarged Edition_, fourth edition (New York: The Museum of Modern Art, 1964), 18.

[3] Newhall, _The History of Photography_, 17-22. 

[4] Charles Eames, _A Computer Perspective: Background To The Computer Age_, 1990 edition (Cambridge, Mass.: Harvard University Press, 1990), 18.

[5] David Bordwell and Kristin Thompson, _Film Art: An Introduction_, fifth edition (New York: The McGraw-Hill Companies), 15.

[6] Eames, _A Computer Perspective_, 22-27, 46-51, 90-91.

[7] Eames, _A Computer Perspective_, 120.

[8] [http://www.mrl.nyu.edu/improv/](http://www.mrl.nyu.edu/improv/), accessed June 29, 1999.

[9] [http://www.white.media.mit.edu/vismod/demos/smartcam/](http://www.white.media.mit.edu/vismod/demos/smartcam/), accessed June 29, 1999.

[10] [http://www.media.mit.edu/people/pattie/CACM-95/alife-cacm95.html](http://www.media.mit.edu/people/pattie/CACM-95/alife-cacm95.html), accessed June 29, 1999.

[11] See, for instance, the work of Gesture and Narrative Language Group at the MIT Media Lab, [http://www.media.mit.edu/groups/gn/](http://www.media.mit.edu/groups/gn/), accessed June 29, 1999.

[12] See [http://www.virage.com/products](http://www.virage.com/products), accessed June 29, 1999.

[13] [http://www.media.mit.edu/groups/agents/projects/](http://www.media.mit.edu/groups/agents/projects), accessed June 29, 1999.

[14] The concept of variability as it is presented here is not dissimilar to Jon Ippolito’s use of the same term. One important difference is that while Ippolito uses this term in the context of conceptual and digital art, I see variability as a basic condition of all new media.

---
# Avant-garde as Software

_author: Lev Manovich_
_year: 1999_

## From "New Vision" to New Media

During the 1920s a number of books with the word "new" in their title were published by European artists, designers, architects and photographers: The New Typography (Jan Tschichold [1], New Vision (Laszlo Moholy-Nagy [2]), Towards a New Architecture (Le Corbusier [3]). Although nobody, as far as I know, published something called New Cinema, all the manifests written during this decade by French, German and Russian filmmakers, in essence, constitute such a book: a call for a new language of film, whether it was to be montage, "Cinéma Pur" (also known as "absolute film"), or "photogénie". Similarly, although not declared in a book, a true visual revolution also took place in graphic design thus "making it new" as well (Aleksander Rodchenko, El Lissitzky, Moholy-Nagy, etc.)

In the 1990s the word "new" re-appeared once again. But now it was paired not with particular media such as photography, print, and film but with media in general. The result was the term "new media". This term was used as a shortcut for new cultural forms which depend on digital computers for distribution: CD-ROMs and DVD-ROMs, Web sites, computer games, hypertext and hypermedia applications. But beyond its descriptive meaning, the term also carried with it some of the same promise which animated the just mentioned books and manifests from the 1920s — that of the radical cultural innovation. If new media is indeed the new cultural avant-garde, how can we understand it in relation to earlier avant-garde movements? Using already noted parallels as a starting point, this article will look at new media in relation to the avant-garde of the 1920s. I will mostly focus on the most radical sites of the avant-garde activities of the 1920s: Russia and Germany. 

The reader may wonder if it is legitimate to compare the revolution in technology with the revolution in art. Looking retroactively on the 1920s from the viewpoint of today we realize that the key artistic innovations of the 1920s were all done in relation to what was then "new media": photography, film, new architectural and new printing technologies. "New Vision" was the new language for photo media; Soviet-montage school and classical film language were the new languages for film media; "New Typography" (Tschichold) was the new language for print media, "New Architecture" (Le Corbusier) was the new language for spatial media (i.e. architecture). Therefore what is being compared here is new media at the beginning of the twentieth century and new media at the turn of the twenty-first century.

But why the 1920s as opposed to some other decade? From the point of art, music and literature, earlier decades were probably as crucial. For example, painting goes abstract between 1910 and 1914. But from the point of view of mass communication, the key decade was the 1920s. Between the second part of the 1910s and the end of the 1920s, all key modern visual communication techniques were developed: photo and film montage, collage, classical film language, surrealism, the use of sex appeal in advertisement, modern graphic design, modern typography. (Not incidentally, during the same decade, the designer, the advertising man, the cinematographer acquire professional status). Of course, in the latter decades of the twentieth century, these techniques are further developed and refined: the quick cutting of such films as The Man with a Movie Camera (Dziga Vertov, 1929) is speeded up in music videos and commercials while its experiments in compositing become the norm of digital filmmaking. The treatment of type as a graphic element, pioneered by "New Typography" of Tschichold and Lissitzky, reaches new intensity in both print and video media (which in large part was stimulated by the availability of such software Photoshop and After Effects). All too classical juxtapositions of Surrealists acquire baroque intensity in modern advertisements. The sex appeal pioneered by J. Walter Thompson’s ads in 1922, as timid as Giotto's first attempts at representing a coherent three-dimensional space, reach after the "sex revolution" 1960s Tintoretto-like mastery and aggressiveness. But no fundamentally new approaches emerge after the 1920s. The techniques introduced by modernist avant-garde turn out to be sufficiently effective to last for the rest of the century. Mass visual culture only pushes further what was already invented, "intensifying" particular techniques and mixing them together in new combinations.

In the 1990s, the technological shift of all cultural communication to computer media gets underway. We may think that finally, the avant-garde techniques of the 1920s will no longer be sufficient and that fundamentally new techniques will start to appear. But, paradoxically, the "computer revolution" does not seem to be accompanied by any significant innovations on the level of communication techniques. While we now rely on computers to create, store, distribute and access culture, we are still using the same techniques developed in the 1920s. Cultural forms which were good enough for the age of the engine turned out to be also good for the age of the "geometry engine" and the "emotion engine". ("Geometry engine" is the name of a computer chip introduced in Silicon Graphics workstations a number of years ago to perform real-time 3D graphics calculations; "emotion engine" is the name of the processor used in Sony’s Playstation 2 introduced in 1999; it allows real-time rendering of facial expressions). In short, as far as the cultural languages are concerned, new media is still old media. Why? If historically each cultural period (Renaissance, Baroque, and so on) brought with it new forms, new expressive vocabulary, why the computer age is satisfied with using the languages of the previous period, in other words, that of the industrial age?

Perhaps we have to give it more time. When radically new cultural forms appropriate for the age of wireless telecommunication, multitasking operating systems, and information appliances do arrive, what will they look like? How would we even know they are here? Would future films look like a "data shower" from the movie Matrix? Does the famous fountain at Xerox PARC, where the strength of the water stream reflects the behavior of the stock market, with stock data arriving in real time over the Internet, represent the future of public sculpture? Or are we asking the wrong question? What if the historical logic of the succession of new forms no longer applies to the information age? What if our growing obsession with mid-twentieth century modernism (exemplified by the popularity of Wallpaper magazine) on the eve of the new millennium is not a temporary aberration but the beginning of new, very different logic?

During its history, the identity of a digital computer kept changing almost every decade: a calculator (the 1940s); a real-time control mechanism; a data processor; a symbol processor; and, in the 1990s, a media distribution machine. This latest identity has very little to do with the original one, since distribution of media does not require much computation. As computing became equated with the Internet use during the second part of the 1990s, the computer, in its original sense, became less and less visible; its identity as a carrier for already established cultural forms — more and more prominent. Music and films streamed over the Internet; M3 music files, to be downloaded and played using stand-alone M3 players; books, to be downloaded into stand-alone electronic book devices; Internet telephony and faxing — all these applications use a computer as a communication channel, without requiring it to compute anything.

The reader may ask how computer’s another new post-Internet role, that of a communication link between individuals (as exemplified by chat, newsgroups and email), fits into this analysis. In my view, we can understood "person-to-person communication channel" identity as a subset of "media distribution channel" identity. For what is being sent over email or posted to a newsgroup is simply another form of media — one’s thoughts formatted as text, i.e. human language. If this perspective may appear strange, it is only because during the history of modern media, from photography to video, a media object was usually (1) created by special type of professional users (artists, designers, filmmakers); (2) mass reproduced; (3) distributed to many individuals via mass printing, broadcasting, etc. The Internet returns us to the age of private media — the eighteenth-century literary salons and similar small intellectual communities where the messages traveled from one individual to another individual or to a small group, rather than being distributed to millions at once. Thus the computer is a new type of media distribution machine which combines public and private media distribution.

## The Avant-garde as Software

The paradox remains: with few notable exceptions like Frank Gerry’s Guggenheim Museum (Frank Gehry), the shift to computer tools in architecture, design, photography, filmmaking did not lead to the invention of radically new forms, at least not in any scale which can be compared to the formal revolutions of the 1920s. In fact, rather than being a catalyst of new forms, computer seems to strengthen already existing ones. How to understand this absence of radically new forms in a culture undergoing rapid and massive computerization? Is new media’s avant-garde promise only an illusion?

Part of the answer is that with new media, 1920s communication techniques acquire a new status. Thus new media does represent a new stage of the avant-garde. The techniques invented in the 1920s Left artists became embedded in the commands and interface metaphors of computer software. In short, the avant-garde vision became materialized in a computer. All the strategies developed to awaken audiences from a dream-existence of bourgeois society (constructivist design, New Typography, avant-garde cinematography and film editing, photo-montage, etc.) now define the basic routine of a post-industrial society: the interaction with a computer. For example, the avant-garde strategy of collage reemerged as a "cut and paste" command, the most basic operation one can perform on any computer data. In another example, the dynamic windows, pull-down menus, and HTML tables all allow a computer user to simultaneously work with practically unrestricted amount of information despite the limited surface of the computer screen. This strategy can be traced to Lissitzky's use of movable frames in his 1926 exhibition design for the International Art Exhibition in Dresden. [4]In this section, I will further analyze the transformation of the 1920s avant-garde techniques into the conventions of modern human-computer interface (HCI) such as overlapping windows. I will also discuss how the avant-garde techniques now function as the strategies of computer-based labor, i.e. different ways we use to organize, access, analyze and manipulate digital data (for instance, discrete data representation, 3-D data visualization, and hyperlinking).

### 1. Visual Atomism / Discrete Ontology

The avant-garde of the 1920s developed a particular approach to visual communication which I will refer to as visual atomism. [5] This approach is based on the idea that a complex visual message can be constructed from simple elements whose psychological effects are known beforehand.

Already in the nineteenth century, Georges Seurat used current psychological theories about the effects of simple visual elements and colors on the viewer to determine directions of lines and colors in his paintings. The next logical step, taken in the 1910s by Kandinsky and others, was to create completely abstract paintings. These paintings in effect were sets of psychological stimuli, similar to the ones used by psychologists to study human perception and the emotional effects of visual elements. Visual atomism acquired a new significance in the 1920s when the artists were searching for ways to rationalize mass communication. If the effect of every simple element is known beforehand, so the logic went, it may be possible to reliably predict viewer's response to complex messages put together from such elements. This approach was most systematically articulated in Soviet Russia. Left artists and designers, who were in charge of State art schools and research institutes, set up a number of psychological laboratories in order to put visual communication on a scientific basis.

The atomistic approach to communication reappears with a new force in computer media. But what was a particular theory of visual meaning and emotional effect grounded in psychology now became a technological basis of all communication. For instance, a digital image consists of atom-like pixels, which makes it possible to automatically generate images, to automatically manipulate them in numerous ways, and, through compression techniques, to transmit them more economically. A digital three-dimensional space has a similar atomistic structure — an agglomerate of simple elements such as polygons or voxels. A digital moving image also consists of a number of separate layers, which can be separately accessed and manipulated.

Another example of the atomistic (i.e., discrete) message construction in computer media is hyperlinking. Hyperlinking separates data from its structure. This makes creation and distribution of messages extremely efficient: the same data can be endlessly assembled in new structures; parts of a single document can exist in physically distinct locations (i.e., a document has a distributed representation). Finally, on yet another level, computer software replaces the traditional process of creating media objects from scratch by a more efficient method. In computer culture, a media object is typically assembled from ready-made elements such as icons, textures, video clips, 3-D models, complete animation sequences, ready-to-use virtual characters, chunks of Javascript code, Director Lingo scripts, etc.

Therefore when a computer user interacts with a Web site, navigates a virtual space, or examines a digital image, she is fulfilling the wildest atomistic fantasies of Kandinsky, Rodchenko, Lissitzky, Eisenstein and other "atomists" of the 1920s. The digital image is made up from pixels and layers; the virtual 3-D space is made from simple polygons; the Web page is made up from separate objects represented by HTML statements; the objects on the Web are connected by hyperlinks. In short, the ontology of computer dataspace as a whole and the individual objects in this space is atomistic on every possible level.

### 2. Montage / Windows

The key feature shared by all modern human-computer interfaces is overlapping windows which were first proposed by Alan Kay in 1969. All modern interfaces display information in overlapping and re-sizable windows arranged in a stack, similar to a pile of papers on a desk. As a result, the computer screen can present the user with practically an unlimited amount of information despite its limited surface.

Overlapping windows of HCI can be understood as a synthesis of two basic techniques of twentieth-century cinema: temporal montage and montage within a shot. In temporal montage, images of different realities follow each other in time, while in montage within the shot, these different realities co-exist within the screen. The first technique defines the cinematic language as we know it; the second is used more rarely. An example of this technique is the dream sequence in The Life of an American Fireman by Edward Porter in 1903, in which an image of a dream appears over a man's sleeping head. Other examples include the split screens beginning in 1908 which show the different interlocutors of a telephone conversation; superimpositions of a few images and multiple screens used by the avant-garde filmmakers in the 1920s; and the use of deep focus and a particular compositional strategy (for instance, a character looking through a window, such as in Citizen Kane, Ivan the Terrible and Rear Window) to juxtapose close and far away scenes. [6]

As testified by its popularity, temporal montage works. However, it is not a very efficient method of communication: the display of each additional piece of information takes time to watch, thus slowing communication. It is not accidental that the European avant-garde of the 1920s was inspired by the engineering ideal of efficiency, experiments with various alternatives, trying to load the screen with as much information at one time as possible. [7] In his 1927 Napoleon Abel Gance uses a multiscreen system which shows three images side by side. Two years later, in A Man with a Movie Camera (1929), we watch Dziga Vertov speeding up the temporal montage of individual shots, more and more, until he seems to realize: why not simply superimpose them in one frame? Vertov overlaps the shots together, achieving temporal efficiency — but he also pushes the limits of a viewer's cognitive capacities. His superimposed images are hard to read — information becomes noise. Here cinema reaches one of its limits imposed on it by human psychology; from that moment on, cinema retreats, relying on temporal montage or deep focus, and reserving superimpositions for infrequent cross-dissolves.

In window interface, the two opposites — temporal montage and montage within the shot — finally come together. The user is confronted with a montage within the shot — a number of windows present at once, each window opening up into its own reality. This, however, does not lead to the cognitive confusion of Vertov's superimpositions because the windows are opaque rather than transparent, so the user is only dealing with one of them at a time. In the process of working with a computer, the user repeatedly switches from one window to another, i.e. the user herself becomes the editor accomplishing montage between different shots. In this way, window interface synthesizes two different techniques of presenting information within a rectangular screen developed by cinema and pushed to the extreme by the filmmakers in the 1920s.

### 3. New Typography / GUI (Graphical User Interface)

The 1920s saw a revolution in typography and graphic design. Traditional symmetrical layouts appropriate for the old age of slow reading and private engagement with the book were replaced by new principles: the clear hierarchy of type sizes, the economy of block type against clean white background, the energy of simple geometric elements designed to grab the attention of the viewer and then to lead her through the message, step by step All these principles received further development in computer interface. On the most simple level, the graphical style of Windows 2000 or MAC OS perfectly follows Tschichold’s thesis that "the essence of the New Typography is clarity." [8] Thus it features clean dark type against neutral background, clean geometry of window frames, clean hierarchy of pull-down menus. But GUI also takes New Typography to the next level. The task of the interface designer is no longer to simply present limited amount of information in a most efficient way as it was for the designer of an invitation card, a magazine layout or a poster. The new task is create an efficient structure and tools for working with arbitrary information, information which is always changing and always growing. Therefore if a modernist designer broke a message into a clearly defined hierarchy — main heading, sub-heading, and so on — GUI provides the user herself with tools for hierarchical organization of arbitrary data. The examples of these tools are nested folders and nested menus; outline display options of word processing applications; zoom and pan controls which can operate on any data, from 3-D spaces to text (Pad++ interface). In this way, the principles of New Typography and modernist design have become the principles of what can be called meta-design: the creation of tools which are employed by a user herself to organize the information on-the-fly. 

### 4. New Vision -\> 3-D Data Visualization

Here is another example of how HCI and computers methods of data analysis inherit aesthetic techniques developed by the 1920s European avant-garde. Putting into practice Russian critic Victor Schklovsky’s notion of "defamiliarization" or "making strange" (In Russian, "otstranenie"), advanced originally in relation to literature, a number of photographers in the 1920s began to use unorthodox viewpoints in their photographs: aerial and "worm's-eye views, diagonal positions of the camera, elimination of the horizon line, extreme close-ups. [9] The most outspoken defenders of this approach to photographic composition were Moholy-Nagy in Germany and Rodchenko in Russia. The latter wrote in 1928 that his task was to "Photograph from all viewpoints except "from the belly button", until they become acceptable. The most interesting viewpoints today are "from above down" and "from below up", and we should work on them." [10] These "defamiliarizing" points of view functioned in a number of ways, being promoted at the same time as the records of the experience of modernization and as the tools to help to bring modernization about. On the simplest level, they were recordings of new quintessentially modern visual experiences — the results of seeing reality from a skyscraper, a moving car, an airplane. They were at the same time perfect metaphors for modernization, with its speed, chaos, new rhythms, and geometric architecture (and it was new architecture which was a favorite subject of "New Vision" photography). They were visual analogs of the Revolutionary process of dismantling and uprooting all social structures, which was underway in Soviet Russia and which was sympathetically watched by the avant-garde Left avant-garde in Europe. They were tools to "cleans perception" in order to bring a new regime of "visual hygiene," literally a kind of new biological vision appropriate for the New Man and New Woman of modernity. Finally, they were also instruments in a brave project of visual epistemology which was advocated most systematically by Dziga Vertov in A Man with a Movie Camera: to decode the world purely through its surfaces visible to the eye, its natural sight being amplified by a mobile camera.

The idea of visual epistemology received a new life in a computer age. It justifies computer version of avant-garde "defamiliarizing" points of view: interactive 3-D computer graphics. This technology allows a computer user to observe any object from an arbitrary viewpoint in order to understand object's structure. Similarly, any quantified data can be turned into a 3-D representation which the user can examine in order to uncover the relations between visualized data. From chemistry and physics to architectural and product design, from financial analysis to pilot training, 3-D visualization is an essential tool of post-industrial labor of information processing. "Defamiliarization" now involves simply a movement of a computer mouse to change the perspective, thus getting a new way of the scene.

While the analogy between 3D interactive graphics and the "defamiliarizing" points of view advocated by Moholy-Nagy, Rodchenko, and their fellow artists is the most direct way to connect "New Vision" and new media, it is not the only one. In fact, all "New Vision" photographic strategies became standard software techniques for the visual analysis of data. In order to reveal the structure of the data translated into a visual image, a computer user may zoom in and out of this image, change the positive image into the negative, re-map the colors, reduce and expands the contrast, and so on.

## "Post-modernism" and Photoshop

To summarize: what was a radical aesthetic vision in the 1920s became a standard computer technology by the 1990s. The techniques which were harnessed to help the viewer to reveal the social structure behind the visible surfaces, to uncover the underlying struggle between the old and the new, to prepare for rebuilding a society from the ground, became the elemental work procedures of a computer age.

The transformation of the avant-garde communication techniques into the principles of HCI and computer-based labor, described here, is yet another, and, as far as I know, previously unnoticed, legacy of the radical avant-garde practices today. According to the standard art historical account, when the radical avant-garde vision of the European avant-garde came to America in the 1930s and 1940s, it was stripped out of its radical politics and put into service of capitalism as a new International Style of architecture and design as well as being turned into a set of formal techniques for "artistic self-expression". It is not difficult to question this story. For instance, since avant-garde artists of the 1920s, both in Russia and in Western Europe, ultimately wanted to participate in building a new modern rational society based on technology, the adaptation of their aesthetics on a mass scale in America can be seen as a fulfillment of this dream. (This would also explain why many radical German artists, architects, and designs had such successful commercial careers in the U.S. after they emigrated there in the 1930s). Art critic Boris Grois argued that the Russian avant-garde project logically moves from creating utopian plans for a future society (the 1910s) to implementing these plans in reality via collaboration with the new state (1920s) and then to Stalin’s dictatorship (1930). Stalin became the ultimate avant-garde artist building a new society according to aesthetic principles. [11] From this perspective, active participation of the European avant-garde artists in building American techno-society, whether through cinema (in Hollywood), architecture or design, can be understood as an equivalent of the Russian artists’ collaboration with the new Revolutionary state. But while the Stalinist state abandoned the techno-dreams of the Russian artists for a new society based on Taylorism, building a few showcase mega-scale projects such as the Moscow Metro instead of mass housing, American capitalism fully embraced Europeans’ techno-utopia — which itself was originally inspired by European fetishism of American technology.

The notion of the revolutionary avant-garde later co-opted by capitalism can be further questioned if we note that already in the 1920s, left avant-garde artists, both in Europe and in Soviet Russia, worked for commercial industries on publicity and advertising campaigns; in short, they "sold out" right away. Rodchenko created advertisements for new Soviet State enterprises; Lissitzky worked on design projects for European companies; Moholy-Nagy was writing about advertising while still a Professor at Bauhaus; eventually, he left the school to start his own commercial practice in Berlin. Also, already in the 1920s, many observers noticed that avant-garde radical techniques were functioning simply as a fashionable style, a convenient and easily decoded sign for what was to become a permanent signified of advertising since then: "being modern". In short, the standard account of the avant-garde’s legacies does not stand up under close scrutiny.

The examples analyzed in this section suggest a different history in which the avant-garde theories and practices gave rise not only to modern and, later, post-modernist style (MTV montage-like aesthetics, for instance) but they also became "materialized" in human-computer interfaces through which post-industrial work is accomplished. To re-phrase the title of the article by photo historian Abigail Solomon-Godeau, the history of Radical Formalism does not end with style; it extends not "from Weapon to Style" but rather "from Weapon to Style and Instrument of Labor." [12]

As the "and" above suggests, it is possible to see this transformation of avant-garde visions into computer software as another example of the larger logic of post-modernism. Post-modernism naturalizes the avant-garde; it gets rid of the avant-garde’s original politics and, through repeated use, makes avant-garde techniques appear totally natural. From this point of view, software naturalizes the 1920s radical communication techniques of montage, collage, "defamiliarization," etc. just as it did in music videos, post-modern design, architecture, and fashion. Of course, as my examples here already demonstrated, software does not simply adopt avant-garde techniques without changing them; on the contrary, these techniques are further developed, formalized in algorithms, codified in software, made more efficient and effective. A hierarchy of two or three subheadings of Tschichold’s design for print becomes the hierarchy of practically endless sub-menus on a computer screen; "defamiliarizing" viewpoint of a Moholy-Nagy’s photograph becomes continuously changing viewpoint of an animated computer walk-through; two overlapping images from a composite shot in Vertov’s A Man with a Movie Camera become dozen windows opened at once on a computer desktop. But post-modern culture similarly does not only replays, samples, comments on and echoes old avant-garde techniques; it also advances them further, "intensifying" them and overlaying them on top of one another. Few photographic fragments brought together in a Rodchenko’s photo-collage become hundreds of image layers in a digitally composited video; quick film cutting of the 1920s is similarly speeded up to the extreme, with limits set by the temporal resolution of the human visual system simply to register individual images (rather by human mental capacity to make sense of the image sequence); the images which originally belonged to the incompatible aesthetic systems of Constructivism and Surrealism are brought together in a space of a single music videos; and so on.

## The New Avant-garde

I began by promising to look at new media in relation to the avant-garde of the 1920s. I also noted that new media does not fit into the traditional history of cultural evolution as it does not use new forms. In contrast, the avant-garde of the 1920s invented a whole set of new formal languages which we are still using today. Given the transformation of the avant-garde techniques into software, described above, shall we conclude that the only claim of new media to an avant-garde status lies in its connection to the old, modernist avant-garde?

The answer is no. New media does introduce an equally revolutionary set of communication techniques. It indeed represents the new avant-garde, and its innovations are at least as radical as the formal innovations of the 1920s. But if we are to look for these innovations in the realm of forms, this traditional area of cultural evolution, we will not find them there. The new avant-garde is radically different from the old: 

1. The old media avant-garde of the 1920s came up with new forms, new ways to represent reality and new ways to see the world. The new media avant-garde is about new ways of accessing and manipulating information. Its techniques are hypermedia, databases, search engines, data mining, image processing, visualization, simulation.

	 

2. The new avant-garde is no longer concerned with seeing or representing the world in new ways but rather with accessing and using in new ways previously accumulated media. In this respect, new media is post-media or meta-media, as it uses old media as its primary material.

As I will show shortly, these two key characteristics of the new avant-garde are logically connected. Beginning with photography, modern media technologies make possible the accumulation of media recordings of reality. Modernism (approximately from the 1860s to 1960s; or from Manet to Warhol; or from Baudelaire to McLuhan), including the avant-garde of the 1920s, corresponds to this period of media accumulation. The artists are concerned with representing the outside world; with "seeing" it in as many different ways as possible. In this, they set themselves up in opposition to the "objective," "mechanical," "documentary" seeing and recording of the world made possible by new media technologies: photography, film, video recording, audio recording, etc. Yet they ultimately participate in the same project as media — reflecting the outside world. That the artists, competing with media machines, interject their artistic "subjectivity" between the world and the recording media, does not change the project. Surrealists put together samples of reality in illogical combinations; Cubists chop up reality in small pieces; abstract artists reduce reality to what they think is its geometric "essence"; "New Vision" photographers show reality from unusual points of view — but, despite these differences, they are all concerned with the same project of reflecting the world. Therefore modernism’s key concern is the invention of new forms, i.e. different ways to "humanize" the "objective" and ultimately alien picture of the world served to us by media technologies.

In the 1960s Andy Warhol serves us hours and hours of unedited film recordings of reality in his famous films, thus refusing his "artistic subjectivity" in favor of the media machine’s vision. He also attempts to rob other subjects of their subjectivity by making them face the disinterested camera in his Screen Tests. In 1961 young East German painter Gerhard Richter moves to Düsseldorf. Where, instead of expressing his new freedom in "subjective" abstract painting as one may expect, he starts to meticulously paint newspaper photographs. He also begins to assemble "Atlas," a database of thousands of media image. Other artists such as Bruce Conner, Robert Rauschenberg, and James Rosenquist similarly give up the idea of creating totally "new" images. Instead, their works come to function as research laboratories where existing media images are juxtaposed together in order to be analyzed. (During the same years Roland Barthes publishes his articles on the semiotics of advertising photography.) And, a little earlier, in 1958, Bruce Conner creates his famous "compilation" film "Movie, movie" totally made from the "found" media material. Such a movie — something would not be conceivable just a three decades earlier, when media society was still young and still excited about the possibility of accumulating media records (so even Vertov thought it was necessary to shoot his own material.)

These artworks of the 1960s signal the arrival of the new stage in history of media, which I will call meta-media society. The tremendous accumulation of media records by that time, along with the shift from industrial society concerned with the production of goods to the information society concerned with the processing of data (which was noted by the early 1970s) changes the game. It becomes more important to find effective and efficient ways to deal with already accumulated volumes of media than to record more or in new ways. I am not saying that the society no longer has any interest in looking outside, in representation and new forms; but the emphasis shifts to find find new ways to deal with the media records obtained by already existing media machines. This shift is paralleled by the new economic importance of data analysis over material production in the information society. The new "information worker" also does not deal with the material reality directly but with its records. Importantly, both meta-media society and the information society adopt digital computer as their key technology to process all types of data and all types of media.

"Post-modernism" (the 1980s -) is one effect of this new historical stage. In evoking this term I follow Fredric Jameson's usage of post-modernism as "a periodizing concept whose function is to correlate the emergence of new formal features in culture with the emergence of a new type of social life and a new economic order." [13] As it became apparent by the early 1980s, culture no longer tries to "make it new". Rather, endless recycling and quoting of the past media content, artistic styles and forms which become the new "international style" of the media-saturated society. In short, culture is now busy re-working, recombining and analyzing the already accumulated media material. So when Jameson notes that post-modern cultural production "can no longer look directly out of its eyes at the real word but must, as in Plato’s cave, trace its mental images of the world on its confining walls," [14] I would add that these walls are made from old media. 

Computer’s post-Internet identity as a distribution machine for older, i.e. already established media forms and content (the 1990s -) is another effect. Meta-media society gives up computation in favor of distribution. 

Yet another effect is the absence of new forms in new media itself. Meta-media society does not need even more ways to represent the world — it has enough trouble dealing with all the already accumulated representations. Consequently, 3-D computer imaging imitates the look of classical cinema, complete with film grain; computer-based virtual spaces usually look like something which was already built in reality; Flash animations on the Web imitate old video graphics; the Web itself combines the layouts of pre-computer print media with moving images which follow the already established conventions of film and television; and so on.

The differences between the two stages of the media society can be illustrated by comparing two media technologies: cinema and computer. Just as cinema was central for the media society, computer is central for the meta-media society. Cinema was the art of seeing (recall A Man with a Movie Camera one more time). Film camera was directed towards the world. Thus, out of all mental functions, cinema foregrounded perception. In contrast, computer foregrounds the function of memory. Meta-media society uses computers first of all to store records of the world accumulated during the previous stage; to access these records, to manipulate them, and to analyze them. And when computers are used to generate new media material, it is made to look like old media. 

So what is the new avant-garde? It is the new computer-based techniques of media access, generation, manipulation and analysis. Forms remain the same, but how these forms can be used changes radically. Here are some examples of these techniques:

### 1. Media access:

Databases allow to store millions of media records and retrieve them almost instantly. Search engines allow to find the required data in the huge unstructured database of the Internet. Multimedia allows to access all the different media types using the same machine (i.e., a computer). Hypermedia adds hyperlinking to multimedia, allowing to create numerous paths through the media material. Networks such as the Internet allow to create distributed media representations in which different parts of a media object may exist in physically remote locations. Hypermedia authoring software (such as Director, Dreamweaver and Generator by Macromedia) and languages (such as HTML and JAVA) allow to create dynamic media documents, i.e. the documents which change as a whole or in part at run-time. To use the most basic example, HTML tables allow parts of a Web page to remain constant while other parts may change.

### 2. Media analysis:

Data mining techniques allow to search for significant relationships in large volumes of data.

Image processing allows to reveal detail which may be hidden in an image and to automatically compare sets of images. Visualization turns numerical data into 3D scenes for easier analysis. Various statistics can be obtained for a given media object in order to determine its authorship, style, etc.

### 3. Media generation and manipulation:

3-D computer graphics technology allows to create highly detailed navigable 3-D scenes. Mathematical techniques can be used to generate images with particular properties (for instance, fractal images display the property of self-similarity.) AL (artificial life) allows to generate systems of objects which display emergent properties. Using scripts and templates, customized media objects can be automatically created from databases. [15] More generally, since a media object has a discrete structure on a number of levels (for instance, a digital image typically consists of a number of layers and each layer is made up from pixels), parts of the object can be easily accessed, modified, substituted by other parts, etc. (This is another benefit of "atomistic" approach to data representation.)

To summarize: from "New Vision," "New Typography," "New Architecture" of the 1920s we move to new media of the 1990s; from "a man with a movie camera" to a user with a search engine, image analysis program, visualization program; from cinema, the technology of seeing, to a computer, the technology of memory; from "defamiliarization" to information design.

In short, the avant-garde becomes software. This statement should be understood in two ways. On the one hand, software codifies and naturalizes the techniques of the old avant-garde. On the other hand, software’s new techniques of working with media represent the new avant-garde of the meta-media society. 

## References:

[1] Jan Tschichold, The New Typography: a Handbook for Modern Designers, trans. Ruari McLean (Berkeley: University of California Press, 1995); 

[2] Although Moholy-Nagy New Vision exhibition took place only in 1932, it was a retrospective of the 1920s movement in photography which took place in the 1920s and which was largely over by the time of the exhibition.

[3] Le Corbusier, Towards a New Architecture, trans. Frederick Etchells (London: Architectural Press; New York, Praeger, 1963).

[4] See El Lissitzky, "Exhibition Rooms," in Sophie Lissitzky-Küppers, El Lisstzky. Life — Letters — Texts (London: Thames and Hudson, 1968), 366-368. 

[5] Lev Manovich, The Engineering of Vision from Constructivism to VR, Ph.D. dissertation, University of Rochester, 1993. 

[6] The examples of Citizen Kane and Ivan the Terrible are from Aumont et al., Aesthetics of Film (Austin: Texas University Press, 1992), 41. 

[7] On the ideal of engineering efficiency in relation to the avant-garde and digital media, see my article "The Engineering of Vision and the Aesthetics of Computer Art," Computer Graphics 28, no. 4 (November 1984): 259-263.

[8] Jan Tschichold, The New Typography, trans. Ruari McLean (Berkeley: University of California Press, 1995), 66. The artist Rainer Ganahl made a reference to the continuity between geometric efficiency in modernism and in human-computer interfaces in a number of his projects, such as Sample, wi. 95/opt.c. 

[9] It is relevant here that Shklovsky connected the origins of his concept of "defamiliarization" to a visual experience which retrospectively can be read as a typical New Vision photograph. He recalled that for months he walked past a Tobacconist shop without ever noticing a sign above the shop — until one day the sign was turned 90 degrees. 

[10] Aleksander Rodchenko, "Downright Ignorance or a Mean Trick?" (1928), trans. John Bowlt, in Christopher Phillips, ed., Photography in the Modern Era (New York: The Metropolitan Museum of Art / Aperture, 1989), 248.

[11] Boris Groys, The Total Art of Stalinism, trans. Charles Rougle (Princeton: Princeton University Press, 1992).

[12] Abigail Solomon-Godeau, "The Armed Vision Disarmed: Radical Formalism from Weapon to Style," in The Contest of Meaning, edited by Richard Bolton (Cambridge, Mass.: The MIT Press, 1989): 86-110.

[13] Fredric Jameson, "Postmodernism and Consumer Society," in Postmodernism and its Discontents, edited by E. Ann Kaplan (London and New York: Verso, 1988): 15

[14] Jameson, "Postmodernism and Consumer Society," 20.

[15] For example, in 1998 Macromedia offered Generator software which can automatically create Web site graphics and interactive applications at run-time. See [http://www.macromedia.com/software/generator](http://www.macromedia.com/software/generator), accessed July 8, 1999.

---

# Computer Simulation and the History of Illusion

 _author: Lev Manovich_
_year: 1999_

Zeuxis was a legendary Greek painter who lived in the fifth century BC. [1] The story of his competition with Parrhasius exemplifies the concern with illusionism which was to occupy Western art throughout much of its history. According to the story, Zeuxis painted grapes with such a skill that the birds began to fly down trying to eat from the painted vine. [2]

RealityEngine is a high-performance graphics computer which was manufactured by Silicon Graphics Inc. in the last decade of the twentieth century AC. Optimized to generate real-time interactive photorealistic 3D graphics, it is used to create computer games and special effects for feature films and TV, to run scientific visualization models, and computer-aided design software. Last but not least, RealityEngine is routinely employed to run high-end VR environments — this latest achievement in West's struggle to outdo Zeuxis.

In terms of the images it can generate RealityEngine may not be superior to Zeuxis. Yet it can do other tricks, unavailable to the Greek painter. For instance, it allows the viewer to move around virtual grapes, touch them, lift them on a palm of a hand. And this ability of a viewer to interact with a representation may be as important in contributing to the overall reality effect as the images themselves. Which makes RealityEngine a formidable contender to Zeuxis.

In the twentieth century art has largely rejected the goal of illusionism, the goal which was so important to it before, and, as a consequence, it lost much of its popular support. The production of illusionistic representations became the domain of mass culture and of media technologies — photography, film and video. The creation of illusions was delegated to optical and electronic machines.

Today, everywhere, these machines are being replaced by new, digital illusion generators — computers. The production of all illusionistic images is becoming the sole province of PCs and Macs, Onyxes and RealityEngines. [3]

This massive replacement is one of the key economic factors which keeps the new media industries expanding. As a consequence, these industries are obsessed with visual illusionism. This obsession is particularly strong in the field of computer imaging and animation. Its annual SIGGRAPH conventions are the competition between Zeuxis and Parrhasius on the industrial scale: about 40,000 people gather on a trade floor around thousands of new hardware and software displays, all competing with each other to deliver the best illusionistic images. The industry frames each new technological advance in image acquisition and display in terms of the ability of computer technologies to catch up and surpass the visual fidelity of analog media technologies. On their side, animators and software engineers are perfecting the techniques for synthesizing photorealistic images of sets and human actors. The quest for a perfect simulation of reality drives the whole field of Virtual Reality (VR). In a different sense, the designers of human-computer interfaces are also concerned with illusion. Many of them believe that their main goal is to make the computer invisible, i.e. to construct an interface which is completely "natural". (In reality, what they usually mean by "natural" is simply older, already assimilated technologies, such as office stationary and furniture, a car, VCR controls, or a telephone.)

Although industry’s obsession with illusionism is not the sole factor responsible for making new media objects — games and CD-ROMs, computer-based films and virtual spaces — look the way they do, it is definitely one of the key. This article addresses different questions raised by the switch of all illusion-generation technologies to being computer-based. The first three sections sketch three different theoretical questions. In the last two sections, I consider in more detail two questions: traditional visual illusionism versus simulation of all aspects of reality in new media; and the effect of interactivity of reality effect.

 \\#\## 1. The Turn to Representation

A parallel can be established between the gradual turn of computer imaging towards representational and photorealistic (the industry term for synthetic images which look as though they were created using traditional photography or cinematography) images throughout the end of the 1970s — beginning of the 1980s and the similar turn towards representational painting and photography in the art world during the same period. [4] In the art world, we witness photorealism, neo-expressionism, "post-modern" "simulation" photography. In computer world, during the same period, we may note the rapid development of the key algorithms for photorealistic 3D image synthesis such as Phong shading, texture mapping, bump mapping, reflection mapping, and cast shadows; also the development of the first paint programs in the mid-1970s which allowed manual creation of representational images and eventually, towards the end of the 1980s, software such as Photoshop which, for a while, made a manipulated photograph the most common type of imagery created on a computer. In contrast, from the 1960s until the late 1970s computer imaging was mostly abstract because it was algorithm-driven and the technologies for inputting photographs into a computer were not easily accessible. [5] Similarly, art world was either dominated by non-representational movements, such as conceptual art, minimalism and performance, or at least was approaching representation with a strong sense of irony and distance, in the case of pop art. (Although it is possible to argue that 1980s "simulation" artists also used "appropriated" images ironically, in their case the distance between the media and artists’ images became visually very small or non-existent.)

### 2. Computer Image as a Meeting of Human Intelligence and Alien Intelligence

In the twentieth century, a very particular looking image created by still photography and cinematography came to dominate modern visual culture. Some of its qualities are linear perspective, depth of field effect (so only a part of 3D space is in focus), particular tonal and color range, and motion blur (rapidly moving objects appear smudged). Considerable research had to be accomplished before it became possible to simulate all these visual artifacts with computers. And even armed with special software, the designer still has to spend significant time manually recreating the look of photography or film. In other words, computer software does not produce such images by default. The paradox of digital visual culture is while all imaging is shifting towards being computer-based, the dominance of photographic and cinematic-looking images is becoming even stronger. But rather than being a direct, "natural" result of photo and film technology, these images are constructed on computers. 3D virtual worlds are subjected to depth of field and motion blur algorithms; digital video is run through the special filters which simulate film grain; and so on.

While visually, these computer-generated or filtered images are indistinguishable from traditional photo and film images, on the level of "material," they are quite different as they are made from pixels or represented by mathematical equations and algorithms. In terms of the kinds of operations which can be performed on them, they are also quite different from images of photography and film. These operations, such as "copy and paste," "add," "multiply," "compress," "filter," reflect first of all the logic of computer algorithms and of human-computer interface; only secondly they refer to the dimensions inherently meaningful to human perception. In fact, we can think of these operations as well as Human-Computer Interface (HCI) in general as balancing between the two poles of computer logic and human logic, by which I mean the everyday ways of perception, cognition, causality and motivation — in short, human everyday existence. Or, to use the terms of the present exhibition, we can say HCI balances between two logics — that of human intelligence and alien intelligence of a computer.

Other aspects of the new logic of computer images can be derived from the general principles of new media: many operations involved in their synthesis and editing are automated; they typically exist in many versions; they include hyperlinks; they act as interactive interfaces (thus an image is something we expect to enter rather than to stay on its surface); and so on. [6] To summarize, the visual culture of a computer age is cinematographic in its appearance, digital on the level of its material, and computational (i.e., software driven) in its logic. What are the interactions between these three levels? Can we expect that cinematographic images (I use this phrase here to include images of both traditional analog and computer-simulated cinematography and photography) will be at some point replaced by some very different images whose appearance will be more in tune with their underlying computer-based logic?

My own feeling is that the answer to this question is no. Cinematographic images are very efficient for cultural communication. Since they share many qualities with natural perception, they are easily processed by the brain. Their similarity to "the real thing" allows the designers to provoke emotions in viewers, as well as effectively visualize non-existent objects and scenes. And since computer representation turns these images into numerically coded data which is discrete (pixels) and modular (layers), they become subject to all economically beneficial effects of computerization: algorithmic manipulation, automation, variability and so on. A digitally-coded cinematographic image thus has two identities, so to speak: one satisfies the demands of human communication, another makes it suitable for computer-based practices of production and distribution.

### 3. The Challenge of Simulation

The available theories and histories of illusion in art and media, from Gombrich’s Art and Illusion and Andre Bazin’s "The Myth of Total Cinema" to Stephen Bann’s The True Vine, only deal with the visual dimensions. [7] In my view, most of these theories have three arguments in common. These arguments concern three different relationships, respectively: between an image and physical reality (1); between an image and natural perception (2); between present and past images (3):

(1) illusionistic images share some features with the represented physical reality (for instance, the number of an object’s angles); 

(2) Illusionistic images share some features with human vision (for instance, linear perspective); 

(3) each period offers some new "features" which are perceived by audiences as "improvement" over the previous period (for instance, the evolution of cinema from silent to sound to color). [8]

Until the arrival of computer media these theories were sufficient since the human desire to simulate reality indeed focused on its visual appearance (although not exclusively — think, for instance, of the tradition of automata). Today, while still useful, the traditional analysis of visual illusionism needs to be supplemented by new theories. The reason is that the reality effect in many areas of new media such only partially depends on image’s appearance. Such areas of new media as computer games, motion simulators, virtual worlds, and VR, in particular, exemplify how computer-based illusionism functions differently. Rather than utilizing the single dimension of visual fidelity, they construct the reality effect on a number of dimensions, of which visual fidelity is just one. These new dimensions include active bodily engagement with a virtual world (for instance, the user of VR moves the whole body); the involvement of other senses besides vision (spatialized audio in virtual worlds and games; use of touch in VR; joysticks with force feedback; special vibrating and moving chairs for computer games play and motion rides), and the accuracy of the simulation of physical objects, natural phenomena, anthropomorphic characters and humans. 

This last dimension, in particular, calls for an extensive analysis, because of the variety of methods and subjects of simulation. If the history of illusionism in art and media largely revolves around the simulation of how things look, for computer simulation this is one goal among many. Besides their visual appearance, simulation in new media aims to realistically model how objects and humans act, react, move, grow, evolve, think and feel. Physically-based modeling is used to simulate the behavior of inanimate objects and their interactions such as a ball bouncing off the floor or a glass being shattered. Computer games extensively use physical modeling to simulate collisions between objects and vehicle behavior — for instance, a car being bounced against the walls of the racing tracks, or behavior of a plane in a flight simulation. Other methods such as AL (Artificial Life), formal grammars, fractal geometry and various applications of the complexity theory (popularly referred to as "chaos theory") are used to simulate natural phenomena such as such as waterfalls and ocean waves, and animal behavior (flocking birds, school of fish). Yet another important area of simulation which also relies on many different methods is virtual characters and avatars, extensively used in movies, games, virtual worlds and human-computer interfaces. The examples are enemies and monsters in Quake; army units in WarCraft and similar games; human-like creatures in Creatures and other AL games and toys; and anthropomorphic interfaces such as Microsoft Office Assistant in Windows 98 — an animated character which periodically pops out in a small window offering help and tips. The goal of human simulation in itself can be further broken into a set of various sub-goals: simulation of human psychological states, human behavior, motivations, and emotions. (Thus, ultimately, the fully "realistic" simulation of a human being requires not only completely fulfilling the vision of the original AI (Artificial Intelligence) paradigm but also going beyond it — since original AI was solely aimed at simulating human perception and thinking processes but not emotions and motivations.) Yet another kind of simulation involve modeling the dynamic behavior over time of whole systems composed from organic and/or non-organic elements (for instance, popular series of Sim games such as SimCity or SimAnts which simulate a city and an ant colony, respectively)

And even on the visual dimension — the one dimension which new media "reality engines" share with the traditional illusionistic techniques — things work very differently. New media changes our concept of what an image is — because it turns a viewer into an active user. As a result, an illusionistic image is no longer something a subject simply looks at, comparing it with her memories of represented reality in order to judge the reality effect of this image. The new media image is something the user actively goes into, zooming in or clicking on individual parts with the assumption that they contain hyperlinks (for instance, imagemaps in Web sites). Moreover, new media turns most images into image-interfaces and image-instruments. Image becomes interactive, i.e. it now functions as an interface between a user and a computer or other devices. The user employs Image-interface to control a computer, asking it to zoom into the image or display another one, start a software application, connect to the Internet, and so on. The user employs image-instruments to directly affect reality:, move a robotic arm in a remote location, fire a missile, change the speed of the car and set the temperature, and so on. To evoke the term often used in film theory, new media moves us from identification to action. What kinds of actions can be performed via an image, how easily they can be accomplished, their range — all this plays part in user’s assessment of the reality effect of the image. 

### 4. Alien Vision: Jurassic Park and Socialist Realism

Consider the film which played a key role in Hollywood's acceptance of computer simulation in the early 1990s: George Lucas’s Jurassic Park. This triumph of computer simulation took more than two years of work by dozens of designers, animators, and programmers of Industrial Light and Magic (ILM), one of the premier companies specializing in the production of computer animation for feature films in the world today. Because a few seconds of computer animation often requires months and months of work, only the huge budget of a Hollywood blockbuster could pay for such extensive and highly detailed computer-generated scenes as seen in Jurassic Park. Most of the 3-D computer animation produced today has a much lower degree of photorealism and this photorealism is uneven, higher for some kinds of objects and lower for others. And even for ILM photorealistic simulation of human beings, the ultimate goal of computer animation, still remains impossible. (Some scenes in 1997 Titanic feature hundreds of synthetic human figures, yet they appear for a few seconds and are quite small, being far away from the camera.)

Typical images produced with 3-D computer graphics still appear unnaturally clean, sharp, and geometric looking. Their limitations especially stand out when juxtaposed with a normal photograph. Thus one of the landmark achievements of Jurassic Park was the seamless integration of film footage of real scenes with computer-simulated objects. To achieve this integration, computer-generated images had to be degraded; their perfection had to be diluted to match the imperfection of film's graininess.

First, the animators needed to figure out the resolution at which to render computer graphics elements. If the resolution were too high, the computer image would have more detail than the film image and its artificiality would become apparent. Just as Medieval masters guarded their painting secrets now leading computer graphics companies carefully guard the resolution of image they simulate.

Once computer-generated images are combined with film images additional tricks are used to diminish their perfection. With the help of special algorithms, the straight edges of computer-generated objects are softened. Barely visible noise is added to the overall image to blend computer and film elements. Sometimes, as in the final battle between the two protagonists in Terminator 2, the scene is staged in a particular location (in this example, a smoky factory) which justifies addition of smoke or fog to further blend the film and synthetic elements together.

So, while we normally think that synthetic photographs produced with computer graphics are inferior to real photographs, in fact, they are too perfect. But beyond that we can also say that paradoxically they are also too real.

The synthetic image is free of the limitations of both human and camera vision. It can have unlimited resolution and an unlimited level of detail. It is free of the depth-of-field effect, this inevitable consequence of the lens, so everything is in focus. It is also free of grain — the layer of noise created by film stock and by human perception. Its colors are more saturated and its sharp lines follow the economy of geometry. From the point of view of human vision it is hyperreal. And yet, it is completely realistic. It is simply a result of a different, more perfect than human, vision.

Whose vision is it? It is the vision of a cyborg or a computer; a vision of Robocop and of an automatic missile. It is a realistic representation of human vision in the future when it will be augmented by computer graphics and cleansed from noise. It is the vision of a digital grid. Synthetic computer-generated image is not an inferior representation of our reality, but a realistic representation of a different reality.

By the same logic, we should not consider clean, skinless, too flexible, and in the same time too jerky, human figures in 3-D computer animation as unrealistic, as imperfect approximation to the real thing — our bodies. They are perfectly realistic representations of a cyborg body yet to come, of a world reduced to geometry, where efficient representation via a geometric model becomes the basis of reality. The synthetic image simply represents the future. In other words, if a traditional photograph always points to the past event, a synthetic photograph points to the future event.

Is this a totally new situation? Was there already an aesthetics which consistently pointed to the future? In order to help us locate these aesthetics historically, I will invoke a painting by Russian-born conceptual artists Komar and Melamud. Called "Bolsheviks Returning Home after a Demonstration" (1981-1982), it depicts two workers, one carrying a red flag, who came across a tiny dinosaur, smaller than a human hand, standing in the snow. Part of "Nostalgic Socialist Realism" series, this painting was created a few years after the painters arrived to the United States, well before Hollywood embraced computer-generated visuals. Yet it seems to comment on such movies as Jurassic Park and on Hollywood as a whole, connecting its fictions with the fictions of Soviet history as depicted by Socialist Realism, the official style of Soviet art from the early 1930s until the late 1950s.

Taking the hint from this panting, we are now in a position to characterize the aesthetics of Jurassic Park. This aesthetic is one of Soviet Socialist Realism. Socialist Realism wanted to show the future in the present by projecting the perfect world of future socialist society on a visual reality familiar to the viewer — streets, interiors, and faces of Russia in the middle of the twentieth century — tired and underfed, scared and exhausted from fear, unkempt and gray. Socialist realism had to retain enough of then everyday reality while showing how that reality would look in the future when everyone's body will be healthy and muscular, every street modern, every face transformed by the spirituality of communist ideology. This is its difference from pure science fiction which does not have to carry any feature of today's reality into the future. In contrast, Socialist realism had to superimpose future into the present, projecting the Communist ideal into the very different reality familiar to the viewers. Importantly, Socialist Realism never depicted this future directly: there is not a single Socialist Realist work of art set in the future. Science fiction as a genre did not exist from the early 1930s until Stalin’s death. The idea was not to make the workers dream about the perfect future closing their eyes to imperfect reality, but rather to make them see the signs of this future in the reality around them. This is one of the meanings behind Vertov’s notion of "communist decoding of the world". To decode the world in such a way means to recognize the future all around you.

The same superimposition of future onto the present happens in Jurassic Park. It tries to show the future of sight itself — the perfect cyborg vision which is free of noise and capable of grasping infinite details. This vision is exemplified by the original computer graphics images before they were blended with film images. But just as Socialist Realist paintings blended the perfect future with the imperfect reality, Jurassic Park blends the future super-vision of computer graphics with the familiar vision of film image. In Jurassic Park, the computer image bends down before the film image, its perfection is undermined by every possible means and is also masked by the film's content. As I already described, computer-generated images, originally clean and sharp, free of focus and grain, are degraded in a variety of ways: resolution is reduced, edges are softened, depth of field and grain effect are artificially added. Additionally, the very content of the film — the prehistoric dinosaurs which came to life — can be interpreted as another way to mask the potentially disturbing reference to our cyborg future. The dinosaurs are present to tell us that computer images belong safely to the past long gone — even though we have every reason to believe that they are messengers from the future still to come.

In that respect, Jurassic Park and Terminator 2 are the opposites. If in Jurassic Park the dinosaurs function to convince us that computer imagery belongs to the past, the Terminator in Terminator 2 is more "honest". He himself is a messenger from the future. Accordingly, he is a cyborg who can take on the human appearance. His true form is that of a futuristic alloy. In perfect correspondence with this logic, this form is represented with computer graphics. While his true body perfectly reflects its surrounding reality, the very nature of these reflections shows to us the future of human and machine sight. The reflections are extra-sharp and clean, without any blur. This is indeed the look produced by the reflection mapping algorithm, one of the standard techniques to achieve photorealism. Thus, to represent the Terminator who came from the future the designers used the standard computer graphics techniques without degrading them; in contrast, in Jurassic Park, the dinosaurs which came from the past were created by systematically degrading computer images. What of course is the past in this movie is the film medium itself: its grain, its depth of focus, its motion blur, its low resolution.

This is, then, the paradox of computer illusionism. The images of 3-D photorealistic computer animation are not inferior to the visual realism of traditional photography. They are perfectly real — all too real. 

### 5. Illusion and Interactivity

Having analyzed computer illusionism from the points of view of its production and the long history of visual illusion, I now want to look at it from a different perspective. While the existing theories of illusionism assume that the subject acts strictly a viewer, the new media more often than not turns the subject into the user — an actant. The subject is expected to interact with a representation: click on menus or the image itself, making selections and decisions. What effect does interactivity has on reality effect of an image? Is the fidelity of simulation of physical laws or human motivation more important for "realism" of a representation than its purely visual qualities? For instance, does a racing game which uses a more precise collision model but poor visuals feel more real than the game which has richer images but less precise model? Or do the simulation dimensions and visual dimensions support each other, adding up to create a total effect?

In this section, I will focus on a particular aspect of this more general question: production of illusionism in interactive computer objects. The aspect which I will consider has to do with time. Web sites, virtual worlds, computer games, and many other types of hypermedia applications are characterized by a peculiar temporal dynamic: constant, repetitive shifts between an illusion and its suspense. These new media objects keep reminding us about their artificiality, incompleteness, and constructedness. They present us with a perfect illusion only to reveal the underlying machinery next.

Web surfing in the 1990s provides a perfect example. A typical user may be spending equal time looking at a page and waiting for the next page to download. During waiting periods, the act of communication itself — bits traveling through the network — becomes the message. The user keeps checking whether the connection is being made, glancing back and forth between the animated icon and the status bar. Using Roman Jakobson's model of communication functions, we can say that communication comes to be dominated by contact, or phatic function — it is centered around the physical channel and the very act of connection between the addresser and the addressee. [9]

Jakobson writes about verbal communication between two people who, in order to check whether the channel works, address each other: "Do you hear me?," "Do you understand me?" But in Web communication, there is no human addresser, only a machine. So as the user keeps checking whether the information is coming, she actually addresses the machine itself. Or rather, the machine addresses the user. The machine reveals itself, it reminds the user of its existence — not only because the user is forced to wait but also because she is forced to witness how the message is being constructed over time. A page fills in part by part, top to bottom; text comes before images; images arrive in low resolution and are gradually refined. Finally, everything comes together in a smooth sleek image — the image which will be destroyed with the next click.

Interaction with most 3D virtual worlds is characterized by the same temporal dynamic. Consider the technique called "distancing" or "level of detail," which for years has been used in VR simulations and later was adapted to 3D games and VRML scenes. The idea is to render the models more crudely when the user is moving through virtual space; when the user stops, details gradually fill in. Another variation of the same technique involves creating a number of models of the same object, each with progressively less detail. When the virtual camera is close to an object, a highly detailed model is used; if the object is far away, a lesser detailed version is substituted to save unnecessary computation. 

A virtual world which incorporates these techniques has a fluid ontology that is affected by the actions of the user. As the user navigates through space the objects switch back and forth between pale blueprints and fully fleshed-out illusions. The immobility of a subject guarantees a complete illusion; the slightest movement destroys it.

Navigating a QuickTime VR movie is characterized by a similar dynamic. In contrast to the nineteenth-century panorama that it closely emulates, QuickTime VR continuously deconstructs its own illusion. The moment you begin to pan through the scene, the image becomes jagged. And, if you try to zoom into the image, all you get are oversized pixels. The representational machine keeps hiding and revealing itself.

Compare this dynamic to traditional cinema or realist theater which aims at all costs to maintain the continuity of the illusion for the duration of the performance. In contrast to such totalizing realism, new media aesthetics has a surprising affinity to twentieth-century leftist avant-garde aesthetics. Playwright Bertold Brecht's strategy to reveal the conditions of an illusion's production, echoed by countless other leftist artists, has become embedded in hardware and software themselves. Similarly, Walter Benjamin's concept of "perception in the state of distraction" [10] has found a perfect realization. The periodic reappearance of the machinery, the continuous presence of the communication channel in the message prevent the subject from falling into the dream world of illusion for very long, making her alternate between concentration and detachment.

While virtual machinery itself already acts as an avant-garde director, the designers of interactive media, such as games, DVD titles, interactive cinema, and interactive television programs, often consciously attempt to structure the subject's temporal experience as a series of periodic shifts. The subject is forced to oscillate between the roles of viewer and user, shifting between perceiving and acting, between following the story and actively participating in it. During one segment the computer screen presents the viewer with an engaging cinematic narrative. Suddenly the image freezes, menus and icons appear and the viewer is forced to act: make choices; click; push buttons. The purest example of such cyclical organization of user’s experience is the computer games which alternate between FMV (full motion video) segments and the segments which require user’s input, such as Wing Commander series. Moscow media theorist Anatoly Prokhorov described these shifts in terms of two different identities of a computer screen: transparent and opaque. The screen keeps shifting from being transparent to being opaque — from a window into a fictional 3D universe to a solid surface, full of menus, controls, text and icons. [11] Three-dimensional space becomes a surface; a photograph becomes a diagram; a character becomes an icon. We can say that the screen keeps alternates between the dimensions of representation and control. What at one moment was a fictional universe becomes a set of buttons which demand action.

The effect of these shifts on the subject is hardly one of liberation and enlightenment. While modernist avant-garde theater and film directors deliberately highlighted machinery and conventions involved in producing and keeping the illusion in their works — for instance, having actors directly address the audience or pulling away the camera to show the crew and the set — the systematic "auto-deconstruction" performed by computer objects, applications, interfaces and hardware do not seem to distract the user from giving in to the reality effect. The cyclical shifts between illusion and its destruction appear to neither distract from it nor support it. It is tempting to compare these temporal shifts to shot / counter-shot structure in cinema and to understand them as a new kind of suturing mechanism. By having periodically to complete the interactive text through active participation the subject is interpolated in it. Thus, if we adopt the notion of suture, it would follow that the periodic shifts between illusion and its suspension are necessary to fully involve the subject in the illusion. [12]

Yet clearly we are dealing with something which goes beyond old-style realism of analog era. We can call this new realism meta-realism since it incorporates its own critique inside itself. Its emergence can be related to a larger cultural change. Old realism corresponded to the functioning of ideology during modernity: totalization of a semiotic field, "false consciousness," complete illusion. But today ideology functions differently: it continuously and skillfully deconstructs itself, presenting the subject with countless "scandals" and "investigations". The leaders of the middle of the twentieth century were presented as invincible; as being always right, and, in the case of Stalin and Hitler, as true saints not capable of any human sin. Today we expect to learn about the scandals involving our leaders, and these scandals do not really diminish their credibility. Similarly, contemporary television commercials often make fun of themselves and advertising in general; this does not prevent them from selling whatever they are designed to sell. Auto-critique, scandal, revelation of its machinery became a new structural component of modern ideology: witness the 1998 episode when MTV created an illusion on its Web site that somebody hacked it. The ideology does not demand that the subject blindly believe it, as it did early in the twentieth century; rather, it puts the subject in a master position of somebody who knows very well that she is being fooled, and generously lets her be fooled. You know, for instance, that creating a unique identity through a commercially mass-produced style is meaningless — but anyway you buy the expensively styled clothes, choosing from a menu: "military," "bohemian," "flower child," "inner city, " clubbing," and so on. The periodic shifts between illusion and its suspension in interactive media, described here, can be seen as another example of the same general phenomenon. Just as classical ideology, classical realism demanded that the subject completely accepted the illusion for as along as it lasted. In contrast, the new meta-realism is based on oscillation between illusion and its destruction, between immersing a viewer in illusion and directly addressing her. In fact, the user is even put in much stronger position of mastery when she ever is by "auto-deconstructing" commercials, newspaper reports of "scandals" and other traditional non-interactive media. Once illusion stops, the user can make choices, re-direct game narrative or get additional information from other Web sites conveniently linked by the designers. The user invests into illusion precisely because she is given control over it.

If this analysis is correct, the counter-arguments that this oscillation is simply an artifact of the current technology and that the advances in hardware will eliminate it, would not work. The oscillation analyzed here is not an artifact of computer technology but a structural feature of modern society, present not just in interactive media but in numerous other social realms and on many different levels.

This may explain the popularity of this particular temporal dynamics in interactive media, but it does not address another question: does it work aesthetically? Can Brett and Hollywood be married? Is it possible to create a new temporal aesthetics, even a language, based on cyclical shifts between perception and action? In my view, the most successful example of such an aesthetics already in existence is a military simulator, the only mature form of interactive narrative. It perfectly blends perception and action, cinematic realism and computer menus. The screen presents the subject with an illusionistic virtual world while periodically demanding quick actions: shooting at the enemy; changing the direction of a vehicle; and so on. In this art form, the roles of a viewer and an actant are blended perfectly — but there is a price to pay. The narrative is organized around a single and clearly defined goal: staying alive.

The games modeled after simulators — first of all, first person shooters such as Doom, Quake and Tomb Raider, but also flight and racing simulators — have been also quite successful. In contrast to interactive narratives such as Wing Commander, Myst, Riven, or Bad Day on the Midway which are based on temporal oscillation between two distinct states, non-interactive movie-like presentation and interactive gameplay, in these games these two states — which are also two states of the subject (perception and action) and the two states of a screen (transparent and opaque) — co-exist together. As you run through the corridors shooting at enemies or control the car on the racetrack, you also keep your eyes on the readouts which tell about the "health" of your character, the damage level of your vehicle, the availability of ammunition, and so on.

As a conclusion, I would like to offer a different interpretation of the temporal oscillation in new media which will relate it not to the social realm outside of new media but to other similar effects specific to new media itself. The oscillation between illusionary segments and interactive segments forces the user to switch between different mental sets — different kinds of cognitive activity. These switches are typical of using modern computer use in general. The user analyses the quantitative data; next, she is using a search engine; next, she starts a new application; next, she navigates through space in a computer game; next, she may go back to using a search engine; and so on. In fact, the modern HCI which allows the user to run a number of programs at the same time and to keep a number of windows open on the screen at once posits multi-tasking as the social and cognitive norm. This multi-tasking demands from the user "cognitive multi-tasking" — rapidly alternating between different kinds of attention, problem-solving and other cognitive skills. All in all, modern computing requires from a user intellectual problem solving, systematic experimentation and the quick learning of new tasks. Thus, just as any particular software application is embedded, both metaphorically and literally, within the larger framework of the operating system, new media embeds cinema-style illusions within the larger framework of an interactive control surface. Illusion is subordinated to action; depth to the surface; a window into an imaginary universe to a control panel. From commanding a dark movie theater, this twentieth-century illusion and therapy machine par excellence, a cinema image becomes just a small window on a computer screen; one stream among many others coming to us through the network; one file among numerous others on our hard drives.

 \\#\# References:

[1] This article uses some material which, in earlier form, appeared in my articles "Paradoxes of Digital Photography," in Photography After Photography, edited by v. Amelunxen, Stefan Iglhaut, Florian Rötzer, 58-66. Münhen: Verlag der Kunst, 1995; and "The Aesthetics of Virtual Worlds: Report from Los Angeles," in Digital Delirium, edited by Arthur and Marilouise Kroker (New York: St. Martin's Press, 1997.

[2] For a detailed analysis of this story, see Stephen Bann, The True Vine. On Western Representation and the Western Tradition (Cambridge: Cambridge University Press, 1989).

[3] Onyx is a faster version or RealityEngine which was also manufactured by Silicon Graphics. See [http://www.sgi.com](http://www.sgi.com/).

[4] I am grateful to Peter Lunenfeld for pointing out this connection to me.

[5] For a good overview of the early history of computer art which includes the discussion of the "turn to illusionism," see Frank Dietrich, "Visual Intelligence: The First Decade of Computer Art," in Computer Graphics, 1985.

[6] I discuss these general principles in my article "New Media: a User's Guide" in NET.CONDITION (ZKM / Zentrum für Kunst und Medientechnologie Karlsruhe and The MIT Press), forthcoming. 

[7] Andre Bazin, What is Cinema? (Berkeley: University of California Press, 1967-71); Stephen Bann, The True Vine: on Visual Representation and the Western Tradition (Cambridge, England, and New York: Cambridge University Press, 1989). 

[8] On the history of illusionism in cinema, see the influential theoretical analysis by Jean-Louis Comolli, "Machines of the Visible, The Cinematic Apparatus, edited by Teresa De Lauretis and Steven Health (New York: St. Martin Press), 1980. 

[9] See Roman Jakobson, "Closing Statement: Linguistics and Poetics," in Style In Language, ed. Thomas Sebeok (Cambridge, Mass.: The MIT Press, 1960). 

[10] Walter Benjamin, "The Work of Art in the Age of Mechanical Reproduction," in Illuminations, ed. Hannah Arendt (New York: Schochen Books, 1969).

[11] Private communication, September 1995, St. Petersburg.

[12] On theories of a suture in relation to cinema, see chapter 5 of Kaja Silverman, The Subject of Semiotics (New York: Oxford University Press, 1983).

---

# Macro-media and Micro-media

_author: Lev Manovich_
_year: 2000_

Web users and producers, especially in the commercial sector, have focused much attention on “broadband” media, a term widely used to describe the ability to access “television quality” video over the Internet. But what will happen when this goal is met, and the current TV look is recreated on the Web? What becomes the next frontier in the evolution of media? Below I explore one scenario: that media will move from “broadband” to _macro-media_.

Media technologies seem typically to move in one direction: toward “more.” More resolution, better color, better visual fidelity, more bandwidth, more immersion. Do digital media technologies simply mimic this pattern? After examining macro-media, I look at another important trajectory in media development: minimalist media or _micro-media_.

## "Television quality"

When I read that the new RealSystem 8 (RS8) from Real Networks can finally deliver streams at full “video quality,” I immediately rushed to their Web site and downloaded the new app. The sample clips posted on RealNetworks did look amazingly sharp and smooth, at least in comparison to what was available on the Internet until now. However, I could not systematically test the company’s claims that the new system offers video at “VHS quality” – because no site was offering video encoded at a high enough rate. The RealNetworks site had just a single sample clip encoded at 1 Mbps. 400 Kbps was all I could find elsewhere.

Given that just five years ago we were amazed to get postage-stamp Quick Time clips, the new RealNetworks technology is indeed a remarkable achievement. But the situation is laden with irony: the Internet was never designed to deliver this type of video stream and yet, for more than fifty years, we have had a system that is capable of delivering a “VHS quality” stream. Even more remarkable, this system is wireless. And, in contrast to today’s streaming video, it never experienced “network time-out”, “rebuffering” and other artifacts of the Internet environment. The video stream never stalled; the image never skipped frames. This system is broadcast television.

I don’t want to undermine the accomplishments of RealNetworks and other companies working hard to deliver video over the Web. However, in our excitement over the ability to duplicate television-quality video on a non-television medium, we may be forgetting something very important: what is new and exciting about the Web is that it is _not_ television.

Although it can deliver a “perfect” video stream, television technology can only handle a dozen of channels at a time. In contrast, a Web user has access to an unlimited number of video streams. One recent estimate puts the current number of streams at 30 million. The count will have grown by the time you read this column. The user can search for these streams using search engines; she can play them at various sizes; she can play a multiple of streams at once. Using languages such as SMIL, the designers can arrange streaming video clips into a dynamic hypermedia program that includes still graphics, text, animation, and other media types in addition to video. Using formats such as QuickTime 4 from Apple, the designers can also embed hyperlinks into particular frames of a video stream, so when a user clicks on a frame or on a particular object within it, a movie launches a Web page. And they can also use software such as VideoLogger to automatically index video streams, identifying various speakers, spoken words, pans, zooms, and other changes in visual content. The user can then search video using this index just as we commonly search text files for particular words. In short, while the Web video may finally look like broadcast video, what is truly important is that it has very different properties and capabilities from it. But even if we forget about these unique qualities and simply focus on the issue of video resolution, “VHS quality” is not the final frontier in the evolution of the Web media.

For a couple of years now, “broadband” was the hot term in the Internet world, fueling the imagination of companies developing network infrastructure, media-enabling software and the new “rich media” content itself. While some take this term simply to mean simply dynamic media featuring content in Shockwave, Flash, and other interactive media formats, most equate “broadband” media with “television-quality” video and audio. To receive such media over the Internet requires a high bandwidth connection (cable modem, DSL, or even “broader” hardware) and special software, such as Real Network’s Real Video, Apple’s QuickTime or Microsoft’s Media Player.

But let us look at a near future when a typical user has enough bandwidth to receive “VHS-quality” video over the Web. What will happen next? Where will Web media go after this benchmark is reached? Let me suggest two directions among other possibilities) which I will call _macro-media_ and _micro-media_. Here I will discuss _macro-media_. I will address _micro-media_ in the next section.

## Macro-media

We will reach _macro-media_ when we have such high bandwidth connections that the whole issue of bandwidth goes away. We will simply not think about it anymore. It would also involve very high-resolution displays: not just the 1024 by 768 or 1280 by 1024 common for computer displays or the 1080 standard for DTV (Digital Television), but 4K, 8K and beyond. These displays will most probably not be desktop-based but would fill the walls of our homes, offices, and other spaces.

The logic of technology development is such that we will get there in a not-too-distant future. If we already gone from the initial 512 by 384 resolution of the first Macs fifteen years ago to 2K resolution available today on computer monitors, why would technology ever stop? If we have gone from 300 kB/s to T1 in Internet access speed, it is only a matter of time before communication bandwidth is no longer an issue.

But what is the unique advantage of delivering television and high-quality video to a platform with unlimited bandwidth, unlimited resolution, and unlimited storage space? The delivery of television- and film-quality video – even old CinemaScope films - to such ultra-high-resolution displays will still leave empty space for more data. This space can then be filled by other video streams or by other kinds of media.

One possible utilization of this space could be new kinds of shows and films in which not just one, but a number of frames coexist on the screen to follow the narrative. (Mike Figgis’ recent film _Timecode_, which uses four frames, is one example of this aesthetic at work today.) For instance, we can follow the activities of different characters simultaneously; different frames can also be used to display past events along with the main action; and so on. (Interestingly, Digital Television technology allows DTV monitors to receive a number of programs simultaneously. This may also encourage producers to adopt a multi-frame aesthetic.)

The second avenue of space allocation - surrounding the video image with other media - already exists both on television and on the Web. Television news and especially financial shows run market tickers, still pictures, graphics, and other dynamic displays alongside the video image. (Probably the most “visually aggressive” example of such aesthetics today can be found on Bloomberg TV.)

Similarly, on the Web, many streaming media sites surround small video frames with other media types: blocks of text copy, lists of hyperlinks, still images and animation. (For typical examples, check videos on the abcnews.com website and its RealVideo Channel. Streaming channels in particular have adopted the practice of embedding a small video image within a larger multi-media composition.)

Rather than being reserved for particular kinds of programs such as news, in the era of _macro-media_ such aesthetics may become the default condition for all programs, including fiction films. The future films and soap operas may look more like Bloomberg TV and less like _Gone with The Wind_.

To return to the present, it is ironic that new media companies have focused on the “television quality” issue at exactly the time when television itself is finally leaving behind the analog standard set half a century ago in order to become digital, thus embracing both a higher resolution and a new digital logic. Let us hope that television quality video is not the final stage in the development of Web video. Hyperlinks, automatic indexing, search, multiple resolutions, and multiple frames – these new dimensions of digital video are waiting to be fully explored!

## Micro-media

My Ericsson T28 cell phone comes with two built-in games: Solitaire and Tetris. Skeptical at first, I tried Tetris and found it to be quite playable. In Japan, which so far leads the US in the use of net-enabled phones, tens of thousands of users download simple computer games and cartoons onto their phones. But in this age of mega-pixeled screens, why would people want to play games on a tiny phone screen?

The world of new media appears to move only in one direction: more of everything. Every year, CPUs run at higher speed. Computer display resolutions increase as does the bandwidth with which you connect to the web, the size of the hard drive in PCs, and the amount of RAM that it comes with. The Internet has moved from being a text-only medium to being a multi-media in the 1990s, with each new platform offering more detail, faster frame rates, and more life-like characters.

This constant movement toward “more realism,” or higher fidelity, is not unique to new media; it can also be traced through the history of old media. For instance, cinema develops from the low-resolution black-and-white images in the 1890s toward sound and then color and then (if you accept VR and games as the next stage of cinema) to interactivity. Similarly, television progressed from just a dozen scan lines in the earlier decades of the twentieth century to the present-day digital standard of over 1,000 lines of resolution.

This trajectory towards more does not seem to leave room for any other development. Yet the history of digital media contains another kind of trajectory. While some media forms get richer, others stay purposefully “poorer.” A more minimalist kind of media, characterized by low resolution, low fidelity, and slow speeds, is born. I call it _micro-media_. Despite the continuous evolution of computer and telecommunication technologies, _micro-media_ is remarkably stable. It just keeps moving from platform to platform, from one technology to another. In fact, given the current prognosis that by 2003-2004 more users worldwide will access the Internet through cell phones than through computers, _micro-media_ seems to be gaining more ground than ever. It will not only successfully compete with _macro-media_ but may even overtake it in popularity.

The fact that companies are now rushing to deliver entertainment to cell phone screens – and that users are bound to enjoy these services – may appear less strange when we realize that the resolution of these tiny screens is not that different from the resolution of video console of the 1970s and the 1980s which, after all, enjoyed huge popularity. So, once you think of your cell phone as an old game console that has just been miniaturized, the idea of delivering games, movies, and other forms of entertainment to its screen makes more sense. This is an example of _micro-media_ at work: having been wiped out by technological evolution on one platform, ultra-low-resolution computer games return at a later time, on another platform.

Here are some examples of _micro-media_ migrations. Consider the history of 3D computer-generated virtual worlds. In the early 1980s, the slow speed of computers did not allow computer animators to render anything more complex than cartoon-looking 3D spaces consisting of flatly shaded surfaces. Today a consumer PC is fast enough to render Hollywood quality virtual worlds. But the hardware limitations of computers used to render virtual worlds in the 1980s have returned in another place: the Internet-based real-time virtual worlds. What used to be the slow speed of CPUs became the slow bandwidth. As a result, the 1990s VRML (Virtual Reality Modeling Language) worlds looked like the pre-rendered animations done ten years earlier.

The same logic can be observed in the history of Web protocols. The original HTML specification allowed text-based pages to include still images, but no other media. Gradually, the extensions of HTML and the development of special formats such as Real Video, ShockWave, Flash, and others turned static text-based Web pages into rich and dynamic multi-media experiences. At the same time, Wireless Application Protocol (WAP) was developed to strip down these rich multi-media pages into simple, low-resolution screens appropriate for delivery to Net-enabled phones. This is an example of _micro-media_ logic: a bare bones Web gradually became media rich, only to reappear once again in a different place in a “poorer” form than ever.

According to the so-called Moore’s Law, the logic density of silicon circuits (and thus the processing capacity of computer chips) doubles every eighteen months. Eventually, the law is supposed to stop working because the engineers will hit the limits of the physical organization of matter. What about _micro-media_? Is this a permanent phenomenon, or will it eventually disappear, with even the smallest displays offering high resolution and full color?

If the evolution of microchips toward being more and more dense is limited by the atomic organization of matter, the limiting factor in the evolution of media toward more and more visual fidelity may be limited by the size of our physical body. I love my Ericsson T28 cell phone, but I do find it to be too small! When I was in the store purchasing the phone, another customer walked in to buy a case for her T28 so that it would have more of a “presence” in her pocket or briefcase and so that it wouldn’t get lost. I followed her example and almost always carry my phone in a case.

Even if it may already be technologically possible to make a much smaller phone, its correspondingly more compact screen will be too small to be of any use. The technological race towards packing richer media experience into the tiniest of packages is limited by the size of our hands and the resolution of our eyes and ears. So, until some futuristic scenarios – i.e., projection glasses that shine the video image directly into the viewer’s retina or direct communication between the computer and the human brain – become reality, we are stuck with _micro-media_.

If ultra-high-resolution media, or _macro-media_, is one direction digital media will go after conquering the current frontier of “broadband” (i.e., “television quality”), _micro-media_ is here to stay as well – at least for the foreseeable future. And as computing, digital media, and Internet platforms move away from bulky desktop systems toward a multitude of small, hand-held devices – electronic organizers such as Palm Pilot and Pocket PC, Net-enabled cell phones, MP3 players, Gameboy players, and other appliances – _micro-media_ is quickly becoming more and more widespread. So, you better get used to playing Tetris or watching Survivor on your cell phone screen because this tiny screen is not going away.

---

# Information and Form: Electrolobby at Ars Electronica 2000

_author: Lev Manovich_
_year: 2000_

Form. Good Form. Ideal Form. Gestalt. Malevich’s abstract compositions, made of geometric primitives floating in outer space. Lissitzky’s Prouns, extending Malevich’s elements into the 3rd dimension in the anticipation of International Style solids, soon to populate every modern city. Mondrian’s grid-making procedure, cutting a rectangle in this or that way with a certainty of some industrial robot. Arp’s and Brancusi’s biomorphic shapes. Forms made of wires by Gabo, Smith, and others. Drawings carefully made by Gestalt psychologists to demonstrate human innate preference, or the need, to delineate “good form” in any pattern which comes into the visual field. Human: the form seeking animal?

The obsession with form in modern art is accompanied by its double – a fascination with “formless”: from Jean Dubuffet’s figures disappearing in the ground to Pollock’s network of drips to Lucio Fontana’s and Gordon Mata-Clark’s form-destroying gestures. [1] But just as iMac’s quirky brilliance only highlights Microsoft’s hegemony, this anti-form stream in modern art only emphasizes the hegemony of form makers whose objects now fill art museums around the world, from MOMA to Tate Modern. Switching off their cell phones, wireless modems, Palm Pilots and other information appliances at the entrance, the citizens of information society can enter these temples of good form to seek the temporary relieves from flows of data which rule their lives outside. There they can feast themselves on Mondrian’s rectangles, Malevich’s triangles, Arp’s and Moore’s disciplined curves, Eames’s chairs, Starck’s famous saucer, and even Pollock’s network of colorful drips. The latter may look at first like a computer network – yet, confined to the frame of a painting, its “pockets” fixed once and for all on canvas, it is a universe apart from a real telecommunication network which never stops to arrive at anything fixed, the bits of data in constant movement.

Information. A click by the user which fires of a server request which fires of a script which fires off an application which extracts some data from a database which is send to another script which formats it and ads a custom Flash animation, the whole thing served back to the user’s screen within a second, while the user already made another click to start another chain of processes. Sixteen millions lines of code which make up current Windows operating system, calling each other to satisfy user’s simple information craving, manifested as a taping on a computer keyboard. A commuter in a television ad accessing his stocks via WAP browser in his cell phone while glancing on an overhead display to see if his plane is already boarding and simultaneously checking the time on his watch; all of these displays constantly shifting (watch readout, airport display board, the information in WAP phone browser) as though in some elaborate nineteenth century ballroom dance. Streams of phone conversations, numerical data, pixels, and sound bits, floating together through a fiber optic channel, entering the gate of network router, to be split into numerous streams, only to reunite at the destination. Demonstrations, protests, or simply large parties, “self-organized” on the spot as participants call each other on cell phones, setting up a chain reaction as a result of which large groups of people gather in one place in half an hour. Jam sessions, “net parties,” and other forms of social networking activities organized around telecommunication and computer networks. The gatherings of net artists and net activists moving from one city to another; endless “projects” which always involve multiple sites and multiple participants. Rarely any of these activities result in something which can be called “good form” or “formless” or even leave behind any finished “art objects” except multi-page proposals and grant applications. And yet this does not mean that this is not genuine “culture” or “art” of our time. 

The contrast between form and information is one of the fundamental cultural dimensions which accompanies the shift from industrial to information society; or from modernism to what I would like to brand “informationalism”. [2] What search for good form was for modernism, information networking is for our own society. And if the first usually resulted in solid objects – geometric abstractions, sculptures and 3-D constructions, chairs and teapots, office skyscrapers and photographs – the second is by its very nature dynamic, never thickening into something solid and fixed.

And yet, as the word inFORMation itself implies, there is a hidden form-making impulse in information society. Or at least, we can say that information processes often leave material residues. Or to be more brutal but more honest, that information processes can be forced to leave material forms. Artistic networks made possible by Internet leave behind some kind of material activity: Web sites, written manifestos (or at least email postings), exhibition catalogs. And Web sites can be reduced to screen shots or listings of computer code, be it XML, CGI, or ASP.

Since modern art, modern aesthetic theory, the museum complex, and the capitalist economy at large are designed to deal with material objects rather than with immaterial information networks, our first automatic response can be to try to force information networks into material traces and objects. More challenging is to figure out how to represent, document, and ultimately support social networking as a genuine cultural practice in its own right; how to present in a museum or gallery setting information networks and processes while giving justice to their dynamic character; in short, the ways to _translate information into form_ which are intrinsic rather than alien to this information.

Following a few experiments where a contemporary art festival became a setting for a real-time social networking activity (such as Workspace at Documenta X, 1997), 2000 edition of Ars Electronica Festival presented _electrolobby_ - “a dedicated area inside the Ars Electronica Festival designed expressively for the net-inspired digital culture and lifestyle.” [3] Skillfully morphing between various speech genres of contemporary culture, Paris-based TNC network which organized _electrolobby_ introduced it as “a marketplace of opinions, projects, branded cultural commodities and their pirated bootlegs — a networked showroom where ideas are on display and communication is the coin… Genetic researchers meet experimental entertainers, food jockeys mingle with MP3 mixers, game designers kibbutz with concept engineers.” Following its I.P.O. (Initial Public Opening), _electrolobby_ ran for the whole duration of festival. I did not see any “food jockeys” in the program, but other announced residents indeed represented an exciting mix of net-inspired culture: Kodwo Eshun, the author of _More Brilliant Than The Sun_; Lincoln Stein who used Napster paradigm to create a program for publication of genome data; Eric Zimmerman, the author of super-addictive SISSY FIGHT 2000; and a dozen or so other personalities and groups, including the bad boys of the Net, the ever present _etoy_.

Did _electrolobby_ work at the end? Have its organizers succeeded in translating information into form? Like the net itself, _electrolobby_ attempted to combine various media paradigms: publishing (the festival catalog and the Web site features interviews with all the participants), Web-casting (a part of _electrolobby_ was reserved for a small Web-casting studio which broadcasted live over Internet daily interviews with the residents and other specials), and a club-like setting whose intention was to create “an atmosphere conducive to communication among participants, and to a playful process of dealing with information.” I am not sure that all these parts came together to form a new gestalt, however. Since _electrolobby_ was taking place alongside with many other activities of a festival, most booths reserved for the participants were always empty; obviously the participants were busy catching other festival offerings. And since _electrolobby_ area also featured a bunch of computers for email access, my sense is that checking and answering their email became more important for festival visitors than focusing on _electrolobby_ presentations. But it is also possible that to expect a form, a single gestalt to emerge here is to apply old logic to net culture. It is possible that ambient, peak-free atmosphere of _electrolobby_ – a few people talking in one corner; one group showing their project to another; no big openings or speeches but something always taking place; things happening in parallel and in small increments rather than in a linear succession and in big jumps – indeed translated the logic of the net into the right spatial-temporal modality. Yes, information can be translated into form, but this form itself is quite different from the old forms of art, be it Mondrian’s “good form” geometric primitives or Pollock’s “formless” drips. 

## References:

[1] See Yve-Alain Bois and Rosalind E. Krauss _Formless: A User's Guide_ (MIT Press, 1997).

[2] See [www.manovich.net/ia](www.manovich.net/ia).

[3] [http://electrolobby.aec.at](http://electrolobby.aec.at).

---

# Fashion Sites

_author: Lev Manovich_
_year: 2001_

In the beginning of the twentieth century art largely abandoned one of its key - if not _the_ key - functions - portraying the human being. Instead, most artists turned to other subjects, such as abstraction, industrial objects, and materials (Duchamp, minimalists), media images (pop art), the figure of artist herself or himself (performance and video art), or, most recently, data (net art). And when the artists did focus on the human figure (for instance, Picasso or De Kooning), often it was just an excuse to investigate the possibilities of painting, or the conditions of representation in general. Those few (Kokoschka, Giacometti, Bacon) who went on to depict a human figure in order to register all the heaviness of the "human condition," registered just that - the dark side of this condition, rather than the whole range of human states.

It is the beginning of the new century, and after the end of Cold War, the exhaustion of post-modernism, and the invention of the Web, we want to feel optimistic. (And if you still feel alienated or simply moody, you are hopelessly behind the times - so just take Prozac and join the global party!) We want to imagine ourselves anew. If visual art, hopelessly stuck in recycling its recent history over and over, can no longer help us, where can we turn to?

Enter fashion. Fashion is everything contemporary art is not: it is concerned with beauty; it is well aware of its history over many centuries, rather than just recent decades; it is more semiotically layered than the most complex Photoshop composite you ever worked on; and it has one ever-present constraint (and only constraints can lead to great art) - the human figure. This constraint gives the art of fashion its vitality, its optimism, and its inventiveness. And while cinema, along with fashion, also can be called the art of a human figure, its representations are too realist, limited to life as it actually exists. In contrast, fashion, or at least its "avant-garde" wing, asks a more playful, more optimistic question - what else a human being could have been? What would have happened if Darwinian evolution took a few steps differently? So, we don't have to wait until scientists start slicing our DNA to re-invent ourselves - because fashion continuously spins out new definitions of the human.

[www.firstview.com](http://www.firstview.com/)

One of the best features of Web media is its comprehensiveness. So if you are in encyclopedic mood, go to [www.firstview.com](http://www.firstview.com/), where you can look up the collections of hundreds of designers, from A A Milano to Zucca, and everybody in between, for the last five years. A separate catalog of video clips is also available. All the collections are free except the most recent (i.e., Fall 2001) which requires a small hourly fee to access. If you look at fashion as fashion, you may be intimidated by the fee; in my case, I simply browse the endless photographs as a kind of atlas of imaginary biology, not particularly caring about the designer or the year (one of my favorite collections on the site is Michiko Koshino Fall 1996 women's ready to wear.)

[www.costumenational.com](http://www.costumenational.com/)

For the ultimate in Web elegance Swedish style, head to Costume National site. Grey background and minimalist layouts create a stark contrast with colorful and theatrical fashion photographs (in "screenplay" sections). Seemingly simple at first, the site contains endless surprises, such as charming abstract compositions which introduce each of the collections. Here, the rich tradition of twentieth century geometric abstraction meets figurative imagination of contemporary fashion presented through the Web design at its best.

[www.vuitton.com](http://vuitton.com/)

Almost as inventive as the site by Costume National and equally elegant (although in distinctly French style), this site for Louis Vuitton luxury creations presents everything from accessories to Vuitton's new collection of travel guides. Although Vuitton's collections themselves are too classical for my taste, I feel renewed just by navigating through the site. It proves that interactivity itself can be as sensual as the best underwear designers by a renowned French designer.

[nikeid.nike.com](nikeid.nike.com)

For one possible future of fashion, head to NIKE iD part of NIKE Web site. While personalization rules new media where you can construct your own path through a narrative or customize your home page, in the word of manufacturing it still remains largely a dream. NIKE iD takes a step towards making personalization a reality: the site lets you build you own "unique" shoes by choosing color combination and your ID (a combination of letters and numbers) which will appear on the shoes, a kind of custom license plate. Once you enter all the information, the customized shoes are delivered to you within a few weeks. As the site boldly explains, "self-expression is at the heart of human nature… And with NIKE id, when you define who you are on your personalized shoes, you add a little soul to your soles." In the realm of consumer culture, Picassos "originality" wins over Duchamp's "ready-made."

---

# From DV Realism to a Universal Recording Machine

_author: Lev Manovich_
_year: 2001_

## Introduction

If Mike Figgis’s remarkable _Timecode_ (2000) exemplifies the difficult search of digital cinema for its own unique aesthetics, it equally demonstrates how these emerging aesthetics borrow from cinema’s rich past, from other media, and from the conventions of computer software. The film splits the screen into the four quadrants to show us four different actions taking place at once. This is of course something that have been common in computer games for a while; we may also recall computer user’s ability to open a new window into a document, which is the standard feature of all popular software programs. In tracking the characters in real time, _Timecode_ follows the principle of unity of space and time that goes back to the seventeenth century classicism. At the same time, since we are presented with video images which appear in separate frames within the screen and which provide different viewpoints on the same building, the film also makes a strong reference to the aesthetics of video surveillance. At the end, we may ask if we are dealing with a film that is borrowing strategies from other media; or with a “reality TV” program that adopts the strategies of surveillance; or with a computer game that heavily relies on cinema. In short, is _Timecode_ still _cinema_ or is it already _new media_? 

This essay will address one of the key themes which accompanies both the evolution of new media technologies during its four-decade long history and the current ongoing shift of cinema towards being computer-based in all aspects of its production, post-production, and distribution. This theme is “realism.” The introduction of every new modern media technology, from photography in the 1840s to Virtual Reality in the 1980s, has always been accompanied by the claims that the new technology allows to represent reality in a new way. Typically, it is argued that the new representations are radically different from the ones made possible by older technologies; that they are superior to the old ones; and that they allow a more direct access to reality. Given this history, it is not surprising that the shift of all moving image industries (cinema, video, television) in the 1980s and 1990s towards computer-based technologies, and the introduction of new computer and network-based moving image technologies during the same decade (for instance, Web cams, digital compositing, motion rides) has been accompanied by similar claims. In this essay I will examine some of these claims by placing them within a historical perspective. How new is the “realism” made possible by DV cameras, digital special effects, and computer-driven Web cams?

Instead of thinking of the evolution of modern media technology as a linear march towards more precise or more authentic representation of reality, we may want to think of a number of distinct aesthetics – particular techniques of representing reality – that keep re-emerging throughout the modern media history. I do not want to suggest that there is no change and that these aesthetics have some kind of metaphysic status. In fact, it would be an important project to trace the history of these aesthetics, to see which ones already appeared in the nineteenth century and which ones only made their appearance later. However, for my purposes here, it is sufficient to assume that the major technological shifts in media, such as the present shift towards computer and network-based technologies, not only lead to the creation of new aesthetic techniques but also activate certain aesthetic impulses already present in the past. 

I will focus on two different aesthetics that at first sight may appear to be unique to the current digital revolution but in fact accompany moving image media throughout the twentieth century. The two aesthetics are opposite of each other. The first treats a film as a sequence of big budget special effects, with may take years to craft during post-production stage. The second gives up all effects in favor of “authenticity” and “immediacy,” achieved with the help of inexpensive DV equipment. I will trace these two aesthetics back to the very origins of cinema. If Georges Méliès was the father of special effects filmmaking, then the Lumière brothers can be called the first _DV realists_. To use the contemporary terms, the Lumière brothers defined filmmaking as production (i.e., shooting), while Méliès defined it as post-production (editing, compositing, special effects). 

The fact that it is not only the theme of “realism” itself but also particular strategies for making media represent reality “better” that keep reappearing in the history of media should not blind us to the radical innovations of new media. I do believe that new media reconfigures a moving image in a number of very important ways. I trace some of them in _The Language of New Media_: the shift from montage to compositing; the slow historical transition from lens-based recording to 3-D image synthesis; the new identity of cinema as a hybrid of cinematography and animation. For me, pointing that some claims about the newness of new media are incorrect (such as tracing the historical heritage of certain realist aesthetics in this essay) is the best way of figuring which claims are correct, as well as discovering the new features of new media which we may have overlooked. In short, the best way to see what is new is to first get clear about what is old. In the case of my topic here, dismissing the originality of digital special effects and digital “immediacy” allows us to notice a truly unique capacity of digital media for representing real, which I will address in the last section of this essay.

This unique capacity can be summed up as the shift from “sampling” to “complete recording.” If both traditional arts and modern media are based on sampling reality, that is, representing/recording only small fragments of human experience, digital recording and storage technologies greatly expand how much can be represented/recorded. This applies to granularity of time, the granularity of visual experience, and also what can be called “social granularity” (i.e., representation of one’s relationships with other human beings.)

In regards to time, it is now possible to record, store and index years of digital video. By this I don't mean simply video libraries of stock footage or movies on demand systems – I am thinking of recording/representing the experiences of the individuals: for instance, the POV of single person as she goes through her life, the POVs of a number of people, etc. Although it presents combined experiences of many people rather than the detailed account of a single person’s life, the work by Spielberg’s Shoah Foundation is relevant here as it shows what can be done with the new scale in video recording and indexing. The Shoah Foundation assembled and now makes accessible massive amount of video interviews with the Holocaust survivors: it would take one person forty years to watch all the video material, stored on Foundation’s computer servers. 

The examples of new finer visual granularity are provided by projects of Luc Courchesne and Jeffrey Shaw which both aim at continuous 360 o moving image recordings of reality. [1] One of Shaw’s custom systems which he called Panosurround Camera uses 21 DV cameras mounted on a sphere. The recordings are stitched together using custom software resulting in a 360o moving image with a resolution of 6000 x 4000 pixels. [2]

Finally, the example of new “social granularity” is provided by the popular _The Sims_. This game that is better referred to as “social simulator” models ongoing relationship dynamics between a number of characters. Although the relationship model itself can hardly compete with the modeling of human psychology in modern narrative fiction, since _The Sims_ is not a static representation of selected moments in the characters’ lives _but a dynamic simulation running in real time_, we can at any time choose to follow any of the characters. While the rest of the characters are off-screen, they continue to “live” and change. In short, just as with the new granularity of time and the new granularity of visual experience, the social universe no longer needs to be sampled but can be modeled as one continuum. 

Together, these new abilities open up vast new vistas for aesthetic experimentation. They give us a wonderful opportunity to address one of the key goals of art – a representation of reality and the human subjective experience of it – in new and fresh ways. 

## Digital Special Effects

By the middle of the 1990s, the producers and directors of feature and short films, television shows, music videos and other _visual fictions_ have widely accepted digital tools, from digital compositing to CGI to DV cameras. According to the clichés used in Hollywood when discussing this digital revolution, filmmakers are now able to “to tell stories that were never possible to tell before”, “achieve new level of realism,” and “impress the audiences with previously unseen effects.” But do these statements hold up under a closer scrutiny?

Let’s begin by considering the first idea. Is it really true that Ridley Scott would not be able to make _Gladiator_ without computers? Of course, computer-generated shots of the Roman Coliseum are quite impressive, but the story could have been told without them. After all, in his 1916 _Intolerance_ Griffith showed the audiences the fall of Babylon, the latter days of Christ’s life and the St. Bartholomew’s Day Massacre – all without computers. Similarly, the 1959 classic _Ben-Hur_ already took the viewers to the ancient Rome, again without computers. 

Shall we then accept the second idea that armed with computers filmmakers can now get closer to reality than ever before? I don’t accept this idea either. More often than not, when you watch special effects shots in films, you are seeing something you never saw before, either in reality or in cinema. You have never before seen prehistoric dinosaurs (_Jurassic Park_). You have never before seen T2 morphing into a tiled floor (_Terminator 2: Judgment Day_). You have never before seen a man gradually become invisible (_The Hollow Man_). So, while in principle filmmakers can use computers to show the viewers ordinary, familiar reality, this almost never happens. Instead, they aim to show us something extra-ordinary: something we have never seen before. 

What about situations when the special effects shots do not show a new kind of character, set or environment? In this case, the novelty involves showing familiar reality _in a new way_ (rather than simply “getting closer to it”). Take, for instance, a special effects shot of a mountain climber who, high up in the mountains, loses his balance and plummets to the ground. Before computers, such a sequence would probably involve cutting between a close-up of the climber and a wide of mountain footage. Now the audience can follow the character as he flies down, positioned several inches from his face. In doing so it creates a new reality, a new visual fiction: imagining what it would be like to fall down together with the character, flying just a few inches from his face. The chances of somebody actually having this experience are pretty much the same as seeing a prehistoric dinosaur come to life. Both are visual fictions, achieved through special effects. 

## DV Realism

A special effects spectacle has not been the only result of digital revolution in cinema. Not surprisingly, the over-reliance of big budget filmmaking on lavish effects has led to a reality check. The filmmakers who belong to what I will call _DV realism_ school on purpose avoid special effects and other post-production tricks. Instead, they use multiple, often handheld, inexpensive digital cameras to create films characterized by a documentary style. The examples would be such American films as Mike Figgis’s _Timecode_ and _Blair Witch Project_ and the European films made by the Dogma 95 group (_Celebration_, _Mifune_). Rather than treating live action as a raw material to be later re-arranged in post-production, these filmmakers place premier importance on the authenticity of the actors’ performances. On the one hand, DV equipment allows a filmmaker to be very close to the actors, to literally be inside the action as it unfolds. In addition to a more intimate filmic approach, a filmmaker can keep shooting for a whole duration of a 60 or 120 minute DV tape as opposed to the standard ten-minute film roll. This increased quantity of (cheaper!) material gives the filmmaker and the actors more freedom to improvise around a theme, rather than being shackled to the tightly scripted short shots of traditional filmmaking. (In fact, the length of _Time Code_ exactly corresponds to the length of a standard DV tape.)

_DV realism_ has a predecessor in an international filmmaking movement that begun in the late 1950s and unfolded throughout the 1960s. Called “direct cinema,” “candid” cinema, “uncontrolled” cinema, “observational” cinema, or _cinéma vérité_ (“cinema truth”), it also involved filmmakers using lighter and more mobile (in comparison to what was available before) equipment. Like today’s DV realists, the 1960s “direct cinema” proponents avoided tight staging and scripting, preferring to let events unfold naturally. Both then and now, the filmmakers used new filmmaking technology to revolt against the existing cinema conventions that were perceived as being too artificial. Both then and now, the key word of this revolt was the same: “immediacy.”

Interestingly, during the same period in the ‘60s, Hollywood also underwent a special effects revolution: widescreen cinema. In order to compete with the new television medium, filmmakers created lavish widescreen spectacles such as the above-mentioned _Ben-Hur._ In fact, the relationship between television, Hollywood and “direct” cinema looks remarkably like what is happening today. Then, in order to compete with a low-res television screen, Hollywood turned to a wide screen format and lavish historical dramas. As a reaction, “direct” cinema filmmakers used new mobile and lightweight equipment to create more “immediacy.” Today, the increasing reliance on special effects in Hollywood can be perceived as a reaction to the new competition of the Internet. And this new cycle of special effects filmmaking has found its own reaction: _DV realism_. 

## Digital Special Effects and _DV Realism_, Historicized

The two ways in which filmmakers use digital technology today to arrive at two opposing aesthetics – special effects driven spectacle and documentary-style realism striving for “immediacy” – can be traced back to the origins of cinema. Film scholars often discuss history of cinema in terms of two complimentary creative impulses. Both originate at the turn of the twentieth century in France. The Lumière brothers established the idea of cinema as reportage. The camera covers events as they occur. The Lumières’s first film, _Workers Leaving the Lumière Factory_, is a single shot that records the movements of people outside of their photographic factory. Another of Lumières’s early films, the famous _Arrival of a Train at a Station_, shows another simple event: the arrival of the train in a Paris train station.

The second idea of cinema equates it with special effects, designed to surprise and even shock the viewer. According to this idea, the goal of cinema is not to record the ordinary but to catch (or construct) the extraordinary. Georges Méliès was a magician in Paris who owned his own film theater. After seeing the Lumières’s film presentation in 1895, Méliès started to produce his own films. His hundreds of short films established the idea of cinema as special effects. In his films, devils burst out of cloud of smoke, pretty woman vanishes, a spaceship flies to the moon, a woman transforms into a skeleton (a predecessor to _Hollow Man_?). Méliès used stop motion, special sets, miniatures, and other special effects to extend the aesthetics of the magician’s performance into a longer narrative form. 

The ways in which filmmakers today use digital technology fits quite well with the two basic ideas of what cinema is, which begun more than a century ago. The Lumières idea of film as a record of reality, as a witness to events as they unfold, survives with _DV realism_. It also animates currently popular “reality TV” shows (_Cops_, _Survivor_, _Big Brother_) where omnipresent cameras report on events as they unfold. Méliès’s idea of cinema as a sequence of magician’s tricks arranged as a narrative receives a new realization in Hollywood’s digital special effects spectacles, from _The Abyss_ to _Star Wars: Episode 1_. 

Therefore, it would be incorrect to think that the two aesthetics of computer-driven special effects and _DV realism_ somehow are results of digital technology. Rather, they are the new realizations of two basic creative impulses that have accompanied cinema from the beginning.

Such an analysis makes for a neat and simple scheme – in fact, too simple to be true. Things are actually more complicated. More recently film scholars such as Thomas Elsaesser revised their take on the Lumières. [3] They realized that even their first films were far from simple documentaries. The Lumières planned and scripting the events, and staged actions both in space in time. For instance, one of the films shown at the Lumières’s first public screening in 1895, _The Waterer Watered_, was a staged comedy: a boy stepping on a hose causes a gardener to squirt himself. And even such supposedly pure example of “reality filmmaking” as _Arrival of a Train at a Station_ turned out to be “tainted” with advanced planning. Rather than being a direct recording of reality, _Arrival of the Train_ was carefully put together, with the Lumières choosing and positioning passers-by seen in the shot. 

_Arrival of the Train_ can be even thought of as a quintessential special effects film. After all, it supposedly shocked the audiences so much they run out of the café where the screening was taking place. Indeed, they have never before seen a moving train presented with photographic fidelity – just as contemporary viewers have never before seen a man gradually being stripped of skin and then skeleton until he vanishes into the air (_The Hollow Man_), or thousands of robot soldiers engaged in battle (_Star Wars: Episode 1_). 

If the Lumières were not first documentarists but rather the directors of _visual fictions_, what about their ancestors – the directors of _DV realism_ films and “reality TV” shows? They do not simply record reality either. According to the statement found on the official Big Brother Web site, “Big Brother is not scripted, but a result of the participants reactions to their environments and interactions with each other on a day-to-day basis.” Yet even the fact that we are watching is not a continuous 24 hours a day recording but short episodes, each episode having a definite end (elimination of one of the house guests from the shows) testifies that the show is not just a window into life as it happens. Instead, it follows well-established conventions of film and television fictions: a narrative that unfolds within a specified period of time and results in a well-defined conclusion. 

In the case of _DV realism_ films, a number of them follow a distinct narrative style. Let us compare it with a traditional film narrative. A traditional film narrative usually takes place over months, years or even decades (for instance, _Sunshine_). We take it for granted that the filmmaker chooses to show us the key events selected from this period, thus compressing many months, or years, or even decades, into a film which runs just for ninety or one hundred and twenty minutes. In contrast, _DV realism_ films often take place in close to real time (in the case of _Time Code_, exactly in real time). Consequently, filmmakers construct special narratives where lots of dramatic events happen in a short period. It is as though they are trying to compensate for the real time of a narrative. 

So, the time that we see is the real time, rather than artificially compressed time of traditional film narrative. However, the narrative that unfolds during this time period is highly artificial, both by the standards of traditional film and TV narrative, and our normal lives. Both in _Celebration_ and in _Time Code_, for instance, we witness people betraying each other, falling in love, having sex, breaking up, revealing incest, making important deals, shooting at each other, and dying – all in the course of two hours. 

## Art of Surveillance

The real time aspect of what can be called _reality filmmaking_ (film and television narratives which take place in real time or close to it, including “reality TV”) has in itself an important historical precedent. Although television as a mass medium became established only in the middle of the twentieth century, television research begins already in the 1870s. During the first decades of this research, television was thought as the technology that would allow people to remotely see what is happening in a distant place – thus its name, television (literally, “distance seeing”). The television experiments were part of the whole set of other inventions which all took place in the nineteenth century around the idea of _telecommunication_: real time transmission of information over a distance. Telegraph was to transmit text over a distance, telephone was to transmit speech over a distance, and television was to transmit images over a distance. It was not until the 1920s when television was redefined as the _broadcasting_ medium, that is, as a technology for transmitting specially prepared programs to a number of people at the same time. In other words, television became a means to _distribute content_ (very much as the Internet today, as opposed to the Internet before mid-1990s) rather than the _telecommunication_ technology. 

The original idea of television has survived, however. It came to define one of the key uses of video technology in modern society: video surveillance. Today, for every TV monitor receiving content one can find a video camera which transmits surveillance images: from parking lots, banks, elevators, street corners, supermarkets, office buildings, etc. Along with having been realized in video surveillance, usually limited to companies, the original meaning of television as seeing over distance in real time received another realization in computer culture – the Web cams, accessible to everybody. Like normal video surveillance cameras that are tracking us everywhere, Web cams rarely show anything of interest. They simply show what is there: the waves on the beach, somebody staring at a computer terminal, an empty office or street. Web cams are the opposites of special effects films: feeding us the banality of the ordinary rather than the excitement of the extra-ordinary. 

Today’s _reality media_ – films that are taking place in real time (such as _Timecode_), “reality TV,” and Web cams – return us to television origins in the nineteenth century. Yet while history repeats itself, it never does it in the same way. The new omnipresence and availability of cheap telecommunication technologies, from Web cams to online chat programs to cell phones has the promise for a new aesthetics which does not have any precursors: the aesthetics which will combine fiction and telecommunication. How can telecommunication and fictional narrative go together? Is it possible to make art out of video surveillance, out of real-time – rather than pre-scripted – signal? 

_Timecode_ can be seen as an experiment in this direction. In _Timecode_ the screen is broken into four frames, each frame corresponding to a separate camera. All four cameras are tracking the events that are happening in different parts of the same location (a production studio on Sunset Boulevard in Hollywood), which is the typical video surveillance setup. It is to the credit of Mike Figgis that he was able to take such a setup and turn it into a new way to present a fictional narrative. Here, telecommunication becomes a narrative art. Television in its original sense of telecommunication – seeing over distance in real time - becomes the means to present human experience in a new way.

Of course, as I already noted, _Timecode_ is not exactly a bare-bones telecommunication. It is not just a real-time recording of whatever happens to be in front of the cameras. The film is tightly scripted. We may think of it as an edited surveillance video: the parts where nothing happens have been taken out; the parts with actions in them have been preserved. But it is more accurate to think of _Timecode_ as a conventional film that adopts visual and spatial strategies of video surveillance (multiple cameras tracking one location) while following traditional dramatic conventions of narrative construction. In other words, the film uses telecommunication-type interface to a traditional narrative. Which means that it does not yet deal with the deeper implications of computer-based surveillance (we can also use other terms which have less negative connotations: “monitoring,” “recording.”)

## Computer as a Universal Recording Machine

What would it mean for cinema, and narrative arts in general, to address these implications? One of the most basic principles of narrative arts is what in computer culture called “compression.” A drama, a novel, a film, a narrative painting or a photograph compresses weeks, years, decades, and even centuries of human existence into a number of essential scenes (or, in the case of narrative images, even a single scene). Non-essential is stripped away; essential is recorded. Why? Narrative arts have been always limited by the capacities of the receiver (i.e., a human being) and of storage media. Throughout history, the first capacity remained more or less the same: today the time we will devote to the reception of a single narrative may range from 15 seconds (a TV commercial) to two hours (a feature film) to a number of short segments distributed over a large period of time (following a TV series or reading a novel). But the capacity of storage media recently changed dramatically. Instead of 10 minutes that can fit on a standard film roll or two hours that can fit on a DV tape, a digital server can hold practically unlimited amount of audio-visual recordings. The same applies for audio only, or for text.

This revolution in the scale of available storage has been accompanied by the new ideas about how such media recording may function. Working within the paradigms of Computer Augmented Reality, Ubiquitous Computing, and Software Agents at places such as MIT Media Lab and Xerox Park, computer scientists advanced the notion of a computer as an unobtrusive but omni-present device which automatically records and indexes all inter-personal communications and other user’s activities. A typical early scenario envisioned in the early 1990s involved microphones and video cameras situated in the business office which record everything taking place, along with indexing software which makes possible a quick search through the years’ worth of recordings. More recently the paradigm has expanded to include capturing and indexing all kinds of experiences of many people. For instance, a DARPA-sponsored research project at Carnegie-Mellon University called Experience-on-Demand which begun in 1997 aims to “developed tools, techniques, and systems that allow users to capture complete records of personal experience and to share them in collaborative settings.” [4] A 2000 report on the project summarizes the new ideas being pursued as follows:

> Capture and abstraction of personal experience in audio and video as a form of personal memory.
> 
> Collaboration through shared composite views and information spanning location and time.
> 
> Synthesis of personal experience data across multiple sources.
> 
> Video and audio abstraction at variable information densities.
> 
> Information visualizations from temporal and spatial perspectives.
> 
> Visual and audio information filtering, “understanding,” and event alerting. [5]

(Given that a regular email program already automatically keeps a copy of all sent and received emails, and allows to sort and search through these emails, and that a typical mailing list archive Web site similarly allows to search through years of dialogs between many people, we can see that in the course of text communication this paradigm has already been realized.) The difficulty of segmenting and indexing audio and visual media is what delays realization of these ideas in practice. However, the recording in mass itself already can be easily achieved: all it takes is an inexpensive Web cam and a large hard drive. 

What is important in this paradigm – and this applies to computer media in general – is that storage media became active. That is, the operations of searching, sorting, filtering, indexing, and classifying, which before were the strict domain of human intelligence, became automated. A human viewer no longer needs to go through hundreds of hours of video surveillance to locate the part where something happens – a software program can do this automatically, and much more quickly. Similarly, a human listener no longer needs to go through years of audio recordings to locate the important conversation with a particular person – software can do this quickly. It can also locate all other conversations with the same person, or other conversations where his name was mentioned, and so on. 

For me, the new aesthetic possibilities offered by computer recording are immense and unprecedented – in contrast to the aesthetics of special effects and _DV realism_, which as I have suggested are not new in cinema history. What maybe truly unique about new media’s capacity to represent reality is the new scale of reality maps it makes possible. Instead of compressing reality to what the author considers the essential moments, very large chunks on everyday life can be recorded, and then put under the control of software. I imagine for instance a “novel” which consists of  complete email archives of thousands of characters, plus a special interface that the reader will use to interact with this information. Or a narrative “film” which a computer program assembles shot by shot in real time, pulling from the huge archive of surveillance video, old digitized films, Web cam transmissions, and other media sources. (From this perspective, Godard’s _History of Cinema_ represents an important step towards such _database cinema_. Godard treats the whole history of cinema as his source material, traversing this database back and forth, as though a virtual camera flying over a landscape made from old media.)

In conclusion, let me once again evoke _Timecode_. Its very name reveals its allegiance to the logic of old media of video: a linear recording of reality on a very limited scale. The film is over when the time code on videotape reaches two hours. Although it adopts some of the visual conventions of computer culture, it does not yet deal with the underlying logic of a computer code. 

Contemporary creators of digital _visual fictions_ need to find new ways to reflect the particular reality of our own time, beyond embracing digital special effects or digital “immediacy.” As I have suggested, computer’s new capacities for automatically indexing massive scale recordings do offer one new direction beyond what cinema has explored so far. Rather than seeing reality in new ways, the trick maybe simply to pour all of it on a hard drive – and then figure out what kind of interface the user needs to work with all the recorded media. In short, a filmmaker needs to become an interface designer. Only then _cinema_ will truly become _new media_. 

## References:

[1] For Courchesne’s Panoscope project, see [http://www.din.umontreal.ca/courchesne/](http://www.din.umontreal.ca/courchesne/); For Jeffrey Shaw’s projects, see [http://www.jeffrey-shaw.net](http://www.vision-ruhr.de/artists/shaw/). Both discuss their projects in relation to previous strategies of “experience representation” in panorama, painting, and cinema in _New Screen Media: Cinema/Art/Narrative_, edited by Martin Rieser and Andrea Zapp (London: BFI and Karlsruhe: ZKM, 2001).

[2] Private communication between Shaw and the author, July 4, 2002. 

[3] This section relies on the analysis of the Lumières by Thomas Elsaesser in his _Cain, Abel or Cable_ (Amsterdam and Ann Arbor: Amsterdam University Press / Michigan University Press, 1998).

[4] [http://www.informedia.cs.cmu.edu/](http://www.informedia.cs.cmu.edu/). For more information on the project, see Howard D. Wactlar et al., “Experience-on-Demand: Capturing, Integrating, and Communicating Experiences Across People, Time, and Space,” [http://www.informedia.cs.cmu.edu/eod/](http://www.informedia.cs.cmu.edu/eod/); also Howard D. Wactlar et al., “Informedia Video Information Summarization and Demonstration Testbed Project Description,” [http://www.informedia.cs.cmu.edu/arda-vace/](http://www.informedia.cs.cmu.edu/arda-vace/). Both of these research projects were conducted at Carnegie-Mellon University; dozens of similar projects are going on at universities and industry research labs around the world.

[5] [http://www.informedia.cs.cmu.edu/eod/EODforWeb/eodquad00d.pdf](http://www.informedia.cs.cmu.edu/eod/EODforWeb/eodquad00d.pdf).

---

# Post-media Aesthetics

_author: Lev Manovich_
_year: 2001_

## Medium in Crisis

In the last third of the twentieth century, various cultural and technological developments have together rendered meaningless one of the key concepts of modern art – that of a medium. However, no new topology of art practice came to replace media-based typology which divides art into painting, works on paper, sculpture, film, video, and so on. The assumption that artistic practice can be neatly organized into a small set of distinct mediums has continued to structure the organization of museums, art schools, funding agencies and other cultural institutions - even though this assumption no longer reflected the actual functioning of culture.

Few different developments have contributed to this conceptual crisis.

From the 1960s onward the rapid development of new artistic forms – assemblage, happening, installation (including its various sub-forms such as site-specific installation and video installation), performance, action, conceptual art, process art, intermedia, time-based art, etc., has threaten the centuries-old typology of mediums (painting, sculpture, drawing) because of the sheer fact of the multiplicity of these forms. In addition, if the traditional typology was based on difference in materials used in art practice, the new mediums either allowed for the use of different materials in arbitrary combinations (installation), or, even worse, aimed to dematerialize the art object (conceptual art). Therefore, the new forms were not really mediums in any traditional sense of the term.

Another mutation in the concept of medium came about as new technological forms of culture were gradually added to the old typology of artistic mediums. Photography, film, television, and video gradually appeared in the curriculum of art schools and were given separate departments in art museums. In the case of traditional (i.e., pre-digital) photography and film, thinking of them as separate mediums in a traditional sense of the term still made sense: they used different material base (photographic paper in the case of photography, film stock in the case of film), and they would also neatly fall on two different sides of another fundamental distinction used by traditional aesthetics in defining the typology of mediums: that of between spatial arts (painting, sculpture, architecture) and temporal arts (music, dance). Since photography dealt with still images and film dealt with moving images whose perception required time, and since they relied on distinct materials, adding these two forms to the typology of artistic media did not threaten the concept of medium.

However, in the case of television and video things were not so easy. Both _mass_ medium of television and art medium of video used the same material base (electronic signal which can be transmitted live or recorded on a tape) and also involved the same conditions of perception (television monitor). The only justifications of treating them as separate mediums were sociological and economic, i.e., the differences in sizes of their respective audiences, in mechanisms of distribution (via television network versus museum and gallery exhibition), and in the number of copies of a tape/program being made.

The case of television versus video is one example of how the old concept of medium used by traditional aesthetics to describe various arts came into conflict with the new set of distinctions important in the twentieth century: between art and mass culture. While modern art system involved circulation of objects which were either unique or existed in small editions, mass culture dealt mass distribution of identical copies – and thus depended on various mechanical and electronic reproduction and distribution technologies. As artists begun to use the technologies of mass media to make art (be it photography, films, radio art, video art, or digital art), the economy of art system dictated that they use technologies designed for mass reproduction for the opposite purpose – to create limited editions. (Thus, while visiting a contemporary art museum, we find such conceptually contradictory objects as “video tape, edition of 6” or “DVD, edition of 3.”) Gradually, this sociological difference in the distribution mechanisms, along with other sociological differences already mentioned (the size of an audience and the space of reception/exhibition), became more important criteria in distinguishing between mediums than the distinctions in material used or conditions of perception. In short, sociology and economics took over aesthetics.

## Digital Attack

Along with the arrival of mass media throughout the twentieth century, and the proliferation of new art forms beginning in the 1960s, another development that threatened the traditional idea of a medium was digital revolution of the 1980s-1990s. The shift of most means of production, storage, and distribution of mass media to digital technology (or various combinations of electronic and digital technologies), and adoption of the same tools by individual artists disturbed both the traditional distinctions based on materials and conditions of perception and the new, more recent distinctions based on distribution model, method of reception/exhibition and payment scheme.

On the material level, the shift to digital representation and the common modification/editing tools which can be applied to most media (copy, paste, morph, interpolate, filter, composite, etc.) and which substitute traditional distinct artistic tools erased the differences between photography and painting (in the realm of still image) and between film and animation (in the realm of a moving image). [1] On the level of aesthetics, the Web has established a _multimedia_ document (i.e., something which _combines_ and mixes different media of text, photography, video, graphics, sound) as a new communication standard. Digital technology has also made much easier to implement the already existing cultural practice of making different versions of the same project for different mediums, different distribution networks and different audiences. And if one can make radically different versions of the same art object (for instance, an interactive and non-interactive versions, or 35mm film version and Web version), the traditional strong link between the identity of an art object and its medium becomes broken. On the level of distribution, the Web has dissolved (at least in theory) the difference between mass distribution, previously associated with mass culture, and limited distribution previously reserved for small subcultures and the art system. (The same Web site can be accessed by one person, ten people, ten thousand people, ten million people, etc.)

These are just some examples of how traditional concept of medium does not work in relation to _post-digital_, _post-net culture_. And yet, despite the obvious inadequacy of the concept of medium to describe contemporary cultural and artistic reality, it persists. It persists through sheer inertia – and also because to put in place a better, more adequate conceptual system is easier said than done. So rather than getting rid of media typology altogether, we keep adding more and more categories: “new genres,” interactive installation, interactive art, net art. The problem with these new categories is that they follow the old tradition of identifying distinct art practices on the basis of the materials being used – only now we substitute different materials by different new technologies.

For instance, all art on the Net, i.e., art which uses the technology of the Net, is lumped onto a single category of “net art.” But why shall we assume that all art objects that share Net technology should have anything in common as far as their reception by users is concerned? [2] The idea of “interactive art” is similarly problematic. 

Used in relation to computer-based media, the concept of interactivity is a tautology. Modern human-computer interface (HCI) is by its very definition interactive. In contrast to earlier interfaces such as batch processing, modern HCI allows the user to control the computer in real-time by manipulating information displayed on the screen. Once an object is represented in a computer, it automatically becomes interactive. Therefore, to call computer media interactive is meaningless - it simply means stating the most basic fact about computers. [3]

Just as we should not assume that all artworks which use the technology of the Net belong to the medium of “net art,” it is a mistake to put all art objects which use - or, more precisely, form a layer on top of - interactive technology of modern computing into one category of “interactive art.” We may want to put forward a proposition that there can be a distinct medium of net art based on the technology of the Net, but it is a mistake to automatically identify all art which uses the Net as “net art.”

## A Program for Post-media Aesthetics

Within the space of this article, I can’t begin to develop a new conceptual system which would replace the old discourse of mediums and which would be able to describe _post-digital_, _post-net_ culture more adequately. However, what I can do is to suggest one particular direction we may want to pursue in developing such a system. This direction would involve substituting the concept of medium by new concepts from computer and net culture. These concepts can be used both literally (in the case of actual computer-mediated communication) and metaphorically (in the case of pre-computer culture). So here is how such post-media aesthetics may look like:

1. Post-media aesthetics needs categories that can describe how a cultural object _organizes data_ and _structures user’s experience of this data_.

2. The categories of post-media aesthetics _should not be tied to any particular storage or communication media_. For instance, rather than thinking of “random access” as a property specific to computer medium, we should think of it as a general strategy of data organization (which applies to traditional books, architecture) and, separately, as a particular strategy of user’s behavior. [4]

3. Post-media aesthetics _should adopt the new concepts, metaphors and operations of a computer and network era_, such as information, data, interface, bandwidth, stream, storage, rip, compress, etc. We can use these concepts both when talking about our own post-digital, post-net culture, and when talking about the culture of the past. I think of a later approach not just as an interesting intellectual exercise but as something which ethically we must do - in order to see old and new culture as one continuum; in order to make new culture richer through the use of the aesthetic techniques of old culture; and in order to make old culture comprehensible to new generations which are comfortable with concepts, metaphors and techniques of a computer and network era. As an example of such approach, we can describe Giotto and Eisenstein not only as an early Renaissance painter and a modernist filmmaker, but also as important _information designers_. The first invented new ways to organize data within a static two-dimensional surface (a single panel) or a 3-D space (a set of panels in a church building); the second pioneered new techniques to organize data over time and to coordinate data in different media tracks to achieve maximum effect on the user. In this way, a future book on information design can include Giotto and Eisenstein alongside Allan Kay and Tim Berners-Lee.

4. The traditional concept of a medium emphasizes the physical properties of a particular material and its representational capacities (i.e., the relationship between the sign and the referent.) As traditional aesthetics in general, this concept encourages us to think about the author’s intentions, the content, and the form of an artwork - rather than the user. In contrast, thinking of culture, media and individual cultural works as software allows us to focus on the operations (called in actual software applications “commands”) that are available to the user. The emphasis shifts on user’s capabilities and user’s behavior. _Rather than using the concept of medium we may use the concept of software to talk about past media, i.e., to ask about what kind of user’s information operations a particular medium allows for_. [5]

5. Both cultural critics and software designers came to draw a distinction between an ideal reader/user inscribed by a text/software and the actual strategies of reading/use/re-use employed by actual users. Post-media aesthetics needs to make a similar distinction in relation to all cultural media, or, to use the just introduced term, _cultural software_. The available operations and the “right” way of using a given cultural object are different from how people actually come to use it. (In fact, a fundamental mechanism of recent culture is a systematic “mis-use” of cultural software, such as scratching the records in DJ culture, or remixing old tracks.)

6. Users’ tactics (to use the term of Michel de Certeau) are not unique or random but follow particular patterns. I would like to introduce another term _information behavior_ to describe a particular way of accessing and processing information available in a given culture. We should not always a priori assume that given information behavior is “subversive”; it may closely correlate to the “ideal” behavior suggested by software, or it may differ from it simply because a given user is just a beginner and has not mastered the best ways to use this software.

## Information Behavior

Just as the term “software” shifts the emphasis from media/text to the user, I hope that the term “information behavior” also can help us to think about the dimensions of cultural communication, which previously went unnoticed. These dimensions have always been there, but in information society they have rapidly became prominent in our lives and thus intellectually visible. Today our daily life consists of  information activities in the most literal way: checking email and responding to email, checking phone messages, organizing computer files, using search engines, etc. In a simplest way, the particular way people organize their computer files, or use search engines, or interact on the phone can be thought of as information behavior. Of course, according to a cognitive science paradigm, human perception and cognition in general can be thought of as information processing – but this is not what I mean here. While every act of visual perception or of memory recall can be understood in information processing terms, today there is much more to see, filter, recall, sort through, prioritize, and plan. In other words, in our society daily life and work to a large extent revolve against new types of behaviors activities which involve seeking, extracting, processing and communication large amounts of information, often quantitative one – from navigating a transport network of a large city to using World Wide Web. Information behaviors of an individual form an essential part of individual identity: they are particular tactics adopted by an individual or a group to survive in information society. Just as our nervous system has evolved to filter information existing in the environment in a particular way suitable for information capacity of a human brain, to survive and prosper in. information society, we evolve particular information behaviors. [6]

Like other concepts of information society such as software, data, and interface, the concept of information behavior can be applied beyond specific information activities of the present, such as our usage of a Palm Pilot, Google, or a metro system. It can be extended into a cultural sphere and also projected into the past. For instance, we may think about information behaviors used in reading literature, visiting a museum, surfing TV, or choosing which tracks to download from Napster. Applied to the past, the concept of information behavior emphasizes that all past culture was not only about representing religious beliefs, glorifying rulers, creating beauty, legitimizing ruling ideologies, etc. – it was also about _information processing_. Artists developed new techniques of encoding information while listeners, readers and viewers developed their own cognitive techniques of extracting this information. The history of art is not only about the stylistic innovation, the struggle to represent reality, human fate, the relationship between society and the individual, etc. – it is also _the history of new information interfaces developed by artists, and the new information behaviors developed by users_. When Giotto and Eisenstein developed new ways to organize information in space and in time, their viewers had to also develop the appropriate ways of navigating these new information structures – just as today every new major release of a new version of familiar software requires us to modify information behaviors we developed in using a previous version.

Surrounded by information interfaces in their everyday life, critics and artists have already begun to selectively think about past culture in terms of information structures. A good example of this is the prominence given to Francis Yates’s book _The Art of Memory_ in new media discussions. What I am suggesting, however, is that such concepts as information interface and information behavior can be applied to any cultural object, past and present. In short, every cultural object is partly a Palm Pilot.

## Software as a New Object of Cultural Analysis

How would post-media aesthetics, as I briefly sketched it here, fit within the history of cultural theory of the last few decades? If we are to think of cultural communication following the basic information theory, that of author - text - reader (or, in proper terms of information theory, sender - message - receiver), this history can be summarized as a gradual shift in attention from the author to the text and then to the reader. Traditional criticism focused on the author, his/her creative intention, biography, and psychology. Arriving in the end of the 1950s, structuralism shifted the focus to the text itself, analyzing it as a system of semiotic codes. After 1968, the critical energy gradually shifts from the text to the reader. This shift has taken place for more than one reason. On the one hand, it became apparent that structuralist approach had severe limitations: in treating every cultural text as an instance of a general system, structuralism did not have a lot to say about what made a given text unique and culturally important. [7] On the other hand, after the events of the 1968 it also became clear that structuralist approach inadvertently supported the status quo, the Law, the System. Because structuralism wanted to describe everything as a closed system and because it treated every individual cultural text as an instance of a more general “deep structure,” structuralism turned out to be on the side of the norm rather than the exception, the majority rather than minority, the society as it existed rather than as it could have been.

The shift from the text to the reader took a number of forms and it can be thought of as following two stages. At the first stage, the abstract text of structuralism is being replaced by an abstract, ideal reader, as imagined by psychoanalysis (Kristeva) and psychoanalytically informed criticism, Apparatus Theory in film theory, or Reception Theory in literature. By the 1980s this abstract reader is being replaced by actual readers and reader communities, both contemporary and historical, as analyzed by Cultural Studies, ethnography, the study of historical reception of early cinema in film studies, etc.

Having traversed the trajectory from the author to the text and to the reader, there can cultural criticism go next? In my view, we need to update information model (author – text – reader) by adding two more components to it – and then focus our critical attentions on these components. These components are _software used by the author and by the reader_. Contemporary author (sender) uses software to create a text (message), and this software influences, or even shapes the kinds of texts being created: from Frank Gehry relaying on special computer software in his architectural design to Andreas Gursky using Photoshop to DJs whose whole practice depends on actual software and/or software in a metaphorical sense: the operations allowed for by turntables, mixers and other electronic equipment originally used by DJs. Similarly, a contemporary reader (receiver) often interacts with a text using actual computer software such as Web browser, or software in a metaphorical sense, that is, older hardwired interfaces - particular controls provided by various electronic devices such as a CD player. (Given that modern computer software often imitates already existing hardware interfaces – for instance, a QuickTime Player simulating controls of a standard VCR - this distinction is not as relevant as it may at first appear.) This software shapes how the reader thinks of a text; in fact, it defines what the given text is, be it a set of separate tracks on a CD or a set of multimedia components and hyperlinks presented as a Web page.  [8]

So far, I talked about communication model as formulated in information theory as consisting of  three components: sender, message, and receiver. In actuality, this model was more complex, having seven components all together sender, sender’s code, message, receiver, receiver’s code, channel, and noise. According to the model, the sender encodes a message using his own code; the message then transmitted over a communication channel; in the course of transmission it was affected by noise. The receiver decodes the message using his own code. Because of the limited bandwidth capacity of the channel, the presence of noise and possible discrepancy between the sender’s and receiver’s codes, the receiver may not receive the same message as send by the sender. Developed originally for such applications as telecommunication (telephone and television transmission) in the 1920s-1930s and code encryption and decoding during the World War II, the goal of information theory was to help engineers construct better communication systems. Different problems emerge as communication model is adopted as a model of cultural communication. The engineers who developed this model were concerned with the accuracy of message transmission, but in cultural communication, the idea of accurate transmission is dangerous: to assume that communication is only successful if the receiver accurately reconstructs the sender’s message is to privilege the sender’s meaning over receiver’s meaning. (We can say that Cultural Studies which focuses on “subversive” uses of dominant culture, goes to another extreme as it assumes that only “unsuccessful” communication is worth studying.)

In addition, communication model treats code and channel (the latter corresponding to “medium” as this term is commonly used) as passive, mechanical components: they are simply the required tools necessary to transmit a pre-existing message. Since the model originally emerged in the context of telecommunication, it assumes that unmediated oral or visual communication – two people talking to each other or a person looking at reality – is ideal. It is only because we want such communication to take place over a distance we need to bother with codes and a channel.

I think that adding the components of author’s software and reader’s software to the model emphasizes the active role technology (i.e., what the original model calls codes and channel) plays in cultural communication. Authoring software shapes how the author understands the medium she/he works in; and consequently, they play a crucial role in shaping the final form of a techno-cultural text. For the reader who accesses this text through the software interface, this interface similarly shapes his/her understanding of the text: what types of data the text contains, how is it organized together, what else is possible what is not possible to communicate. In addition, software tools (again, both actual computer software and software in a metaphorical sense, i.e., a set of data operations and metaphors employed by a particular media or representational technology) are what allow the authors and the users to re-mix new cultural texts out of existing texts. Again, the example of DJ practice can be evoked here.

What are the dangers of a post-media aesthetic theory sketched here? As any other paradigm, it privileges some directions of research at the expense of others. So, while it can be productive to begin approaching history of culture as the history of information interfaces, information behaviors, and software, such a perspective can make us less attentive to other aspects of culture. The most immediately obvious danger is that in its emphasis on information structures and information behaviors post-media aesthetics privileges cognitive dimensions of culture without providing any obvious way to think about affect.

Affect has been neglected in cultural theory since the late 1950s when, under the influence of mathematical theory of communication, Roman Jakobson, Claude Levi-Strauss, Roland Barthes, and others began treating cultural communication solely as a matter of encoding and decoding messages. Barthes begins his well-known article _The Photographic Message_ published in 1961 in the following way:

> The press photograph is a message. Considered overall this message is formed by a source of emission, a channel of transmission and a point of reception. The source of emission is the staff of the newspaper, the group of technicians certain of whom take the photo, some of whom choose, compose and treat it, while others, finally, give it a title, a caption and a commentary. The point of reception is the public which reads the paper. As for the channel of transmission, this is the newspaper itself. [9] 

Although later critics avoided such direct application of the terms of mathematical theory of communication to cultural communication, the legacy of this approach continued to linger for decades as the general paradigm of cultural criticism that even today stills focuses on the concepts of “text” and “reading.” By approaching any cultural object/situation/process as “text” which is “read” by audience and/or by critics, cultural criticism privileges informational and cognitive dimensions of culture over affective, emotional, performative, and experiential dimensions. Other influential approaches to cultural criticism of the last decades similarly neglect these dimensions. Neither Lacan’s psychoanalysis (1960s-) nor cognitive approach in literary studies and film theory (1980s-) deal with affect.

Post-media, or informational aesthetics I am sketching here can’t directly deal with affect either, and thus its approach will need to be supplemented by some other paradigms. But it is important to remember that we can’t do full justice to contemporary culture by considering an information worker working on his/her computer and ignoring the music he/she is likely to listen to simultaneously on CD/MP3 player. In short, we can’t just consider the office and ignore the club.

The office and the club: both rely on the same machine (digital computer). What is different between the two is software. At the office we use Web browsers, databases, spreadsheets, information managers, compilers, scripting tools, etc. At the club DJ uses mixing and music authoring software, either directly on stage, or indirectly, by playing tracks composed beforehand in the studio.

If the same data processing machine can be used for highly rational, cognitive processes (for instance, writing a computer code) and for making possible affective, bodily experience of clubbing, this means that data does not just belong to the side of cognition. If in our society data streams move our brains and our bodies, perhaps informational aesthetics will eventually learn how to think about affective data as well.

## References:

[1] For a more extensive discussion of this shift, see the chapter “Digital Cinema and the History of a Moving Image,” in _The Language of New Media_ (Cambridge, Mass.: The MIT Press, 2001).

[2] Outside of art, the Net maybe is best thought of as a number of distinct mediums that share some technologies and communication but ultimately have their own distinct identities. For instance, Net used for email is one medium, commercial Web sites is another medium.

[3] Manovich, _The Language of New Media_.

[4] An excellent example of a new category which takes into account recent computer-based texts but at the same time can be used to talk about pre-computer texts is “ergodic literature” developed by Espen Aarseth in his _Cybertext: Perspectives on Ergodic Literature_(Baltimore: Johns Hopkins University Press, 1997). 

[5] We can make a parallel here with the trajectory of cultural criticism in the last few decades. Beginning in the 1970s, cultural criticism shifted attention from the author and the text to the strategies/practices of readership (psychoanalysis, cultural studies, ethnography). Critics emphasized that each reader constructs her/his own text and that readers employ various strategies of reading/interpreting/re-using cultural texts. In parallel, the designers of human-computer interfaces and software in general started to study the actual ways users employ software and other information technology.

[6] Geert Lovink’s ironic description of a figure of “Data Dandy” focuses our attention on the extent to which dealing with information has become a defining cultural characteristic of our time. See Adilkno, _The Media Archive_ (Brooklyn, New York: Autonomedia,1998), 99.

[7] In that respect, Roland Barthes's _S/Z_ which describes the functioning of five semiotic codes in Balzac’s short story, represents the unintentional admission of structuralism’s defeat: Barthes selectively chooses to show the functioning of some codes in the story, unsystematically using different parts to illustrate the work of this or that code. So rather than producing a scientific structural analysis he ends up writing a stimulating but completely idiosyncratic work of cultural interpretation. Roland Barthes, _S/Z_, translated by Richard Miller (New York, Hill and Wang, 1974).

[8] Earlier I said that the concept of software allows us to think about particular information operations that a user can perform in a given medium. It is interesting that historically modern media theory and modern cultural criticism never systematically met, except in the works of Friedrich Kittler and his students and followers.

[9] Roland Barthes, (1961), "The Photographic Message," in _Image, Music, Text_, ed. Stephen Heath (New York: Hill and Wang, 1977).

---

# New Media from Borges to HTML

_author: Lev Manovich_
_year: 2001_

## New Media Field: a Short Institutional History

The appearance of _New Media Reader_ is a milestone in the history a new field that, just a few years ago, was somewhat of a cultural underground. Before taking up the theoretical challenge of defining what new media actually is, as well as discussing the particular contributions this reader makes to answering this question, I would like very briefly to sketch the history of the field for the benefit of those who are newcomers to it.

If we are to look at any modern cultural field sociologically, measuring its standing by the number and the importance of cultural institutions devoted to it such as museum exhibitions, festivals, publications, conferences, and so on, we can say that in the case of new media (understood as computer-based artistic activities) it took about ten years for it to move from cultural periphery to the mainstream. Although SIGGRAPH in the U.S. and Ars Electronica in Austria have already acted as annual gathering places of artists working with computers since the late 1970s, the new media field begin to take real shape only in the end of the 1980s. At the end of the 1990s new institutions devoted to the production and support for new media art are founded in Europe: ZKM in Karlsruhe (1989), New Media Institute in Frankfurt (1990) and ISEA (Inter-Society for the Electronic Arts) in the Netherlands (1990). (Jeffrey Shaw was appointed to be director of the part of ZKM focused on visual media while Frankfurt Institute was headed by Peter Weibel). In 1990 as well, Intercommunication Center in Tokyo begins its activities in new media art (it moves into its own building in 1997.) Throughout the 1990s, Europe and Japan remained to be the best places to see new media work and to participate in high-level discussions of the new field. Festivals such as ISEA, Ars Electronica, DEAF have been required places of pilgrimage for interactive installation artists, computer musicians, choreographers working with computers, media curators, critics, and, since the mid-1990s, net artists.

As it was often the case throughout the twentieth century, countries other than the States would be first to critically engage with new technologies developed and deployed in the U.S. There are a few ways to explain this phenomenon. Firstly, the speed with which new technologies are assimilated in the U.S. makes them “invisible” almost overnight: they become an assumed part of the everyday existence, something which does not seem to require much reflection about. The slower speed of assimilation and the higher cost gives other countries more time to reflect upon new technologies, as it was the case with new media and the Internet in the 1990s. In the case of Internet, by the end of the 1990s it became as commonplace in the U.S. as the telephone, while in Europe Internet still remained a phenomenon to reflect upon, both for economic reasons (in the U.S. subscribers would play very low monthly flat fee; in Europe they had to pay by the minute) and for cultural reasons (more skeptical attitude towards new technologies in many European countries which slows down their assimilation). (So, when in the early 1990s Soros Foundation has set up contemporary art centers throughout the Eastern Europe, it wisely gave them a mandate to focus their activities on new media art, both in order to support younger artists who had difficulty getting around the more established “art mafia” in these countries; and also in order to introduce general public to the Internet.)

Secondly, we can explain the slowness of the U.S. engagement with new media art during the 1990s by the very minimal level of the public support for the arts there. In Europe, Japan, and Australia, festivals for media and new media art such as the ones I mentioned above, the commissions for artists to create such work, exhibition catalogs and other related cultural activities were funded by the governments. In the U.S. the lack of government funding for the arts left only two equally commercial art and culture (i.e., the art market). For different reasons, neither of these players would support new media art nor would they foster intellectual discourse about it. Out of the two, commercial culture (in other words, culture designed for mass audiences) has played a more progressive role in adopting and experimenting with new media, even though for obvious reasons the content of commercial new media products had severe limits. Yet without commercial culture we would not have computer games using Artificial Intelligence programming, network-based multimedia, including various Web plug-ins which enable distribution of music, moving images and 3-D environment over the Web, sophisticated 3-D modeling, animation and rendering tools, database-driven Web sites, CD-ROMs, DVD, and other storage formats, and most other advanced new media technologies and forms.

The 1990s U.S. art world proved to be the most conservative cultural force in contemporary society, lagging behind the rest of cultural and social institutions in dealing with new media technologies. (In the 1990s a standard joke at new media festivals was that a new media piece requires two interfaces: one for art curators, and one for everybody else.) This resistance is understandable given that the logic of the art world and the logic of new media are exact opposites. The first is based the romantic idea of authorship which assumes a single author; the notion of a unique, one-of-a-kind art object; and the control over the distribution of such objects which takes place through a set of exclusive places: galleries, museums, actions. The second privileges the existence of potentially numerous copies, infinitely large number of different states of the same work, author-user symbiosis (the user can change the work through interactivity), the collective, collaborative authorship, and network distribution (which bypasses the art system distribution channels). Moreover, exhibition of new media requires a level of technical sophistication and computer equipment which neither U.S. museums nor galleries were able to provide in the 1990s. In contrast, in Europe generous federal and regional funding allowed not only for mountings of sophisticated exhibitions but also for the development of a whole new form of art: interactive computer installation. It is true that after many years of its existence, the U.S. art world learned how to deal with and in fact fully embraced video installation, but video installations require standardized equipment and they don’t demand constant monitoring, as it is the case with interactive installations and even with Web pieces. While in Europe equipment-intensive form of interactive installation has flourished throughout the 1990s, U.S. art world has taken an easy way by focusing on “net art” i.e., Web-based pieces whose exhibition do not require much resources beyond an off-the-shelf computer and a Net connection.

All this started to change with the increasing speed by the end of the 1990s. Various cultural institutions in the U.S. finally begun to pay attention to new media. The first were education institutions. Around 1995 Universities and the art schools, particularly on the West Coast, begin to initiate program in new media art and design as well as open faculty positions in these areas; by the beginning of the new decade, practically every University and art school on the West Coast had both undergraduate and graduate programs in new media. A couple of years later museums such as Walker Art Center begun to mount a number of impressive online exhibitions and started to commission online projects. 2000 Whitney Biannual included a room dedicated to net art (even though its presentation conceptually was ages behind the presentation of new media in such places as Ars Electronica Center in Linz, Intercommunication Center in Tokyo, or ZKM in Germany). Finally in 2001, both Whitney Museum in New York and San Francisco Museum of Modern art (SFMOMA) have mounted large survey exhibitions of new media art (“Bitstreams” at the Whitney, ”010101: Art in Technological Times“ at SFMOMA). Add to this the constant flow of conferences and workshops mounted in such bastions of American Academia as the Institute for Advanced Studies in Princeton; fellowships in new media initiated by such prestigious funding bodies as Rockefeller Foundation and Social Science Research Council (both were started in 2001); book series on new media published by such well-respected presses as the MIT Press (this book is a part of such a series). What ten years ago was a cultural underground, became an established academic and artistic field; what has emerged from on the ground interactions of individual players has solidified, matured, and acquired institutional forms.

Paradoxically, at the same time as new media field has started to mature (the end of the 1990s), its very reason for existence came to be threatened. If all artists now, regardless of their preferred media, also routinely use digital computers to create, modify and produce works, do we need to have a special field of new media art? As digital and network media are rapidly becoming an omni-presence in our society, and as most artists came to routinely use them, new media field is facing a danger of becoming a ghetto whose participants would be united by their fetishism of latest computer technology, rather than by any deeper conceptual, ideological, or aesthetic issues – a kind of local club for photo enthusiasts. I personally do think that the existence of a separate new media field now and in the future makes very good sense, but it does require a justification – something that I hope the rest of this text that will take up more theoretical questions will help to provide.

## Software Design and Modern Art: Parallel Projects

Ten years after the appearance of first cultural institutions solely focused on new media, the field has matured and solidified. But what exactly is new media? And what is new media art? Surprisingly, these questions remain to be not so easy to answer. The book you are now holding in your hands does provide very interesting answers to these questions; it also provides the most comprehensive foundation for new media field, in the process redefining it a very productive way. In short, this book is not just a map of the field as it already exists but a creative intervention into it.

Through the particular selections and their juxtaposition this book re-defines new media as parallel tendencies in modern art and computing technology after the World War II. Although the editors of the anthology may not agree with this move, I would like to argue that eventually this parallelism changes the relationship between art and technology. In the last few decades of the twentieth century, modern computing and network technology materialized certain key projects of modern art developed approximately at the same time. In the process of this materialization, the technologies overtake art. That is, not only new media technologies – computer programming, graphical human-computer interface, hypertext, computer multimedia, networking (both wired-based and wireless) – have actualized the ideas behind the projects by artists, but they extended them much further than the artists originally imagined. As a result, these technologies themselves have become the greatest art works of today. The greatest hypertext text is the Web itself, because it is more complex, unpredictable, and dynamic than any novel that could have been written by a single human writer, even James Joyce. The greatest interactive work is the interactive human-computer interface itself: the fact that the user can easily change everything which appears on her screen, in the process changing the internal state of a computer, or even commanding reality outside of it. The greatest avant-garde film is software such as Final Cut Pro or After Effects which contains the possibilities to combining together thousands of separate tracks into a single movie, as well as setting various relationships between all these different tracks – and it thus it develops the avant-garde idea a film as an abstract visual score to its logical end – and beyond. Which means that computer scientists who invented these technologies – J.C. Licklider, Douglas Engelbart, Ivan Sutherland, Ted Nelson, Seymour Papert, Tim Berners-Lee, and others – are the important artists of our time – maybe the only artists who are truly important and who will be remembered from this historical period.

To prove the existence of historical parallelism, _New Media Reader_ systematically positions next to each other the key texts by modern art that articulate certain ideas and the key texts by modern computer scientists which articulate similar ideas in relation to software and hardware design. Thus, we find next to each other the story by Jorge Borges (1941) and the article by Vannevar Bush (1945) which both contain the idea of a massive branching structure as a better way to organize data and to represent human experience. [1]

The parallelism between texts by artists and by computer scientists involves not only the ideas in the texts but also the form of the texts. In the twentieth century artists typically presented their ideas either by writing manifestos or by creating actual art works. In the case of computer scientists, we either have theoretical articles that develop plans for particular software and/or hardware design or more descriptive articles about already created prototypes or the actual working systems. Structurally manifestos correspond to the theoretical programs of computer scientists, while completed artworks correspond to working prototypes or systems designed by scientists to see if their ideas do work, to demonstrate these ideas to colleagues, sponsors and clients. Therefore _New Media Reader_ to a large extent consists of these two types of texts: either theoretical presentations of new ideas and speculations about projects or types of projects that would follow from them; or the descriptions of the projects actually realized.

Institutions of modern culture that are responsible for selecting what makes it into the canon of our cultural memory and what is left behind are always behind the times. It may take a few decades or even longer for a new field which is making an important contribution to modern culture to “make it” into museums, books, and other official registers of cultural memory. In general, our official cultural histories tend to privilege art (understood in a romantic sense as individual products of individual artists) over mass industrial culture. For instance, while modern graphic and industrial designers do have some level of cultural visibility, their names, with the exception of a few contemporary celebrity designers such as Bruce Mau and Philippe Starck are generally not as well-known as the names of fine artists or fiction writers. Some examples of key contemporary fields that so far have not been given their due are music videos, cinematography, set design, and industrial design. But no cultural field so far remained more unrecognized than computer science and, in particular, its specific branch of human-computer interaction, or HCI (also called human-computer interface design, or HCI).

It is time that we treat the people who have articulated fundamental ideas of human-computer interaction as the major modern artists. Not only they invented new ways to represent any data (and thus, by default, all data which has to do with “culture,” i.e., the human experience in the world and the symbolic representations of this experience) but they have also radically redefined our interactions with all of the old culture. As a window of a Web browser comes to supplement cinema screen, a museum space, a CD player, a book, and a library, the new situation manifest itself: all culture, past and present, is being filtered through a computer, with its particular human-computer interface. Human-computer interface comes to act as a new form through which all older forms of cultural production are being mediated.

_New Media Reader_ contains essential articles by some of the key interface and software designers in the history of computing so far, from Engelbart to Berners-Lee. Thus in my view this book is not just an anthology of new media but also the first example of a radically new history of modern culture – a view from the future when more people will recognize that the true cultural innovators of the last decades of the twentieth century were interface designers, computer game designers, music video directors and DJs - rather than painters, filmmakers or fiction writers whose fields remained relatively stable during this historical period.

## What is New Media: Eight Propositions

Having discussed the particular perspective adopted by _New Media Reader_ in relation to the broader cultural context we may want to place new media in – the notion of parallel developments in modern art and in computing - I now want to go through other possible concepts of new media and its histories (including a few proposed by the present author elsewhere). Here are eight answers; without a doubt, more can be invented if desired.

### 1. New Media Versus Cyberculture

To begin with, we may distinguish between new media and cyberculture. In my view they represent two distinct fields of research. I would define cyberculture as the study of various social phenomena associated with Internet and other new forms of network communication. Examples of what falls under cyberculture studies are online communities, online multi-player gaming, the issue of online identity, the sociology and the ethnography of email usage, cell phone usage in various communities; the issues of gender and ethnicity in Internet usage; and so on. [2] Notice that the emphasis is on the _social_ phenomena; cyberculture does not directly deals with new cultural objects enabled by network communication technologies. The study of these objects is the domain of new media. In addition, new media is concerned with cultural objects and paradigms enabled by all forms of computing and not just by networking. To summarize: cyberculture is focused on the social and on networking; new media is focused on the cultural and computing.

### 2. New Media as Computer Technology used as a Distribution Platform

What are these new cultural objects? Given that digital computing is now used in most areas of cultural production, from publishing and advertising to filmmaking and architecture, how can we single out the area of culture that specifically owes its existence to computing? In my _The Language of New Media_ I begin the discussion of new media by invoking its definition which can be deduced from how the term is used in popular press: new media are the cultural objects which use digital computer technology for distribution and exhibition. [3] Thus, Internet, Web sites, computer multimedia, computer games, CD-ROMs and DVD, Virtual Reality, and computer-generated special effects all fall under new media. Other cultural objects which use computing for production and storage but not for final distribution - television programs, feature films, magazines, books, and other paper-based publications, etc. – are not new media.

The problems with this definition are three-fold. Firstly, it has to be revised every few years, as yet another part of culture comes to rely on computing technology for distribution (for instance, the shift from analog to digital television; the shift from film-based to digital projection of feature films in movie theatres; e-books, and so on). Secondly, we may suspect that eventually most forms of culture will use computer distribution, and therefore the term “new media” defined in this way will lose any specificity. Thirdly, this definition does not tell us anything about the possible effects of computer-based distribution on the aesthetics of what is being distributed. In other words, do Web sites, computer multimedia, computer games, CD-ROMs and Virtual Reality all have something in common because they are delivered to the user via a computer? Only if the answer is at least partial yes, it makes sense to think about new media as a useful theoretical category.

### 3. New Media as Digital Data Controlled by Software

_The Language of New Media_ is based on the assumption that, in fact, all cultural objects that rely on digital representation and computer-based delivery do share a number of common qualities. In the book I articulate a number of principles of new media: numerical representation, modularity, automation, variability and transcoding. I do not assume that any computer-based cultural object will necessary be structured according to these principles today. Rather, these are tendencies of a culture undergoing computerization that gradually will manifest themselves more and more. For instance, the principle of variability states that a new media cultural object may exist in potentially infinite different states. Today the examples of variability are commercial Web sites programmed to customize Web pages for every user as she is accessing the site particular user, or DJs remixes of already existing recordings; tomorrow the principle of variability may also structure a digital film which will similarly exist in multiple versions.

I deduce these principles, or tendencies, from the basic fact of digital representation of media. New media is reduced to digital data that can be manipulated by software as any other data. This allows automating many media operations, to generate multiple versions of the same object, etc. For instance, once an image is represented as a matrix of numbers, it can be manipulated or even generated automatically by running various algorithms, such as sharpen, blur, colorize, change contrast, etc.

More generally, extending what I proposed in my book, I could say that two basic ways in which computers models reality – through data structures and algorithms – can also be applied to media once it is represented digitally. In other words, given that new media is digital data controlled by particular “cultural” software, it makes sense to think of any new media object in terms of particular data structures and/or particular algorithms it embodies. [4] Here are the examples of data structures: an image can be thought of as a two-dimensional array (x. y), while a movie can be thought of as a three-dimensional array (x, y, t). Thinking about digital media in terms of algorithms, we discover that many of these algorithms can be applied to any media (such as copy, cut, paste, compress, find, match) while some still retain media specificity. For instance, one can easily search for a particular text string in a text but not for a particular object in an image. Conversely, one can composite a number of still or moving images together but not different texts. These differences have to do with different semiotic logics of different media in our culture: for example, we are ready to read practically any image or a composite of images as being meaningful, while for a text string to be meaningful we require that it obeys the laws of grammar. On the other hand, language has a priori discrete structure (a sentence consists of  words which consist of morphemes, and so on) that makes it very easily to automate various operations on it (such as search, match, replace, index), while digital representation of images does not by itself allow for automation of semantic operations.

### 4. New Media as the Mix Between Existing Cultural Conventions and the Conventions of Software

As particular type of media is turned into digital data controlled by software, we may expect that eventually it will fully obey the principles of modularity, variability, and automation. However, in practice these processes may take a long time and they do not proceed in a linear fashion – rather, we witness “uneven development.” For instance, today some media are already totally automated while in other cases this automation hardly exists – even though technologically it can be easily implemented.

Let us take as the example contemporary Hollywood film production. Logically we could have expected something like the following scenario. An individual viewer receives a customized version of the film that takes into account her/his previous viewing preferences, current preferences, and marketing profile. The film is completely assembled on the fly by AI software using pre-defined script schemas. The software also generates, again on the fly characters, dialog, and sets (this makes product placement particularly easy) that are taken from a massive “assets” database.

The reality today is quite different. Software is used in some areas of film production but not in others. While some visuals may be created using computer animation, cinema sill centers around the system of human stars whose salaries amount for a large percent of a film budget. Similarly, script writing (and countless re-writing) is also trusted to humans. In short, the computer is kept out of the key “creative” decisions and is delegated to the position of a technician.

If we look at another type of contemporary media - computer games – we will discover that they follow the principle of automation much more thoroughly. Game characters are modeled in 3D; they move and speak under software control. Software also decides what happens next in the game, generating new characters, spaces, and scenarios in response to user’s behavior. It is not hard to understand why automation in computer games is much more advanced than in cinema. Computer games is one of the few cultural forms “native” to computers; they begun as singular computer programs (before turning into a complex multimedia productions which they are today) - rather than being an already established medium (such as cinema) which is now slowly undergoing computerization.

Given that the principles of modularity, automation, variability, and transcoding are tendencies that slow and unevenly manifest themselves, is there a more precise way to describe new media, as it exists today? _The Language of New Media_ analyzes the language of contemporary new media (or, to put this differently, “early new media”) as the mix (we can also use software metaphors of “morph” or “composite”) between two different sets of cultural forces, or cultural conventions: on the one hand, the conventions of already mature cultural forms (such as a page, a rectangular frame, a mobile point of view) and, on the other hand, the conventions of computer software and, in particular, of HCI, as they developed until now.

Let me illustrate this idea with two examples. In modern visual culture a representational image was something one gazed at, rather than interacted with. An image was also one continuous representational field, i.e., a single scene. In the 1980s GUI redefined an image as a figure-ground opposition between a non-interactive, passive ground (typically a desktop pattern) and active icons and hyperlinks (such as the icons of documents and applications appearing on the desktop). The treatment of representational images in new media represents a mix between these two very different conventions. An image retains its representational function while at the same time is treated as a set of hot spots (“image-map”). This is the standard convention in interactive multimedia, computer games and Web pages. So, while visually an image still appears as a single continuous field, in fact it is broken into a number of regions with hyperlinks connected to these regions, so clicking on a region opens a new page, or re-starts game narrative, etc.

This example illustrates how a HCI convention is “superimposed” (in this case, both metaphorically and literally, as a designer places hot spots over an existing image) over an older representational convention. Another way to think about this is to say that a technique normally used for control and data management is mixed with a technique of fictional representation and fictional narration. I will use another example to illustrate the opposite process: how a cultural convention normally used for fictional representation and narration is “superimposed” over software techniques of data management and presentation. The cultural convention in this example is the mobile camera model borrowed from cinema. In _The Language of New Media_ I analyze how it became a generic interface used to access any type of data:

> Originally developed as part of 3D computer graphics technology for such applications as computer-aided design, flight simulators and computer movie making, during the 1980's and 1990's the camera model became as much of an interface convention as scrollable windows or cut and paste operations. It became an accepted way for interacting with any data which is represented in three dimensions — which, in a computer culture, means literally anything and everything: the results of a physical simulation, an architectural site, design of a new molecule, statistical data, the structure of a computer network and so on. As computer culture is gradually spatializing all representations and experiences, they become subjected to the camera's particular grammar of data access. Zoom, tilt, pan, and track: we now use these operations to interact with data spaces, models, objects and bodies. [5]

To sum up: new media today can be understood as the mix between older cultural conventions for data representation, access and manipulation and newer conventions of data representation, access, and manipulation. The “old” data are representations of visual reality and human experience, i.e., images, text-based and audio-visual narratives – what we normally understand by “culture.” The “new” data is numerical data.

As a result of this mix, we get such strange hybrids as clickable “image- maps,” navigable landscapes of financial data, QuickTime (which was defined as the format to represent any time-based data but which in practice is used exclusively for digital video), animated icons – a kind of micro-movies of computer culture – and so on.

As can be seen, this particular approach to new media assumes the existence of historically particular aesthetics that characterizes new media, or “early new media,” today. (We may also call it the “aesthetics of early information culture.”) This aesthetics results from the convergence of historically particular cultural forces: already existing cultural conventions and the conventions of HCI. Therefore, it could not have existed in the past and it unlikely to stay without changes for a long time. But we can also define new media in the opposite way: as specific aesthetic features which keep re-appearing at an early stage of deployment of every new modern media and telecommunication technologies.

### 5. New Media as the Aesthetics that Accompanies the Early Stage of Every New Modern Media and Communication Technology

Rather than reserving the term new media to refer to the cultural uses of current computer and computer-based network technologies, some authors have suggested that every modern media and telecommunication technology passes through its “new media stage.” In other words, at some point photography, telephone, cinema, television each were “new media.” This perspective redirects our research efforts: rather than trying to identify what is unique about digital computers functioning as media creation, media distribution, and telecommunication devices, we may instead look for certain aesthetic techniques and ideological tropes which accompany every new modern media and telecommunication technology at the initial stage of its introduction and dissemination. Here are a few examples of such ideological tropes: new technology will allow for “better democracy; it will give us a better access to the “real” (by offering “more immediacy” and/or the possibility “to represent what before could not be represented”); it will contribute to “the erosion of moral values”; it will destroy the “natural relationship between humans and the world” by “eliminating the distance” between the observer and the observed.

And here are two examples of aesthetic strategies that seem to often accompany the appearance of new media and telecommunication technology (not surprisingly, these aesthetic strategies are directly related to ideological tropes I just mentioned). In the mid-1990s a number of filmmakers started to use inexpensive digital cameras (DV) to create films characterized by a documentary style (for instance, _Timecode_, _Celebration_, _Mifune_). Rather than treating live action as a raw material to be later re-arranged in post-production, these filmmakers place premier importance on the authenticity of the actors’ performances. The smallness of DV equipment allows a filmmaker to literally be inside the action as it unfolds. In addition to adopting a more intimate filmic approach, a filmmaker can keep shooting for a whole duration of a 60 or 120 minute DV tape as opposed to the standard ten-minute film roll. This gives the filmmaker and the actors more freedom to improvise around a theme, rather than being shackled to the tightly scripted short shots of traditional filmmaking. (In fact, the length of _Time Code_ exactly corresponds to the length of a standard DV tape.)

These aesthetic strategies for representing real which at first may appear to be unique to digital revolution in cinema and in fact not unique. DV-style filmmaking has a predecessor in an international filmmaking movement that begun in the late 1950s and unfolded throughout the 1960s. Called “direct cinema,” “candid” cinema, “uncontrolled” cinema, “observational” cinema, or cinéma vérité (“cinema truth”), it also involved filmmakers using lighter and more mobile (in comparison to what was available before) equipment. Like today’s DV realists,” the 1960s “direct cinema” proponents avoided tight staging and scripting, preferring to let events unfold naturally. Both then and now, the filmmakers used new filmmaking technology to revolt against the existing cinema conventions that were perceived as being too artificial. Both then and now, the key word of this revolt was the same: “immediacy.”

My second example of similar aesthetic strategies re-appearing more than deals with the development of moving image technology throughout the nineteenth century, and the development of digital technologies to display moving images on a computer desktop during the 1990s. In the first part of the 1990s, as computers' speed kept gradually increasing, the CD-ROM designers have been able to go from a slide show format to the superimposition of small moving elements over static backgrounds and finally to full-frame moving images. This evolution repeats the nineteenth century progression: from sequences of still images (magic lantern slides presentations) to moving characters over static backgrounds (for instance, in Reynaud's Praxinoscope Theater) to full motion (the Lumières’s cinematograph). Moreover, the introduction of QuickTime by Apple in 1991 can be compared to the introduction of the Kinetoscope in 1892: both were used to present short loops, both featured the images approximately two by three inches in size, both called for private viewing rather than collective exhibition. Culturally, the two technologies also functioned similarly: as the latest technological “marvel.” If in the early 1890s the public patronized Kinetoscope parlors where peep-hole machines presented them with the latest invention — tiny moving photographs arranged in short loops; exactly a hundred years later, computer users were equally fascinated with tiny QuickTime Movies that turned a computer in a film projector, however imperfect. Finally, the Lumières’s first film screenings of 1895 which shocked their audiences with huge moving images found their parallel in 1995 CD-ROM titles where the moving image finally fills the entire computer screen (for instance, in _Jonny Mnemonic_ computer game, based on the film by the same title). Thus, exactly a hundred years after cinema was officially "born," it was reinvented on a computer screen.

Interesting as they are, these two examples also illustrate the limitations of thinking about new media in terms of historically recurrent aesthetic strategies and ideological tropes. While ideological tropes indeed seem re-appearing rather regularly, many aesthetic strategies may only reappear two or three times. Moreover, some strategies and/or tropes can be already found in the first part of the nineteenth century while others only make their first appearance much more recently. [6] In order for this approach to be truly useful it would be insufficient to simply name the strategies and tropes and to record the moments of their appearance; instead, we would have to develop a much more comprehensive analysis which would correlate the history of technology with social, political and economic histories of the modern period.

So far, my definitions of new media focused on technology; the next three definitions will consider new media as material re-articulation, or encoding, of purely cultural tendencies – in short, as ideas rather than technologies.

### 6. New Media as Faster Execution of Algorithms Previously Executed Manually or Through Other Technologies

A modern digital computer is a programmable machine. This simply means that the same computer can execute different algorithms. An algorithm is a sequence of steps that need to be followed to accomplish a task. Digital computers allow to execute most algorithms very quickly, however in principle an algorithm, since it is just a sequence of simple steps, can be also executed by a human, although much more slowly. For instance, a human can sort files in a particular order, or count the number of words in a text, or cut a part of an image and paste it in a different place.

This realization gives us a new way to think about both digital computing, in general, and new media, in particular, as a massive speed-up of various manual techniques that all have already existed. Consider, for instance, computer’s ability to represent objects in linear perspective and to animate such representations. When you move your character through the world in a first-person shooter computer game (such as _Quake_), or when you move your viewpoint around a 3D architectural model, a computer re-calculates perspectival views for all the objects in the frame many times every second (in the case of current desktop hardware, frame rates of 80 frames of second are not uncommon). But we should remember that the algorithm itself was codified during the Renaissance in Italy, and that, before digital computers came along (that is, for about five hundred years) it was executed by human draftsmen. Similarly, behind many other new media techniques there is an algorithm that, before computing, was executed manually. (Of course, since art has always involved some technology – even as simple as a stylus for making marks on stone – what I mean by “manually” is that a human had to systematically go through every step of an algorithm himself, even if he was assisted by some image making tools.) Consider, for instance, another very popular new media technique: making a composite from different photographs. Soon after photography was invented, such nineteenth century photographers as Henry Peach Robinson and Oscar G. Rejlander were already creating smooth "combination prints" by putting together multiple photographs.

While this approach to thinking about new media takes us away from thinking about it purely in technological terms, it has a number of problems of its own. Substantially speeding up the execution of an algorithm by implementing this algorithm in software does not just leave things as they are. The basic point of dialectics is that a substantial change in quantity (i.e., in speed of execution in this case) leads to the emergence of qualitatively new phenomena. The example of automation of linear perspective is a case in point. Dramatically speeding up the execution of a perspectival algorithm makes possible previously non-existent representational technique: smooth movement through a perspectival space. In other words, we get not only quickly produced perspectival drawings but also computer-generated movies and interactive computer graphics.

The technological shifts in the history of “combination prints” also illustrate the cultural dialectics of transformation of quantity into quality. In the nineteenth century, painstakingly crafted “combination prints” represented an exception rather than the norm. In the twentieth century, new photographic technologies made possible photomontage that quickly became one of the basic representational techniques of modern visual culture. And finally, the arrival of digital photography via software like Photoshop, scanners, and digital cameras in the late 1980s and 1990s not only made photomontage much more omnipresent than before but it also fundamentally altered its visual characteristics. In place of graphic and hard-edge compositions pioneered by Moholy-Nagy and Rodchenko we now have smooth multi-image composites which use transparency, blur, colorization and other easily available digital manipulations and which often incorporate typography that is subjected to exactly the same manipulations (thus in Post-Photoshop visual culture the type becomes a subset of a photo-based image). To see this dramatic change, it is enough to compare a typical music video from 1985 and a typical music video from 1995: within ten years, visual aesthetics of photomontage have undergone a fundamental change.

Finally, thinking about new media as speeding up of algorithms which previously were executed by hand foregrounds the use of computers for fast algorithm execution, but ignores its two other essential uses: real-time network communication and real-time control. The abilities to interact with or control remotely located data in real-time, to communicate with other human beings in real-time, and control various technologies (sensors, motors, other computers) in real time constitute the very foundation of our information society – phone communications, Internet, financial networking, industrial control, the use of micro-controllers in numerous modern machines and devices, and so on. They also make possible many forms of new media art and culture: interactive net art, interactive computer installations, interactive multimedia, computer games, real-time music synthesis.

While non-real time media generation and manipulation via digital computers can be thought of as speeding up of previously existing artistic techniques, _real-time_ networking and control seem to constitute qualitatively new phenomena. When we use Photoshop to quickly combine photographs together, or when we compose a text using a Microsoft Word, we simply do much faster what before we were doing either completely manually or assisted by some technologies (such as a typewriter). However, in the cases when a computer interprets or synthesizes human speech in real time, monitors sensors and modifies programs based on their input in real-time, or controls other devices, again in real-time, this is something which simply could not be done before. So, while it is important to remember that, on one level, a modern digital computer is just a faster calculator, we should not ignore its other identity: that of a cybernetic control device. To put this in different way, while new media theory should pay tributes to Alan Turing, it should not forget about its other conceptual father – Norbert Weiner.

### 7. New Media as the Encoding of Modernist Avant-Garde; New Media as Meta-media

The approach to new media just discussed does not foreground any particular cultural period as the source of algorithms that are eventually encoded in computer software. In my article “Avant-garde as Software” I have proposed that, in fact, a particular historical period is more relevant to new media than any other– that of the 1920s (more precisely, the years between 1915 and 1928). [7] During this period the avant-garde artists and designers have invented a whole new set of visual and spatial languages and communication techniques that we still use today. 

According to my hypothesis, with new media, 1920s communication techniques acquire a new status. Thus, new media does represent a new stage of the avant-garde. The techniques invented by the 1920s Left artists became embedded in the commands and interface metaphors of computer software. In short, the avant-garde vision became materialized in a computer. All the strategies developed to awaken audiences from a dream-existence of bourgeois society (constructivist design, New Typography, avant-garde cinematography and film editing, photomontage, etc.) now define the basic routine of a post-industrial society: the interaction with a computer. For example, the avant-garde strategy of collage reemerged as a "cut and paste" command, the most basic operation one can perform on any computer data. In another example, the dynamic windows, pull-down menus, and HTML tables all allow a computer user to simultaneously work with practically unrestricted amount of information despite the limited surface of the computer screen. This strategy can be traced to Lissitzky's use of movable frames in his 1926 exhibition design for the International Art Exhibition in Dresden.

The encoding of the 1920s avant-garde techniques in software does not mean that new media simply qualitatively extends the techniques which already existed. Just as it is the case with the phenomenon of real-time computation that I discussed above, tracing new media heritage in the 1920s avant-garde reveals a qualitative change as well. The modernist avant-garde was concerned with “filtering” visible reality in new ways. The artists are concerned with representing the outside world, with “seeing” it in as many different ways as possible. Of course, some artists already begin to react to the emerging media environment by making collages and photomontages consisting of  newspaper clippings, existing photographs, pieces of posters, and so on; yet these practices of manipulating existing media were not yet central. But a number of decades later they have come to the foreground of cultural production. To put this differently, after a century and a half of media culture, already existing media records (or “media assets,” to use the Hollywood term) become the new raw material for software-based cultural production and artistic practice. Many decades of analog media production resulted in a huge media archive, and it is the contents of this archive – television programs, films, audio recordings, etc. – which became the raw data to be processed, re-articulated, mined, and re-packaged through digital software – rather than raw reality. In my article I formulate this as follows:

New Media indeed represents the new avant-garde, and its innovations are at least as radical as the formal innovations of the 1920s. But if we are to look for these innovations in the realm of forms, this traditional area of cultural evolution, we will not find them there. For the new avant-garde is radically different from the old:

1. The _”old media avant-garde“_ of the 1920s came up with new forms, new ways to represent reality and new ways to see the world. The _”new media avant-garde“_ is about new ways of accessing and manipulating information. Its techniques are hypermedia, databases, search engines, data mining, image processing, visualization, and simulation.

2. The new avant-garde is no longer concerned with seeing or representing the world in new ways but rather with accessing and using in new ways previously accumulated media. In this respect new media is post-media or _”meta-media“_, as it uses old media as its primary material

My concept of “meta-media” is related to a more familiar notion of “post-modernism” – the recognition that by the 1980s the culture became more concerned with reworking already existing content, idioms, and style rather than creating genially new ones. What I would like to stress (and what I think the original theorists of post-modernism in the 1980s have not stressed enough) is the key role played by the material factors in the shift towards post-modernist aesthetics: the accumulation of huge media assets and the arrival of new electronic and digital tools which made it very easy to access and re-work these assets. This is another example of quantity changing into quality in media history: the gradual accumulation of media records and the gradual automation of media management and manipulation techniques eventually recoded modernist aesthetics into a very different post-modern aesthetics.

### 8. New Media as Parallel Articulation of Similar Ideas in Post WWII Art and Modern Computing

Along with the 1920s, we can think of other cultural periods that generated ideas and sensibilities particularly relevant to new media. In the 1980s a number of writers looked at the connections between Baroque and post-modern sensibilities; given the close linked between post-modernism and new media I just briefly discussed, it would be logical if the parallels between Baroque and new media can also be established. [8] It can be also argued that in many ways new media returns us to a pre-modernist cultural logic of the eighteenth century: consider for instance, the parallel between eighteenth century communities of readers who were also all writers and participants in Internet newsgroups and mailing lists who are also both readers and writers.

In the twentieth century, along with the 1920s, which for me to represent the cultural peak of this century (because during this period more radically new aesthetic techniques were prototyped than in any other period of similar duration), the second culturally peak –1960s – also seem to contain many of new media genes. A number of writers such as Söke Dinkla have argued that interactive computer art (1980s -) further develops ideas already contained in the new art of the 1960s (happenings, performances, installation): active participation of the audience, an artwork as a temporal process rather than as a fixed object, an artwork as an open system. [9] This connection makes even more sense when we remember that some of the most influential figures in new media art (Jeffrey Shaw, Roy Ascott) have started their art careers in the 1960s and only later moved to computing and networking technologies. For instance, in the end of the 1960s Jeffrey Shaw was working on inflatable structures for film projections and performances which were big enough to contain a small audience inside – something which he later came back to in many of his VR installations, and even more directly in EVE project. [10]

There is another aesthetic project of the 1960s that also can be linked to new media not only conceptually but also historically, since the artists who pursued this project with computers (such as Manfred Mohr) knew of minimalist artists who during the same decade pursued the same project “manually” (most notably, Sol LeWitt). [11] This project can be called “combinatorics.” [12] It involves creating images and/or objects by systematically varying a single parameter or by systematically creating all possible combinations of a small number of elements. [13] “Combinatorics” in computer art and minimalist art of the 1960s led to the creation of remarkably similar images and spatial structures; it illustrates well that the algorithms, this essential part of new media, do not depend on technology but can be executed by humans.

## Four Decades of New Media

Along with the ones I already mentioned, more connections between 1960s cultural imagination and new media exist. _New Media Reader_ contains a number of important texts by the radical artists and writers from the 1960s which have conceptual affinity to the logic of computing technology: Allan Kaprow, William Borrows; Oulipo movement (whose members pursued combinatorics project in relation to literature), Nam June Paik and others. “The Complex, the Changing, and the Intermediate” part of the reader presents the most comprehensive, to date, set of cultural texts from the 1960s whose ideas particularly resonate with the developments in computing in the same period.

Although modern computing has many conceptual fathers and mothers, from Leibnitz to Ada Lovelace, and its prehistory spans many centuries, I would argue that the paradigm that still defines our understanding and usage of computing was defined in the 1960s. During the 1960s the principles of modern interactive graphical user interface (GUI) where given clear articulation (although the practical implementation and refined of these ideas took place later, in the 1970s at Xerox Parc). The articles by Licklider, Sutherland, Nelson, and Engelbart from the 1960s included in the reader are the essential documents of our time; one day the historians of culture would rate them on the same scale of importance as texts by Marx, Freud, and Saussure. (Other key developments that also took place in the 1960s – early 1970s were Internet, Unix, and object- oriented programming. A number of other essential ideas of modern computing such as networking itself, the use of computers for real-time control, and the graphical interactive display were articulated earlier, in the second part of the 1940s and the first part of the 1950s.) [14]

The first section of the reader takes us to the end of the 1970s; by this time the key principles of modern computing and GUI were already practically implemented and refined by the developers at Xerox Parc but they were not yet commercially available to consumers. The second section “Media Manipulation, Media Design” covers the late 1970s and the 1980s. During this period Macintosh (released in 1984) popularized GUI; it also shipped with a simple drawing and painting programs which emphasized the new role of a computer as a creative tool; finally, it was the first inexpensive computer which came with a bit-mapped display. Atari computers made computer-based sound manipulation affordable; computer games achieved a new level of popularity; cinema started to use computers for special effects (Tron released by Disney in 1982 contained seventeen minutes of 3-D computer generated scenes); towards the very end of the decade, Photoshop, which can be called the key software application of post-modernism, was finally released. All these developments of the 1980s created new set of roles for a modern digital computer: a manipulator of existing media (Photoshop); a media synthesizer (film special effects, sound software), and a new medium (or rather, a set of new mediums) in its own right (computer games). _New Media Reader_ collects essential articles by computer scientists from the 1980s that articulate ideas behind these new roles of a computer (Bolt, Shneiderman, Laurel, and others).

As computing left the strict realm of big business, the military, the government, and the university and entered society at large, cultural theorists begin to think about its effects, and it is appropriate that _New Media Reader_ also reprints key theoretical statements from the 1980s (Turkle, Haraway). I should note here that European cultural theorists reacted to computerization earlier than the Americans: both Lyotard’s _The Post-Modern Condition_ (1979) and Baudrillard’s _Simulacra and Simulations_ (1981) contain detailed discussions of computing, something which their 1980s American admirers did not seem to notice.

The last section of the reader “Revolution, Resistance, and the Web’s Arrival” continues to weave texts by computer scientists, social researchers, cultural theorists, and critics from the end of the 1980s onward; it also takes us into the early 1990s when the rise of the Web redefined computing one again. If the 1980s gradually made visible the new role of a computer as a media manipulator and an interface to media – the developments which eventually were codified around 1990 in the term “new media” – in the 1990s another role of a digital computer (which was already present since the late 1940s) came to the foreground: that of a foundation for real-time multi-media networking, available not just for selected researchers and the Military (as it was for decades) but for millions of people.

In the 1960s we can find strong conceptual connections between computing and radical art of the period, but with the sole exception of Ted Nelson (the conceptual father of hypertext) no computer scientist was directly applying radical political ideas of the times to computer design. In fact, these ideas had a strong effect of the field, but it was delayed until the 1970s when Alan Kay and his colleagues at Xerox Parc pursued the vision of personal computer workstation that would empower an individual rather than a big organization. In the late 1980s and early 1990s, however, we seem to witness a different kind of parallel between social changes and computer design. Although causally unrelated, conceptually it makes sense that the end of cold War and the design of the Web took place at exactly the same time. The first development ended the separation of the world into separate parts closed to each other, making it a single global system; the second development connected world’s computers into a single networking. The early Web (i.e., before it came to be dominated by big commercial portals towards the end of the 1990s) also practically implemented a radically horizontal, non-hierarchical model of human existence in which no idea, no ideology and no value system can dominate the rest – thus providing a perfect metaphor to a new post-Cold War sensibility.

The emergence of new media studies as a field testifies to our recognition of the key cultural role played by digital computers and computer-enabled networking in our global society. For a field in its infancy, we are very lucky to now have such a comprehensive record of its origins as the one provided by _New Media Reader_; I believe that its readers would continue to think about both the ideas in its individual texts and the endless connections which can be found between different texts for many years to come.

## References:

[1] More subtle but equally convincing is the relationship between _Panopticism_ by Michel Foucault which comes from his book _Discipline and Punish_ (1975) and _Personal Dynamic Media_ by Alan Kay and Adele Goldberg (1997). In 1960s and 1970s the prevalent model of computer use was time sharing. It was Panopticon-like in so far as it involved a single centralized computer with terminals connected to it and thus was conceptually similar to an individual prisoner’s cell connected by lines of site to the central tower in Panopticon. At the end of the 1960s, computer scientist Alan Kay pioneered a radically different idea of a personalized computer workstation, a small and mobile device that he called Dynabook. This idea came to be realized only in 1984 with the introduction of Macintosh. (It is not accidental that the famous Apple commercial - directed by Ridley Scott who two years earlier made _Blade Runner_ - explicitly invokes the images of Orwellian-like society of imprisonment and control, with Macintosh bringing liberation to the users imprisoned by an older computing paradigm.)

[2] For a good example of cyberculture paradigm, see online _Resource Center for Cyberculture Studies_[www.otal.umd.edu/%7Erccs/](www.otal.umd.edu/~rccs/)).

[3] Lev Manovich, _The Language of New Media_ (Cambridge, Mass.: The MIT Press, 2001).

[4] I don’t mean here the actual data structures and algorithms which may be used by particular software – rather, I am thinking of them in more abstract way: what is the structure of a cultural objects and what kind of operations it enables for the user.

[5] Manovich, _The Language of New Media_, 80.

[6] I believe that the same problems apply to Erkki Huhtamo’s very interesting theory of media archeology which is close to the approach presented here and which advocates the study of tropes which accompany the history of modern media technology, both the ones which were realized and the ones which were only imagined.

[7] Lev Manovich, “Avant-Garde as Software,” in _Ostranenie_, edited by Stephen Kovats (Frankfurt and New York: Campus Verlag, 1999). Available online at [www.manovich.net](www.manovich.net). (The subsequent quotes are from the online text.)

[8] See Norman Klein’s book _From Vatican to Las Vegas: A History of Special Effects_ that is discussing in detail the connections between the treatment of space in Baroque and in cyberculture. Norman Klein, _From Vatican to Las Vegas: A History of Special Effects_ (New Press, 2004.)

[9] See, for instance, Söke Dinkla, "From Participation to Interaction: Towards the Origins of Interactive Art," in _Clicking In: Hots Links to a Digital Culture_, edited by Lynn Hershman Leeson (Seattle: Bay Press, 1996). 

[10] Jeffrew Shaw, ed., _Jeffrey Shaw-A User's Manual_ (DAP, 1997). 

[11] For Manfred Mohr, see [http://www.emohr.com/](http://www.emohr.com/). 

[12] Frank Dietrich has used the term “combinatorics” to talk about a particular direction in the early computer art of the 1960s. See Frank Dietrich, "Visual Intelligence: The First Decade of Computer Art," (Computer Graphics, 1985). 

[13] It is interesting that Sol LeWitt was able to produce works “by hand” which often consisted of more systematic variations of the same elements than similar works done by other artists who used computers. In other words, we can say that Sol LeWitt was better in executing certain minimalist algorithms than the computers of the time.

[14] See Paul N. Edwards, _The Closed World: Computers and the Politics of Discourse in Cold War America_, reprint edition (The MIT Press, 1997).

---

# The Poetics of Augmented Space

_author: Lev Manovich_
 _year: 2002, updated 2005_

How is our experience of a spatial form affected when the form is filled in with dynamic and rich multimedia information? (The examples of such environments are _particular urban spaces_ such as shopping and entertainment areas of Tokyo, Hong Kong, and Seoul where the walls of the buildings are completely covered with electronic screens and signs; convention and trade shows halls; department stores, etc.; and at the same time, _any human-constructed space_ where the subject can access various information wirelessly on her cell phone, PDA, or laptop). Does the form become irrelevant, being reduced to functional and ultimately invisible support for information flows? Or do we end up with a new experience in which the spatial and information layers are equally important? In this case, do these layers add up to a single phenomenological gestalt or are they processed as separate layers?

Although historically built environments were almost always covered with ornament, texts (for instance, shop signs), and images (fresco paintings, icons, sculptures, etc. – think of churches in most cultures), the phenomenon of the dynamic multimedia information in these environments is new. Also new is the delivery of such information to a small personal device such as a cell phone, which a space dweller can carry with her.

Therefore, this essay will discuss how the general dynamic between spatial form and information which has been with us for a long time and which I outlined above functions differently in computer culture of today. Since the kinds of environments I offered, as examples above do not have a recognizable name yet, I will give it a new name - _an augmented space_. The term will be explained in more detail below, but here is the brief definition: augmented space is the physical space overlaid with dynamically changing information. This information is likely to be in multimedia form and it is often localized for each user.

I want to focus on the experience of the human subject in augmented space as opposed to particular electronic, computer, and network technologies through which the augmentation is achieved. I also want to re-conceptualize augmentation as an idea and cultural and aesthetic practice rather than as technology. To do this, I will discuss how various practices in professional and vernacular architecture and build environments, cinema, 20th century art, and media art can be understood in terms of augmentation. I hope that this will firmly position the concept of augmented space in historical and cultural as opposed to purely technological sphere.

## Augmentation and Monitoring

The 1990s were about the virtual. We were fascinated by the new virtual spaces made possible by computer technologies. Images of an escape into a virtual space that leaves -physical space useless, and of cyberspace – a virtual world that exists in parallel to our world – dominated the decade. This phenomenon started with the media obsession with Virtual Reality (VR). In the middle of the decade graphical browsers for the World Wide Web made cyberspace a reality for millions of users. During the second part of the 1990s, yet another virtual phenomenon – dot coms – rose to prominence, only to crash in the real-world laws of economics. By the end of the decade, the daily dose of cyberspace (using the Internet to make plane reservations, check e-mail using a Hotmail account, or download MP3 files) became so much the norm that the original wonder of cyberspace so present in the early cyberpunk fiction of the 1980s and still evident in the original manifestos of VRML evangelists of the early 1990s - was almost completely lost. [1] The virtual became domesticated. Filled with advertisements and controlled by big brands, it was rendered harmless. In short, to use Norman Klein’s expression, it became an “electronic suburb.”

At the beginning of the twenty first century the research agendas, media attention, and practical applications have come to focus on a new agenda – the physical – that is, physical space filled with electronic and visual information. The previous icon of the computer era – a VR user traveling in virtual space – has been replaced by a new image: a person checking her e-mail or making a phone call using her PDA/cell phone combo while at the airport, on the street, in a car, or any other actually existing space. But this is just one example of what I see as a larger trend. Here are a few more examples of the technological applications that _dynamically deliver dynamic data to, or extract data from, physical space_ – and which already are widely employed at the time of this writing: [2]

1. _Video surveillance_ is becoming ubiquitous. No longer employed only by governments, the military, and businesses but also by individuals; cheap, tiny, wireless, and Net-enabled, video cameras can now be placed almost anywhere. (For instance, by 2002, many taxis already had video cameras continuously recording the inside of the cab.)

2. If video and other types of surveillance technologies translate the physical space and its dwellers into data, _cellspace technologies_ (also referred to as mobile media, wireless media, or location-based media) work in the opposite direction: delivering data to the mobile physical space dwellers. Cellspace is physical space that is “filled” with data, which can be retrieved by a user via a personal communication device. [3] Some data may come from global networks such as the Internet; some may be embedded in objects located in the space around the user. Moreover, while some data may be available regardless of where the user is in the space, it can also be location-specific. The examples of the cellspace applications which are not localized is using GPS to determine your coordinates, or surfing and checking email using a cell phone. The examples of location specific applications are using a cell phone to check in at the airport, pay for a road toll, or retrieve information about a product in a store. [4]

3. While we can think of cellspace as the invisible layer of information that is laid over physical space and is customized by an individual user, publicly _located computer / video displays_ present the same visible information to passersby. These displays are gradually becoming larger and thinner; they are no longer confined to flat surfaces; they no longer require darkness to be visible. In the short term, we may expect large thin displays to become more pervasive in both private and public spaces (perhaps using technology such as e-ink). In the longer term, every object may become a screen connected to the Net with the whole of built space eventually becoming a set of display surfaces. [5] Of course, physical space has long been augmented by images, graphics, and type; but replacing all of these with electronic displays makes it possible to present dynamic images, to mix images, graphics, and type, and to change the content at any time.

If we consider the effect of these three technological applications (surveillance, cellspace, electronic displays) on our concept of space and, consequently, on our lives as far as they are lived in various spaces, I believe that they very much belong together. They make physical space into a dataspace: extracting data from it (surveillance) or augmenting it with data (cellspace, computer displays).

It also makes sense to conceptually connect the surveillance/monitoring of physical space and its dwellers, and the augmentation of this space with additional data, because technologically these two applications are in a symbiotic relationship. For instance, if you know the location of a person equipped with a cell phone, you can send them particular information relevant to that specific location via their cell phone. A similar relationship exists in the case of software agents, affective computing, and similar interfaces, which take a more active role in assisting the user than the standard Graphical User Interface (GUI). By tracking the user – her mood, her pattern of work, her focus of attention, her interests, and so on – these interfaces acquire information about the user, which they then use to automatically perform the tasks for her.

The close connection between surveillance/monitoring and assistance/augmentation is one of the key characteristics of the high-tech society. This is how such technologies are made to work, and this is why I am discussing data flows from physical space (surveillance, monitoring, tracking) and into physical space (cellspace applications, computer screens, and other examples below) together.

## Panopticum and Information Theory

Let us now add to these three examples of the technologies that are already at work by citing a number of the research paradigms which are being actively conducted in university and industry labs. Note that many of them overlap, mining the same territory but with a somewhat different emphasis:

1. _Ubiquitous Computing_: the shift which away from computing centered in desktop machines and towards smaller multiple devices distributed throughout the space. [6]

2. _Augmented Reality_: a paradigm that originated around the same time as ubiquitous computing (1990) – the laying of dynamic and context-specific information over the visual field of a user (see below for more details). [7]

3. _Tangible Interfaces_: treating the whole of physical space around the user as part of a human-computer interface (HCI) by employing physical objects as carriers of information. [8]

4. _Wearable Computers_: embedding computing and telecommunication devices into clothing.

5. _Intelligent Buildings_ (or Intelligent Architecture): buildings wired to provide cellspace applications.

6. _Intelligent Spaces_: spaces that monitor user’s interaction with them via multiple channels and provide assistance for information retrieval, collaboration, and other tasks (think of Hal in 2001). [9]

7. _Context-aware Computing_: an umbrella term used to refer to all or some of the developments above, signaling a new paradigm in the computer science and HCI fields. [10]

8. _Ambient Intelligence_: alternative term, which also refers to all or some of the paradigms, summarized above.

9. _Smart Objects_: objects connected to the Net; objects that can sense their users and display “smart” behavior.

10. _Wireless Location Services_: delivery of location-specific data and services to portable wireless devices such as cell phones (i.e., similar to cellspace).

11. _Sensor Networks_: networks of small sensors that can be used for surveillance and environmental monitoring, to create intelligent spaces, and similar applications.

12. _E-paper_ (or _e-ink_): a very thin electronic display on a sheet of plastic, which can be flexed into different shapes and which displays information that is received wirelessly. [11]

While the technologies imagined by these research paradigms accomplish their intentions in a number of different ways, the end result is the same: _overlaying dynamic data over the physical space_. I will use the term “augmented space” to refer to this new kind of physical space. As I have already mentioned, this overlaying is often made possible by the tracking and monitoring of users. In other words, the delivery of information to users in space, and the extraction of information about those users, are closely connected. Thus, _augmented space_ is also _monitored space_.

Augmented space is the physical space which is “data dense,” as every point now potentially contains various information which is being delivered to it from elsewhere. At the same time, video surveillance, monitoring, and various sensors can also extract information from any point in space, recording the face movements, gestures and other human activity, temperature, light levels, and so on. Thus, we can say that various augmentation and monitoring technologies add new dimensions to a 3D physical space, making it multi-dimensional. As a result, the physical space now contains many more dimensions than before, and while from the phenomenological perspective of the human subject, the “old” geometric dimensions may still have the priority, from the perspective of technology and its social, political, and economic uses, they are no longer more important than any other dimension.

This demise in importance of geometry as seen in augmented spaces can be understood as a part of a larger paradigm shift. If modern society as summed up in Michel Foucault’s metaphor of Panopticon was organized around the strait lines of human sight, i.e., the geometry of the visible, this is no longer the case for our society. While some technologies such as video surveillance and infrared communication still require a line of sight, most do not. The examples are cellular and Bluetooth communication, radar, and environmental sensors. Instead of the binary logic of visible/invisible, the new spatial logic can be described using such terms as functions or fields, since from the point of view of these new technologies, every point in space has a particular value on a possible continuum. (Think for instance of a strength of your cellular signal which varies depending how close you are to a cell or whether you outside or inside.) In the case of information delivery into space, these values determine how much, how quickly and how successfully this information can be delivered – in other words, it corresponds to communication bandwidth. In the case of monitoring or surveillance, these values similarly affect how much and how successfully information can be extracted from a point, or region in space. In either case, if the old binary logic of visible/invisible (or present/absent) had still applied in this case, we would either register a signal or not. Instead, we witness a new logic, which is described by the key intellectual paradigm of information society - mathematical theory of communication developed by Claude Shannon and others in the 1940s. According to this theory, communication is always accompanied by noise, and therefore a received signal always has some noise mixed in. [12] In practical terms, this means that any information delivered to or extracted from augmented space always occupies some position on the continuous dimension whose poles is a perfect signal and complete noise. In a typical situation, we are usually somewhere in between: our cell phone conversation is accompanied by some background noise; a surveillance system delivers blurry or low-res images, which needs to be interpreted, i.e., a decision needs to be made by somebody what is the signal being present. Thus, along with providing a theoretical framework to describe all electronic communication, mathematical theory of communication by Shannon turns to also perfectly capture the practical reality of our communications, at least up until now. That is, in the majority of cases, the signals we receive are accompanied by noise visible to us.

## Augmentation and Immersion

I derived the term “augmented space” from the already established term “augmented reality” (AR). [13] Coined around 1990, the concept of “augmented reality” is normally opposed to “virtual reality” (VR). [14] in the case of VR, the user works on a virtual simulation, in the case of AR, she works on actual things in actual space. Because of this, a typical VR system presents a user with a virtual space that has nothing to do with that user’s immediate physical space; while, contrast, a typical AR system adds information that is directly related to the user’s immediate physical space.

But we don't necessarily have to think of immersion in the virtual and augmentation of the physical as opposites. On one level, whether we think of a particular situation as immersion or augmentation is simply a matter of scale - i.e., the relative size of a display. When you are watching a movie in a movie theatre or on big TV monitor, or when you are playing a computer game on a game console that is connected to the TV, you are hardly aware of your physical surroundings. Practically speaking, you are immersed in virtual reality. But when you watch the same movie, or play the same game, on the small display of a cell phone or PDA that fits in your hand, then the experience is different. You are still largely present in physical space, and while the display adds to your overall phenomenological experience, it does not take over. So, whether we should understand a particular situation in terms of immersion or augmentation depends on how we understand the idea of addition: we may add new information to our experience – or we may add an altogether different experience.

“Augmented space” may bring associations with one of the founding ideas of computer culture: Douglas Engelbart’s concept of a computer augmenting human intellect that was articulated 40 years ago. [15] The association is appropriate, but we also need to be aware of the differences. For the vision of Engelbart, and the related visions of Vannevar Bush and J.C.R. Licklider, assumed a stationary user – a scientist or engineer at work in his office. Revolutionary for the time, these ideas anticipated the paradigm of desktop computing. Today, however, we are gradually moving into the next paradigm, one in which computing and telecommunication capacities are delivered mobile user. [16] Thus, augmenting the human also comes to mean augmenting the whole space in which she lives, or through which she passes.

## Augmentation as an Idea

Having analyzed at some length the concept of augmented space, we are now ready to move to the key questions of this essay. What is the phenomenological experience of being in a new augmented space? What can be the new cultural applications of new computer and network enabled augmented spaces? What are possible poetics and aesthetics of an augmented space?

One way to begin thinking about these questions is to approach the design of augmented space as an architectural problem. Augmented space provides a challenge and an opportunity for many architects to rethink their practice, since architecture will have to take into account the fact that virtual layers of contextual information will overlay the built space.

But is this a completely new challenge for architecture? If we assume that the overlaying of different spaces is a conceptual problem that is not connected to any particular technology, we may start to think about which architects and artists have already been working on this problem. To put it another way, the layering of dynamic and contextual data over physical space is a particular case of a general aesthetic paradigm: how to combine different spaces together. Of course, electronically augmented space is unique - since the information is personalized for every user, it can change dynamically over time, and it is delivered through an interactive multimedia interface, etc. Yet it is crucial to see this as a conceptual rather than just a technological issue – and therefore something that in part has already - been a part of other architectural and artistic paradigms.

Augmented space research gives us new terms with which to think about previous spatial practices. If before we would think of an architect, a fresco painter, or a display designer working to combine architecture and images, or architecture and text, or to incorporate different symbolic systems in one spatial construction, we can now say that all of them were working on the problem of augmented space. The problem, that is, of how to overlay physical space with layers of data. Therefore, in order to imagine what can be done culturally with augmented spaces, we may begin by combing cultural history for useful precedents.

To make my argument more accessible, I have chosen two well-known contemporary figures as my examples. Janet Cardiff is a Canadian artist who became famous for her “audio walks”. She creates her pieces by following a trajectory through a space and narrating an audio track that combines instructions to the user (“go down the stairs”; “look in the window”; “go through the door on the right”) with narrative fragments, sound effects, and other aural ‘data’. To experience the piece, the user dons earphones connected to a CD player and follows Cardiff’s instructions. [17] In my view - even though Cardiff does not use any sophisticated computer, networking, or projection technologies - her “walks” represent the best realization of the augmented space paradigm so far. They demonstrate the aesthetic potential of laying new information over a physical space. Their power lies in the interactions between the two spaces - between vision and hearing (what the user is seeing and what she is hearing), and between present and past (the time of the user’s walk versus the audio narration, which, like any media recording, belongs to some undefined time in the past).

The Jewish Museum Berlin by Daniel Libeskind can be thought of as another example of augmented space research. For, if Cardiff lays a new dataspace over the existing architecture and/or landscape, then Libeskind uses the existing dataspace to drive the new architecture that he constructs. After putting together a map that showed the addresses of Jews who were living in the neighborhood of the museum site before World War II, the architect connected different points on the map and then projected the resulting net onto the surfaces of the building. The intersections of the projected net and the Museum walls gave rise to multiple irregular windows. Cutting through the walls and the ceilings at different angles, these windows evoke many visual references: the narrow eyepiece of a tank; the windows of a medieval cathedral; the exploded forms of the cubist/abstract/suprematism paintings of the 1910s-1920s. Just as in the case of Cardiff’s audio walks, here the virtual becomes a powerful force that re-shapes the physical. In the Jewish Museum Berlin, the past literally cuts into the present. Rather than something ephemeral, an immaterial layer over the real space, here dataspace is materialized to become a sort of monumental sculpture.

## White Cube as Cellspace

While we may interpret the practices of selected architects and artists as having particular relevance to thinking about the ways in which augmented space can be used culturally and artistically, there is another way to link the augmented space paradigm with modern culture. Here is how it works.

One trajectory that can be traced in 20th century art runs from the dominance of a two-dimensional object placed on a wall, towards the use of the whole 3-D space of a gallery. (Like all other cultural trajectories in the 20th century, this one is not a linear development; rather, it consists of steps forward and steps back that occur in rhythm with the general cultural and political rhythm of the century: the highest peak of creativity took place in the 1910s-1920s, followed by a second peak in the 1960s). Already in the 1910s, Tatlin’s reliefs broke the two-dimensional picture plane and exploded a painting into the third dimension. In the 1920s, Lissitzky, Rodchenko, and other pioneering exhibition designers moved further away from an individual painting or sculpture towards using all surfaces of an exhibition space – yet their exhibitions activate only the walls rather than the whole space.

In the mid-1950s, assemblage legitimized the idea of an art object as a three-dimensional construction (”The Art of Assemblage,” MoMA, 1961). In the 1960s, minimalist sculptors (Carl Andre, Donald Judd, Robert Morris) and other artists (Eva Hesse, Arte Povera) finally started to deal with the whole of the 3-D space of a white cube. Beginning in the 1970s, installation (Dan Graham, Bruce Nauman) grew in importance to become, in the 1980s, the most common form of artistic practice of our times – and the only thing that all installations share is that they engage with 3-D space. Finally, the white cube becomes a cube – rather than just a collection of 2-D surfaces.

If we follow this logic, augmented space can be thought of as the next step in the trajectory from a flat wall to a 3-D space which has animated modern art for the last hundred years. For a few decades now, artists have already dealt with the entire space of a gallery: rather than creating an object that a viewer would _look at_, they placed the viewer _inside_ the object. Now the artists have a new challenge: placing a user inside a space filled with dynamic, contextual data with which the user can interact. Alternatively, if we want to be more modest, we can say that the arrival of augmented space in the 1980s and 1990s as deployed in urban sphere was paralleled by the development of a similar concept of space by installation artists. If before 3D space was in practice reduced to a set of surfaces – walls in the case of the built environment; flat paintings or gallery walls in an art environment – now it is finally used as 3D space.

## White Cube Versus Black Box

Before we rush to conclude that the new technologies do not add anything substantially new to the old aesthetic paradigm of overlaying different spaces together, let me note that - in addition to their ability to deliver dynamic and interactive information - the new technologically implemented augmented spaces also differ in one important aspect from Cardiff’s walks, Libeskind’s Jewish Museum, and other similar works. Rather than laying a new 3-D virtual dataspace over the physical space, Cardiff and Libeskind overlay only a 2-D plane, or a 3-D path, at best. Indeed, Cardiff’s walks are new 3-D paths placed over an existing space, rather than complete spaces. Similarly, in the Jewish Museum Berlin, Libeskind projects 2-D maps onto the 3-D shapes of his architecture. [18]

In contrast, GPS, wireless location services, surveillance technologies, and other augmented space technologies all define dataspace – if not in practice, then at least in theory - as a continuous field that completely extends over, and fills in, all of physical space. Every point in space has a GPS coordinate that can be obtained using a GPS receiver. Similarly, in the cellspace paradigm, every point in physical space can be said to contain some information that can be retrieved using a PDA or similar device. With surveillance, while in practice video cameras, satellites, Echelon (the set of monitoring stations that are used by the U.S. to monitor all kinds of electronic communications globally), and other technologies, can so far only reach some regions and layers of data but not others; the ultimate goal of the modern surveillance paradigm is to able to observe every point at every time. To use the terms of Borges’ famous story, all of these technologies want to make the map equal to the territory. And if, in accord with Foucault’s famous argument in _Discipline and Punish_, the modern subject internalizes surveillance and thereby removes the need for anybody to be actually present in the center of the Panopticon to watch him/her, modern institutions of surveillance insist that s/he should be watched and tracked everywhere all the time.

It is important, however, that, in practice, dataspaces are almost never continuous: surveillance cameras’ look at some spaces but not at others, wireless signals are stronger in some areas and non-existent in others, and so on. As Matt Locke eloquently describes this,

> Mobile networks have to negotiate the architecture of spaces that they attempt to inhabit. Although the interfaces have removed themselves from physical architectures, the radio waves that connect cell spaces are refracted and reflected by the same obstacles, creating not a seamless network but a series of ebbs and flows. The supposedly flat space of the network is in fact flat, pulled into troughs and peaks by the gravity of architecture and the users themselves. [19]

The contrast between the continuity of cellspace in theory and its discontinuity in practice should not be dismissed. Rather, it itself can be the source of interesting aesthetics strategies.

My third example of already existing augmented space – electronic displays mounted in shops, streets, lobbies, train stations, and apartments – follows a different logic. Rather than overlaying all of the physical space, here dataspace occupies a well-defined part of the physical space. This is the tradition of Alberti’s window, and, consequently, of post-Renaissance painting, the cinema screen, the TV screen, and the computer monitor. However, if the screen has, until recently, most usually acted as a window into a virtual 3-D space; in the last two decades of the 20th century it has turned into a shallow surface in which 3-D images co-exist with 2-D design and typography. Live-action footage shares space with motion graphics (animated type), scrolling data (for instance, stock prices or weather), and 2-D design elements. In short, the Renaissance painting became an animated Medieval illustrated book.

My starting point for the discussion of the poetics of this type of augmented space is the current practice of video installation, which came to dominate the art world in the 1990s. Typically, these installations use video or data projectors. They turn a whole wall or even a whole room into a display or a set of displays, thus previewing and investigating (willingly or not) the soon-to-come future of our apartments and cities when large and thin displays covering most surfaces may become the norm. At the same time, these laboratories of the future are rooted in the past: in the different traditions of “image within a space” of 20th century culture.

What are these traditions? Among the different oppositions that have structured the culture of the 20th century, and which we have inherited, has been the opposition between the art gallery and the movie theatre. One was high-culture; the other was low-culture. One was a white cube; the other was a black box.

Given the economy of art production – one-of-a-kind objects created by individual artists – 20th century artists expended lots of energy experimenting with what could be placed inside the neutral setting of a white cube: by breaking away from a flat and rectangular frame and going into the third dimension; covering a whole floor; suspending objects from the ceiling; and so on. In other words, if we are to make an analogy between an art object and a digital computer, we can say that, in modern art, both the “physical interface” and the ”software interface” of an art object were not fixed but open for experimentation. Put differently, both the physical appearance of an object and the proposed mode of interaction with an object were open for experimentation. Artists also experimented with the identity of a gallery: from a traditional space of aesthetic contemplation to a place for play, performance, public discussion, lectures, and so on.

In contrast, since cinema was an industrial system of mass production and mass distribution, the physical interface of a movie theatre and the software interface of a film itself were pretty much fixed: a 35-mm image of fixed dimensions projected on a screen with the same frame ratio, dark space where viewers were positioned in rows, and the fixed time of a movie itself. Not accidentally, when the experimental filmmakers of the 1960s started to systematically attack the conventions of traditional cinema, these attacks were aimed at both its physical and its software interfaces. Robert Breer, for example, projected his movies on a board that he would hold above his head as he walked through a movie theatre towards the projector; Stan Vanderbeek constructed semi-circular tents for the projection of his films; etc.

The gallery was the space of refined high taste while the cinema served to provide entertainment for the masses, and this difference was also signified by what was deemed to be acceptable in the two kinds of spaces. Despite all the experimentation with its “interface,” until recently the gallery space was primarily reserved for static images; to see moving images, the public had to go a movie theatre. Thus, until at least the 1980s, moving images in a gallery were indeed an exception (Duchamp’s rotoscopes, Acconci’s masturbating performance, which can be thought as a kind of animation within the gallery).

Given this history, the 1990s’ phenomena of omni-present video installations taking over the gallery space goes against the whole paradigm of modern art – and not only because installations bring moving images into the gallery. Most video installations adopt the same physical interface: a dark enclosed or semi-enclosed rectangular space with a video projector at one end and the projected image appearing on the opposite wall. Therefore, from a space of constant innovation in relation to the physical and software interface of an art object, a gallery space has turned into what was, for almost a century, its ideological enemy – a movie theatre that is characterized by the rigidity of its interface.

Since the early days of computer culture in the 1960s, many software designers and software artists – from Ted Nelson and Alan Kay to Perry Hoberman and IOD – have revolted against the hegemony of mainstream computer interfaces, such as the keyboard and mouse, GUI, or commercial Web browsers. Similarly, the best of video or, more generally, moving image installation artists, go beyond the standard video installation interface – a dark room with an image on one wall. Examples of such artists include Diana Thater, Gary Hill, and Doug Aitken, as well as the very first “video artist” – Nam Juke Paik. The founding moment of what would come to be called “video art” was Paik’s attack on the physical interface of a commercial moving image – his first show consisted of televisions with magnets attached to them, and TV monitors ripped out of their enclosures.

## The Electronic Vernacular

When we look at what visual artists are doing with a moving image in a gallery setting in comparison with other contemporary fields, we can see that the white gallery box still functions as a space of contemplation – quite different from the aggressive, surprising, overwhelming spaces of a boutique, trade show floor, airport, or retail/entertainment area of a major metropolis. [20] While a number of video artists continue the explorations of the 1960s ‘expanded cinema’ movement by pushing moving image interfaces in many interesting directions, outside of a gallery space we can find much richer field of experimentation. I can single out four areas. First, contemporary urban architecture - in particular, many proposals of the last decade that incorporate large projection screens into architecture and project the activity inside onto these screens. Example include Rem Koolhaas’ unrealized 1992 project for the new ZKM building in Karlsruhe; a number of projects again so far mostly unrealized by Robert Venturi to create what he calls “architecture as communication” (buildings covered with electronic displays); realized architectural/media installations by Diller + Scofilio such as _Jump Cuts and Facsimile_ [21]; the highly concentrated use of video screens and information displays in certain cities such as Seoul, Hong Kong and Tokyo, or in Times Square, NYC; and, finally, imaginary future architecture as seen in movies from _Blade Runner_ (1982) to _Minority Report_ (2002), which use electronic screens on a scale that is not yet possible. Second is the use of video displays in certain kind of contemporary spaces where communication of information to public is the key functions: trade show design, such as the annual SIGGRAPH and E3 conventions; company showrooms; airports and train stations. The third is the best of retail environments. These range from small high-end boutiques (I will discuss this type of space in more detail shortly) to mega-size shopping centers / eating/ entertainment complexes which incorporate projection screens, dynamic lighting systems, mirrors, transparent and translucent surfaces to create an experience of an animated and dynamic space. The fourth is the multi-media design of music performances, from the concerts of the brand name pop starts, to the numerous VJs performing nightly in clubs in most major cities on earth, to “hybrid” groups which situate themselves between club and art culture, such as brilliant collective Light Surgeons based in London.

While at this moment they are still imagined and implemented by the practitioners from different fields, we start slowly seeing the different species of augmented spaces being combined into one. A shopping complex leads to an interior shopping street which leads to a multiplex; or an airport complex combines information displays about airline departures and arrivals and shopping areas with their own promotions playing on LCD screens, and so on. Although at present the small electronic screens are usually distributed throughout these spaces (for instance, small LCD monitors mounted in elevators of new hi-rise buildings in Hong Kong and China such as CITIC Plaza in Guangzhou), the single larger screen (or other method for large image creation) has a potential to unite them all, offering a kind of symbolic unity to a typically heterogeneous urban program: a shopping center + entertainment center + hotel + residential units. Asan example, consider Langham Place (Mongkok, Hong Kong, opened November 2004) developed by The Jerde Partnership, the pioneers of the urban version of ”experience design” they refer as “placemaking.” An entertainment complex with an area of 1.8 million square feet, it combines a 15-store shopping mall with 300 shops, a 59-level Grade A office tower and the 5-star Langham Place Hotel. The focal point of the complex is Digital Sky which is spanning the entire roof of the mall. Showing continuous visuals, this giant “screen” is made possible by 200 projectors, PCs, speakers, and special effects lights. [22] No longer a square superimposed on a façade or a wall, here an image envelops the whole space as an ambient “elevator music” sky to shop under.

To discuss the use of electronic images in architecture further, let us turn to Robert Venturi. His projects and theories deserve special consideration here since, for him, an electronic display is not an optional addition but the very center of architecture in the information age. Since the 1960s, Venturi continuously argued that architecture should learn from vernacular and commercial culture (billboards, Las Vegas, strip malls, architecture of the past). Appropriately, his books _Complexity and Contradiction in Architecture_ and _Learning from Las Vegas_ are often referred to as the founding documents of post-modern aesthetics. Venturi proposed that we should refuse the modernist desire to impose minimalist ornament-free spaces, and instead embrace complexity, contradiction, heterogeneity, and iconography in our built environments. [23] In the 1990s, he articulated the new vision of “architecture as communication for the Information Age (rather than as space for the Industrial Age).” [24] Venturi wants us to think of “architecture as an iconographic representation emitting electronic imagery from its surfaces day and night.” Pointing to some of the already mentioned examples of the aggressive incorporation of electronic displays in contemporary environments, such as Times Square in NYC, and arguing that traditional architecture _always_ included ornament, iconography, and visual narratives (for instance, a Medieval cathedral with its narrative window mosaics, narrative sculpture covering the façade, and narrative paintings), Venturi proposed that architecture should return to its traditional definition as iconography, i.e. as _information surface_. [25] Of course, if the messages communicated by traditional architecture were static and reflected the dominant ideology, today’s electronic dynamic interactive displays make it possible for these messages to change continuously; making the information surface a potential space of contestation and dialog, which functions as the material manifestation of the often invisible public sphere.

Although this has not been a part of Venturi’s core vision, it is relevant to mention here a growing number of projects in which the large publicly mounted screen is open for programming by the public who can send images via Internet or information being displayed via their cell phones. Even more suggestive is the project Vectorial Elevation, _Relational Architecture #4_ by artist Rafael Lozano-Hemmer [26]. This project made it possible for people from all over the world to control a mutant electronic architecture made from search lights in Mexico City’s Zócalo Square. To quote from the statement of the 2002 Prix Ars Electronica jury, which awarded this project the Golden Nica in the Interactive Art category:

> Vectorial Elevation was a large scale interactive installation that transformed Mexico City’s historic center using robotic searchlights controlled over the Internet. Visitors to the project web site at [http://www.alzado.net](http://www.alzado.net) could design ephemeral light sculptures over the National Palace, City Hall, the Cathedral and the Templo Mayor Aztec ruins. The sculptures, made by 18 xenon searchlights located around the Zócalo Square, could be seen from a 10-mile radius and were sequentially rendered as they arrived over the Net.
> 
> The website featured a 3D-java interface that allowed participants to make a vectorial design over the city and see it virtually from any point of view. When the project server in Mexico received a submission, it was numbered and entered into a queue. Every six seconds the searchlights would orient themselves automatically and three webcams would take pictures to document a participant’s design. [27]

If we focus completely on the idea of architecture as information surface, we may forget that traditional architecture communicated messages and narratives not only through flat narrative surfaces but also through the particular articulation of space. To use the same example of a medieval cathedral, it communicated Christian narratives not only through the images covering its surfaces but also through its whole spatial structure. In the case of modernist architecture, it similarly communicated its own narratives (the themes of progress, technology, efficiency, and rationality) through new spaces constructed from simple geometric forms – and also through its bare, industrial-looking surfaces. (Thus, the absence of information from the surface, articulated in the famous “ornament is crime” slogan of Adolf Loos, itself became a powerful communication technique of modern architecture.)

An important design problem of our own time is how to combine the new functioning of a surface as an electronic display with the new kind of spaces and forms being imagined by contemporary architects. [28] While Venturi fits electronic displays on to his buildings, which closely follow traditional vernacular architecture, this is obviously not the only possible strategy. The well-known Freshwater Pavilion by NOX/Lars Spuybroek (Netherlands, 1996) follows a much more radical approach. To emphasize that the interior of the space constantly mutates, Spuybroek eliminates all straight surfaces and straight angles; he makes the shapes defining the space appear to move; and he introduces computer-controlled lights that change the illumination of the interior. [29] As described by Ineke Schwartz, “There is no distinction between horizontal and vertical, between floors, walls and ceilings. Building and exhibition have fused: mist blows around your ears, a geyser erupts, water gleams and splatters all around you, projections fall directly onto the building and its visitors, the air is filled with waves of electronic sound.” [30]

I think that Spuybroek’s building is a successful symbol for the Information Age. Its continuously changing surfaces illustrate the key effect of the computer revolution: the substitution of every constant by a variable. In other words, the space that symbolizes the Information Age is not the symmetrical and ornamental space of traditional architecture, the rectangular volumes of modernism, nor the broken and blown-up volumes of deconstruction. Rather, it is a space whose shapes are inherently mutable and whose soft contours act as a metaphor for the key quality of computer-driven representations and systems: variability.

## Learning from Prada

Venturi wants to put rich electronic ornamentation and iconography on traditional buildings. In contrast, in his Freshwater Pavilion Lars Spuybroek constructs a new kind of space which he then fills with information – but information reduced to abstract color fields and sound. In other words, in the Freshwater Pavilion, the information surface functions in a very particular way, displaying color fields rather than text, images, or numbers. Where can we find today interesting architectural spaces combined with electronic displays that show the whole range of information, from ambient color fields to figurative images and numerical data?

Beginning in the mid-1990s, the avant-garde wing of the retail industry began to produce rich and intriguing spaces, many of which incorporate moving images. Leading architects and designers such as Droog/NL, Marc Newson, Herzog & de Meuron, Renzo Piano, and Rem Koolhaas created stores for Prada, Mandarina Duck, Hermes, Comme des Garcons, and other high-end brands; while architect Richard Gluckman collaborated with artist Jenny Holzer to create Helmut Lang’s stunning New York perfumery, which incorporates Holzer’s signature use of LCD displays. A store featuring dramatic architecture and design, and the mixing of a restaurant, fashion, design, and art gallery became a new paradigm for high-end brands. Otto Riewoldt describes this paradigm using the term “brandscaping” – promoting the brand by creating unique spaces. According to Riewoldt: “Brandscaping” is the hot issue. The site at which goods are promoted and sold has to reinvent itself by developing unique and unmistakable qualities.” [31]

OMA / Rem Koolhaas’ Prada store in New York (2002) pushes brandscaping to a new level. Koolhaus seems to achieve the impossible by creating a flagship store for the Prada brand – and at the same time an ironic statement about the functioning of brands as new religions. [32] The imaginative use of electronic displays designed by Reed Kram of Kramdesign is an important part of this statement. On entering the store, the visitor discovers glass cages hanging from the ceiling throughout the space. Just as a church would present the relics of saints in special displays, here the glass cages contain the new objects of worship – Prada clothes. The special status of Prada clothing is further enhanced by the placement of small flat electronic screens throughout the store on horizontal shelves right alongside the merchandize. The clothes are equated with the ephemeral images playing on the screens, and, vice versa, the images acquire a certain materiality, as though they are themselves objects. By positioning screens showing moving images right next to the clothes, the designers ironically refer to what everybody today already knows: we buy objects not for themselves but in order to emulate the specific images and narratives that are presented by the advertisements of these objects. Finally, on the basement level of the store, you discover a screen displaying the Prada Atlas. Designed by Kram, the Atlas maybe be mistaken for an interactive multimedia presentation of OMA (Office for Metropolitan Architecture, the name of Koolhaas’s studio) research for its Prada commission. It looks like the kind of information that brands normally communicate to their investors but not to their consumers. In designing the Atlas, as well as the whole media of the store, Kram’s goal was to make “Prada reveal itself, make it completely transparent to the visitors.” [33] The Atlas lets you list all of the Prada stores throughout the world by square footage, look at an analysis of optimal locations for store placement, and study other data sets that underlie Prada’s brandscaping. This ‘unveiling’ of Prada does not break our emotional attachment with the brand; on the contrary, it seems to have the opposite result. Koolhaas and Kram masterfully engage the “I know it is an illusion but nevertheless I believe it” effect: we know that Prada is a business that is governed by economic rationality and yet we still feel that we are not simply in a store but in a modem church.

It is symbolic that Prada NYC has opened in the space that was previously occupied by a branch of the Guggenheim Museum. The strategies of brandscaping are directly relevant to museums and galleries that, like all other physical spaces, now have to compete against that new information, entertainment, and retail space: a computer or a cell phone screen connected to the Net. Although museums in the 1990s have similarly expanded their functionality, often combining galleries, a store, film series, lectures, and concerts, design-wise they can learn from retail design, which, as Riewoldt points out, “has learnt two lessons from the entertainment industry. First: forget the goods, sell thrilling experience to the people. And secondly: beat the computer screen at its own game by staging real objects of desire – and by adding some spice to the space with maybe some audio-visual interactive gadgetry.” [34]

In a high-tech society, cultural institutions usually follow the technology industry. A new technology is developed for military, business, or consumer use, and after a while cultural institutions notice that some artists are experimenting with that technology and so they start to incorporate it in their programming. Because they have the function of collecting and preserving artworks, the art museums today often look like historical collections of media technologies from previous decades. Thus, one may well mistake a contemporary art museum for a museum of obsolete technology. Today, while outside one finds LCDs and PDAs, data projectors, and HDTV cameras, inside a museum we may expect to find slide projectors, 16-mm film equipment, and 3/4-inch video decks.

Can this situation be reversed? Can cultural institutions play an active, even a leading, role, acting as laboratories where alternative futures are tested? Augmented space – which is slowly becoming a reality – is one opportunity for these institutions to take a more active role. While many video installations already function as laboratories for developing new configurations of images within space, museums and galleries as a whole could use their own unique asset – a physical space – to encourage the development of distinct new spatial forms of art and new spatial forms of the moving image. In this way, they can take a lead in testing out one part of the augmented space future.

Having stepped outside the picture frame into the white cube walls, floor, and the whole space, artists and curators should feel at home taking yet another step: treating this space as layers of data. This does not mean that the physical space becomes irrelevant; on the contrary, as the practice of Cardiff and Libeskind shows, it is through the interaction of the physical space and the data that some of the most amazing art of our time is being created.

Augmented space also represents an important challenge and an opportunity for contemporary architecture. As the examples discussed in this essay demonstrate, while many architects and interior designers have actively embraced electronic media, they typically think of it in a limited way: as a screen, i.e., as something that is attached to the “real” stuff of architecture, i.e., surfaces defining volumes. Venturi’s concept of architecture as “information surface” is only the most extreme expression of this general paradigm. While Venturi logically connects the idea of surface as electronic screen to the traditional use of ornament in architecture and to such features of vernacular architecture as billboards and window product displays, this historical analogy also limits our visions of how architecture can use new media. For, in this analogy, an electronic screen becomes simply a moving billboard or a moving ornament.

Going beyond the “surface as electronic screen paradigm,” architects now have the opportunity to think of the material architecture that most usually preoccupies them and the new immaterial architecture of information flows within the physical structure as a whole. In short, I suggest that the design of electronically augmented space can be approached as an architectural problem. In other words, architects along with artists can take the next logical step to consider the “invisible” space of electronic data flows as _substance_ rather than just as void – something that needs a structure, a politics, and a poetics.

## References:

[1] VRML stands for the Virtual Reality Modeling Language. In the first part of the 1990s, the inventors of this language designed it to model and access 3-D interactive virtual worlds over the Internet and promoted it as the material realization of the idea of cyberspace. See, for instance, Mark Pesce, "Ontos, Eros, Noos, Logos," the keynote address for ISEA (International Symposium on Electronic Arts) 1995, [http://www.xs4all.nl/~mpesce/iseakey.html](http://www.xs4all.nl/~mpesce/iseakey.html) As of the time of writing (May 2002), Internet-based 3-D virtual worlds have failed to become popular. 

[2] This text was originally written in early 2002; current edit was done in September 2004.

[3] Coined in 1998 by David S. Bennahum, the term “cellspace” originally referred to the then new ability to access e-mail or the Internet wirelessly. Here I am using the term in a broader sense. 

[4] It is interesting to think of GPS (Global Positioning System) as a particular case of cellspace. Rather than being tied to an object or a building, here the information is a property of the Earth as a whole. A user equipped with a GPS receiver can retrieve a particular type of information relative to their location – the coordinates of this location. GPS systems are gradually being integrated into various telecommunication and transportation technologies, from cell phones, to PDAs, to cars. 

[5] Recall the opening scene of _Blade Runner_ (1982) in which the whole side of a high-rise building acts as a screen. 

[6] M. Weiser, “The Computer for the Twenty-first Century,” _Scientific American_, 265(3):94–104, September 1991. 

[7] W. MacKay, G. Velay, K. Carter, C. Ma, and D. Pagani, “Augmenting Reality: Adding Computational Dimensions to Paper,” _Communications_ of the ACM\_, 36(7):96–97, 1993. Kevin Bonsor, “How Augmented Reality Will Work,” [http://www.howstuffworks.com/augmented-reality.htm](http://www.howstuffworks.com/augmented-reality.htm). 

[8] See the “Tangible Bits” project at the MIT Media Lab, [http://tangible.media.mit.edu/projects/Tangible\_Bits/projects.htm](http://tangible.media.mit.edu/projects/Tangible_Bits/projects.htm).

[9] Guido Appenzeller, Intelligent Space Project, [http://gunpowder.Stanford.EDU/~appenz/ISpace/](http://gunpowder.Stanford.EDU/~appenz/ISpace/); Intelligent Room Projects, AI Lab, MIT, [http://www.ai.mit.edu/projects/iroom/projects.shtml](http://www.ai.mit.edu/projects/iroom/projects.shtml). 

[10] Tom Moran and Paul Dourish, “Introduction to the Special Issue on Context-aware Computing,” _Human Computer Interaction_, 16:108, 2001. 

[11] Ivan Noble, “E-paper Moves a Step Nearer,” _BBC News Online_, 23 April, 2001. [http://news.bbc.co.uk/hi/english/sci/tech/newsid\_1292000/1292852.stm](http://news.bbc.co.uk/hi/english/sci/tech/newsid_1292000/1292852.stm). 

[12] If the noise falls below a certain threshold, we are able to reconstruct the send signal perfectly; conversely, if noise is above a particular threshold, the signal disappears. These thresholds are never absolute; they are specific to a particular communication situation, influenced by the bandwidth of a communication channel and also the content of a message.

[13] For AR research sites and conferences, see [http://www.augmented-reality.org](http://www.augmented-reality.org). 

[14] With a typical VR system, all work is done in a virtual space; physical space becomes unnecessary, and it’s the user’s visual perception of physical space is completely blocked. In contrast, an AR system helps the user to work in a physical space by augmenting that space with additional information. This end is achieved by laying information over the user’s visual field. An early scenario of a possible AR application that was developed at Xerox PARC involved a wearable display for copier repairman, which overlaid a wireframe image of the copier’s insides over the actual copier as it was being repaired. 

Today, additional scenarios for everyday use can be imagined: for instance, AR glasses for a tourist that layer dynamically changing information about the sites in a city over her visual field. Military and artistic applications are also being developed, as presented for instance in the exhibition showcasing AR projects developed by Ars Electronica FutureLab (Ars Electronica Festival 2003). In this new iteration, AR becomes conceptually similar to wireless location services. The idea shared by both is that when the user is in the vicinity of particular objects, buildings, or people, then information about them is delivered to the user. But while this information is displayed, in cellspace, on a cell phone or PDA; in AR the information is laid over the user’s visual field. 

The decrease in the popularity of VR in mass media and a slow but steady rise in AR-related research in the last five years is one example of the ways in which the augmented space paradigm is now overtaking the virtual space paradigm. Interestingly, this reversal can be said to be anticipated in the very origins of VR. In the late 1960s, Ivan Sutherland developed what we came to know as the first VR system. The user of the system saw a simple wireframe cube whose perspectival view would change as the user moved his head. The wireframe cube appeared overlaid over whatever the user was seeing. Because the idea of a 3-D computer graphics display whose perspective changes in real time according to the position of the user became associated with subsequent virtual reality systems, Sutherland is credited with inventing the first VR system. But it can be also argued that this was not a VR but rather an AR system because the virtual display was overlaid over the user’s field of vision without blocking it. In other words, in Sutherland’s system, new information was added to the physical environment: a virtual cube.

[15] Vannevar Bush, “As We May Think” (1945); Douglas Engelbart, “Augmenting Human Intellect: A Conceptual Framework” (1962). Both in Noah Wardrip-Fruin and Nick Montfort, eds., _The New Media Reader_ (MIT Press, forthcoming 2002). 

[16] And while it may still be more efficient to run, say, CAD, 3-D modeling, or Web design software while sitting comfortably in front of a 30-inch LCD display, there are many other types of computing and telecommunication activities that do not require or encourage stationary use.

[17] I only experienced one of her “walks” that she created for P.S. 1 in New York in 2001.

[18] For whose readers familiar with these concepts, the artistic augmented spaces I have evoked can be thought of as 2-D texture maps, while technologically augmented spaces can be compared to a solid texture.

[19] Matt Locke, in _Mobile Minded_, eds. Geert Lovink and Mieke Gerritzen (Corte Madera, CA: Ginko Press, 2002), 111.

[20] This passive and melancholic quality of video art was brilliantly staged in a recent exhibition design by LO/TEK, “Making Time: Considering Time as a Material in Contemporary Video & Film,” in the Hammer Museum in Los Angeles (February 4 - April 29, 2001). As Norman Klein pointed out to me, LO/TEK designed a kind of collective tomb - a cemetery for video art.

[21] Overview of Diller + Scofilio projects can be found at [http://www.labiennaledivenezia.net/it/archi/7mostra/architetti/diller/open.htm](http://www.labiennaledivenezia.net/it/archi/7mostra/architetti/diller/open.htm).

[22] Raymond Wang, “Langham Place offices to roll next month,” _The Standard_ (Greater China’s Business Newspaper), 19 June 2004 [www.thestandard.com.hk/thestandard/news\_detail\_frame.cfm?articleid=48588&intcatid=1](www.thestandard.com.hk/thestandard/news_detail_frame.cfm?articleid=48588&intcatid=1). 

[23] Robert Venturi, _Complexity and Contradiction in Architecture_ (New York: Museum of Modern Art, 1966); Robert Venturi, Denise Scott Brown, and Steven Izenour, _Learning from Las Vegas_ (Cambridge, Mass.: MIT Press, 1972.) 

[24] Robert Venturi, _Iconography and Electronics upon a Generic Architecture: A View from the Drafting Room_ (MIT Press, 1996).

[25] Robert Venturi in a dialog with George Legrady at the Entertainment and Value Conference, University of California, Santa Barbara, May 4, 2002. The term “information surface” is mine. 

[26] See [http://prixars.aec.at/history/interactive/2000/E00int\_01.htm](http://prixars.aec.at/history/interactive/2000/E00int_01.htm).

[27] Ibid. 

[28] See [http://www.manovich.net/IA](http://www.manovich.net/IA). 

[29] See Ineke Schwartz, “Testing Ground for Interactivity: The Water Pavilions by Lars Spuybroek and Kas Oosterhuis,” [http://synworld.t0.or.at/level3/text\_archive/testing\_ground.htm](http://synworld.t0.or.at/level3/text_archive/testing_ground.htm).

[30] Ibid.

[31] Otto Riewoldt, qtd. in Mark Hooper, “Sex and Shopping,” _ID_, The DNA Issue (2001), 94. 

[32] For an insightful analysis of the branding phenomenon, see Naomi Klein, _No Logo_ (New York: Picador, 2000). 

[33] Reed Kram, personal communication with the author, June 5, 2002. For more Kram projects, see [www.kramdesign.com/](www.kramdesign.com/).

[34] Riewoldt, qtd. in Hooper, 2000.

---

# Who is the Author? Sampling / Remixing / Open Source

_author: Lev Manovich_
_year: 2002_

New media culture brings with it a number of new models of authorship which all involve different forms of collaboration. Of course, collaborative authorship is not unique to new media: think of medieval cathedrals, traditional painting studios which consisted of  a master and assistants, music orchestras, or contemporary film productions which, like medieval cathedrals, involve thousands of people collaborating over a substantial period of time. In fact, in we think about this historically, we will see collaborative authorship represents a norm rather than exception. In contrast, romantic model of a solitary single author occupies a very small place in the history of human culture. New media, however, offers some new variations on the previous forms of collaborative authorship. In this essay I will look at some of these variations. I will try to consider them not in isolation but in a larger context of contemporary cultural economies. As we will see, new media industries and cultures systematically pioneer new types of authorship, new relationships between producers and consumers, and new distribution models, thus acting as the avant-garde of the culture industry. 

## 1. Collaboration of Different Individuals and/or Groups

The most often discussed new type of authorship associated with new media is collaboration (over the network or in person, in real time or not) between a group of artists to create a new media work / performance / event / “project.” Often, no tangible objects or an even definite event like a performance ever comes out from these collaborations, but this does not matter. People meet people with common interests and start a “project” or a series of “projects.” We can think of this as a “social culture”; we may also note that while the new media culture may not have produced any “masterpieces”, it definitely had a huge impact on how people and organizations communicate. Along with database, navigable space, simulation and interactivity, new cultural forms enabled by new media also include new patterns of social communication. In short, the network-enabled process of collaboration, networking, and exchange is a valuable form of contemporary culture, regardless of whether it results in any “objects” or not.

## 2. Interactivity as Miscommunication Between the Author and the User

In the first part of the 1990s when interactivity was a new term, it was often claimed that an interactive artwork involves collaboration between an author and a user. Is this true? The notion of collaboration assumes some shared understanding and the common goals between the collaborators, but in the case of interactive media these are often absent. After an author designs the work, s/he has no idea about the assumptions and intentions of a particular user. Such a user, therefore, can’t be really called a collaborator of the author. From the other side, a user coming to a new media artwork often also does not know anything about this work, what is supposed to do, what its interface is, etc. For this user, therefore, an author is not really a collaborator. Instead of collaborators, the author and the user are often two total strangers, two aliens which do not share a common communication code. 

While interactivity in new media art often leads to “miscommunication” between the author and the user, commercial culture employs interactive feedback to assure that no miscommunication will take place. It is common for film producers to test a finished edit of a new film before a “focus group.” The responses of the viewers are then used to re–edit the film to improve comprehension of the narrative or to change the ending. In this practice, rather than presenting the users with multiple versions of the narrative, a single version that is considered the most successful is selected. 

## 3. Authorship as Selection from a Menu

I discuss this type of authorship in detail in _The Language of New Media_; here I just want to note that it applies to both professional designers and users. [1] The design process in new media involves selection from various menus of software packages, databases of media assets, etc. Similarly, a user is often made to feel like a “real artist” by allowing her/him to quickly create a professional looking work by selecting from a few menus. The examples of such “authorship by selection” are the Web sites that allow the users to quickly construct a postcard or even a short movie by selecting from a menu of images, clips, and sounds.

Three decades ago, Roland Barthes elegantly defined a cultural text as “a tissue of quotations”: “We know now that a text is not a line of words releasing a single ‘theological’ meaning (the ‘message’ of the Author-God) but a multi-dimensional space in which a variety of writings, none of them original, blend and clash. The text is a tissue of quotations drawn from innumerable centers of culture.” [2] In software-driven production environment, these quotations come not only from the creators’ memories of what they previously saw, read, and heard, but also directly from the databases of media assets, as well as numerous other words that in the case of the World Wide Web are just a click away. 

## 4. Collaboration Between a Company and the Users

When it released the original _Doom_ (1993), id software also released detailed descriptions of game files formats and a game editor, thus encouraging the players to expand the game, creating new levels. Adding to the game became its essential part, with new levels widely available on the Internet for anybody to download. Since _Doom_, such practices became commonplace in computer game industry. Often, the company would include elements designed by the users in a new release. 

With another widely popular game _Sims_ (2001), this type of collaboration reached a new stage. The Web site for the game allows users to upload the characters, the settings, and the narratives they constructed into the common library, as well as download characters, settings, and narratives constructed by others. [3] Soon it turned out that the majority of users do not even play the game but rather use its software to create their own characters and storyboard their adventures. In contrast to earlier examples of such practice – for instance the 1980s _Star Trek_ fans editing their own video tapes by sampling from various _Star Trek_ episodes or writing short stories involving main _Star Trek_ characters – now it came into the central place, being legitimized and encouraged by game producers. 

Another way in which a company can be said to collaborate with the users of its software is by incorporating their suggestions about new features into the new version of the software. This is common practice of many software companies. 

## 5. Collaboration Between the Author and Software

Authoring using Al or AI is the most obvious case of human-software collaboration. The author sets up some general rules, but s/he has no control over the concrete details of the work – these emerge as a result of the interactions of the rules. More generally, we can say that all authorship that uses electronic and computer tools is a collaboration between the author and these tools that make possible certain creative operations and certain ways of thinking while discouraging others. Of course, humans have designed these tools, so it would be more precise to say that the author who uses electronic/ software tools engages in a dialog with the software designers (see #4). 

## 6. Remixing

Remixing originally had a precise and a narrow meaning that gradually became diffused. Although precedents of remixing can be found earlier, it was the introduction of multi-track mixers that made remixing a standard practice. With each element of a song – vocals, drums, etc. – available for separate manipulation, it became possible to “re-mix” the song: change the volume of some tracks or substitute new tracks for the old ounces. Gradually the term became more and more broad, today referring to any reworking of an original musical work(s). 

In his _DJ Culture_ Ulf Poschardt singles out different stages in the evolution of remixing practice. In 1972 DJ Tom Moulton mixed his first disco remixes; as Poschardt points out, they “show a very chaste treatment of the original song. Moulton sought above all a different weighting of the various soundtracks, and worked the rhythmic elements of the disco songs even more clearly and powerfully…Moulton used the various elements of the sixteen or twenty-four track master tapes and remixed them.” [4] By 1987, “DJs started to ask other DJs for remixes” and the treatment of the original material became much more aggressive. For example, “Coldcut used the vocals from Ofra Hanza’s ‘Im Nin Alu’ and contrasted Rakim’s ultra-deep bass voice with her provocatively feminine voice. To this were added techno sounds and a house-inspired remix of a rhythm section that loosened the heavy, sliding beat of the rap piece, making it sound lighter and brighter.” [5] In another example, London DJ Tim Simenon produced a remix of his personal top ten of 1987. Simenon: “We found a common denominator between the songs we wanted to use, and settled on the speed of 114 beats per minute. The tracks of the individual songs were adapted to this beat either by speeding them up or slowing them down.” [6]

In the last few years people started to apply the term “remix” to other media: visual productions, software, literary texts. With electronic music and software serving as the two key reservoirs of new metaphors for the rest of culture today, this expansion of the term is inevitable; one can only wonder why it did not happen earlier. Yet we are left with an interesting paradox: while in the realm of commercial music remixing is officially accepted [7], in other cultural areas it is seen as violating the copyright and therefore as stealing. So, while filmmakers, visual artists, photographers, architects, and Web designers routinely remix already existing works, this is not openly admitted, and no proper terms equivalent to remixing in music exist to describe these practices.

The term that we do have is “appropriation.” However, this never left its original art world context where it was first applied to the works of post-modern artists of the early 1980s based on re-working older photographic images. Consequently, it never achieved the same wide use as “remixing.” Anyway, “remixing” is a better term because it suggests a systematic re-working of a source, the meaning which “appropriation” does not have. And indeed, the original “appropriation artists” such as Richard Prince simply copied the existing image as a whole rather than re-mixing it. As in the case of Duchamp’s famous urinal, the aesthetic effect here is the result of a transfer of a cultural sign from one sphere to another, rather than any modification of a sign.

The only other commonly used term across media is “quoting” but I see it as describing a very different logic than remixing. If remixing implies systematically rearranging the whole text, quoting means inserting some fragments from old text(s) into the new one. Thus, it is more similar to another new fundamental authorship practice that, like remixing, was made possible by electronic technology – sampling. 

## 7. Sampling: New Collage?

According to Ulf Poschardt, “The DJ’s domination of the world started around 1987.” [8] This take-over is closely related to the new freedom in the use of mixing and sampling. That year M/A/R/S released their record “Pump Up the Volume”; as Poschardt points out, “This record, cobbled together from a crazy selection of samples, fundamentally changed the pop world. As if from nowhere, the avant-garde sound collage, unusual for the musical taste of the time, made it to the top of the charts and became the year’s highest-selling 12-inch single in Britain.” [9]

Theorizing immediately after M/A/R/S, Coldcut, Bomn The Bass and S-Xpress made full use of sampling, music critic Andrew Goodwin defined sampling as “the uninhibited use of digital sound recording as a central element of composition. Sampling thus becomes an aesthetic program.” [10] We can say that with sampling technology, the practices of montage and collage that were always central to twentieth century culture, became industrialized. Yet we should be careful in applying the old terms to new technologically driven cultural practices. While the terms “montage” and “collage” regularly pop up in the writings of music theorists from Poschardt to Kodwo Eshun and DJ Spooky, I think these terms that come to us from literary and visual modernism of the early twentieth century do not adequately describe new electronic music. To note just three differences: musical samples are often arranged in loops; the nature of sound allows musicians to mix pre-existent sounds in a variety of ways, from clearly differentiating and contrasting individual samples (thus following the traditional modernist aesthetics of montage/collage), to mixing them into an organic and coherent whole [11]; finally, the electronic musicians often conceive their works beforehand as something that will be remixed, sampled, taken apart and modified. Poschardt: “house (like all other kinds of club music) has relinquished the unity of the song and its inviolability. Of course the creator of a house song thinks at first in terms of his single track, but he also thinks of it in the context of a club evening, into which his track can be inserted at a particular point.” [12]

Last but not least, it is relevant to note here that the revolution in electronic pop music that took place in the second part of the 1980s was paralleled by similar developments in pop visual culture of the same period. The introduction of electronic editing equipment such as switcher, keyer, paintbox, and image store made remixing and sampling a common practice in video production towards the end of the decade; first pioneered in music videos, it later took over the whole visual culture of TV. Other software tools such as Photoshop (1989) had the same effect on the fields of graphic design, commercial illustration, and photography. And, a few years later, World Wide Web redefined an electronic document as a mix of other documents. Remix culture has arrived. 

## 8. Open Source Model

Open Source model is just one among a number of different models of authorship (and ownership) which emerged in software community and which can be applied (or are already being applied) to cultural authorship. The examples of such models are the original project Xanadu by Ted Nelson, “freeware,” and “shareware.” In the case of Open Source, the key idea is that one person (or group) writes software code, which can be then modified by another user; the result can be subsequently modified by a new user, and so on. 

If we apply this model to a cultural sphere, do we get any new model of authorship? It seems to me that the models of remixing, sampling and appropriation conceptually are much richer than the Open Source idea. There are, however, two aspects of Open Source movement that make it interesting. One is the idea of license. There are approximately 30 different types of licenses in Open Source movement. [13] The licenses specify the rights and responsibilities of a person modifying the code. For instance, one license (called Gnu Public License) specifies that the programmer have to provide the copy of the new code to the community; another stipulates that the programmer can sell the new code and he does not have to share with the community, but he can’t do things to damage the community.

Another idea is that of the kernel. At the “heart” of Linux operating system is its kernel - the code essential to the functioning of the system. While users add and modify different parts of Linux system, they are careful not to change the kernel in fundamental ways. [14] Thus all dialects of Linux share the common core. [15]

I think that the ideas of license and of kernel can be directly applied to cultural authorship. Currently appropriation, sampling, remixing, and quoting are controlled by a set of heterogeneous and often outdated legal rules. These rules tell people what they are _not_ allowed to do with the creative works of others. Imagine now a situation where an author releases her/his work into the world accompanied by a license that will tell others both what they should not do with this work and also what they _can_ do with it (i.e., the ways in which it can be modified and re-used) Similarly we may imagine a community formed around some creative work; this community would agree on what constitutes the kernel of this work. Just as in the case of Linux, it would be assumed that while the work can be played with and endlessly modified, the users should not modify the kernel in dramatic ways. 

Indeed, if music, films, books, and visual art are our _cultural software_, why not apply the ideas from software development to cultural authorship? In fact, I believe that we can already find many communities and individual works that employ the ideas of license and kernel, even though these terms are not explicitly used. One example is Jon Ippolito’s _Variable Media Initiative_. [16] Ippolito proposed that an artist who accepts variability in how her/his work will be exhibited and/or re-created in the future (which is almost inevitable in the case of net art and other software-based work) should specify what constitutes the legitimate exhibition/recreation; in short, s/he should provide the equivalent of the software license.

Among the cultural projects inspired by Open Source Movement, OPUS project (2002) stands out from the rest in how it tackles with the question of authorship in computer culture. [17] Importantly, OPUS, created by Raqs Media Collective (New Delhi), is both a software package and an accompanying “theoretical package.” Thus, the theoretical ideas about authorship articulated by Raqs collective do not remain theory but are implemented in software available for everybody to use. In short, this is “software theory” at its best: theoretical ideas translated into a new kind of cultural software. 

OPUS software designed to enable possible multi-user cultural collaboration in a digital network environment. In OPUS (which stands for “Open Platform for Unlimited Signification), anybody can start a new project and invite other people to download and upload media objects to the project’s area on OPUS site (it is also possible to download OPUS software itself and put it on new servers). When the author uploads a new media object (anything from a text to a piece of music), s/he can specify what modifications by others will be allowed. Subsequently, OPUS software keeps track of every new modification to this object.

Each media objects archived, exhibited, and made available for transformation within OPUS carries with it data that can identify all whose who worked on it. This means that while OPUS enables collaboration, it also preserves the identity of authors/creators (no matter how big or small their contribution may be) at each stage of a work’s evolution. [18]

The Raqs Collective introduces a new term “rescension” to address this type of collaborative authorship. [19] In my view, “rescension” presents a sophisticated comprise between the two extreme ideologies of digital authorship commonly invoked and used today: on the one hand, completely open model that lets everybody modify anything; on the other hand, tight control of all permissible uses of a cultural object by traditional copyright practices. 

Importantly, as distribution of culture, from texts to music to videos, is increasingly moving online, economically dominant ideas about authorship and copyright in our society will be implemented in actual software that will control who can access, copy, and modify the cultural objects, and at what price. For instance, while MPEG-1 through MPEG-7 media formats focused on “compression and the coordination of different media tracks, the recent proposal for MPEG-21 focuses on digital rights management.” The authors of the proposal imagine a future “multimedia framework” where “all people on Earth take part in a network involving content providers, value adders, packages, service providers, consumers, and resellers.” Like XML, MPEG-21 consists of  a number of separate components, those very names reveal its aim to manage all the difficult issues of content creation and distribution in digital network environment through technological solutions: “Intellectual property Management and Protection,” “Rights Data Dictionary,” “Rights Expression Language.” [20] OPUS anticipates this kind of future by providing an intellectually sophisticated alternative paradigm of cultural authorship and access implemented in software. 

## 9. Brand as the Author

Who are the people behind Nike? Prada? Sony? Gap? Consumer brands do not make visible their design teams, engineers, stylists, writers, programmers, and other creative individuals who make their individual products and product lines. Competing in already crowded semantic space, the company wants the consumers to remember one thing only: the brand name. To bring in the names of individuals involved in creating brand products - which are numerous and which continuously change - would dissolve brand identity. Note that a company does not try to hide these names - you can find them if you want - but they are just not part of brand publicity. Unless, of course, the name involved itself represents another brand, like Rem Koolhaas or Bruce Mau. Koolhaas and Mau are brands because they function exactly like all other brands: they have big teams working on different projects, but the names of individual contributors are not made visible. A museum hires Rem Koolhaas to have a building by Rem Koolhaas - not because it wants the skills of a particular media designer, lighting designer, or an architect working for Koolhaas. The same goes for most well-known musicians, artists, and architects. In contrast to “corporate brands,” these are "individual brands." 

When we think of these individual brands, we’re not supposed to also think of all the people involved in their creations. We can see here the romantic ideology with its emphasis on a solitary genius still at work. In a certain sense, corporate brands are more "progressive" in that they don’t hide (although they don’t foreground it either) the fact that everything they sell is created by collectives of individuals. And while in the last decade a number of artists’ collectives have presented themselves as corporate brands, in most case their masquerades still followed the conventions of artworld rather than of commercial brand environment. For instance, when jodi.org burst into the emerging net art scene with their Web site a number of years ago, the fact that for the first couple of years we only knew the project by the name of its URL but not the artist’s names was part of the attraction. However, eventually the names of the creators, Joan Heemskerk and Dirk Paesmans, became public. And Etoy, the most systematic among artists’ collectives simulating as brands, still has not been completely consistent in following the rules of corporate authorship. Etoy presents itself as a company which consists of  a small number of Etoy agents which go by their first names: etoy.zak, etoy.zai, and so on. Thus, it foregrounds all the individuals involved in brand management, even though they go by semi-fictional names.

My aim here is not to criticize Jodi or Etoy but rather to point that high culture and consumer culture follow very different models of authorship, which makes it hard even for smartest artists to completely simulate the corporate model. Still, artist-as-anonymous-brand phenomenon that already existed before Internet became much more common on the Web, with many artists, designers and design groups choosing to focus visibility on the name of their site rather than their individual names: from Jodi and Etoy to future farmers, unclickable.com, uncontrol.com, and many others.

## Conclusion

The commonality of menu selection / remixing / sampling / synthesis / “open sourcing” in contemporary culture calls for a whole new critical vocabulary to adequately describe these operations, their multiple variations and combinations. One way to develop such a vocabulary is to begin correlate the terms that already exist but are limited to particular media. Electronic music theory brings to the table analysis of mixing, sampling, and synthesis; academic literary theory can also make a contribution, with its theorizations of intertext, paratext [21], and hyperlinking; the scholars of visual culture can contribute their understanding of montage, collage and appropriation. Having a critical vocabulary that can be applied across media will help us to finally accept these operations as legitimate cases of authorship, rather than exceptions. To quote Poschardt one last time, “however much quoting, sampling and stealing is done – in the end it is the old subjects that undertake their own modernization. Even an examination of technology and the conditions of productions does not rescue aesthetics from finally having to believe in the author. He just looks different.” [22]

## References:

[1] Lev Manovich, _The Language of New Media_ (Cambridge, Mass.: The MIT Press, 2001).

[2] Roland Barthes, _Image, Music, Text_, translated by Stephen Heath (New York: Hill and Wang, 1977), 146.

[3] [http://www.ea.com/eagames/games/pccd/thesims/thesims.jsp](http://www.ea.com/eagames/games/pccd/thesims/thesims.jsp).

[4] Ulf Poschardt, _DJ Culture_, trans. Shaun Whiteside (London: Quartet Books Ltd, 1998), 123.

[5] Ibid, 271.

[6] Ibid., 273.

[7] For instance, Web users are invited to remix Madonna songs at [http://madonna.acidplanet.com/default.asp?subsection=madonna](http://madonna.acidplanet.com/default.asp?subsection=madonna).

[8] Poschardt, _DJ Culture_, 261.

[9] Ibid., 261-262.

[10] Ibid., 280.

[11] To use the term of Barthes’s quote above, we can say that if modernist collage always involved a “clash” of elements, electronic and software collage also allows for “blend.”

[12] Poschardt, _DJ Culture_, 252.

[13] Cindy Shirky, presentation during Human Generosity Project Summit, Banff Center for the Arts, September 2001.

[14] Some modification of Linux kernel becomes necessary when Linux is adapted for embedded systems which usually have less memory and less processing power than desktop PCs. See [http://www.linuxdevices.com/news/NS6362696390.html](http://www.linuxdevices.com/news/NS6362696390.html).

[15] Linux community will condemn any modifications that will change the kernel in fundamental ways. See [http://slashdot.org/articles/99/02/27/076204.shtml](http://slashdot.org/articles/99/02/27/076204.shtml).

[16] Ippolito is a new media artist and an Associate Curator of Media Arts at the Guggenheim Museum. For more information on _Variable Media Initiative_, see [www.guggenheim.org](www.guggenheim.org).

[17] See [http://www.opuscommons.net/main.php](http://www.opuscommons.net/main.php).

[18] [http://www.opuscommons.net/templates/doc/manual.html](http://www.opuscommons.net/templates/doc/manual.html).

[19] See [http://www.opuscommons.net/templates/doc/manual\_left.htm](http://www.opuscommons.net/templates/doc/manual_left.htm).

[20] See [http://mpeg.selt.it/](http://mpeg.selt.it/).

[21] For definitions of these terms introduced by Gerard Genette, see [http://www.ht01.org/presentations/Session4b/dalgaard\_HT01/](http://www.ht01.org/presentations/Session4b/dalgaard\_HT01/html\_with\_notes/tsld006.html.

[22] Poschardt, _DJ Culture_, 284.

---

# 10 Key Texts on New Media Art, 1970-2000

_author: Lev Manovich_
_year: 2002_

1. Gene Youngblood, _Expanded Cinema_ (New York: Dulton, 1970).

2. Jasia Reichardt, _The Computer in Art_ (London: 1971).

3. Cynthia Goodman, _Digital Visions: Computers and Art_, (New York: 1987).

4. Friedrich Kittler, _Discourse Networks_ (Stanford, 1990). (Original German edition 1985).

5. Michael Benedikt, ed., _Cyberspace: First Steps_ (Cambridge, Mass.: 1991).

6. _Artintact 1: Artists’ Interactive CD-ROM Magazine_ (Karlsruhe, 1994).

7. Minna Tarkka et al., eds., _The 5th International Symposium on Electronic Art Catalogue_ (ISEA), (Helsinki, 1994.)

8. Peter Weibel et al., eds., _Mythos Information: Welcome to the Wired World_. Ars Electronica 1995 Festival Catalog, edited by Peter Weibel (Vienna and New York: 1995).

9. Espen Aarseth, _Cybertext: Perspectives on Ergodic Literature_ (Baltimore: 1997).

10. Ulf Poschardt, _DJ Culture_ (London, 1998). (Original publication in German, 1995).

Working on my assignment to select “written works considered important to the history of digital art, culture and technology” turned out to be quite difficult. In contrast to other art fields, the short memory of digital art field is very short, while its long-term memory is practically absent. As a result, many artists working with computers, as well as curators and critics who exhibit and write about these artists, keep reinventing the wheels over and over and over. And while other fields usually have certain critical / theoretical texts which are known to everybody and which usually act as starting points for the new arguments and debates, digital art field has nothing of a kind. No critical text on digital art so far has achieved a familiarity status that can be compared with the status of the classic articles by Clement Greenberg and Rosalind Krauss (modern art), or Andre Bazin and Laura Mulvey (film). So, what does it mean to select “written works considered important to the history of digital art”? The field did produce many substantial texts that were important to it at particular historical points, but since these texts are not remembered, they have no bearings to its current development.

If you think that I am overstating my point, consider the following example. Think of important museum shows and their catalogs that act as key reference points in the field of modern art. How many among visitors to ”Bitsreams” (The Whitney Museum, 2001) and ”010101: Art in Technological Times” (SFMOMA, 2001) knew that thirty years ago the major art museums in New York and London presented a whole stream shows on the topics of art and technology. Taken together, these shows were more radical and more conceptually interesting than the current attempts of art museums to come to terms with new media. Here are some of them: ”Cybernetic Serendipity” (ICA, curated by Jasia Reichardt, 1968), ”The Machine as Seen at the End of the Mechanical Age” (MOMA, curated by K.G. Pontus Hulten, 1968), ”Software, Information Technology: its Meaning for Art” (Jewish Museum, New York, curated by Jack Burnham, 1970), ”Information” (MOMA, curated by Kynaston McShine, 1970), ”Art and Technology“ (LACMA, curated by Maurice Tuchman, 1970).

While the number of online exhibitions which were organized by Steve Dietz at the Walker, the recent exhibitions at the Z Lounge at the New Museum in NYC (Anne Barlow and Anne Ellegoood), the shows/events curated by Christiane Paul at the Whitney and Jon Ippolito at the Guggenheim all are quite sophisticated, all of them are also small-scale affairs. In terms of large-scale museum recent museum surveys, only the one at SFMOMA (2001) can be compared to the exhibitions of the thirty years ago. It was an ambitious attempt to sample the whole landscape of contemporary culture in order to present how artists _and_ designers across a number of disciplines engage with computing on a variety of levels: as a tool, as a medium, as iconography, as a source of new perceptual, cognitive and communication skills and habits. In comparison, the show at The Whitney was a truly reactionary affair. Here was a show on new media art that did not include any computers or interactive works. Instead, new media was reduced to flat images on the walls: stills presented as digital prints or moving images presented with projectors or plasma screens. The descriptions on the works positioned them within the familiar and well-rehearsed narratives and categories of standard twentieth century art textbooks. In short, new media was neutralized, diluted, rendered harmless, similar to the way commercial culture takes over most of the new radical cultural developments, from hip-hop to techno.

In contrast, just reading the titles of the exhibitions that took place thirty years ago you can see that they engaged with the new categories and dimensions of the emerging techno-culture. In terms of the works and projects presented, the museums similarly were not afraid to invite new technologies and new types of artistic practice within their spaces. [1] For example, ”The Machine as Seen at the End of the Mechanical Age” presented works by 100 artists, including commissioned collaborations between artists and engineers under the umbrella of EAT (compare this to current practice of US art museums to commission “net art” which then can be safely “tucked away” on museum Web sites instead of the actual galleries.) ”Software” exhibition included a number of works which used PDP-8 computer in the museum, while ”Information” engaged with information and communication revolution on a conceptual level by presenting a number of projects which asked the viewers to engage in particular communication scenarios constructed by artists, who included Vito Acconci and Hans Haacke). 

Given the systematic absence of long-term memory in digital art field, just ten texts would not be enough to reconstruct its rich fifty-year history. So here is the selection algorithm I ended up following:

(1) Given my limit of ten texts, I decided to be a little subjective and to give weight to the texts that were particularly important for me since I first learned about digital art. 

(2) Given that the digital art field does not really has a set of “canonical” critical texts, I instead selected a few texts which at different decades acted as key reviews of the field (_The Computer in Art_, 1971; _Expanded Cinema_, 1970; _Digital Visions_, 1987). 

(3) Since the annual festivals/exhibitions such as Ars Electronica, ISEA and SIGGRAPH played the key role in development of the field, I next included couple of representative catalogs from the particularly important meetings (ISEA 94, Ars Electronica 95). 

(4) I then added the first publication from ZKM’s _Artintact_ series (_artintact_ 1, 1994). Early on, ZKM solved the two key problems of the digital art field – distribution and criticism – in a particularly elegant and efficient way. Every year since 1994 ZKM published a CD-ROM/book. CD-ROM would contain 3 interactive art projects while the book would present critical texts about each of the projects (today ZKM continues this successful format with new series which use DVD-ROM instead of CD-ROM). By following the book format and by teaming up with a major German book publisher, ZKM assured that _artintact_ would be distributed through the standard book distribution channels. (It only took the Whitney eight years to catch up: Whitney 2002 biennial catalog similarly included a CD-ROM attached to the front cover.) [2]

(5) While digital art field does not have a canon of critical texts about the art itself, most people in it are familiar with at least some _theoretical_ texts dealing with the larger topics of digital technology / culture / society. I think that in fact a number of such theoretical texts act as equivalent of canonical _critical_ texts in other art fields. Since I had the limit of ten texts total, I could only include a small sample of such theoretical works. I choose _Discourse Networks_ by Friedrich Kittler (1985; English edition 1990); _Cyberspace: First Steps_, edited by Michael Benedikt (1991), _DJ Culture_ by Ulf Poschardt (1995; English edition 1998); and _Cybertext_ by Espen Aarseth (1997). But I could have equally well selected books by Katherine Hayles, Sherry Turkle, W.J.T. Mitchell, Paul Virilio, Peter Lunenfeld, Jay David Bolter, Pierre Levy, Geert Lovink, Norman Klein, Vivian Sobchack, Peter Weibel, Slavoj Zizek, Erkki Huhtamo, Margaret Morse, Alex Galloway, Matt Fuller, and many others (and this is just the people who write in English or whose work is available in English translation; internationally, the list of brilliant commentators on techno-culture goes on and on). [3] 

I think that each of the four theoretical books I selected has something unique about it. Benedikt’s best-selling collection is exemplarily in bringing together theorists, artists and computer designers or early cyberspaces such as Habitat – and somehow forcing the designers to write clear and theoretically sophisticated descriptions of their projects and research programs. The best of the anthologies and conferences on digital arts and new media culture try to create such a mix, but few succeed in doing it the way _Cyberspace: First Steps_ did. 

Kittler is probably the most important media theorist after McLuhan, and in his master opus _Discourse Networks_ he is able to accomplish another difficult “convergence” trick – bringing together “the best of” what in the US called “critical theory” (in his case it is Lacan and Foucault) with his own brilliant ideas about the effects of communication networks and media recording/storage/access technologies on culture. Again, this is a kind of “convergence” which many try to do but probably only Kittler has succeeded so far. 

Many would agree that the two areas of culture where the new logic of digital computing always shows up significantly earlier than in other fields is computer games and electronic music. While I know next to nothing about popular electronic music, I found _DJ Culture_ to be a brilliant mix of broad social, cultural, and technological history of the field and provocative theoretical speculations. Too many books and anthologies on electronic music put you to sleep with too much detail about this or that piece of technology - _DJ Culture_ manages to stay focus on the concepts. In his writing, Munich-based Ulf Poschardt also successfully integrates “remix” inspired style of exposition and a more standard historical structure that keeps you on track through this think book. 

Finally, in his thin but dense _Cybertext_ Espen Aarseth offers a particularly elegant solution to the key question of digital arts and culture field: how to separate new and old media? Although he is concerned with texts, his approach can be extended to other media, providing a reach paradigm for thinking about the relationships between the old and the new media. Read this book if you missed it! (I don't want to do his complex and clear arguments injustice by trying to sum them in two sentences here…)

At the end, it is probably to the best that the arguments in digital arts do not always return to the same few “master” texts over and over and over, the way it often happens in the art world and in humanities. As Norman Klein once put it, “to paint with a computer is to paint with a machine gun” – meaning that a digital computer is unprecedented in being the key engine of modern economy, the key control and communication technology of modern societies, and _also_ their key representational machine. Given this unprecedented “convergence,” any serious reflection on the social and cultural dynamics of our time has to engage with digital computing. 

The fact that the theoretical texts which address the general issues in techno-culture – new functioning of space and time, info-subjectivity, new dynamics of cultural production and consumption, and so on - are more important to digital artists and designers than digital art criticism per se is ultimately very healthy. It means that the people in our field have a keen interest in how computerization affects society and culture at large, rather than just being concerned about the narrow history of their own field. So, while we should all be more familiar with this history than we currently are, let’s not make it into a fetish. 

## References:

[1] For more information on these shows and other important milestones in the fifty year history of computer and telecommunication art, see excellent Telematic Timeline produced as a part of the show curated by Steve Dietz,[http://telematic.walkerart.org/timeline/](http://telematic.walkerart.org/timeline/).

[2] In 2002 Hatje Cantz Publishers published _The Complete Artintact 1994-99 CD-ROMamagazine on DVD-ROM_. 

[3] I decided not to include in my final “top 10” list any works by my Southern California colleagues: Hayles, Lunenfeld, Klein, and Sobchack. Why am I being so naïve? New York people only curate/publish themselves all the time…

---

# Generation Flash

_author: Lev Manovich_
_year: 2002_

This essay that consists of  a number of self-contained segments looks at the phenomenon of Flash graphics on the Web that attracted a lot of creative energy in the last few years. More than just a result of a particular software / hardware situation (low bandwidth leading to the use of vector graphics), Flash aesthetics exemplifies cultural sensibility of a new generation. [1] This generation does not care if their work is called art or design. This generation is no longer is interested in "media critique" which preoccupied media artists of the last two decades; instead it is engaged in software critique. This generation writes its own software code to create their own cultural systems, instead of using samples of commercial media. [2] The result is the new modernism of data visualizations, vector nets, pixel-thin grids and arrows: Bauhaus design in the service of information design. Instead the Baroque assault of commercial media, Flash generation serves us the modernist aesthetics and rationality of software. Information design is used as tool to make sense of reality while programming becomes a tool of empowerment. [3] 

## Turntable and Flash Remixing

> for [www.whitneybiennial.com](http://www.whitneybiennial.com/)
> 
> Turntable is a web-based software that allows the user to mix in real-time up to 6 different Flash animations, in addition manipulating color palette, size of individual animations and other parameters. For [www.whitneybiennial.com](http://www.whitneybiennial.com/), the participating artists were asked to submit short Flash animations that were exhibited on the site both separately and as part of Turntable remixes. Some remixes consisted of  animations of the same artists while others used animations by different artists.

It has become a cliché to announce that “we live in remix culture.” Yes, we do. But is it possible to go beyond this simple statement of fact? For instances, can we distinguish between different kinds of remix aesthetics? What is the relationship between our remixes made with electronic and computer tools and such earlier forms as collage and montage? What are the similarities and differences between audio remixes and visual remixes? 

Think loop. The basic building block of an electronic soundtrack, the loop also conquered surprisingly strong position in contemporary visual culture. Left to their own devices, Flash animations, QuickTime movies, the characters in computer games loop endlessly - until the human user intervenes by clicking. As I have shown elsewhere, all nineteenth century pre-cinematic visual devices also relied on loops. Throughout the nineteenth century, these loops kept getting longer and longer - eventually turning into a feature narrative… Today, we witness the opposite movement – artists sampling short segments of feature films or TV shows, arranging them as loops, and exhibiting these loops as “video installations.” The loop thus becomes the new default method to “critique” media culture, replacing a still photograph of post-modern critique of the 1980s. At the same time, it also replaces the still photograph as the new index of the real: since everybody knows that a still photography can be digitally manipulated, a short moving sequence arranged in a loop becomes a better way to represent reality - for the time being.

Think Internet. What was referred in post-modern times as quoting, appropriation, and pastiche no longer needs any special name. Now this is simply the basic logic of cultural production: download images, code, shapes, scripts, etc.; modify them, and then paste the new works online - send them into circulation. (Note: with Internet, the always-existing loop of cultural production runs much faster: a new trend or style may spread overnight like a plague.) When I ask my students to create their own images by making photographs or by shooting video, they have a revelation: images do not have to come from Internet! Shall I also reveal to them that images do not have to come from a technological device that record reality – that instead they can be drawn or painted?

Think image. Compare it to sound. It seems possible to layer many many sounds and tracks together while maintaining legibility. The result just keeps getting more complex, more interesting. Vision seems to be working differently. Of course, commercial images we see every day on TV and in cinema are often made from layers as well, sometimes as many as thousands – but these layers work together to create a single illusionistic (or super-illusionistic) space. In other words, they are not being heard as separate sounds. When we start mixing arbitrary images together, we quickly destroy any meaning. (If you need proof, just go and play with the classic “The Digital Landfill.”) [4] How many separate image tracks can be mixed together before the composite becomes nothing but noise? Six seems to be a good number – which is exactly the number of image tracks one can load onto Turntable.

Think sample versus the whole work. If we are indeed living in a remix culture, does it still make sense to create whole works – if these works will be taken apart and turned into samples by others anyway? Indeed, why painstakingly adjust separate tracks of Director movie or After Effects composition getting it just right if the “public” will “open source” them into their individual tracks for their own use using some free software? Of course, the answer is yes: we still need art. We still want to say something about the world and our lives in it; we still need our own “mirror standing in the middle of a dirty road,” as Stendahl called art in the nineteenth century. Yet we also need to accept that for others our work will be just a set of samples, or maybe just one sample. Turntable is the visual software that makes this new aesthetic condition painfully obvious. It invites us to play with the dialectic of the sample and the composite, of our own works and the works of others. Welcome to visual remixing Flash style.

Think Turntable.

## Art, Media Art, and Software Art

Recently “software art” has emerged as the new dynamic area of new media arts. Flash’s ActionScript, Director’s Lingo, Perl, MAX, JavaScript, Java, C++, and other programming and scripting languages are the medium of choice of a steadily increasing number of young artists. Thematically, software art often deals with data visualization; other areas of creative activity include the tools for online collaborative performance / composition (Keystroke), DJ/VJ software, and alternatives to / critiques of commercial software (Auto-illustrator), especially the browsers (early classics like Netomat, Web Stalker, and many others since then). Often, artists create not singular works, but software environments open for others to use (such as Alex Galloway’s Carnivore.) Stylistically, many works implicitly reference visual modernism (John Simon seems to be the only one so far to weave modernist references in his works explicitly). 

Suddenly, programming is cool. Suddenly, the techniques and imagery that for two decades were associated with SIGGRAPH geek ness and were considered bad taste – visual output of mathematical functions, particle systems, RGB color palette – are welcomed on the plasma screens of the gallery walls. It is no longer _October_ and _Wallpaper_ but Flash and Director manuals that are the required read for any serious young artist. 

Of course, from its early days in 1960s computer artists have always written their own software. In fact, until the middle of the 1980s, writing own software or at least using special very high-end programming languages designed by others (such as Zgrass) was the only way to do computer art. [5] So what is new about the recently emerged phenomenon of software art? Is it necessary?

Let’s distinguish between three figures: an artist; a media artist; and a software artist. 

A romantic/modernist artist (the nineteenth century and the first half of the twentieth century) is a genius who creates from scratch, imposing the phantoms of his imagination on the world. 

Next, we have the new figure of a media artist (the 1960s – the 1980s) that corresponds to the period of post-modernism. Of course, modernist artists also used media recording technologies such as photography and film, but they treated these technologies similar to other artistic tools: as means to create an original and subjective view of the world. In contrast, post-modern media artists accept the impossibility of an original, unmediated vision of reality; their subject matter is not reality itself, but representation of reality by media, and the world of media itself. Therefore, these media artists not only use media technologies as tools, but they also use the content of commercial media. A typical strategy of a media artist is to re-photograph a newspaper photograph, or to re-edit a segment of TV show, or to isolate a scene from a Hollywood film / TV shows and turn it into a loop (from Nam June Paik and Dara Birnbaum to Douglas Gordon, Paul Pffefer, Jennifer and Kevin McCoy). Of course, a media artist does not have to use commercial media technologies (photography, film, video, new media) – s/he can also use other media, from oil paint to printing to sculpture. 

The media artist is a parasite who lives at the expense of the commercial media – the result of collective craftsmanship of highly skilled people. In addition, an artist who samples from / subverts / pokes at commercial media can ultimately never compete with it. Instead of a feature film, we get a single scene; instead of a complex computer game with playability, narrative, AI, etc., we get just a critique of its iconography. 

Thirty years of media art and post-modernism have inevitably led to a reaction. We are tired of always taking existing media as a starting point. We are tired of being always secondary, always reacting to what already exists. 

Enter a software artist – the new romantic. Instead of working exclusively with commercial media – and instead of using commercial software – software artist marks his/her mark on the world by writing the original code. This act of code writing itself is very important, regardless of what this code actually does at the end. 

A software artist re-uses the language of modernist abstraction and design – lines and geometric shapes, mathematically generated curves and outlined color fields – to get away from figuration in general, and cinematographic language of commercial media in particular. Instead of photographs and clips of films and TV, we get lines and abstract compositions. In short, instead of QuickTime, we use Flash. Instead of computer as a media machine – a vision being heavily promoted by computer industry (and most clearly articulated by Apple who promotes a MAC as a “digital hub” for other media recording / playing devices), we go back to computer as a programming machine. 

Programming liberates art from being secondary to commercial media. The similar reason may be behind the recent popularity of “sound art.” While commercial media now uses every possible visual style, commercial sound environments still have not appropriated all of sound space. While rock and roll, hip-hop, and techno have already become standard elevator music (at least in more hip elevators such as the Hudson Hotel in NYC), it seems that the rhythm-less regions of sound space are still untouched – at least for now.

## UTOPIA in Shockwave

UTOPIA is a Shockwave project by Futurefarmers for Tirana Biennale 01 Internet section.
> 
> Futurefarmers: Amy Franceschini and Sascha Merg,
>  [http://nutrishnia.org/level/](http://nutrishnia.org/level/)

UTOPIA is playful and deceitful - because it pretends to be more innocent, more simple, and more light than it actually is. At first glance it can be taken for something made for children - or for adults whose references are not Karl Marx, Sigmund Freud, Rem Koolhaas, and Philip Stark, but text messaging, gnuttela, retro Atari graphics, and nettime. This is the new generation that emerged in the 1990s. In contrast to visual and media artists of the 1960s-1980s, whose main target was media - ads, cinema, television - the new generation does not waste its energy on media critique. Instead of bashing commercial media environment, it creates its own: Web sites, mixes, software tools, furniture, clothes, digital video, Flash / Shockwave animations and interactives.

The new sensibility, which Utopia exemplifies so well, is soft, elegant, restrained, and smart. This is the new software intelligentsia. Look at the thin low-contrast lines of UTOPIA, praystation.com, and so many Flash projects included in Tirana Biennale 01. If images of the previous generations of media artists, from Nam June Paik to Barbara Krueger, were screaming, trying to compete with the intensity of the commercial media, the new data artists such as Franceschini/Merg whisper in our ears. In contrast to media's arrogance, they offer us intelligence. In contrast to media stream of endless repeated icons and sound bytes, they offer us small and economical systems: stylized nature, ecology, or the game / music generator / Lego-like parade in UTOPIA.

Futurefarmers are among the few Flash/Schockwave masters who use their skills for social rather than simply a formal end. Their project theyrule.net is a great example of how smart programming and smart graphics can be used politically. Instead of presenting a packaged political message, it gives us data and the tools to analyze it. It knows that we are intelligent enough to draw the right conclusion. This is the new rhetoric of interactivity: we get convinced not by listening / watching a prepared message but by actively working with the data: reorganizing it, uncovering the connections, becoming aware of correlations.

UTOPIA does not have explicit political content; instead it presents its message through a visual allegory. Like _SimCity_ and similar sims, the program presents us with a whole miniature world which runs according to its own system of rules. (All the animation in UTOPIA is result of code execution – nothing is hand animated.) The cosmogony of this world reflects our new understanding of our own planet - post Cold War, Internet, ecology, Gaia, and globalization. Notice the thin barely visible lines that connect the actors and the blocks. (This is the same device used in theyrule.net.) In the universe of UTOPIA, everything is interconnected, and each action of an individual actor affects the system as a whole. Intellectually, we know that this is how our Earth functions ecologically and economically - but UTOPIA represents this on a scale we can grasp perceptually.

The lines also serve another purpose. Despite CNN, Greenpeace, the glass roof of Berlin’s Reichstag and other institutions and devices working to make the functioning of modern societies transparent to their citizens, most of it is not visible. This is not only because we don't know the motives behind this or that Government policy or because advertising and PR constantly work to make things appear differently from what they really are – the societies’ functioning is not visible in a literal sense. For instance, we don't know where the cells are which make our cell phones work; we don’t know the layout of private financial network that circle the Earth; we don’t know what companies are located in a building we pass every day on a way to work; and so on. But in UTOPIA, we do know – because the links are made visible. UTOPIA is Utopia because it is a society where cause and effect connection are rendered visible and comprehensible. The program re-writes Marxism as vector graphics; it substitutes the figure of “connections” for the old figure of “unveiling.” 

UTOPIA is serious business behind its playful façade – but it is not all business. Drawing on our current fascination with computer games and interactive image-sound software, UTOPIA is a visual and intellectual delight, UTOPIA draws on the current fascination with computer games and interactive image-sound software. It is Tetris that meets Marx that meets data mining that meets the club dance floor. It is a game for the new generation that know that the world is a network, that the media is not worth taking very seriously, and that programming can be used as a political tool.

## The Unbearable Lightness of FLASH

Tirana Biennale 01 Internet section [www.electronicorphanage.com/biennale](http://www.electronicorphanage.com/biennale) was organized by Miltos Manetas / Electronic Orphanage. The exhibition consisted of  a few dozen projects by Web designers and artists, many of whom work in Flash or Schockwave. Manetas commissioned me, Peter Lunenfeld, and Norman Klein to write the analysis of the show. This text is my contribution; many ideas in it developed out of the conversations the three of us had about the works in the show. The names in parentheses below refer to the artists in the show; go to the show site to see their projects. 

### Biology

Flash artists are big on biological references. Abstract plants, minimalist creatures, or simply clouds of pixels dance in patterns which to a human eye signal “life” (Geoff Stearns: deconcept.com, Vitaly Leokumovich: unclickable.com, Danny Hobart: dannyhobart.com; uncontrol.com). Often we see self-regenerating systems. But this is not life as it naturally developed on Earth; rather, it looks like something we are likely to witness in some biotech laboratory where biology is put in the service of industrial production. We see hyper accelerated regeneration and evolution. We see complex systems emerging before our eyes: millions of years of evolution are compressed into a few seconds. 

There is another feature that distinguishes life a la Flash from real life: the non-existence of death. Biological organisms and systems are born, they develop, and eventually they die. In short, they have teleology. But in Flash projects life works differently: since these projects are loops, there is no death. Life just keeps running forever – more precisely, until your computer maintains Net connection.

### Amplification: Flash aesthetics and Computer Games

Abstract ecosystems in Flash projects have another characteristic that makes playing so pleasurable (Joel Fox). They brilliantly use the power of the computer to amplify user’s actions. This power puts a computer in line with other magical devices; not accidentally, the most obvious place to see it is in games, although it is also at work in all of our interactions with a computer. For instance, when you tell Mario to step to the left by moving a joystick, this initiates a small delightful narrative: Mario comes across a hill; he starts climbing the hill; the hill turns to be too steep; Mario slides back onto the ground; Mario gets up, all shaking. None of these actions required anything from us; all we had to do is just to move the joystick once. The computer program amplifies our single action, expanding it into a narrative sequence. 

Historically, computer games were always a step ahead from the general human computer interface. In the 1960s and 1970s users communicated with a computer using non-graphical interfaces: entering the program onto a stack of punch cards, typing on a command line, and so on. In contrast since their beginnings in the late 1950s, computer games adopted interactive graphical interface – something that only came to personal computers in the 1980s. 

Similarly, today’s games already use what many computer scientists think will be the next paradigm in HCI: active amplification of user’s actions. In the future, we are told, agent programs would watch our interactions with a computer, notice the patterns, and then automate many tasks we do regularly, from backing up the data at regular intervals to filtering and answering our email. The computer would also monitor our behavior and attention level, adjusting its behavior accordingly: speeding up, slowing down, and so on. In some ways this new paradigm is already at work in some applications: for instance, Internet browser offers us the list of sites relevant to the topic we are searching on; Microsoft Office Assistant trying to guess when we need help. However, there is a crucial problem with moving to such active amplification across the whole of HCI. The more power we delegate to a computer, the more we lose control over what it is doing. How do we know that the agent program identified a correct pattern in our daily use of email? How do we know that a commerce agent we send on the Web to negotiate with other agents the lowest price for a product was not corrupted by them? In short, how do we know that a computer amplified our actions correctly? 

Computer games are games, and the worst that may happen is that we lose. Therefore, active amplification is present in practically every game: Mario embarking on mini-narratives of its own with a single move of a joystick; troops conducting complex military maneuvers while you directly control only their leader in Rainbow Six; Lora Craft executing whole acrobatic sequences with a press of a keyboard key. (Note that in “normal” games this amplification does not exist: when you move a single figure on a chessboard, this is all that happens; your move does not initiate a sequence of steps.) 

Flash projects heavily use active amplification. It gives many projects the magical feeling. Often we are confronted with an empty screen, but a single click brings to life a whole universe: abstract particle systems, plant-like outlines, or a population of minimalist creatures. The user as a God controlling the universe is something we also often encounter in computer games; but Flash projects also give us the pleasure of creating the universe from scratch. 

The active amplification is not the only feature Flash projects share with games. More generally, as Peter Lunenfeld suggested, computer games are for Flash generation what movies were for Warhol. Cinema and TV colonized the unconscious of the previous generations of media artists who continue to use the gallery as their therapy coach, spilling bits and pieces of their childhood media archives in public (for instance, Douglas Gordon). Flash artists are less obsessed with commercial time-based media. Instead, their iconography, temporal rhythms, and interaction aesthetics come from games (Mike Clavert: mikeclavert.com). Sometimes the user participation is needed for the Flash game to work; sometimes the game just plays itself (UTOPIA by futurefarmers.com; dextro.org).

### Flash versus Net Art

Tirana Biennale 01 Internet exhibition: this title is deeply ironic. The exhibition did not include any projects from Albania, or any other post-communist East European country for that matter. This was quite different from many early net art exhibitions of the middle of the 1990s whose stars came from the East: Vuk Cosic, Alexei Shulgin, Olia Lialina. 1990s net art was the first international art movement since the 1960s that included east Europe in a big way. Prague, Ljubljana, Riga, and Moscow counted as much as Amsterdam, Berlin, and New York. Equally including artists from the West and the East, net art perfectly corresponded to the economic and social utopia of a new post Cold War world of the 1990s.

Now this utopia is over. The power structure of the global Empire has become clear, and the demographics of Tirana Biennale 01 Internet section reflected this perfectly. Many artists included in Tirana Biennale 01 Internet exhibition work in key IT regions of the world: San Francisco (Silicon Valley), New York (Silicon Alley), and Northern Europe. 

What happened? In the mid-1990s, net art relied on simple HTML that run well on both fast and slow connections – and this is enabled active participation of the artists from the East. But the subsequent colonization of the Web by multimedia formats – Flash, Shockwave, QuickTime, and so on – restored the traditional West/East power structure. Now Web art requires fast Internet connections for both the artist and the audiences. With its slow connections, East is out of the game. The Utopia is over; welcome to the Empire.

(Tirana Biennale 01 did include one artist from China who contributed a beautiful animation of martial arts fighters. But we never found who he was. All we knew about him was his email address: [zhu\_zhq@sohu.com](mailto:zhu_zhq@sohu.com). Maybe he did not even live in China.)

## Generation FLASH: FAQ

After I posted the preceding segments on popular mailing lists dealing with new media art and cyberculture (rhizome.org and nettime.org), I received lots of responses. Here are my answers to two most common questions which appeared in a number of responses. 

_Question_:

Is not “soft modernism” you describe simply a result of particular technological limitations of multimedia on the Net? You seem to mistake the particular features of Flash designed to deliver animation over the narrow bandwidth for a larger zeitgeist.

_Answer_:

Now that the new release of Flash (Flash MX) allows for import and streaming of video, it is possible that soon "Flash generation" / "soft modernism" aesthetics will leave Flash sites. This is fine. My concern in this essay is _not_ with Flash software and its limitations/capabilities per se, but with the new sensibility that during the last couple of years manifested in many Flash projects. In other words, I am interested in "generation Flash" that is quite different from Flash software/format. 

Therefore, the number of people who after reading my text accused me of confusing a technical standard with aesthetics missed my argument. The vector-oriented look of "soft modernism" is not simply a result of narrow bandwidth or a nostalgia for 1960s design - it _always_ happens when people begin to generate graphics through programming and discover that they can use simple equations, etc. This is also why "soft modernism" of Flash projects and other software artists replays, sometimes in amazing detail, the aesthetics of early computer art (1950s-1970s) when people were only able to create images and animations through programming. 

_Question_: 

There is no reason software art cannot use representation images or any other form. Why do you associate software art with non-representational, abstract vector-based graphics?

_Answer_: 

Of course, software artists can use representational images or any other “conventional” form or media. It was not accidental that soon after his arrival at Xerox PARC in the 1970s, Alan Kay and his associates created a paint program and an animation program, alongside with overlapping windows, icons, Smalltalk, and other principles of modern interactive graphical computing. The abilities to manipulate and generate media are not after-thoughts to a modern computer - they are central to its identity as a "personal dynamic medium" (Alan Kay.) To put his differently: computer is a simulation machine, and as such it can and should be used to simulate other media. 

So, I have nothing against software artists using/creating media, but I hope that "Flash generation" will extend its programming work to representational media! In other words, if in the early 1970s the paint program and the animation program were revolutionary in changing people idea about a computer away from computation and towards a (creative) medium, after almost two decades of menu-based media manipulation programs and the use of computers as media distribution machine (greatly accelerated by World Wide Web), a little programming can be quite revolutionary! In short, we now are so used to think of a computer as a "personal dynamic medium," that we need to remind ourselves and others that it is also a programmable machine.

Now, think about how programming has been used so far to create/use still images, animation, and film/video. There are three trajectories that can be traced historically. One trajectory extends from the earliest works of computer art - the films by the Whitney made with an analog computer already in the mid-1950s (who were the students of Oscar Fischinger and thus represent a direct link with the early twentieth century modernism) - to today's "soft modernism" of Flash projects and data visualization artworks. In other words, this is the use of programming to generate and control abstract images. 

The second trajectory begins in the 1980s when Hollywood and TV designers started to use computer-generated imagery (CGI). Now, programming was put in the service of traditional cinematic realism. Particle systems, formal grammars, AI, and other software techniques became the means to generate flying bats, hilly landscapes, ocean waves, explosions, alien creatures, and other figurative elements integrated in a photorealistic universe of a narrative film. 

What about using algorithms not simply to generate figurative elements of a narrative but to control the whole fictional universe? This is the third trajectory: programming in computer games (1960-). Here algorithms may control the narrative events, the behavior of characters, camera movement, and other characteristics of the game world - all in real time. Unfortunately, as we all know, aesthetically revolutionary computer and player driven game worlds feature formula-driven content that makes even a bad Hollywood film appear original and inspiring by comparison. (_Grand Theft Auto 3_ is no exception here - despite its breakthroughs in simulating a more compelling and open universe.) 

I think this brief survey shows that there is still an untouched space completely open for experimentation and creative research - using programming to generate and/or control figurative/fictional media. For instance, in the case of a movie, programming can be used to generate characters on the fly, to composite in real-time characters shot against a blue screen with backgrounds, to control the sequence of scenes, to apply filters to any scene in real-time, to combine pre-recorded scene with on the imagery generated on the fly, to have characters interact with the viewer, etc., etc. In short, programming can be used to control _any_ aspect of a fictional media work. 

Of course, once in a while one encounters projects moving in this direction at places like SIGGRAPH or ISEA, but they are typically research demos created in universities that do not reach culture at large. Of course, you can object that having an algorithmically controlled complex fictional universe requires the kind of programming investment only possible in a commercial game company or in a university. After all, this is not the same as writing a script that draws a few lines that keep moving in response to user input...yes, but why our fictional/figurative works have to follow the formulas of commercial media? If one accepts that the characters do not have to be "photorealistic," that the fictional world does not have to be exclusively three-dimensional, that chance and randomness can co-exist with narrative logic, or that stick figures can co-exist with 3-D characters and video footage, etc., programming figuration / fiction becomes less formidable. In short, while I welcome programming Flash, I think it is much more challenging to program QuickTime.

## Postscript: On the Lightness of Flash

When I first visited the most famous Flash site – praystation.net – I was struck by the lightness of its graphics. More quiet than a whisper, more elegant than Dior or Chanel, more minimal than 1960s minimalist sculptures of Judd, more subdued than the winter landscape in heavy fog, the site pushed the contrast scale to the limits of legibility. The similar lightness and restrain can be found in many projects included in Biennale 01 show. Again, the contrast with screaming graphics of commercial media and the media art of the previous generations is obvious. 

The lightness of Flash can be thought of as a visual equivalent of electronic ambient music. Every line and every pixel counts. Flash appeals to our visual intelligence - and cognitive intelligence. After the century of RGB color which begun with Matisse and ended with aggressive spreads of Wired, we are asked to start over, to begin from scratch. Flash generation invites us to undergo a visual cleansing – this is why we see a monochrome palette, white and light gray. It uses neo-minimalism as a pill to cure us from post-modernism. In Flash, the rationality of modernism is combined with the rationality of programming and the affect of computer games to create the new aesthetics of lightness, curiosity, and intelligence. Make sure your browser has the right plug-in: welcome to generation Flash.

I am not advocating a revival of modernism. Of course, we don't want to simply replay Mondrian and Klee on computer screens. The task of the new generation is to integrate the two key aesthetic paradigms of the twentieth century: (1) belief in science and rationality, emphasis on efficiency and basic forms, idealism, and heroic spirit of modernism; (2) skepticism, interest in “marginality” and “complexity,” deconstructive strategies, baroque opaqueness, and excess of post-modernism (1960s-). At this point all the features of the second paradigm became tired clichés. Therefore, a partial return to modernism is not a bad first step, as long as it is just a first step towards developing the new aesthetics for the new age.

Of course, this aesthetics should also fully engage with the difficult questions of globalization. The _remix culture_ we are living now is not only engaged in remixing all previous cultural forms and texts of but also in remixing various features which come from what used to be call national cultures as well as from already existing remixes between immigrant populations and their “host” cultures. The solution offered by multinational conglomerates – a composite which takes certain signifiers from a few national cultures – for instance, French idea of elegance, Japanese manga iconography, “cool Britannia” references, and so on, and integrates it all into a rather bland and monolithic text which is then being send back to all the places around the world – is obviously not a satisfactory solution. (It reminds me of Soviet-style centralized economy when the all the output of collective farms was sent to the center where it was decided how it was distributed nationally.) Luckily, numerous remixes which follow different logics are being explored around the world by musicians, theatre groups, dancers, designers, architects, and so on. Nobody knows what will emerge from this global cultural laboratory – and this is what makes out times so interesting. 

Although most of my arguments in this book are about visual culture and visual aesthetics, it is relevant at this point to evoke a different practice. Music historically has been the artistic field that was always been ahead of other fields in using computers to enable new aesthetic paradigms. The whole practice of popular electronic music in the last three decades is a testament to how empowering new technologies are in welding new complex and rich remixes between different cultures, styles, and sensibilities. Without electronic and computing technologies – from a turntable and a tape recorder to peer-to-peer file sharing networks and music synthesis software running on a regular laptop, most of this culture would never come to be. The field of electronic sound (which pretty much means most sounds today) with its multitude voices and a real bottom-up, “emergent” logic, is a powerful alternative to the “top-down” cultural composites sold by global media conglomerates around the world. Let us hope that other artists and designers in other fields will follow music lead in using a computer to enable similarly rich remix cultures. 

## References:

[1] This article is about “Flash Generation” and not about the Web sites made with Flash software. Many of the sites which inspired me to think of “Flash aesthetics” are not necessarily made with Flash; they use Shockwave, DHTML, Quicktime, and other Web multimedia formats. Thus, the qualities I describe below as specific to “Flash aesthetics” are not unique to Flash sites.

[2] For instance, the work of Lisa Jevbratt, John Simon, and Golan Levin.

[3] “Generation Flash” incorporates revised versions of the texts commissioned for [www.whitneybiennial.com](http://www.whitneybiennial.com/) and [http://www.electronicorphanage.com/biennale](http://www.electronicorphanage.com/biennale). Both exhibitions were organized by Miltos Manetas / Electronic Orphanage. “On UTOPIA” was commissioned by Futurefarmers. 

[4] See [http://www.potatoland.org/landfill/](http://www.potatoland.org/landfill/)

[5] After GUI-based applications such as Hypercard, Director, Photoshop and others became commonplace, many computer artists continued to do their own programming: writing custom code to control an interactive installation, programming in LINGO an interactive multimedia work, etc. This was not referred to as software art; it was taken for granted that even in the age of GUI-based applications a really serious artistic engagement with computers requires getting one’s hands dirty in code.

---

# Metadata, Mon Amour

_author: Lev Manovich_
_year: 2002_

Metadata is the data about data: keywords assigned to an image in a media database, a number of words in a text file, the type of codec used to compress an audio file. Metadata is what allows computers to “see” and retrieve data, move it from place to place, compress it and expand it, connect data with other data, and so on.

The title of this chapter refers to the ongoing modern struggle between the visual data, i.e., images, and their creators and masters – the humans. The later want to control images: make new images which would precisely communicate the intended meanings and effects; yield the exact meanings contained in all the images already created by human cultures; and, more recently, automate these and all-over possible image operations by using computers. The former can be said to “resist” all these attempts. This struggle has intensified and became more important in a computer age – more important because the ease with which computers copy, modify, and transmit images allows humans to daily multiply the number of media records available.

Creating metadata is not, however, only the economic and industrial problem to be solved – it is also a new paradigm to “interface reality” and the human experience in new ways. This is already demonstrated by a number of successful art projects that focus on new ways to describe, organize and access large numbers of visual records. Importantly, these projects propose not only _new interfaces_ but also _new types of images_, or, more generally, “records” of human individual and collective experience: film/video recordings embedded within virtual space (Sauter, _Invisible Shape of Things Past_; Fujihata, _Field-Work@Alsace_); photographs of people/objects organized into networks/maps based on their semantic similarity (Legrady, _Pockets Full of Memories_; Walitzky, _Focus_).

In summary, in terms of its creative and “generative” potential, “metadating the image” paradigm means following four related directions: (1) inventing new systems of image description and categorization; (2) inventing new interfaces to image collections; (3) inventing new kinds of images which go beyond such familiar types as “a still photograph” or a “digital video”; (4) approaching the new “super-human” scale of visual data available (images on the Web, web cam recordings, etc.) not as a problem but as a creative opportunity.

In short: new structure – new interface – new image – new scale.

## Description

Ancient and modern cultures developed rich and precise systems to describe oral and written communication: phonetics, syntax, semantics, pragmatics, rhetoric, poetics, narratology, and so on. Dictionaries and thesauruses help us to create new texts; the search engines and the ever present “find…” command in our software applications help us to locate the particular texts already created, or their parts; narratology and poetics provide us with concepts to describe the semantics and the formal structure of literary texts.

Paradoxically, while the role of visual communication has dramatically increased over the last two centuries, no similar descriptive systems and/or search tools were developed for images. While we do have some concepts such as Panofsky’s iconography and iconology, or Pierce’s index – symbol – icon, they do not approach the richness, the generality, and the precision of concepts available to describe the texts. While In the last four decades there have been many attempts to import concepts from literary theory and linguistics into art history and visual culture studies, these imported concepts have not been widely adopted. 

Often the professionals working in some cultural field develop their own terms and taxonomies that are more precise than the terms used by the theorists studying the same field from the outside. In the case of images, there are a few professional practices we can look at – for instance, Hollywood cinematography or Bauhaus art education – but overall, the image taxonomies used in various contemporary professional fields are also quite limited. Stock photography agencies divide millions of photographs into a few dozen categories, with names such as “joy,” “business,” and” achievement”. Graphic designers and their clients typically use even more limited range of categories to describe their projects: “clean,” “futuristic,” “corporate,” “conservative.” 

In short, the way we usually deal with the problem of image description is to reduce the image to one or a few verbal labels (called “keywords” in software applications). In other words, we use natural languages (English, Spanish, Russian, etc.) as metalanguages for images. 

Interestingly, when modern theorists have tried to address the questions of visual signification, they often ended up performing similar reduction. This tendency in modern thought even received a special label – “verbocentrism.” For instance, while Roland Barthes stimulated the interest in visual semiotics with his pioneering articles published in the late 1950s and early 1960s, he simultaneously strongly questioned the possibility of an autonomous visual language. In "Rhetoric of the Image" [1] Barthes investigated significations conveyed by the objects and their arrangement and in fact disregarded any contribution to meaning by the picture itself. [2] In _Elements of Semiotics_ Barthes directly denied that a specifically visual language is possible: "It is true that objects, images and patterns of behavior can signify, and do so on a large scale, but never autonomously; every semiological system has its linguistic admixture." [3] And finally, in _The Fashion System_ Barthes explicitly analyzed not clothes but "written clothes." [4]

While semioticians, art historians, and art critics were going back and forth between stating, a la Barthes, that images do not have meanings without a linguistic support and, on the contrary, searching for a unique pictorial language, these subtle debates concerning what happens _inside a single image_ became now somewhat irrelevant. Computerization of media society introduced a new set of conceptual and practical challenges. Forget our inability to understand and describe how a single image may signify this or that – we now have to worry about more banal problems: how to organize, archive, filter and search billions and billions of images being stored on our laptops, network drives, memory cards, and so on. 

Of course, the questions of visual semiotics and hermeneutics still matter – but they need to be re-calibrated. _The cultural unit is no longer a single image, but a large scale structured or unstructured (such as the Web) image database_. This shift becomes clearly visible if we compare how visual epistemology works in _Blow-Up_ (Antonioni, 1966), _Blade Runner_ (Scott, 1982), and _Minority Report_ (Spielberg, 2002). The protagonists of the first two films are looking for truth within a single photographic image. Panning and zooming into this image reveals new information about reality: the killer hiding in the bushes, the identity of a replicant. In contrast, the protagonist of _Minority Report_ is looking for truth _outside a single image_: he works by matching the image of a future murder to numerous images of the city contained in a database to identify the location of the murder. The message is clear: by itself, a single image is useless – it only acquires significance in relation to a larger database. 

## Structure

How did computer scientists and the image industries respond to the dramatic increase in the amount of media data available? The response has been to gradually shift towards more structured ways to organize and describe this data. The industries are moving from HTML to XML to Semantic Web; from MPEG-1 to MPEG-4 to MPEG-7; from “flat” lens-based images to “layered” image composites to discrete 3D computer generated spaces. [5] In all these cases the shift is from a “low-level” metadata (the fonts used in a PDF file, the resolution and compression settings of a digital video file) to a “high-level” metadata that describes the structure of a media composition and ultimately its semantics. 

This gradual shift occurs in two complementary ways. One involves adding metadata to all the media data _already_ accumulated during the last hundred or fifty years of media society. Slides, photographs, recordings of television programs, typewritten records stored in numerous archives, state, university, and corporate libraries – all of these are being digitized and stored in computer databases with the metadata usually entered manually. (Often the reports on these efforts read as though they came from fiction by Borges or Lem: for instance, as I write this, hundreds of thousands of slides in an art collection at my university library are being digitized and logged; the recent report proudly announced that the speed of the process has reached 12,500 slides a month.)

The second is to assure that any media data generated _in the future_ – from a page of text on the Web to an image snapped by a cell phone camera to a TV show – will contain “high-level” metadata. This involves implementing various structured media formats such as already mentioned MPEG-4 and MPEG7 that I will focus on here as my examples. [6] The designers of MPEG-4 describe it as “the content representation standard for multimedia information search, filtering, management and processing.” MPEG-4 standard is based on the concept of a media composition that consists of a number of a media objects of various types, from video and audio to 3D models and facial expressions, and the information on how these objects are combined. MPEG-4 provides an abstract language to describe such a composition. 

MPEG-7 represents the next logical step in a gradual transition towards structured media data that comes with machine and code readable descriptions of its structure and contents. MPEG-7 is defined as “a standard for describing the multimedia content data that supports some degree of interpretation of the information’s meaning, which can be passed onto, or accessed by, a device or a computer code.” It is worth quoting the longer passage from the ISO/IEC document describing the standard as it explains well the importance of the last part of this definition:

> More and more audiovisual information is available from many sources around the world. The information may be represented in various forms of media, such as still pictures, graphics, 3D models, audio, speech, and video. Audiovisual information plays an important role in our society, be it recorded in such media as film or magnetic tape or originating, in real time, from some audio or visual sensors and be it analogue or, increasingly, digital. While audio and visual information used to be consumed directly by the human being, there is an increasing number of cases where the audiovisual information is created, exchanged, retrieved, and re-used by computational systems. This may be the case for such scenarios as image understanding (surveillance, intelligent vision, smart cameras, etc.) and media conversion (speech to text, picture to speech, speech to picture, etc.). Other scenarios are information retrieval (quickly and efficiently searching for various types of multimedia documents of interest to the user) and filtering in a stream of audiovisual content description (to receive only those multimedia data items which satisfy the user’s preferences)…

Audiovisual sources will play an increasingly pervasive role in our lives, and there will be a growing need to have these sources processed further. This makes it necessary to develop forms of audiovisual information representation that go beyond the simple waveform or sample-based, compression-based (such as MPEG-1 and MPEG-2) or even objects-based (such as MPEG-4) representations. Forms of representation that allow some degree of interpretation of the information’s meaning are necessary. These forms can be passed onto, or accessed by, a device or a computer code. 

MPEG-7 and similar schemes call for the inclusion of high-level metadata along with the media data that will enable computers to automatically process this data in a variety of data. But where would this metadata come from? I have briefly discussed above our overall tendency to describe images in terms of verbal labels. Can computers at least generate such labels automatically? Or maybe they would even finally allow us to describe image with more precision than natural languages? 

Computerization creates a promise that images that traditionally resisted the human attempts to adequately describe them will be finally conquered. After all, we now easily find out that a particular digital image contains so many pixels and so many colors; we can also generate a histogram (in Photoshop 7.0 it is a command found under “image” menu) that shows up how frequently each value appears in the image; etc. In short, by turning an image into a mathematical object digital computers gave us a _new metalanguage for images – numbers_. Building on such simple statistics, a computer can also tease out some indications of image structure and semantics – for instance, it can easily automatically find most edges in photograph and sometimes even segment it into parts corresponding to individual objects. 

Yet this promise may be only the illusion. The metadata provided by a image database software I use to organize my digital photos (iView MediaPro 1.1) tells me all kinds of technical details such as what aperture my digital camera used to snap this or that image – but nothing about the image content (in technical terms, this is typical “low-level” metadata). Visual search engines that can deal with the queries such as “find all images which have a picture of X” or “find all images similar in composition to this one” are still in their infancy. More generally, after almost fifty years of research, computer vision systems still can only recognize objects in photographs or video when they know what these objects would be beforehand – presented with an arbitrary image, they become “blind.” 

In short, while computerization made the image acquisition, storage, manipulation, and transmission much more efficient than before, it did not help us much in dealing with its side effects – how to more efficiently describe and access the vast quantities of digital images being generated by digital cameras and scanners, by the endless “digital archives” and “digital libraries” projects around the world, by the sensors and the museums. Although standards such as MPEG-7 would allow computers to automatically process visual data based on metadata, there still remains a basic and very time-consuming task: entering this metadata. In other words, computers can help us but only after we help them first by feeding image descriptions. 

## Scale

The constantly growing quantities of media data which are already available in numerous public and private various archives and databases or which can be generated on purpose (by storing all access logs of a Web site, by continuously recording the output of some sensors or video cameras, and so on) represents not only the problem to be solved (if it can be solved at all) but also a unique artistic opportunity. [7] This unique opportunity can be summed up as the shift from “sampling” to “complete recording.” 

One of the most basic principles of narrative arts is what in computer culture called “compression.” A drama, a novel, a film, a narrative painting or a photograph compresses weeks, years, decades, and even centuries of human existence into a number of essential scenes (or, in the case of narrative images, even a single scene). Non-essential is stripped away; essential is recorded. Why? Narrative arts have been always limited by the capacities of the receiver (i.e., a human being) and of storage media. Throughout history, the first capacity remained more or less the same: today the time we will devote to the reception of a single narrative may range from 15 seconds (a TV commercial) to two hours (a feature film) to forty hours (the average time spend by a player on a new computer game) to maybe hundreds of hours (following a TV series or soap opera). But the capacity of storage media recently changed dramatically. Instead of 10 minutes that can fit on a standard film roll or two hours that can fit on a DV tape, a digital server can hold practically unlimited amount of audio-visual recordings. The same applies for audio only, or for text.

In short, if both traditional narrative arts and modern media technologies are based on sampling reality, that is, representing/recording only small fragments of human experience, digital recording and storage technologies greatly expand how much can be represented/recorded. This applies to granularity of time, the granularity of visual experience, and also to what can be called “social granularity” (i.e., representation of one’s relationships with other human beings).

In regards to time, it is now possible to record, store and index years of digital video. By this I don't mean simply video libraries of stock footage or movies on demand systems – I am thinking of recording/representing the experiences of the individuals: for instance, the POV of single person as she goes through her life, the POVs of a number of people, etc. Although it presents combined experiences of many people rather than the detailed account of a single person’s life, the work by Spielberg’s Shoah Foundation is a relevant here as it shows what can be done with the new scale in video recording and indexing. The Shoah Foundation assembled and now makes accessible massive amount of video interviews with the Holocaust survivors: it would take one person forty years to watch all the video material, stored on Foundation’s computer servers. 

The examples of new finer visual granularity are provided by projects of Luc Courchesne and Jeffrey Shaw which both aim at continuous 360 o moving image recordings of visual reality. [8] One of Shaw’s custom systems which he called Panosurround Camera uses 21 DV cameras mounted on a sphere. The recordings are stitched together using custom software resulting in a 360o moving image with a resolution of 6000 x 4000 pixels. [9]

Finally, the example of new “social granularity” is provided by the popular computer game _The Sims_. This game that is better referred to as “social simulator” models ongoing relationship dynamics between a number of characters. Although the relationship model itself can hardly compete with the modeling of human psychology in modern narrative fiction, since _The Sims_ is not a static representation of selected moments in the characters’ lives but a dynamic simulation running in real time, we can at any time choose to follow any of the characters. While the rest of the characters are off-screen, they continue to “live” and change. In short, just as with the new granularity of time and the new granularity of visual experience, the social universe no longer needs to be sampled but can be modeled as _one continuum_. 

Together, these new abilities open up vast new vistas for aesthetic experimentation. They give us an unprecedented opportunity to address one of the key goals of art – a representation of reality and the human social and subjective experience of it – in new ways. In other words, what for the industry and computer science are difficult questions which need urgent solutions instead should be viewed as possibilities to play with. For instance, if it already possible to record and store practically unlimited number of still and moving images of one’s existence, what kind of interface can we use to organize and navigate these images? Or, given that we now can use database software to classify, link, and retrieve images and image sequences along with other media, how can a database structure be used to represent the life of a modern city, the history of a place, etc. In short, behind the problem of visual metadata that became more urgent because of the new scale of media data available there is an exciting promise – the promise to rethink the nature of representation. 

## Re-inventing media

Has the revolution in the scale of available storage been accompanied by the new ideas about how such media recording may function? It is not hard to see that most of the commercial and academic research into new structures and interfaces for organizing and accessing media data takes for granted commercially supported media formats and media conventions the way they exist today– photographs, consumer video, professional television programs, and the like. For example, when ISO/IEC document which specifies MPEG-7 standard talks about various types of media that can be supported by this standard, the list include not only such “general” types as video and 3D models, but also more particular ones such as “talking heads” (an obvious reference to television and industrial video convention). Given that most of this research is geared towards _existing_ applications by the industry, government agencies, and the military, this orientation towards media formats and conventions the way they exist today can be expected. However, some research projects are trying to re-invent media formats and their uses beyond what exists today. These projects come from different research paradigms that are not tied in to broadcasting and commercial video production industries the way MPEG community is. 

Since the beginning of the 1990s, working within the paradigms of Computer Augmented Reality, Ubiquitous Computing, and Software Agents at places such as MIT Media Lab and Xerox Park, computers scientists advanced the notion of a computer as an unobtrusive but omni-present device which automatically records and indexes all inter-personal communications and other user’s activities. A typical early scenario envisioned in the early 1990s involved microphones and video cameras situated in the business office which record everything taking place, along with indexing software which makes possible a quick search through the years’ worth of recordings. More recently the paradigm has expanded to include capturing and indexing all kinds of experiences of many people. For instance, a DARPA-sponsored research project at Carnegie-Mellon University called Experience-on-Demand which begun in 1997 aims to “developed tools, techniques, and systems that allow users to capture complete records of personal experience and to share them in collaborative settings.” [10] A report on the project from 2000 summarizes the new ideas being pursued as follows:

> Capture and abstraction of personal experience in audio and video as a form of personal memory.
> 
> Collaboration through shared composite views and information spanning location and time.
> 
> Synthesis of personal experience data across multiple sources.
> 
> Video and audio abstraction at variable information densities.
> 
> Information visualizations from temporal and spatial perspectives.
> 
> Visual and audio information filtering, “understanding,” and event alerting. [11]

Given that a regular email program already automatically keeps a copy of all send and received emails, and allows to sort and search through these emails, and that a typical mailing list archive Web site similarly allow to search through years of dialogs between many people, we can see that in the course of text communication this paradigm has already been realized. However, the difficulties of segmenting and indexing audio and visual media already discussed above are what delays realization of these ideas in practice in relation to other media. But the recording in mass itself is already can be easily achieved: all is takes is an inexpensive Web cam and a large hard drive. 

What is important in this paradigm –- and this applies for computer media in general – is that _storage media became active_. That is, the operations of searching, sorting, filtering, indexing, and classifying which before were the strict domain of human intelligence, become automated. A human viewer no longer needs to go through hundreds of hours of video surveillance to locate the part where something happens – a software program can do this automatically, and much more quickly. Similarly, a human listener no longer needs to go through years of audio recordings to locate the important conversation with a particular person – software can do this quickly. It can also locate all other conversations with the same person, or other conversations where his name was mentioned, and so on. 

To refer to the famous story by Borges, not only can computers make maps as big or larger than the territory, but they can also be used to make new types of maps impossible before. Instead of compressing reality to what the author considers the essential moments, very large chunks on everyday life can be recorded, and then put under the control of software. I imagine for instance a “novel” which consists of  complete email archives of thousands of characters, plus a special interface that the reader will use to interact with this information. Or a narrative “film” in which a computer programs assembles shot by shot in real time, pulling from the huge archive of surveillance video, old digitized films, Web cam transmissions, and other media sources. (From this perspective, Godard’s _History of Cinema_ represents an important step towards such database cinema. Godard treats the whole history of cinema as his source material, traversing this database back and forth, as though a virtual camera flying over a landscape made from old media.) 

As this essay has tried to suggest, “metadating the image” paradigm can be looked at as a problem to be solved or as a unique creative opportunity to pursue. This paradigm points toward four directions for artistic research – new structure / new interface / new image / new scale – which are interrelated. New _scale_ in the quantity of media available makes it difficult to use this data efficiently without automation. The automation – that is, processing of media by computers – requires new _structured_ media formats such as MPEG-7 that include _metadata describing the semantics_ of the data. The same change in scale calls for new _interfaces_ that would allow human users to navigate and access media collections efficiently. But since the interface can be approached not just as a tool but also as a cultural form – a mechanism to “interface reality” as well as to construct new reality – working on such new interfaces to media becomes an important task for media/software arts. (While new media artists have extensively critiqued existing software interfaces in general and developed many particular alternatives, surprisingly little energy has been spend so far thinking on how we can interface image and other media collections in new ways.) Finally, along with creating new structures and new interfaces to existing media forms, both researchers and artists are also working on new media forms including new forms of visual media – new _images_ which by themselves already “interface reality” in new ways. 

## References:

[1] Barthes, Ronald, trans. (1964). "Rhetoric of the Image." _Image -Music - Text_. Ed. Stephen Heath. New York: Hill and Wang, 1977. 32-51.

[2] Sonesson, Göran. _Pictorial Concepts. Inquiries into the Semiotic Heritage and its Relevance for the Analysis of the Visual World_. Lund, Sweden: Lund University Press, 1989. Page 127.

[3] Barthes, Ronald, trans. (1964). _Elements of Semiology_. New York: Hill and Wang, 1968. Page 10.

[4] Barthes, Ronald, trans. (1967) _The Fashion System_. New York: Hill and Wang, 1983.

[5] For a detailed discussion of compositing in terms of this shift, see the section “From Image Streams to Modular Media” in my _The Language of New Media_ (MIT Press, 2001).

[6] MPEG-4 is an ISO/IEC standard developed by MPEG (Moving Picture Experts Group), the committee that also developed the successful standards known as MPEG-1 (1992) and MPEG-2 (1994). Version 1 of MPEG-4 was approved in1998, and version 2 in 1999. All quotations in this section are from [http://mpeg.telecomitalialab.com/standards/](http://mpeg.telecomitalialab.com/standards/).

[7] This and the following section use the material from my article “Reality Media” published as “Old Media as New Media: Cinema” in _The New Media Book_, edited by Dan Harries (London: BFI Publishing, 2002).

[8] For Courchesne’s Panoscope project, see [http://www.din.umontreal.ca/courchesne/](http://www.din.umontreal.ca/courchesne/); For Jeffrey Shaw’s projects, see [http://www.jeffrey-shaw.net](http://www.vision-ruhr.de/artists/shaw/). Both discuss their projects in relation to previous strategies of “experience representation” in panorama, painting, and cinema in _New Screen Media: Cinema/Art/Narrative_, edited by Martin Rieser and Andrea Zapp (London: BFI and Karlsruhe: ZKM, 2001).

[9] Private communication between Shaw and the author, July 4, 2002. 

[10] [http://www.informedia.cs.cmu.edu/](http://www.informedia.cs.cmu.edu/). For more information on the project, see Howard D. Wactlar et al., “Experience-on-Demand: Capturing, Integrating, and Communicating Experiences Across People, Time, and Space,” [http://www.informedia.cs.cmu.edu/eod/](http://www.informedia.cs.cmu.edu/eod/); see also Howard D. Wactlar et al., “Informedia Video Information Summarization and Demonstration Testbed Project Description,” [http://www.informedia.cs.cmu.edu/arda-vace/](http://www.informedia.cs.cmu.edu/arda-vace/). Both of these research projects were conducted at Carnegie-Mellon University; dozens of similar projects are going on at universities and industry research labs around the world.

[11] [http://www.informedia.cs.cmu.edu/eod/EODforWeb/eodquad00d.pdf](http://www.informedia.cs.cmu.edu/eod/EODforWeb/eodquad00d.pdf).

---

# Data Visualization as New Abstraction and Anti-Sublime

_author: Lev Manovich_
_year: 2002_

## Visualization and Mapping

Along with a Graphical User Interface, a database, navigable space, and simulation, _dynamic data visualization_ is one of the genuinely new cultural forms enabled by computing. [1] Of course the fans of Edward Tufte will recall that it is possible can find examples of graphical representation of quantitative data already in the eighteenth century, but the use of computer medium turns such representations from the exception into the norm. It also makes possible a variety of new visualization techniques and uses for visualization. With computers we can visualize much larger data sets; to create visualizations which are dynamic (i.e., animated and interactive); to feed in real-time data; to base graphical representations of data on its mathematical analysis using variety of methods from classical statistics to data mining; to map one type of representation into another (images into sounds, sounds into 3D spaces, etc.).

Since Descartes introduced the system for quantifying space in the seventeenth century, graphical representation of functions has been the cornerstone of modern mathematics (if you need to remember how it works and you have a Mac, start Graphing Calculator and run the demo). In the last few decades, the use of computers for visualization enabled development of a number of new scientific paradigms such as chaos and complexity theories, and artificial life. It also forms the basis of a new field of scientific visualization. Modern medicine relies on visualization of body and its functioning; modern biology similarly is dependent on visualization of DNA and proteins. But while contemporary pure and applied sciences, from mathematics and physics to biology and medicine heavily relies on data visualization, in the cultural sphere visualization until recently has been used on a much more limited scale, being confined to 2D graphs and charts in the financial section of a newspaper, or on occasional 3D visualization on television to illustrate the trajectory of a space station or of a missile.

I will use the term _visualization_ for the situations when quantified data which by itself is _not visual_ – the output of meteorological sensors, stock market behaviors, the set of addresses describing the trajectory of a message through a computer network, and so on – is transformed into a visual representation. [2]

The concept of _mapping_ is closely related to visualization but it makes sense to keep it separate. By representing all data using the same numerical code, computers make it easy to map one representation into another: grayscale image into 3D surface, a sound wave into an image (think of visualizers in music players such as iTunes), and so on. Visualization then can be thought of as a particular subset of mapping in which a data set is mapped into an image. 

Human culture practically never uses more than four dimensions in its representations because we humans live in 4D space. Therefore, we have difficulty imagining data in more than these four dimensions: three dimensions of space (X, Y, Z) and time. However, more often than not, the data sets we want to represent have more than four dimensions. In such situations designers and their clients have to choose which dimensions to use and which to omit, and how to map the selected dimensions.

This is the new _politics of mapping_ of computer culture. Who has the power to decide what kind of mapping to use, what dimensions are selected; what kind of interface is provided for the user – these new questions about data mapping are now as important as more traditional questions about the politics of media representation by now well rehearsed in cultural criticism (who is represented and how, who is omitted). More precisely, these new questions around the politics of _quantified data representation_ run parallel to the questions about the content of the _iconic and narrative media representations_. In the later case we usually deal with the visual images of people, countries, and ethnicities, in the former case, the images are abstract 3D animations, 3D charts, graphs, and other types of visual representation used for quantified data. 

## Data Modernism

Mapping one data set into another, or one media into another, is one of the most common operations in computer culture, and it is also common in new media art. [3] Probably the earliest mapping project which received lots of attention and which lies at the intersection of science and art (because it seems to function well in both contexts) was Natalie Jeremijenko’s “live wire.” Working in Xerox PARC in the early 1990s, Jeremijenko created a functional wire sculpture which reacts in real time to network behavior: more traffic causes the wire to vibrate more strongly. In the last few years, data mapping has emerged as one of the most important and interesting areas in new media art, attracting the energy of some of the best people in the field. It is not accidental that out of 10 Net Art projects included in 2002 Whitney Biennale, about a half presented different kinds of mapping: the visual map of the space of Internet addresses (Jevbratt), 3D navigable model of Earth presenting a range of information about the Earth in multiple layers (Klima), another 3D model illustrating the algorithm used for genome searches (Fry); the diagrams of corporate power relationships in the United States (John On & Futurefarmers). [4]

In order to ground my general observations about data mapping in art in concrete material, I would like now to briefly discuss a few projects by some of the best artists dealing with data visualization. One of my favorites is John Simon (New York). His work is unique for a number of reasons. First of all, he makes explicit connections in his pieces between the new ideas of new media and various traditions, movements and figures of modern art, in particular Mondrian, Klee, and Sol LeWitt. Given that art world and culture at large are still largely treating new media as a phenomena in itself which has no connections to the past, Simon’s explicit and systematic explorations of conceptual linkages between new media and modern art is very important. In addition, while new media art field has been rapidly growing in size over the last years, and while artists in all disciplines are now routinely computer as a tool in their work, there are still literally only a few artists out there who focus on one of the most fundamental and radical concepts associated with digital computers – that of computation itself (rather than interactivity, network, or multimedia). Simon systematically researches how real-time computation can be used to create engaging artworks which are both conceptual and strongly material, offering the viewer rich visual experiences. In his earlier work online piece _Every Icon_ (1998) and his wall-mounted pieces included in _Bitstreams_ exhibition at the Whitney Museum (2001) he uses real-time computation to create artworks that have a starting point in time but no end point; as the time progresses, they constantly change. While we can find certain precedents for such artworks in modern art (for instance, kinetic art, early computer art of the 1960s, and conceptual art), Simon pursues a unique strategy of his own: he uses artificial life, cellular automata and other computational techniques to create complex and nuanced images which combine figurative and abstract and which explicitly insert themselves within the history of modernist visual research.

If Simon’s images are the result of real-time computation internal to a work itself, those of Lisa Jevbratt (Santa Barbara) often are driven by the Internet data. Jevbratt received her training at CADRE. [5] This program was created Joel Slayton at San Jose State University who was able to strategically exploit its unique location right in the middle of Silicon Valley to encourage creation of computer artworks which critically engage with commercial software being created in Silicon Valley for the rest of the world: Internet browsers, search engines, databases, data visualization tools, etc. With his ex-students, Slayton formed a “company” called C5 to further develop critical software tools and environments. Jevbratt is the most well-known artist to emerge from the C5 group. While “software art” has emerged as a new separate category within new media field only about two years ago, Jevbratt, along with other members of CADRE community, have been working in this category for much longer. In their complexity and functionality, many software projects created at C5 match commercial software, which is still not the case for most new media artists. 

In her earlier well-known project _1:1_ Jevbratt created a dynamic database containing IP addresses for all the hosts on the World Wide Web, along with five different ways to visualize this information. [6] As the project description by Jevratt points out:

> When navigating the web through the database, one experiences a very different web than when navigating it with the "road maps" provided by search engines and portals. Instead of advertisements, pornography, and pictures of people's pets, this web is an abundance of non-accessible information, undeveloped sites, and cryptic messages intended for someone else…The interfaces/visualizations are not maps of the web but are, in some sense, the web. They are super-realistic and yet function in ways images could not function in any other environment or time. They are a new kind of image of the web and they are a new kind of image.

In a 2001 project _Mapping the Web Infome_ Jevbratt continues to work with databases, data gathering and data visualization tools; and she again focuses on the Web as the most interesting data depository corpus available today. [7] For this project Jevbratt wrote special software that enables easy menu-based creation of Web crawlers and visualization of the collected data (crawler is a computer programs which automatically moves from a Web site to a Web site collecting data from them). She then invited a number of artists to use this software to create their own crawlers and also to visualize the collected data in different ways. This project exemplifies a new functioning of an artist as a designer of software environments that are then made available to others.

Alex Gallaway/RSG collective uses the similar approach in his network visualization project _Carnivore_ (2002). Like Jevbratt, RSG collective created a software system that he opened up to other artists to use. Physically _Carnivore_ is styled like a moth between a non-distinct box for telephone surveillance such the ones used in GDR, and a modernist sculpture; connected to some point in the network, it intercepts all data going through it. This by itself does not make it art, since a number of commercial software packages perform similar functions. For instance, Etherpeek 4.1 is a LAN analyzer that captures packets from attached Ethernet or AirPort networks and uses decodes to break these packets into their component fields. It can decode FTP, HTTP, POP, IMAP, Telnet, Napster, and hundreds of other network protocols. It performs real-time statistical analysis of captured packets, and it can reconstruct complete e-mail messages from the captured packets. As it is often the case with the artist software (software by CADRE community being an exception), _Carnivore_ only offers a small fraction of the capabilities of its commercial counterparts such as Etherpeek. What it does offer instead is the open architecture that allows other artists to write their own visualization clients that display the intercepted data in a variety of different ways.

Some of the most talented artists working with the Net have written visualization clients for _Carnivore_. The result is a diverse and rich menu of forms, all driven by the network data. Just as in the first decades of the twentieth century modernist artists of the mapped the _visual chaos_ of the metropolitan experience into simple geometric images, data visualization artists transform the _informational chaos_ of data packets moving through the network into clear and orderly forms. And if modernism reduced the particular to its Platonic schemas (think of Mondrian, for instance, systematically abstracting the image of a tree in a series of paintings), data visualization is engaged in a similar reduction as it allows us to see patterns and structures behind the vast and seemingly random data sets. Thus, it is possible to think of data visualization as a new abstraction. But if modernist abstraction was in some sense anti-visual – reducing the diversity of familiar everyday visual experience to highly minimal and repetitive structures (again, Mondrian’s art provides a good example) – data visualization often employs the opposite strategy: the same data set drives endless variations of images (think of various visualization plug-ins available for music players such as iTunes.) Thus, data visualization moves from the concrete to the abstract, and then again to the concrete. The quantitative data is reduced to its patterns and structures that are then exploded into many rich and concrete visual images. 

## Meaningful Beauty: Data Mapping as Anti-sublime

Having looked at the particular examples of data visualization art, we are now in the position to make a few observations and pose a few questions. I often find myself moved by these projects emotionally. Why? Is it because they carry the promise of rendering the phenomena that are beyond the scale of human senses into something that is within our reach, something visible and tangible? This promise makes data mapping into the exact opposite of the Romantic art concerned with the sublime. In contrast, data visualization art is concerned with the _anti-sublime_. If Romantic artists thought of certain phenomena and effects as un-representable, as something which goes beyond the limits of human senses and reason, data visualization artists aim at precisely the opposite: to map such phenomena into a representation whose scale is comparable to the scales of human perception and cognition. For instance, Jebratt’s _1:1_ reduces the cyberspace – usually imagined as vast and maybe even infinite – to a single image that fits within the browser frame. Similarly, the graphical clients for _Carnivore_ transform another invisible and “messy” phenomena – the flow of data packets through the network that belong to different messages and files – into ordered and harmonious geometric images. The macro and the micro, the infinite and the endless are mapped into manageable visual objects that fit within a single browser frame.

The desire to take what is normally falls outside of the scale of human senses and to make visible and manageable aligns data visualization art with modern science. Its subject matter, i.e., data, puts it within the paradigm of modern art. In the beginning of the twentieth century art largely abandoned one of its key – if not the key – function – portraying the human being. Instead, most artists turned to other subjects, such as abstraction, industrial objects and materials (Duchamp, minimalists), media images (pop art), the figure of artist herself or himself (performance and video art) – and now data. Of course, it can be argued that data art represents the human being indirectly by visualizing her or his activities (typically the movements through the Net). Here again I would like to single out the works of Simon who makes explicit references to the tradition of modernist abstraction (one of his works, for instance, refers to Piet Mondrian’s _Broadway Boogie-Woogie_, 1942-43) – and also includes figurative elements in his compositions, such as outlines of Manhattan Midtown buildings and street traffic. In fact, Simon refers to this piece as a view from his studio window – a type of image that has a well-known history in modern art (for instance, views of Paris by the impressionists).

Another important question worth posing is about arbitrary versus motivated choices in mapping. Since computers allow us to easily map any data set into another set, I often wonder why the artist chose this or that mapping when endless other choices were also possible. Even the very best works which use mapping suffer from this fundamental problem. This is the “dark side” of mapping and of computer media in general – its built-in existential angst. By allowing us to map anything into anything else, to construct infinite number of different interfaces to a media object, to follow infinite trajectories through the object, and so on, computer media simultaneously makes all these choices appear arbitrary – unless the artist uses special strategies to motivate her or his choices.

Let’s look at one example of this problem. One of the most outstanding architectural buildings of the last decade is Jewish Museum Berlin by Daniel Libeskind. The architect put together a map that showed the addresses of Jews who were living in the neighborhood of the museum site before World War II. He then connected different points on the map together and projected the resulting net onto the surfaces of the building. The intersections of the net projection and the design became multiple irregular windows. Cutting through the walls and the ceilings at different angles, the windows point to many visual references: narrow eyepiece of a tank; windows of a Medieval cathedral; exploded forms of the cubist/abstract/suprematist paintings of the 1910s-1920s. Just as in the case of Janet Cardiff's audio walks, here the virtual becomes a powerful force that re-shapes the physical. In Jewish Museum, the past literally cuts into the present. Rather than something ephemeral, here data space is materialized, becoming a sort of monumental sculpture.

But there was one problem which I kept thinking about when I visited still empty museum building in 1999 – the problem of motivation. On the one hand, Libeskind’s procedure to find the addresses, make a map and connect all the lines appears very rational, almost the work of scientist. On the other hand, as far as I know, he does not tell us anything about why he projected the net in this way as opposed to any other way. So, I find something contradictory in fact that all painstakingly collected and organized data at the end is arbitrary "thrown" over the shapes of the building. I think this example illustrates well the basic problem of the whole mapping paradigm. Since usually there are endless ways to map one data set onto another, the particular mapping chosen by the artist often is not motivated, and as a result the work feels arbitrary. We are always told that in good art "form and content form a single whole" and that "content motivates form." Maybe in a "good" work of data art the mapping used have to somehow relate to the content and context of data - although I am not sure how this would work in general.

One way to deal with this problem of motivation is to not to hide but to foreground the arbitrary nature of the chosen mapping. Rather than try to always being rational, data art can instead make the method out of irrationality.[8] This of course was the key strategy of the twentieth century Surrealists. In the 1960s the late Surrealists – the Situationists – developed a number of methods for their “the dérive” (the drift). The goal of “the dérive” was a kind of spatial “ostranenie” (estrangement): to let the city dweller experience the city in a new way and thus politicize her or his perception of the habitat. One of these methods was to navigate through Paris using a map of London. This is the kind of poetry and conceptual elegance I find missing from mapping projects in new media art. Most often these projects are driven by the rational impulse to make sense out of our complex world, the world there many process and forces are invisible and are out of our reach. The typical strategy then is to take some data set – Internet traffic, market indicators, amazon.com book recommendation, or weather – and map it in some way. This strategy echoes not the aesthetics of the Surrealists but a rather different paradigm of the 1920s left avant-garde. The similar impulse to "read off" underlying social relations from the visible reality animated many left artists in the 1920s, including the main hero of my _The Language of New Media_ – Dziga Vertov. Vertov' 1929 film _A Man with a Movie Camera_ is brave attempt at visual epistemology – to reinterpret the often banal and seemingly insignificant images of everyday life as the result of the struggle between old and the new.

Important as the data mapping new media projects are, they miss something else. While modern art tried to play the role of "data-epistemology," thus entering in completion with science and mass media to explain to us the patterns behind all the data surrounding us, it also always played a more unique role: to show us other realities embedded in our own, to show us the ambiguity always present in our perception and experience, to show us what we normally don't notice or don't pay attention to. Traditional "representational” forms - literature, painting, photography, and cinema – played this role very well. For me, the real challenge of data art is _not_ about how to map some abstract and impersonal data into something meaningful and beautiful – economists, graphic designers, and scientists are already doing this quite well. The more interesting and at the end maybe more important challenge is how to represent the personal subjective experience of a person living in a data society. If daily interaction with volumes of data and numerous messages is part of our new “data-subjectivity,” how can we represent this experience in new ways? How new media can represent the ambiguity, the otherness, the multi-dimensionality of our experience, going beyond already familiar and “normalized” modernist techniques of montage, surrealism, absurd, etc.? In short, rather than trying hard to pursue the anti-sublime ideal, data visualization artists should also not forget that art has the unique license to portray human subjectivity – including its fundamental new dimension of being “immersed in data.” 

## References:

[1] Graphical User Interface itself includes a set of techniques: interactive control, direct manipulation, multiple views, and others. Used not just for data access or computer control but also for media access and manipulation, each of these techniques itself opens up a new paradigm in cultural representation. For the discussion of a database and navigable space, see my _The Language of New Media_ (MIT Press, 2001). Simulation (as in _The Sims_) will be discussed in my next book _Info-Aesthetics_.

[2] Of course, if we also think of all 3D computer animation as a type of data visualization in a different sense – after all, any 3D representation is constructed from a data set describing the polygons of objects in the scene or from mathematical functions describing the surfaces – the role played by data visualization becomes significantly larger. After all, 3D animation is routinely used in industry, science and in popular culture. But I don’t think we should accept such an argument since 3D computer images closely follow traditional Western perspectival techniques of space representation, and therefore from the point of view of their visual appearance do not constitute a new phenomenon.

[3] Most mappings in both science and art go from non-visual media to visual media. Is it possible to create mappings that will go into the opposite direction?

[4] [http://artport.whitney.org/exhibitions/index.shtml](http://artport.whitney.org/exhibitions/index.shtml).

[5] [http://cadre.sjsu.edu](http://cadre.sjsu.edu/)

[6] [http://www.c5corp.com/1to1/](http://www.c5corp.com/1to1/)

[7] [http://dma.sjsu.edu/jevbratt/lifelike/](http://dma.sjsu.edu/jevbratt/lifelike/)

[8] Read “against the grain,” any descriptive or mapping system which consists of  quantitative data – a telephone directory, the trace route of a mail message, etc. - acquires both grotesque and poetic qualities. Conceptual artists explored this well, and data visualization artists may learn from these explorations.

---

# Don’t Call it Art: Ars Electronica 2003

_author: Lev Manovich_
_year: 2003_

In choosing CODE as its theme, Ars Electronica 2003 has capitalized on (some would say: appropriated) developments within the field of new media art that already have been going on for a few years. As Andreas Broeckmann (the Artistic Director of Transmediale festival, Berlin) reminded the audience in his concluding presentation during the Ars Electronica symposium, already 5 years ago New York based artist John Simon suggested that it would be useful to treat software-based art as a separate category. Consequently, since 2001 the Transmediale festival competition has included “artistic software” as one of its categories, and devoted a significant space to it in the festival’s symposiums. Another important platform for presenting software art has become the Whitney Museum in New York and its Artport web site where curator Cristiane Paul has organized a number of important exhibitions during the last few years. As of 2002, software art became the subject of a new, smaller-scale but very significant festival, README. The 2002 README took place in Moscow, while 2003’s was in Helsinki. Finally, in January 2003, festival organizers (Alexei Shulgin, Olga Goriunova, Alex McLean, and others) established a comprehensive web portal for software art RUNME.ORG. Containing at present more than 60 categories, RUNME is an evolving conceptual map of what I see as the larger meaning of the term “software art”: the significant, diverse, and real creative activities at the intersections between culture, art, and software.

Given that Ars Electronica has much more significant resources than probably any other festival of media or new media art in the world, one would expect that it would correspondingly take the discussions of software art and culture to a new level. Unfortunately, my impression of the festival (note that although I spent five full days at the festival, I still could not make it to every single panel and performance) is that instead it narrowed the focus of these discussions. Intentionally or not, software art became equated with algorithmically generated media: still and moving images and sound. To quote the definition of “art created out of code” from Ars Electronica program, it is “a generative artform that has been derived and developed from computational processes” (the statement by the directors of Ars Electronica, festival program, p. 2). More than once I had to check my program to make sure that I was indeed at Ars Electronica 2003 rather than SIGGRAPH – or an earlier Ars Electronica edition from the 1980s when computer imaging indeed represented the key creative area of digital arts field. In a strange loop, Ars Electronica festival came full circle to include its own past. In the mid-1990s, recognizing that production of computer images was no longer confined to the digital “avant-garde” but became the norm in culture at large, Ars Electronica dropped this category, replacing it with “Net Vision / Net Excellence.” So why in 2003, would the Ars Electronica exhibition and symposium once again devote such significant space to algorithmically generated visuals and sound? As even a quick look through README depository demonstrates, “software art” constitutes an extremely diverse set of contexts, interests, and strategies, with algorithmic media generation being only one direction among many others.

It is true that the Ars Electronica 2003 symposium has made important gestures towards addressing larger social and political issues, since along with the discussions of code as software and the corresponding area of “software art,” it also included discussions of “law code” and biological code.” And the Festival statements describing these topics were right on target, for instance: “software sets the standards and norms, and determines the rules by which we communicate in a networked world, do business, and gather and disseminate information” (Gerfried Stocker, statement in the Festival catalog). Yet by having only a few speakers to cover each of these areas, the symposium could not explore these important areas in much depth. I see this in general as simultaneously both positive and negative feature of many European media festivals. On the one hand it is very stimulating and entertaining to attend a festival which includes art exhibitions, film screenings, music performances, intellectual discussions, and late-night parties – these kinds of hybrid events are practically non-existent in North America where one goes a museum to see a thematic exhibition, to a university to attend a conference on intellectual topics, to a club to dance, and so on. On the other hand, just as a typical software program which tries to cover a number of different areas rarely has as much depth as the programs dedicated to these separate areas, often after attending a European media festival I have a feeling that the broadness of coverage prevented analysis of anything with much depth.

This definitely was my feeling at the end of this year’s Ars Electronica – in spite of the brilliance of individual participants such as media theory veteran - Friedrich Kittler and emerging star Florian Cramer; virtuoso graphics programmers / designers Lia, Ben Fry, Casey Reas, Schoenerwissen, and others; the faculty and the students from the Department of Media and Art at University of Art, Media, and Design in Zurich who put on the show of student projects which I found to be the best exhibition at this year festival; Giaco Schiesser, Christian Hubler, Christiane Paul, Andreas Broeckmann (and I am sure many others speaking in the sessions I missed); last but not least, the musicians who put on what for me and many others I talked to was the highlight of the festival – a five hour marathon concert entitled Principles of Indeterminism: an Evening from Score to Code which presented a number of key works in the history of electronic music with a focus on Iannis Xenakis.

While CODE exhibition and Electrolobby staged at Brucknerhaus presented a lively and diverse set of artistic practice in and around the theme of software art, I felt that the larger questions about the role of software in cultural production were not taken up. Yet outside of Ars Electronica festival these questions are being already actively discussed. For instance, only during 2003 summer and fall exhibition seasons one could see a number of large museum exhibitions which go much further in addressing this area. I am thinking, for instance, of the presentations of the architects whose practice is closely linked with software: solo exhibitions of Zaha Hadid (MAK, Vienna), Greg Lynn (also at MAK), Asymptote (NAI, Rotterdam). In another example, the works of a number of the software artists who were shown at Ars Electronica exhibition were also included in a large exhibition “Abstraction Now” currently on display in Vienna’s Künstlerhaus. By combining these software-driven works with the works of many other contemporary artists who do not use computers directly but instead practice what can be called “conceptual software” approach – that is, they base their output on particular conceptual procedures (sometimes closely approximating algorithms) - this show by two young curators Norbert Pfaffenbichler and Sandro Droschl (both ex-students of Peter Weibel) successfully achieved precisely the effect which was missing from Ars Electronica’s CODE exhibition. That is, “Abstraction Now” inserted software art within the larger fields of contemporary cultural production and thought, giving its visitors enough intelligently and provocatively organized material to reflect about the relationships between modern and contemporary art, media, visual culture, and software.

If I extend the context beyond the current exhibition season, Peter Weibel’s curatorial practice after he left Ars Electronica in 1999 to become the director of ZKM exemplifies one effective strategy for new media field’s survival. After his arrival, ZKM mounted a number of large-scale shows devoted to large questions of cultural history (CTRL[Space], ICONOCLASH, and others); while new media was an essential components of these shows, it never provided the whole context. The recent show “Future Cinema” which more centrally focused on new media pursued another successful strategy: similar to “Abstraction Now,” it presented a larger context by including a range of artists, from hard-core “new media artists” (Masaki Fujihata, Luc Courchesne) to art world “media artists” (Eija-Liisa Ahtila, Isaac Julien, Gary Hill) and older experimental filmmakers (Michael Snow, Chris Marker).

In the 1980s and first part of the 1990s when few outside of digital arts field used computers, the existence of the festival devoted to this field was very important. In the last few years, however, the situation changed dramatically. If pretty much everybody in the cultural field now uses digital media, computer networks, and the like, what exactly then do we see in Ars Electronica exhibitions during the last few years? What exactly is the phenomenon of “software art” - or larger phenomena of “digital art,” “new media art”, “cyberart,” etc.? The key participants of Ars Electronica 2003 themselves take different positions here: Casey Reas told me (if I remember correctly) that he and Ben Fry think of themselves as designers while Golan Levin thinks of himself as artist (all three are ex-students of John Maeda from MIT Media Lab who himself acts in different roles of a designer, software designer, and artist). While this review does not give me space for a comprehensive analysis, lets briefly review the possible answers to these questions.

For instance, can “digital art” be considered a branch of contemporary art? Since the end of 1960s, modern art has become fundamentally a conceptual activity. That is, beyond conceptualism proper, art came to focus not on medium or techniques but on concepts. How these concepts are executed is either secondary, or simply irrelevant. When an artist asks gallery visitors to complete a questionnaire and then compiles and exhibits statistics (Hans Haacke), takes up a job as a maid in a hotel and documents hotel rooms (Sophie Calle), cooks a meal for gallery visitors (Rirkrit Tiravanija), presents a found video tape shot by Russian troops in Chechnya (Sergei Bugaev, a.k.a. Africa), the traditional questions of artistic techniques, skills, and media become largely unimportant. As the well-known Russian artist Africa has put it: “the role of modern art is not to uncover a secret but instead to steal it.” Put differently, more and more contemporary artists act as a kind of journalists, researching and presenting various evidence through different media including text, still photographs, video, etc. What matters is the initial idea, a strategy, a procedure, rather than the details of how the findings or documentation are presented.

Of course, not all artists today act as journalists – I am simply taking this as the most clear example of the new role of an artist, in contrast to the older roles of artist as craftsman, as the creator of symbols, allegories, and “representations,” etc. In short, a typical contemporary artist who was educated in the last two decades is no longer making paintings, or photographs, or video – instead, s/he is making “projects.” This term appropriately emphasizes that artistic practice has become about organizing agents and forces around a particular idea, goal, or procedure. It is no longer about a single person crafting unique objects in a particular media.

(Of course, contemporary art is also characterized by a fundamental paradox – what collectors collect are exactly such old-fashioned objects rather than “projects.” Indeed, artists selling their works for highest prices in contemporary art market usually do produce such objects. This paradox is partially resolved if you consider the fact that these artists always employ a staff of assistants, technicians, etc. – i.e., like everybody else they are making “projects” – only the collective nature of production in this case if concealed in favor of individual artists’ “brand names.”)

Although its highly social nature (people exchanging code, collaborating on projects together, treating audiences as equal participants, etc.) aligns “software art” with contemporary art, since it is firmly focused on its medium rather than medium-free concepts, “software art” cannot be considered “contemporary art.” This is one reason why it is indeed excluded by the art world. The logics of “contemporary art” and “digital art” are fundamentally at odds which each other, and I don’t see any easy way around this. So, for instance, when Ars Electronica program asks “In which direction is artists’ work with the new instruments like algorithms and dynamic systems transforming the process of artistic creativity?” (festival program, p. 9), the very assumptions behind such a question put it outside of the paradigm of contemporary art.

If “software art” does not belong to the cultural field of ”contemporary art,” does it perhaps follows the earlier logic of artistic modernism? In other words, are we dealing here with a kind of “Modernism ver. 2,” since “software” and “digital artists” clearly spend lots of energy investigating new possibilities offered by digital computers and computer-based networks for representation and social communication and cooperation? This interpretation does not work either. Contrary to what you might have learned in art school, modernist artists were not formalists – at least in first half of a twentieth century. The incredible and unprecedented energy which went during these decades into inventing fundamentally new languages of visual communication, new forms, new artistic concepts of space and time, and so on, was rarely driven by purely formal concerns – i.e., investigating the specificity of a particular medium and purifying it from other influences to create works which did not refer to anything outside themselves (Greenberg). Instead, artists’ inventions were driven by multitude of larger questions and goals – representing absolute values and spiritual life; creating new visual language for a working class; representing the dynamism of contemporary city and the experience of war; representing the concepts of Einstein’s relativity theory; translating principles of engineering into visual communication; and so on. In contrast, today’s “digital artists” are typically proper formalists, with their discussions firmly centered on their particular medium – i.e., software. In short, they are not “new modernists,” because modernists were always committed to larger political, social, and spiritual values.

(Of course, many European modernists were also quick to “sell” themselves, translating their achievements into simply a new style. By mid 1920s, Lissitzky, Rodchenko, Moholy-Nagy and others often took on commercial jobs for commercial clients who were happy to have ads and graphic identity done in new style. In short, within a few years modern art also became modern design. Yet this does not negate my argument because at least on the level of theory, the modernist artists were always advocating larger ideas and values, even when working for commercial or state clients.)

If “digital art” does not qualify as “contemporary art” or “modern art,” does it then belong to “design”? Although some designers today indeed focus their energy on systematically investigating new representational and communication possibilities of digital media – John Maeda and his students being a perfect example – these designers represent a very small percentage of the overall design field. A typical designer simply takes the client’s brief and does something using already established conventions, techniques, and iconography. Thus, to identify “digital art” with design is to wrongly assume that contemporary design field as a whole is devoted to “basic research” rather than “applications.”

If there is one social field whose logic is similar to the logic of “digital art,” or “new media art” in general, in my view this field is not contemporary art, modern art, or design, but computer science. Like digital artists, computer scientists working with computer graphics, multimedia, networking, interfaces, and other “cultural” parts of computer science (as opposed to, say, chip design or computer architecture) are true formalists – that is, they are investigating new possibilities for representation, social and human-machine communication. Like software artists, these computer scientists routinely translate their ideas into various working demos and prototypes which often do not have life outside of their own professional domain: academic papers, conferences, demo presentations. (However, in contrast to the works of digital artists, some of these ideas do enter into mainstream computing and thus have huge impact on culture: think of GUI, hyperlinking, or World Wide Web.)

At the end of the day, if new media artists want their efforts to have a significant impact on cultural evolution, they indeed to generate not only brilliant images or sounds but more importantly, solid discourse. That is, they need to situate their works in relation to ideas that are not only about the techniques of making these works. The reason that we continue discussing Duchamp’s urinal or as Paik’s early TV sculptures as though these works were created today has nothing to do with the artistic and technological skills of these artists – it has to do with their concepts, i.e., the discursive statements these artists were making through their objects. In short, if modern and contemporary art is a particular discourse (or a game) where the statements (or moves) are made via particular kind of material objects identified as “artworks,” digital artists need to treat their works as such statements if they are to enter the larger cultural conversation. This means referring to the historical and presently circulating statements in the fields of contemporary art and/or contemporary culture at large. And while Ars Electronica 2003 festival organizers seem to understand this – “A media art that is coherently and consistently conceived will never be limited to the artistic use of technical media” (Gerfried Stocker, statement in the 2003 Festival Program, p. 7) – the festival itself, in my view, did not encourage the real dialogue between new media art and contemporary art, simply because it did not include anybody from the latter field.

If brilliant computer images are not supported by equally brilliant cultural ideas, their life span is very limited. Either they are destined to be simply forgotten, the way it happened with the great deal of media art – simply because the software and the hardware they required to run on no longer exists. Alternatively – and it hard to say which fate is worse – they would end up as buttons or plug-ins in mainstream graphics and multimedia software. This the ever-present danger of anybody working on the cutting edge of technology – if the results do not become part of other cultural conversations, they inevitably stay within the field of technology itself: either simply erased by new generations of software and hardware or incorporated within it as elementary building blocks.

In saying all this I don’t want to imply that contemporary art is somehow “better” than digital art. Every culture has a need for different discourses, statements, and practices; historically they are distributed across - -varied cultural fields.

Today, for instance, you will find that the development of new styles is mostly done with design; the tradition of portraiture (representation of a particular human being) is primary carried on in commercial photography; literature and cinema have taken on the role representing human existence via narratives, which in classical period was the function of theatre; and so on. Some fields within computer science, the research-oriented wing of designers, and digital art are playing their own unique and extremely important role: devising new representational and communication methods and techniques. As for contemporary art, it does not actually have a well-defined role within this cultural division of labor. Rather, it is a field there one can make statements which are not possible to make in all any other field, be it science, media, etc. These statements are unique in terms of their subject matter, how they are arrived at, and how they are presented. Not every contemporary artist fully takes advantage of this unique situation, but the best do.

While the fields of contemporary art and digital art play very different roles and both are culturally important for different reasons, they are also are both limited in a complementary way. If the two fields can learn from each other, the results can be very exciting. Contemporary art is too historical: a typical statement in this field either by artist or by critic inevitably refers to another statement or statements made during the last few decades in the field. Digital art has the opposite illness: it has no memory of its own history, so it can benefit from remembering its past more systematically.

To conclude: this brief analysis was not meant as attack on the whole fields of “digital art” or “software art.” Its best practitioners are concerned with larger social and political questions. Moreover, the best works of digital art are able to find just the right balance between the strong concept that is not inherently technological and the attention to software medium (I am thinking of such classics as Carnivore and Auto-Illustrator). Others may be more concerned with technological or design issues but, here as well, the best works are making a unique contribution to the larger dialog: for instance, Ben Fry’s visualizations which allow us to see relationships in data and its dynamic development – something which was until now not possible to do in the history of visual representations. Still, others are programmers who do not even consider themselves as artists, which allows them even though they may not know it - to make genuinely interesting artistic statements. (runme.org recognizes that some of the most interesting activities in “software art” come from the outsiders – in the same way that Shulgin’s much earlier “medal for web art” was awarded to web sites which were not done by self-proclaimed artists but displayed “original artistic sensibility.” As – the runme.org site states, “Software art is an intersection of two almost non-overlapping realms: software and art… The repository is happy to host different kinds of projects - ranging from found, anonymous software art to famous projects by established artists and programmers.”)

What I wanted to critique was not the extremely dynamic and important field of “software art” but the way it was represented by Ars Electronica 2003 festival. Its paradigm can only be described as cultural isolationism. This is a dangerous position to take. Today, when pretty much every artist and cultural producer is widely using computers while also typically being motivated by many other themes and discourses, is it in fact possible that “digital art” happens everywhere else but not within the spaces of Ars Electronica festival?

## Links:

[www.aec.at/en/festival/](www.aec.at/en/festival/)
 [www.transmediale.de/](www.transmediale.de/)

[www.runme.org](www.runme.org)

[www.m-cult.org/read\_me/](www.m-cult.org/read_me/)

[www.abstraction-now.net](www.abstraction-now.net)
 [www.zkm.de/futurecinema/](www.zkm.de/futurecinema/)

---

# Introduction to Korean Edition of _The Language of New Media_

_author: Lev Manovich_
_year: (Barcelona, October 10, 2003)_

I am delighted to introduce the Korean translation of my book. In some ways, Korea today is the quintessential “new media civilization” - and at the same time, it is not. Of course, given the speed of adaptation of new communication and network technologies in different countries around the world, it is difficult to observe how these technologies interact with the local cultural and social patterns without being there "on the ground," so the following observation, based on the impressions from my week-long visit to Seoul a few years ago, maybe incorrect. It seems that while leading the world in the use of a number of new communication services, Korean society at the same time in many ways remains "traditional" and highly "hierarchical" - the values which appear to contradict the “open source” and “hypertext” ideologies which go along with the new communication technologies. 

Such contradictions, or rather, creative and generative cultural tensions, are exactly one of the subjects of this book. While it may appear on the first sight to be the systematic exposition of the unique features and creative techniques of new media filed, after reading any few pages in the book you will see that in fact, it is animated by a different desire. The “language” of new media which this book aims to map out – or, more precisely, numerous separate “languages” (this is one change I would love to do post factum to the title of the book) – are always hybrids, incorporating memories, expertise, and techniques of already well established cultural forms such as cinema, theatre, printed books, and so on, as well as new more recent techniques which come from the new engine of global information society – digital networked computer. Every section of the book therefore takes up a particular dimension of new media and examines it as a meeting ground - a field of struggle, competition, and creative tension – between the energies of the past and present.

A few years ago such an approach appeared strange to some participants and observes of cyberculture. Why did the references to the old culture when Internet was supposed to bring us all into the new brave world as painted in _Wired_ and similar publications? Yet I am happy to say that today the global audiences for my book that positioned new media in a longer historical context keep growing, while many futuristic pronouncements of cyber gurus from the 1990s look quite embarrassing. Our culture is undergoing computerization, and every one of its layers is changing as a result, but these changes can take a very long time to become visible. (Geological metaphors are not out of place in this respect.) Think for instance of a culture which accompanied the development of new industrial society in the nineteenth century. If we time the beginning of this society to the introduction of engine in the first decades of the nineteenth century, we see that it took about one hundred years for the cultural super-structure to catch up. It was only in the 1920s when artists, designers, and architects have clearly formulated new set of aesthetic forms and principles that together formed new twentieth century culture of “industrial modernism”: spaces made of geometric forms devoid of ornament, aggressive use of type, compositions made from simple abstract elements, new color schemes, and so on. I do have a strong sense that many cultural phenomena and styles which surround us today are equivalents of academic painting or architectural eclecticism of the nineteenth century – something which does not at all belong to the twenty first century and which one day, when we will find proper cultural responses to a new global information society, will look hopeless irrelevant. Yet today it is not easy to say which current impulses are messages from the future, and which are simply here through inertia.

It is my strong feeling that the emerging “information aesthetics” (i.e., the new culture of information society different from the old culture of industrial society) has or will have a very different logic from “industrial modernism.” The later was driven by a strong desire to erase the old, visible as much in the avant-garde artists’ (particularly the futurists) statements that museums should be burn, and in the dramatic erasure of all social and spiritual realities of many of people in Russia after the 1917 revolution, and in other countries after they became Soviet satellites after 1945. Culture and ideology of industrial modernism wanted to start with “tabula rasa,” radically distancing themselves from the past. It was only in the 1960s that this move started to feel inappropriate, as manifested both in loosening of ideology in communist countries and the beginnings of new post-modern sensibility in the West. “Learning from Las Vegas,” to quote the title of a famous book by Robert Venturi et al. (published in 1972, it was the first systematic manifestation of new sensibility), was to admit that real, organically developing culture had a very different rhythm and logic than Bauhaus-grown “international style” which was still practiced by architects world-wide at that time. We can say that in 1990 when Soviet Empire collapsed post-modernism has won world over. 

Today we have a very real danger of being imprisoned by new “international style” - something which we can “global international.” The cultural globalization, of which cheap international flights and Internet are just two among other carriers, erases certain cultural specificity with the energy and speed impossible for modernism. Yet we also witness today a different logic at work: the desire to creatively place together old and new in various combinations. It is this logic, for instance, which in many ways made Barcelona where I am writing this right now, such a “hip” and “in” place today. All over the city, architectural styles of many past centuries co-exist with new “cool” spaces of bars, hotels, museums, and so on. Medieval meets multi-national, Gaudi meets Dolce and Gabbana, Mediterranean time meets Internet time. The result is the incredible sense of energy which one feels physically just walking along the street. It is this hybrid energy, which characterizes in my view the most successful cultural phenomena today. This book then is a systematic investigation of a particular slice of contemporary culture driven by this hybrid aesthetics: the slice where the logic of digital networked computer intersects the numerous logics of already established cultural forms. 

In conclusion let me offer you a different metaphor to think with about this cultural slice which we also call “new media.” This metaphor is that of “remix.” I often look at contemporary culture in terms of three key processes – three different kinds of remixes. The first remix is what already for a few decades we referred to as “post-modernism”– the remixing of previous cultural contents and forms within a given media or cultural form (most visible today in music, architecture, and fashion). The second type of remixing is that of national cultural traditions, characters, and sensibilities intermingling both between themselves and also interacting with a new “global international” style. In short, this is the remix of “globalization.” “New media” then can be thought alongside these two types of remixes as the third type. It is the remix between the interfaces of various cultural forms and the new software techniques – in short, the remix between culture and computers. Its cultural logic is new not because this is “modernist new” which tried to erase the past – on the contrary, it is new because of the scale of the remix process at work, its speed, and the components themselves involved. Some of the results, which are being generated, are trivial, some are OK, and some are brilliant. While computer is a very powerful remix instrument, what comes out from it is ultimately up to the creative individuals who are at the controls of the computers – you.

Welcome to the hybrid!

---

# Moving Image after Computerization - Extending Traditional Elements of Cinema. An Outline

_author: Lev Manovich_
_year: 2003_

## 1. Frame / Camera / Space

- panoramic cinema / "total" recording

- QTVR, ipix, etc.

- Jeffrey Shaw (EVE; Place: a User Manual; Panosurround Camera)

- Luc Courchesne hybrid space: combining best features of 2D lens based and 3D synthetic representations;

- 3-D compositing: film/video mapped into surfaces positioned in a virtual space;

- virtual camera moves through this space

- Christian Boustani/Alain Escale: Brugge, Viagem
- film/video recordings embedded within virtual space

- Joachim Sauter/ARCT+COM, Invisible Shape of Things Past (1997),

- computer sets for "The Marlowe, The Jew of Malta" (2002)

- Fujihata, Field-Work@Alsace

	 

## 2. Image

- "velvet revolution" in moving image culture (1985-1995)

- new visual aesthetics of moving images: If we define a digital moving image as compositing of various elements created/modified via different methods
	   live action +

- image processing +

- 2-D animation +

- 3-D animation +

- typography +

- generative/procedural image construction +

- 2D graphic design / motion graphics +

- filters applied to any of the above +

We can define a number of visual aesthetics in contemporary moving image culture depending on which method dominates, for instance:

- live action (most feature and short film/video)

- typography - "motion graphics" ("typographic cinema"): typography becomes an image (After Effects)

- 3D computer animation (Final Fantasy, 2001; children 3D animated cartoons
	   2D graphic design + generative/procedural image construction
 (graphical music videos using shockwave / Flash - see schockwave.com; algorithmic abstract animation; "Generation Flash")

We can also single out certain sensibilities/styles based on privileging a few of the methods, for instance:
	   "Post-Flash cinema": Web designers bringing their aesthetics to short films: uses stylized live action, 2D animation, 3-D animation (flat planes, vectors) + typography, but with 2D graphics design as the dominant code

New imaging / recording techniques: infrared, web cams, GPS, etc.
-  Financial TV programs

- contemporary information workspaces

- CNN television coverage of the war

- Jordan Crandal

- Harun Farocki

## 3. Editing / Time / Narrative

"Continuous" cinema (Timecode, Russian Arc) - from montage to very long takes

- database cinema (database + metadata + algorithmic construction) Stan Douglas

- Klein/Kratky/Comella, Bleeding Through: Layers of Los Angeles, 1920-1980
- Florian Thalhofer - Korsakow Syndrom, Love Story Project [www.LoveStoryProject.com](www.LoveStoryProject.com)

- Sebastian Campion, interactive documentary for Danish Film Institute switching.dk

- Michael Lew (Media Lab Dublin)
 [www.mle.ie/~michael/research/voodoo](www.mle.ie/~michael/research/voodoo)
  [www.thickspace.net](www.thickspace.net)

- Thompson/Craighead, Short Films About Flying

Spatial image - a single image broken into a number of frames
	   Gance Abel

- Peter Greenaway

- Mike Figgis

- recent TV programs

spatial image - multiple images positioned in space- 1960s "expanded cinema"

- Gary Hill

- Eija Liisa Ahtila

- Doug Aitken

- Wilson Twins

- club culture

Real-time improvisation/generation- 1960s-1970s video synthesizers / processors

- 242 Pilots

- VJ culture

---

# Abstraction and Complexity

_author: Lev Manovich_
_year: 2004_

What kind of images are appropriate for the needs of a global informational networked society – the society which in all of its areas needs to represent more data, more layers, more connections than the preceding its industrial society? [1] The complex systems which have become super-complex [2]; the easy availability of real-time information coming from news feeds, networks of sensors, surveillance cameras; more fragmented and limited access to the senses of any subject in a consumer economy – all this puts a new pressure on the kinds of images human culture already developed and ultimately calls for the development of new kinds. This does not necessarily mean inventing something completely unprecedented – instead it is apparently quite productive to simply give old images new legs, so to speak, by expanding what they can represent and how they can be used. This is, of course, exactly what computerization of visual culture has been all about since it began in the early 1960s. While it made production and distribution of already existing kinds of images (lens-based recordings, i.e., photographs, film and video, diagrams, architectural plans, etc.) efficient, more importantly the computerization made possible for these images to function in various novel ways by “adding” interactivity, by making turning static images into navigable virtual spaces, by opening images to all kinds of mathematical manipulations which can be encoded in algorithms. 

This short essay of course will not be able to adequately address all these transformations. It will focus instead on a particular kind of image – software driven abstraction. Shall the global information society include abstract images in its arsenal of representational tools? In other words, if we take an abstraction and wire it to software, do we get anything new and useful beyond what already took place in the first part of the twentieth century – than the new abstract visual language was adopted by graphic design, product design, advertising and all other communication, propaganda, and consumer fields? 

## After Effects

Let’s begin by thinking about abstraction in relation to its opposite. How did computerization of visual culture have affected the great opposition of twentieth century between abstraction and figuration? In retrospect, we can see that this opposition was one the defining dimensions of the twentieth century culture since it was used to support so many other oppositions – between “popular culture” and “modern art,” between “democracy” and “totalitarianism,” and so on. Disney against Malevich, Pollock against Socialist Realism, MTV versus Family Channel. Eventually, as the language of abstraction took over all of modern graphic design while abstract paintings migrated from artists’ studios to modern art museums as well as corporate offices, logos, hotel rooms, bags, furniture, and so on, the political charge of this opposition has largely dissolved. And yet in the absence of new and more precise categories we still use figuration/abstraction (or realism/abstraction) as the default basic visual and mental filter through which we process all images which surround us.

In thinking about the effects of computerization on abstraction and figuration, it is much easier to address the second term than the first. While “realistic” perspective images of the world are as common today as they were throughout the twentieth century, photography, film, video, drawing, and painting are no longer the only ways to generate them. Since the 1960s, these techniques were joined by a new technique of computer image synthesis. Over the next decades, 3D computer images gradually became more and more widespread, gradually coming to occupy a larger and larger part of the whole visual culture landscape. Today for instance practically all of computer games rely on real-time 3D computer images - and so are numerous feature films, TV shows, animated features, instructional video, architectural presentations, medical imaging, military simulators, and so on. And while the production of highly detailed synthetic images is still a time-consuming process, as the role of this technique is gradually expanding, various shortcuts and technologies are being developed to make it easier: from numerous ready-to-use 3D models available in online libraries to scanners which capture both color and shape information and software which can automatically reconstruct a 3D model of an existing space from a few photographs.

While computerization has “strengthened” the part of the opposition occupied by figurative images by providing new techniques to generate these images – and even more importantly, making possible new types of media which rely on them (3D computer animation, interactive virtual spaces) – it simultaneously had “blurred” the “figurative” end of the opposition. Continuous developments in “old” analog photo and film technologies (new lenses, more sensitive films, etc.) combined with the development of software for digital retouching image processing and compositing eventually completely collapsed the distance which previously separated various techniques for constructing representational images: photography, photo-collage, drawing and painting in various media, from oil, acrylic, and airbrush to crayon and pen and ink. Now the techniques specific to all these different media can be easily _combined_ within the metamedium of digital software. [3]

One result of this shift from _separate representational and inscription media_ to _computer metamedium_ is proliferation of _hybrid_ images - images that combine traces and effects of a variety of media. Think of a typical magazine spread, a TV advertisement or a home page of a commercial web site: maybe a figure or a face of person against a white background, some computer elements floating behind or in front, some Photoshop blur, funky Illustrator typography, and so on. (Of course, looking at the Bauhaus graphic design we can already find some hybridity as well similar treatment of space combining 3D and 3D elements – yet because a designer had to deal with the actual media, the boundaries between elements in different media were sharply defined.)

This leads us to another effect - the liberation of the techniques of a particular media from its material and tool specificity. Simulated in software, these techniques can now be freely applied to visual, spatial or audio data that has nothing to do with the original media. [4] In addition to populating the tool pallets of various software applications, these virtualized techniques came to form a separate type of software – filters. You can apply reverb (a property of sound when it propagates in particular spaces) to any sound wave; apply depth of field effect to a 3D virtual space; apply blur to type, and so on.

The last example is quite significant in itself: simulation of media properties and interfaces in software has not only made possible the development of numerous separate filters but also whole new areas of media culture such as _motion graphics_ (animated type which exist on its own or combined with abstract elements, video, etc.). By allowing the designers to move type in 2D and 3D space, and filter it in arbitrary ways, After Effects has affected the Guttenberg universe of text at least as much if not more than Photoshop affected photography.

The cumulative result of all these developments – 3D computer graphics, compositing, simulation of all media properties and interfaces in software – is that the images which surround us today are usually very beautiful and often very stylized. The perfect image is no longer something which is expected in particular areas of consumer culture – instead it is an entry requirement. To see this difference you only have to compare an arbitrary television program from twenty years ago to one of today. Just as the actors that appear in them, all images have been put through a plastic surgery of Photoshop, After Effects, Flame, or similar software. At the same time, the mixing of different representational styles which until a few decades ago was only found in modern art (think of Moholy-Nagy photograms or Rauschenberg’s prints from 1960) has become a norm in all areas of visual culture. 

## Modernist Reduction

As can be seen even from this brief and highly compressed account, computerization has affected the figurative or “realistic” part of the visual culture spectrum in a variety of significant ways. But what about the opposite part of the spectrum – pure abstraction? Are the elegant algorithmically driven abstract images which started to populate more and more web sites since the late 1990s have a larger ideological importance, comparable to any of the political positions and conceptual paradigms which surrounded the birth of modern abstract art in the beginning of the 1920s century? Is there some common theme can be deduced from the swirling streams, slowly moving dots, dense pixel fields, mutating and flickering vector conglomerations coming from the contemporary masters of Flash, Shockwave, Java, and Processing?

If we compare 2004 with 1914, we will in fact see a similar breadth of abstract styles: strict northern diet of horizontal and vertical lines in Mondrian, more flamboyant orgy of circular forms in Robert Delaunay working in Paris, even more emotional fields of Wassily Kandinsky, the orgy of motion vectors of Italian futurists. The philosophical pre-suppositions and historical roots which have led to the final emergence of “pure” abstraction in the 1910s are similarly multiple and diverse, coming from a variety of philosophical, political, and aesthetic positions: the ideas of synesthesia (the correspondence of sense impressions), symbolism, theosophy, communism (abstraction as the new visual language for the proletariat in Soviet Russia), and so on. And yet it possible and appropriate to point at a single paradigm which both differentiates modernist abstraction from realist painting of the nineteenth century and simultaneously connects it to modern science. This paradigm is _reduction_.

In the context of art, abstraction of Mondrian, Kandinsky, Delaney, Kupka, Malevich, Arp, and others represents the logical conclusion of a gradual development of a number of preceding decades. From Manet, impressionism, post-impressionism, symbolism to fauvism and cubism, the artists progressively streamline and abstract the images of visible reality until all recognizable traces of the world of appearances are taken out. While in general this reduction of visual experience in modern art was a very gradual process which begins already in early nineteenth century [5], in the beginning of the twentieth century we often see the whole development replayed from the beginning to the end within a single decade – such as in the paintings by a tree created by Mondrian between 1908 and 1914. Mondrian starts with a detailed realistic image of a tree. By the time Mondrian has finished his remarkable compression operation, only the essence, the idea, the law, the genotype of a tree is left.

This visual reduction that took place in modern art perfectly parallels with the dominant scientific paradigm of the nineteenth and early twentieth century. [6] Physics, chemistry, experimental psychology and other sciences were all engaged in the deconstruction of the inanimate, biological and psychological realms into simple, further indivisible elements, governed by simple and universal laws. Chemistry and physics postulated the levels of molecules and atoms. Biology saw the emergence of the concepts of cell and chromosome. Experimental psychology applied the same reductive logic to the human mind by postulating the existence of further indivisible sensorial elements, the combination of which would account for perceptual or mental experience. For instance, in 1896 E.B. Titchener (former student of Wundt who brought experimental psychology to the U.S.) proposed that there are 32,800 visual sensations and 11,600 auditory sensory elements, each just slightly distinct from the rest. Titchener summarized his research program as follows: "Give me my elements, and let me bring them together under the psychophysical conditions of mentality at large, and I will guarantee to show you the adult mind, as a _structure_, with no omissions and no superfluity." [7]

It can be easily seen that the gradual move towards pure abstraction in art during the same period follows exactly the same logic. Similarly to physicists, chemists, biologists and psychologists, the visual artists have focused on the most basic pictorial elements – pure colors, strait lines, and simple geometric shapes. For instance, Kandinsky in _Point and Line to Plane_ advocated "microscopic" analysis of three basic elements of form (point, line, and plane) claiming that there exist reliable emotional responses to simple visual configurations. [8] Equally telling of Kandinsky's program are the titles of the articles he published in 1919: "Small Articles About Big Questions. I. About Point," and "II. About Line." [9]

While the simultaneous deconstruction of visual art into its most basic elements and their simple combinations by a variety of artists in a number of countries which has taken place in the first two decades of the twentieth century echoes the similar developments in contemporary science, in some cases the connection was much more direct. Some of the key artists who were involved in the “birth” of abstraction were closely following the research into the elements of visual experience conducted by experimental psychologists. As experimental psychologists split visual experience into separate aspects (color, form, depth, motion) and subjected these aspects to a systematic investigation, their articles begin to feature simple forms such as squares, circles, and straight lines of different orientations, often in primary colors. Many of the abstract paintings of Mondrian, Klee, Kandinsky, and others look remarkably similar to the visual stimuli already widely used by psychologists in previous decades. Since we have documentation that at least in some cases the artists have followed the psychological research, it is appropriate to suggest that they have directly copied the shapes and compositions from the psychology literature. Thus, abstraction was in fact born in psychological laboratories before it ever reached the gallery walls.

## Complexity

Beginning in the 1960s, scientists in different fields gradually realize that the classical science which aims to explain the world through simple universally applicable rules (such as the three laws of Newtonian physics) can’t account for a variety of physical and biological phenomena. Soon after, artificial intelligence research that tried to reduce human mind to symbols and rules, also run out of steam.

The new paradigm begins to emerge across a number of scientific and technical fields, eventually reaching popular culture as well. It includes a number of distinct areas, approaches, and subjects: chaos theory, complex systems, self-organization, autopoiesis, emergence, artificial life, the use of the models and metaphors borrowed from evolutionary biology (genetic algorithms, “memes”), neural networks. While distinct from each other, most of them share certain basic assumptions. They all look at complex dynamic and non-linear systems and they model the development and/or behavior of these systems as the interaction of a population of simple elements. This interaction typically leads to emergent properties - a priori unpredictable global behavior. In other words, the order that can be observed in such systems emerges spontaneously; it can’t be deduced from the properties of elements that make up the system. Here are the same ideas as expressed in somewhat different terms: “orderly ensemble properties can and do arise in the absence of blueprints, plans, or discrete organizers; interesting wholes can arise simply from interacting parts; enumeration of parts cannot account for wholes; change does not necessarily indicate the existence of an outside agent or force; interesting wholes can arise from chaos or randomness.” [10]

According to the scientists working on complexity, the new paradigm is as important as the classical physics of Newton, Laplace, and Descartes, with their assumption of the "clockwork universe." But the significance of the new approach is not limited to its potential to describe and explain the phenomena of the natural world that were ignored by classical science. Just as the classical physics and mathematics fitted perfectly the notion of a highly rational and orderly universe controlled by God, the sciences of complexity seem to be appropriate in the world which on all levels – political, social, economic, technical – appears to us to be more interconnected, more dynamic, and more complex than ever before. (As Rem Koolhaas has put it recently, “globalization is about connecting everything to everything else.”) [11] So, at the end it does not matter if frequent invocations of the ideas of complexity in relation to just about any contemporary phenomenon – from financial markets to social movements – are appropriate or not. [12] What is important is that having realized the limits of linear top-down models and reductionism, we are prepared to embrace a very different approach, one which looks at complexity not as a nuisance which needs to be quickly reduced to simple elements and rules, but instead as _the_ source of life – something which is essential for a healthy existence and evolution of natural, biological, and social systems.

Let us now return to the subject this text is about – contemporary software abstraction and its role in a global information society. I am now finally ready to name the larger paradigm I see behind the visual diversity of this practice – from stylish animations and backgrounds which populate commercial web sites to the online and offline works which are explicitly presented by their creators as art (a wonderful and carefully created selection of software works in the ”Abstraction Now” exhibition represents this diversity very well). This paradigm is _complexity_. If modernist art followed modern science in reducing the mediums of art – as well as our sensorial, ontological, and epistemological experiences and models reality – to basic elements and simple structures, contemporary software abstraction instead recognizes the essential complexity of the world. It is therefore not accidental that often software works develop in a way that is directly opposite to the reduction that took place over the number of years in Mondrian’s paintings – from a detailed figurative image of a tree to a composition consisting of  a just a few abstract elements. Today we are more likely to encounter the opposite: animated or interactive works that begin with an empty screen or a few minimal elements that quickly evolve into a complex and constantly changing image. And while the style of these works is often rather minimal – vector graphics and pixel patterns rather than an orgy of abstract expressionism (see my “Generation Flash” for a discussion of this visual minimalism as a new modernism [13]) – the images formed by these lines are typically the opposite of the geometric essentialism of Mondrian, Malevich, and other modernists. The patterns of lines suggest the inherent complexity of the world that is not reducible to some geometric phenotype. The lines curve and form unexpected arabesques rather than traversing the screen in strict horizontals and verticals. The screen as a whole becomes a constantly changing fields rather than a static composition.

When I discussed modernist abstraction, I pointed out that its relationship to modern science was two-fold. In general, the reductionist trajectory of modern art that eventually led to a pure geometric abstraction in the 1910s parallels the reductionist approach of contemporary sciences. At the same time, some of the artists actually follow the reductionist research in experimental psychology, adopting the simple visual stimuli used by psychologists in their experiments for their paintings.

Since designers and artists who pursue software abstraction are our contemporaries and since we share the same knowledge and references, it is easy for us to see the strategy of direct borrowing at work. Indeed, many designers and artists use the actual algorithms from the scientific publications on chaos, artificial life, cellular automata and related subjects. Similarly, the iconography of their works often closely followed the images and animations created by scientists. And some people actually manage to operate simultaneously in the scientific and cultural universes, using same algorithms and same images in their scientific publications and art exhibitions. (One example is Karl Sims who in the early 1990s created impressive animations based on artificial life research that were later shown at Pompidou Center in Paris.) What is less obvious is that in addition to the extensive cases of direct borrowing, _the aesthetics of complexity_ is also present in the works that do not use any models from complexity research directly. In short, I argue that just as it was the case with modernist abstraction, the abstraction of the information era is connected to contemporary scientific research both directly and indirectly – both through a direct transfer of ideas and techniques and indirectly as being part of the same historically specific imagination.

Here are some examples all drawn from a recent “Abstraction Now” exhibition. I decided to test my hypothesis by systematically going from piece to piece one by one rather than selecting only one a few works that would fit my preconceived ideas. I have also looked at all the accompanying statements – none of which as far I could see explicitly evoke the sciences of complexity. My experiment worked even better than I expected since almost all pieces in the online component of the show turn out to follow the aesthetics of complexity, invoking complex systems in natural world even more often and even more literally than I expected. 

Golan Levin’s _Yellowtail_ software amplifies the gestures of the user, producing ever-changing organic-looking lines of constantly varying thickness and transparency. The complexity of the lines and their dynamic behavior of the lines make the animation look like a real-time snapshot of some possible biological universe. The works perfectly illustrates how the same element (i.e., abstract line) that in modernist abstraction represented the abstract structure of the world now evokes instead the world’s richness and complexity. (The piece by Manny Tan also can be used as an example here.) In other words, if modernist abstraction assumes that behind sensorial richness of the world there are simple abstract structures that generate all this richness, such separation of levels is absent from software abstractions. What they show us instead is the dynamic interaction of the elements that periodically leads to certain orderly configurations.

_Insertsilence_ by James Paterson and Amit Pitaru works in the same manner: a click by the user immediately increases the complexity of the already animated line cob, making lines multiply, break, mutate, and oscillate until they “cool down” to from a complex pattern which sometimes contains some figurative references. While the artists’ statement makes no allusions to complexity sciences, the animation in fact looks like a perfect illustration of the concept of emergent properties.

As I already noted, often software works deploy vector graphics to create distinctly biologically looking patterns. However, a much more modernist looking rectangular composition can also be reworked to function as an analog to the complex systems studied by scientists. The pieces by Peter Luining, Return, and James Tindall evoke typical compositions created by students at Bauhaus and Vkhutemas (Russian equivalent of Bauhaus in the 1920s). But again, with a single click of the user the compositions immediately come to life, turning into dynamic systems whose behavior lo longer evokes the ideas of order and simplicity. As in many others software pieces which subscribe to the aesthetics of complexity, the behavior of the system is neither linear nor random – instead we are witnessing a system which seems to change from state to state, oscillating between order and chaos – again exactly like complex systems found in natural world.

While some of the software pieces in ”Abstraction Now” exhibition adopt the combinatorial aesthetics common to both early modernist abstraction and 1960s minimalism (in particular, the works by Sol LeWitt), this similarly only makes more apparent a very different logic at work today. For instance, instead of systematically displaying all possible variations of a small vocabulary of elements, _Arp_ code by Julian Saunderson from Soda Creative Ltd constantly shifts the composition without ever arriving at any stable configurations. The animation suggests that the modernist concept of “good form” no longer applies. Instead of right and wrong forms (think for instance of the war between Mondrian and Theo van Doesburg), we are in the presence of a dynamic process of organization that continuously generates different forms, all equally valid. 

If the works described so far were able to reference complexity mainly through the dynamic behavior of rather minimal line patterns, the next group of works uses algorithmic processes to generate dense and intricate fields which often cover the whole screen. Works by Glen Murphy, Casey Reas, Dexto, Meta, Ed Burton (also from Soda) all fit into this category. But just as with the works described so far, these fields are never static, symmetrical, or simple – instead they constantly mutate, shift and evolve.

I can go on multiplying examples, but the pattern should be quite clear by now. The aesthetics of complexity which dominates the online works selected for “Abstraction Now” show is not unique to it; scanning works regularly included in other exhibitions such as [www.whitneybiennial.com](www.whitneybiennial.com) (curated by Miltos Manetas), Ars Electronica 2003, or Flash Forward festivals demonstrates that this aesthetics is as central for contemporary software abstraction as the reductionism was for early modernist abstraction.

The space limitations of this text do not allow me to go into an important question of what is happening today in abstract painting (which is a very active scene in itself) and how its developments connect (or not) to the developments in software art and design as well as contemporary scientific paradigms. Instead, let me conclude by returning to the question that I posed in the beginning: the need for a new types of representations adequate for the needs of a global information society, characterized by the new levels of complexity (in this case understood in descriptive rather than in theoretical terms). As I already suggested, practically all of the developments in computer imaging so far can be understood as the responses to this need. But this still leaves open the question of representing the new social complexity symbolically. While software abstraction usually makes more direct references to the physical and biological than the social, it maybe also appropriate to think of many works in this paradigm as such symbolic representations. For they seem to quite accurately and at the same time poetically capture our new image of the world – world as the dynamic networks of relations, oscillating between order and disorder – always vulnerable ready to change with a single click of the user. 

## References:

[1] I rely here on the influential analysis of Manuel Castells who characterizes new economy which emerged in the end of the twentieth century as Informational, global and networked. See Manuel Castells, _The Rise of the Network Society. The Information Age_, v. 1, second edition (Blackwell, 2000), p. 77.

[2] Lars Qvortrup, _Hypercomplex Society_ (Peter Lang Publishing, 2003.) 

[3] The notion of computer as metamedium was clearly articulated by the person who, more than anybody, was responsible for making it a reality by directing the development of GUI at Xeroc Parc in the 1970s – Alan Kay. See Alan Kay and Adele Golberg, “Personal Dynamic Media” (1997), in Noah Wardrip-Fruin and Nick Monfort, _The New Media Reader_ (MIT Press, 2003), 394.

[4] In _The Language of New Media_ I describe this effect in relation to the cinematic interface, i.e., the camera model which in computer culture has become a general interface to any data which can be represented in 3D virtual space. But this is just a particular case of a more general phenomenon: simulation of any media in software allows for the “virtualization” of its interface. Lev Manovich, _The Language of New Media_ (MIT Press, 2001.)

[5] See, for instance, the exhibition _The Origins of Abstraction_, Musee d’Orsay, Paris, Nov 5 2003 – Feb 23 2004.

[6] For a detailed reading of modern art as the history of reduction which parallels the reductionism of modern science and in particular experimental phycology, see little known but remarkable book _Modern Art and Modern Science_. This section is based on the ideas and the evidence presented in this book. Paul Vitz and Arnold Glimcher _Modern Art and Modern Science: The Parallel Analysis of Vision_ (Praeger Publishers, 1984).

[7] Qtd. in Eliot Hearst, "One Hundred Years: Themes and Perspectives," in _The First Century of Experimental Psychology_, 25.

[8] Wassily Kandinsky, (1926), _Point and Line to Plane_ (New York: Solomon R. Guggenheim Foundation, 1947).

[9] Yu. A. Molok, "'Slovar simvolov' Pavla Florenskogo. Nekotorye margonalii" (Pavel Florensky's 'dictionary of symbols.' A few margins), _Sovetskoe Iskusstvoznanie_ 26 (1990): 328. 

[10] See [http://serendip.brynmawr.edu/complexity/complexity.html](http://serendip.brynmawr.edu/complexity/complexity.html).

[11] _CONTENT – Rem Koolhaas/OMA/AMO_, section on Prada stores, exhibition at Neue Nationalgalerie Berlin, November 2003 – January 2004.

[12] For examples of works which apply the ideas of complexity to a range of fields, see Manuel de Landa, _Thousand Years of Non-linear History_ (MIT Press, 1997); Howard Rheingold, _Smart Mobs: The Next Social Revolution_ (Perseus Publishing, 2002); Steven Johnson, _Emergence: Connected Lives of Ants, Brains, Cities, and Software_ (Scribner, 2003).

[13] Available at [www.manovich.net](www.manovich.net).

---

# The Shape of Information

_author: Lev Manovich_
_year: 2005_

To explain what I mean by _info-aesthetics_, let me start by noting something simple but nevertheless quite significant: the word “information” contains within it the word “form.” For a while now social theorists, economists, and politicians were telling us that we are living in a new “information society.” The term was first used already in the 1960s, even before computer revolution got underway. Today, a few decades later, what was once a theoretical hypothesis became a practical everyday reality that can be easily observed by anybody leaving in any developed country. All kinds of work became reduced to handling data on one’s computer screen – in short, processing information. As you walk or drive past office buildings in any city, all offices regardless of what a company does look the same: rows of computer screens and keyboards. Regardless of their actual profession, financial analysts, city officials, secretaries, architects, accountants, and pretty much everybody else engaged in white-collar work is actually processing information. And when we leave work, we don’t leave information society. In our everyday life, we use search engines and retrieve data from databases; we rely on “personal information appliances” and “personal information managers.” We complain that there is too much information to keep track or make sense - while the libraries and museums around the world constantly add to the global information pile by systematically digitizing everything they got. We turn our own lives into an information archive by storing all our emails, SMS, digital photos, and other “digital traces” of our existence. One day we get tired from all this, so we start planning to take “e-mail free” holidays – but even this requires information work: searching for best deals on the internet, comparing fares, inputting credit card information into a reservation web site. In short, “information society” is where the citizens of the developed world live today, experiencing it in their everyday practice.

The question that I have been interested in for the last five years is this: What is “the shape of information”? What are the forms contained within information, so to speak? To put this more explicitly: has the arrival of information society been accompanied by new vocabularies of forms, new aesthetic sensibilities, and new iconologies? And can there be forms specific to information society, given that software and computer networks redefine the very concept of form? (Instead of being solid, stable, finite, discrete, and limited in space and time, the new forms are often variable, emergent, distributed, and not directly observable.) Can information society be represented iconically, if the activities that define it – information processing, interaction between a human and a computer, telecommunication, networking – are all dynamic processes? How does the super-human scale of our information structures – from 16 million lines of computer codes making Windows OS, to forty years which would take one viewer to watch all video interviews stored on digital servers of the Shoah Foundation, to the Web itself which cannot be even mapped as a whole – be translated to the scale of human perception and cognition? In short, if the shift from modernism to informationalism (the term of Manuel Castells) has been accompanied by a shift _from form to information flows_, can we still map these information flows to forms, meaningful to a human?

When I started looking at contemporary culture from the perspective of these questions, I decided that I need a term to label my future findings. _Info-aesthetics_ is the term I chose. _Info-aesthetics_ project scans contemporary culture to detect emerging aesthetics and cultural forms specific to a global information society. I don’t mean to suggest that there is some single _info-aesthetics_ style that already exists today or may emerge in the future. Rather, _info-aesthetics_ refers to various new contemporary cultural practices which can be best understand as responses to the new priorities of information society: making sense of information, working with information, producing knowledge from information. And while I think that these responses already occupy a prominent place, I should make it clear that the whole eco-system of diverse styles and forms in contemporary aesthetics should not be simply correlated to the shift to information society and key role played by information management in the social, economic, and political life of contemporary societies. Various other factors such as economic globalization, the ideas of complexity, emergence, and evolution, the ecological thinking (manifested in such paradigms as “cradle to cradle’ manufacturing, or recyclable and sustainable design), the new materials and new manufacturing processes are as important.

So, what are some of the ways in which the forms we create today are information-driven? The most obvious example is the new field of information visualization. Designers, computer scientists, and artists working with information visualization create new forms which are no longer related to a human form (classical art) or abstract from it (modern art) – instead they represent quantitative data of all kinds, helping us to understand it – and sometimes simply anesthetizing it.

There are many other less obvious ways in which new forms in our culture are the result of our new information powers. We could store, index, and manipulate exponentially larger amounts of media (video, audio, etc.) than it was possible even recently; we can also record altogether new kinds of data (such as GPS); and we can also map recorded data into numerous other formats. These capacities are utilized by architects, graphic and industrial designers, filmmakers, media artists. Many architects including such leading figures as Koolhaas/OMA, UN Studio, NOX, Hadid, and others (and of course many architectural students who grew up with computers) record, analyze, and map information flows, and then utilize resulting records and diagrams to drive the design of architectural forms and spaces. Media artists and designers create new types of representations that combine different media recording of the same space into a new type of representation (for instance, Masaki Fujihata combines video, GPS data and 3D virtual space in his elegant and poetic _Field Works_ series). The ability to record and store media data on a new scale makes possible new forms of cinematic narrative (_Timecode_, _Russian Arc_) and new forms of portraiture (_MyLifeBits_ project by Microsoft).

The recently launched DVD publishing label real23 ([www.real23.com](http://www.real23.com/)) features the following statement on its home page: “Life is data, progress is optional.” I would like to add the following question: If life indeed became data, what are the new forms that will help us make sense of it?

---

# Remixability and Modularity

_author: Lev Manovich_
_year: 2005_

## Remixing and Remixability

The dramatic increase in quantity of information greatly speeded up by Internet has been accompanied by another fundamental development. Imagine water running down a mountain. If the quantity of water keeps continuously increasing, it will find numerous new paths and these paths will keep getting wider. Something similar is happening as the amount of information keeps growing - except these paths are also all connected to each other and they go in all directions; up, down, sideways. Here are some of these new paths which facilitate movement of information between people, listed in no particular order: SMS, forward and redirect function in email clients, mailing lists, Web links, RSS, blogs, social bookmarking, tagging, publishing (as in publishing one’s playlist on a web site), peer-to-peer networks, Web services, Firewire, Bluetooth. These paths stimulate people to draw information from all kinds of sources into their own space, remix and make it available to others, as well as to collaborate or at least play on a common information platform (Wikipedia, Flickr). Barb Dybwad introduces a nice term “collaborative remixability” to talk about this process: “I think the most interesting aspects of Web 2.0 are new tools that explore the continuum between the personal and the social, and tools that are endowed with a certain flexibility and modularity which enables collaborative remixability — a transformative process in which the information and media we’ve organized and shared can be recombined and built on to create new forms, concepts, ideas, mashups and services.” [1]

If a traditional twentieth century model of cultural communication described movement of information in one direction from a source to a receiver, now the reception point is just a temporary station on information’s path. If we compare information or media object with a train, then each receiver can be compared to a train station. Information arrives, gets remixed with other information, and then the new package travels to other destination where the process is repeated.

We can find precedents for this “remixability” – for instance in modern electronic music where remix has become the key method since the 1980s. More generally, most human cultures developed by borrowing and reworking forms and styles from other cultures; the resulting “remixes” were to be incorporated into other cultures. Ancient Rome remixed Ancient Greece; Renaissance remixed antiquity; nineteenth century European architecture remixed many historical periods including the Renaissance; and today graphic and fashion designers remix together numerous historical and local cultural forms, from Japanese Manga to traditional Indian clothing. At first glance it may seem that this traditional cultural remixability is quite different from “vernacular” remixability made possible by the computer-based techniques described above. Clearly, a professional designer working on a poster or a professional musician working on a new mix is different from somebody who is writing a blog entry or publishing her bookmarks.

But this is a wrong view. The two kinds of remixability are part of the same continuum. For the designer and musician (to continue with the sample example) are equally affected by the same computer technologies. Design software and music composition software make the technical operation of remixing very easy; the Internet greatly increases the ease of locating and reusing material from other periods, artists, designers, and so on. Even more importantly, since every company and freelance professionals in all cultural fields, from motion graphics to architecture to fine art, publish documentation of their projects on their Web sites, everybody can keep up with what everybody else is doing. Therefore, although the speed with which a new original architectural solution starts showing up in projects of other architects and architectural students is much slower than the speed with which an interesting blog entry gets referenced in other blogs, the difference is quantitative than qualitative. Similarly, when H&M or Gap can “reverse engineer” the latest fashion collection by a high-end design label in only a few weeks, this is part of the same new logic of speeded up cultural remixability enabled by computers. In short, a person simply copying parts of a message into the new email she is writing, and the largest media and consumer company recycling designs of other companies are doing the same thing – they practice remixability.

The remixability does not require modularity - but it greatly benefits from it. Although precedents of remixing in music can be found earlier, it was the introduction of multi-track mixers that made remixing a standard practice. With each element of a song – vocals, drums, etc. – available for separate manipulation, it became possible to ‘re-mix’ the song: change the volume of some tracks or substitute new tracks for the old ounces. According to the book _DJ Culture_ by Ulf Poschardt, first disco remixes were made in 1972 by DJ Tom Moulton. As Poschardt points out, “Moulton sought above all a different weighting of the various soundtracks, and worked the rhythmic elements of the disco songs even more clearly and powerfully…Moulton used the various elements of the sixteen or twenty-four track master tapes and remixed them.” [2]

In most cultural fields today we have a clear-cut separation between libraries of elements designed to be sampled – stock photos, graphic backgrounds, music, software libraries – and the cultural objects that incorporate these elements. For instance, a graphic design may use photographs that the designer bought from a photo stock house. But this fact is not advertised; similarly, the fact that this design (if it is successful) will be inevitably copied and sampled by other designers is not openly acknowledged by the design field. The only fields where sampling and remixing are done openly are music and computer programming, where developers rely on software libraries in writing new software.

Will the separation between libraries of samples and “authentic” cultural works blur in the future? Will the future cultural forms be deliberately made from discrete samples designed to be copied and incorporated into other projects? It is interesting to imagine a cultural ecology where all kinds of cultural objects regardless of the medium or material are made from Lego-like building blocks. The blocks come with complete information necessary to easily copy and paste them in a new object – either by a human or machine. A block knows how to couple with other blocks – and it even can modify itself to enable such coupling. The block can also tell the designer and the user about its cultural history – the sequence of historical borrowings which led to the present form. And if original Lego (or a typical twentieth century housing project) contains only a few kinds of blocks that make all objects one can design with Lego rather similar in appearance, computers can keep track of unlimited number of different blocks. At least, they can already keep track of all the possible samples we can pick from all cultural objects available today.

The standard twentieth century notion of cultural modularity involved artists, designers or architects making finished works from the small vocabulary of elemental shapes, or other modules. The scenario I am entertaining proposes a very different kind of modularity that may appear like a contradiction in terms. It is modularity without a priori defined vocabulary. In this scenario, any well-defined part of any finished cultural object can automatically become a building block for new objects in the same medium. Parts can even ”publish” themselves and other cultural objects can “subscribe” to them the way you subscribe now to RSS feeds or podcasts. 

When we think of modularity today, we assume that a number of objects that can be created in a modular system is limited. Indeed, if we are building these objects from a very small set of blocks, there are a limited number of ways in which these blocks can go together. (Although as the relative physical size of the blocks in relation to the finished object get smaller, the number of different objects which can be built increases: think IKEA modular bookcase versus a Lego set.) However, in my scenario modularity does not involve any reduction in the number of forms that can be created. On the contrary, if the blocks themselves are created using one of many already developed computer designed methods (such as parametric design), every time they are used again they can modify themselves automatically to assure that they look different. In other words, if pre-computer modularity leads to repetition and reduction, post-computer modularity can produce unlimited diversity.

I think that such “real-time” or “on-demand” modularity can only be imagined today after online stores such as Amazon, blog indexing services such as Technorati, and architectural projects such as Yokohama International Port Terminal by Foreign Office Architects and Walt Disney Concert Hall in Los Angeles by Frank Gehry visibly demonstrated that we can develop hardware and software to coordinate massive numbers of cultural objects and their building blocks: books, bog entries, construction parts. But whether we will ever have such a cultural ecology is not important. We often look at the present by placing it within long historical trajectories. But I believe that we can also productively use a different, complementary method. We can imagine what will happen if the contemporary techno-cultural conditions which are already firmly established are pushed to their logical limit. In other words, rather than placing the present in the context of the past, we can look at it in the context of a logically possible future. This “look from the future” approach may illuminate the present in a way not possible if we only “look from the past.” The sketch of logically possible cultural ecology I just made is a little experiment in this method: futurology or science fiction as a method of contemporary cultural analysis.

So, what else can we see today if we will look at it from this logically possible future of complete remixability and universal modularity? If my scenario sketched above looks like a “cultural science fiction,” consider the process that is already happening on the one end of remixability continuum. Although strictly speaking it does not involve increasing modularity to help remixability, ultimately its logic is the same: helping cultural bits move around more easily. I am talking about a move in Internet culture today from intricately packaged and highly designed “information objects” which are hard to take apart – such as web sites made in Flash – to “straight” information: ASCII text files, feeds of RSS feeds, blog entries, SMS messages. As Richard MacManus and Joshua Porter put it, “Enter Web 2.0, a vision of the Web in which information is broken up into “microcontent” units that can be distributed over dozens of domains. The Web of documents has morphed into a Web of data. We are no longer just looking to the same old sources for information. Now we’re looking to a new set of tools to aggregate and remix microcontent in new and useful ways.” [3] And it is much easier to “aggregate and remix microcontent” if it is not locked by a design. Strait ASCII file, a JPEG, a map, a sound, or video file can move around the Web and enter into user-defined remixes such as a set of RSS feeds; cultural objects where the parts are locked together (such as Flash interface) can’t. In short, in the era of Web 2.0, “information wants to be ASCII.” [4]

If we approach the present from the perspective of a potential future of “ultimate modularity / remixability,” we can see other incremental steps towards this future which are already occurring. For instance, Orange (orange.blender.org, an animation studio in Amsterdam) has set up a team of artists and developers around the world to collaborate on an animated short film; the studio plans to release all of their production files, 3D models, textures, and animation as Creative Commons open content on an extended edition DVD.

Creative Commons offers a special set of Sampling Licenses which “let artists and authors invite other people to use a part of their work and make it new.” [5] Flickr offers multiple tools to combine multiple photos (not broken into parts – at least so far) together: tags, sets, groups, Organizer. Flickr interface thus position each photo within multiple “mixes.” Flickr also offers “notes” which allows the users to assign short notes to individual parts of a photograph. To add a note to a photo posted on Flickr, you draw a rectangle on any part of the phone and then attach some text to it. A number of notes can be attached to the same photo. I read this feature as another a sign of modularity/remixability mentality, as it encourages users to mentally break a photo into separate parts. In other words, “notes” break a single media object – a photograph – into blocks.

In a similar fashion, the common interface of DVDs breaks a film into chapters. Media players such as iPod and online media stores such as iTunes break music CDs into separate tracks – making a track into a new basic unit of musical culture. In all these examples, what was previously a single coherent cultural object is broken into separate blocks that can be accessed individually. In other words, if “information wants to be ASCII,” “content wants to be granular.” And culture as a whole? Culture has always been about remixability – but now this remixability is available to all participants of Internet culture. 

Since the introduction of first Kodak camera, “users” had tools to create massive amounts of vernacular media. Later they were given amateur film cameras, tape recorders, video recorders... But the fact that people had access to "tools of media production" for as long as the professional media creators until recently did not seem to play a big role: the amateur and professional media pools did not mix. Professional photographs traveled between photographer’s darkroom and newspaper editor; private pictures of a wedding traveled between members of the family. But the emergence of multiple and interlinked paths which encourage media objects to easily travel between web sites, recording and display devices, hard drives, and people changes things. Remixability becomes practically a built-in feature of digital networked media universe. In a nutshell, what maybe more important than the introduction of a video iPod, a consumer HD camera, Flickr, or yet another exciting new device or service is how easy it is for media objects to travel between all these devices and services - which now all become just temporary stations in media’s Brownian motion.

## We Have Never Been Modular

While the topics of remixability and modularity are connected, it is important to note that modularity is something which does not only apply to RSS, social bookmarking, or Web Services. We are talking about the logic which extends beyond the Web and digital culture. 

Modularity has been the key principle of modern mass production. Mass production is possible because of the standardization of parts and how they fit with each other - i.e., modularity. Although there are historical precedents for mass production, until twentieth century they have separate historical cases. But soon after Ford installs first moving assembly lines at his factory in 1913, others follow, and soon modularity permeates most areas of modern society. ("An assembly line is a manufacturing process in which interchangeable parts are added to a product in a sequential manner to create an end product.") Most products we use are mass produced, which means they are modular, i.e., they consist of  standardized mass produced parts which fit together in standardized way. Moderns also applied modularity principle outside of factory. For instance, already in 1932 – long before IKEA and Logo sets – Belgian designer Louis Herman De Kornick developed first modular furniture suitable for smaller council flats being built at the time. 

Today we are still living in an era of mass production and mass modularity, and globalization and outsourcing only strengthen this logic. One commonly evoked characteristic of globalization is greater connectivity – places, systems, countries, organizations etc., becoming connected in more and more ways. Although there are ways to connect things and processes without standardizing and modularizing them – and the further development of such mechanisms is probably essential if we ever want to move beyond all the grim consequences of living in a standardized modular world produced by the twentieth century – for now it is much easier just to go ahead and apply the twentieth century logic. Because society is so used to it, it’s not even thought of as one option among others.

In the fall of 2005 I was at Design Brussels event where the designer Jerszy Seymour speculated that once Rapid Manufacturing systems become advanced, cheap, and easy, this will give designers in Europe a hope for survival. Today, as soon as some design becomes successful, a company wants to produce it in large quantities – and its production goes to China. Seymour suggested that when Rapid Manufacturing and similar technologies would be installed locally, the designers can become their own manufactures and everything can happen in one place. But obviously this will not happen tomorrow, and it’s also not at all certain that Rapid Manufacturing will ever be able to produce complete finished objects without any humans involved in the process, whether its assembly, finishing, or quality control. 

Of course, modularity principle did not stay unchanged since the beginning of mass production a hundred years ago. Think of just-in-time manufacturing, just-in-time programing, or the use of standardized containers for shipment around the world since the 1960s (over %90 of all goods in the world today are shipped in these containers). The logic of modularity seems to be permeating more layers of society than ever before, and computers – which are great to keeping track of numerous parts and coordinating their movements – only help this process. 

The logic of culture often runs behind the changes in economy – so while modularity has been the basis of modern industrial society since the early twentieth century, we only start seeing the modularity principle in cultural production and distribution on a large scale in the last few decades. While Adorno and Horkheimer were writing about "culture industry" already in the 1940s, it was not then - and it’s not today - a true modern industry. [6] In some areas such as production of Hollywood animated features or computer games we see more of the factory logic at work with extensive division of labor. In the case of software engineering (i.e., programming), software is put together to a large extent from already available software modules - but this is done by individual programmers or teams who often spend months or years on one project – quite different from Ford production line assembling one identical car after another. In short, today cultural modularity has not reached the systematic character of the industrial standardization circa 1913. 

But this does not mean that modularity in contemporary culture simply lags behind industrial modularity, responsible for mass production. Rather, cultural modularity seems to be governed by a different logic than industrial modularity. On the one hand, “mass culture” is made possible by a complete industrial-type modularity on the levels of packaging and distribution. In other words, all the materials carriers of cultural content in the modern period have been standardized, just as it was done in the production of all goods - from first photo and films formats in the end of the nineteenth century to game cartridges, DVDs, memory cards, interchangeable camera lenses, etc. But the actual making of content was never standardized in the same way. [7] So while mass culture involves putting together new products – films, television programs, songs, games – from a limited repertoire of themes, narratives, icons using a limited number of conventions, this is done by the teams of human authors on a one by one basis. And while more recently we see the trend toward the refuse of cultural assets in commercial culture, i.e., media franchising – characters, settings, icons which appear not in one but a whole range of cultural products – film sequels, computer games, theme parks, toys, etc. – this does not seem to change the basic “pre-industrial” logic of the production process. For Adorno, this individual character of each product is part of the ideology of mass culture: “Each product affects an individual air; individuality itself serves to reinforce ideology, in so far as the illusion is conjured up that the completely reified and mediated is a sanctuary from immediacy and life.” [8]

On the other hand, what seems to be happening is that the "users" themselves have been gradually "modularising" culture. In other words, modularity has been coming into modern culture from the outside, so to speak, rather than being built-in, as in industrial production. In the 1980s musicians start sampling already published music; TV fans start sampling their favorite TV series to produce their own “slash films,” game fans start creating new game levels and all other kinds of game modifications. (Mods “can include new items, weapons, characters, enemies, models, modes, textures, levels, and story lines.”) And of course, from the very beginning of mass culture in early twentieth century, artists have immediately started sampling and remixing mass cultural products – think of Kurt Schwitters, collage and particularly photomontage practice which becomes popular right after WWI among artists in Russia and Germany. This continued with Pop Art, appropriation art, and video art.

Enter the computer. In _The Language of New Media_ I named modularity as one of the principles of computerized media. If before modularity principle was applied to the packaging of cultural goods and raw media (photo stock, blank videotapes, etc.), computerization modularizes culture on a structural level. Images are broken into pixels; graphic designs, film and video are broken into layers. Hypertext modularizes text. Markup languages such as HTML and media formats such as QuickTime and MPEG-7 modularize multimedia documents in general. We can talk about what this modularization already did to culture – think of World Wide Web as just one example - but this is a whole new conversation.

In short: in culture, we have been modular already for a long time already. But at the same time, “we have never been modular” - which I think is a very good thing.

## References:

[1] “Approaching a definition of Web 2.0,” The Social Software Weblog (socialsoftware.weblogsinc.com), accessed October 28, 2005. 

[2] Ulf Poschardt, _DJ Culture_, trans. Shaun Whiteside (London: Quartet Books Ltd, 1998), 123.

[3] “Web 2.0 Design: Bootstrapping the Social Web,” _Digital Web Magazine_, [http://www.digital-web.com/types/web\_2\_design/](http://www.digital-web.com/types/web_2_design/), accessed October 28, 2005.

[4] Modern information environment is characterized by a constant tension between the desires to “package” information (Flash design for instance) and strip it from all packaging so it can travel easier between different media and sites.

[5] [http://creativecommons.org/about/sampling](http://creativecommons.org/about/sampling), accessed October 31, 2005.

[6] Theodor W. Adorno and Max Horkheimer. _The Culture Industry. Enlightenment as Mass Deception_, 1947.

[7] In “Culture industry reconsidered,” Adorno writes: “the expression ’industry’ is not to be taken too literally. It refers to the standardization of the thing itself — such as that of the Western, familiar to every movie-goer — and to the rationalization of distribution techniques, but not strictly to the production process… it [culture industry] is industrial more in a sociological sense, in the incorporation of industrial forms of organization even when nothing is manufactured — as in the rationalization of office work — rather than in the sense of anything really and actually produced by technological rationality.” Theodor W. Adorno, “Culture Industry Reconsidered,” _New German Critique_, 6, Fall 1975, pp. 12-19.

[8] Ibid.

---

# Scale Effects

_author: Lev Manovich_
_year: 2005_

I would like to suggest that we can understand multiple cultural effects of computerization in terms of one overarching concept - scaling. Computer radically scales up already existing cultural and social forms leading to new qualitative effects. 

For example, Wikipedia which currently offers over 800,00 articles in its English version and continues to grow exponentially in several languages, is a result of scaling up the number of contributors and the speed of editing (real-time). 

The rest of my examples will deal with visual culture and visual media simply because these are the areas I know more about.

_From “New Media” to “More Media”: examples of new data storage, calculation. and communication capacities:_

University of New Hampshire researcher Will Gilbert, who has taken to ”carrying around the human genome on his 5 MB iPod.”

The UCSD Division of Calit2 purchased a Silicon Graphics Prism visualization system with 48GB RAM and 8 Intel® Itanium® 2 processors running the Linux® environment and an SGI InfiniteStorage RM600 system with ”21.6TB of disk storage” and dual 10 Gigabit network interfaces.

BCC would like to put its whole program archive – ”over 1 million hours of programming – online”, accessible at HDTV resolution by 20+ million UK households simultaneously.

During iGRID 2005, the Netherlands Computing and Networking Center SARA set a world record for “bandwidth usage by one single application showing scientific content” when it was ”streaming visualizations” of various large scientific objects from Amsterdam to the LambdaVision display in San Diego ”at a sustained rate of 18 Gigabits per second” (Gbps).

Company in San Diego makes 6 Giga-pixel images. For comparison: at iGRID we played with a panoramic view of Delft. The resolution of this image: _78,797x 31,565 pixels_. Yes, that is correct: seventy-eight thousand by thirty-one thousand pixels, plus some – which adds up to _2.48 Giga-pixels_.

Current consumer developments:

Video iPod – using video in blogs (vlogging) – video podcasts (vodcasts), mobile blogs (moblogs).

From FLICKR site: “we want to _get photos into and out of the system in as many ways as we can_: from the web, from mobile devices, from the users' home computers and from whatever software they are using to manage their photos. And we want to be able to push them out in as many ways as possible: on the Flickr website, in RSS feeds, by email, by posting to outside blogs or ways we haven't thought of yet.”

HDTV output from PSP3.

## Concept of Scale in Media Theory

When we think of technology’s impact on culture, we are used to considering the effects of _new_ technological inventions (including visual technologies). We are _not_ used to thinking about the effects of scaling up already _widely_ _used_ technologies. For instance, generations of art historians have discussed the introduction of a new technique of one-point linear perspective during the Renaissance in western Europe. Similarly, endless volumes have been written about the inventions of photography in the 19th century and how it affected arts, culture, warfare, etc. To take a more recent example, it’s obvious that a whole series of new medical imaging techniques developed over the last two decades in addition to the century old X-ray technique – CAT, MRI, CT, PET, and others – have had a fundamental impact on medical practice. Similarly, the introduction of graphical browsers around 1993 is what allowed the World Wide Web – which at this point had already existed for a few years - to quickly take off.

But what about the impact of scaling up existing media technologies – for instance, faster networks or higher-resolution computer images? This is harder to think about – although if we are to go to the very source of contemporary thinking about visual media – Marshall McLuhan’s 1964 book _Understanding Media_ – we will discover that the idea of scale is central to McLuhan’s thinking. McLuhan writes: “For the ‘message’ of any medium or technology is the change of scale or pace or pattern that it introduces into human affairs. The railway did not introduce movement or transportation or wheel or road into human society, but it accelerated and enlarged the scale of previous human functions, creating totally new kinds of cities and new kinds of work and leisure. This happened whether the railway functioned in a tropical or a northern environment, and is quite independent of the freight or content of the railway medium.” 

As we can see, for McLuhan, new media technologies accelerate, expand, or scale already existing technologies, which leads to qualitative changes in society and culture. Yet these ideas were not taken up by subsequent writers, possibly because the table of contents of _Understanding Media_ reads like a catalog of new communication inventions, with chapter names like “print,” “telegraph,” “telephone,” “car,” “television,” etc. – without mentioning the idea of scale itself.

Another example of how scale thinking – considering quantitative changes as well as qualitatively new phenomena – can be useful in media theory:

Normally we talk about print, cinema, computational media etc. as though they have been single unchanging media throughout their histories. Although more detailed analysis focused on the detailed development of a particular media of course focuses on changing, in a more general discussion these differences drop out. But if we are to look at history of each of these media, we can see that the qualitative development in effect make the media fundamental change its identity a number of times – so while some things stay constant, such as an interface (book binding; cinema space; interactive graphical computer), other – mostly representational [?] qualities change dramatically so it’s no longer meaningless to think of cinema or computational media as “one medium.” For instance, compare the quality of a film image in 1895 and in 2005; or the construction of first camera with the contemporary cameras featuring steady cams, etc., etc. 

## Examples of Cultural Effects of Scaling

### Speed

Consider, for instance, computer’s ability to represent objects in linear perspective and to animate such representations. When you move your character through the world in a first-person shooter computer game (such as _Quake_), or when you move your viewpoint around a 3D architectural model, a computer re-calculates perspectival views for all the objects in the frame many times every second (in the case of current desktop hardware, frame rates of 80 frames of second are not uncommon). But we should remember that the algorithm itself was codified during the Renaissance in Italy, and that, before digital computers came along (that is, for about five hundred years) it was executed by human draftsmen. In this case, speeding up the calculation of perspectival images leads to emergence of new media of 3D computer animation and virtual navigable spaces.

### Size

CALIT2 building house the EVL LambdaVision display consists of 55 tiled LCD screens (11 horizontally x 5 vertically), resulting in a total resolution of 17,600 x 6,000 pixels (in total, 105,600,000 pixels, or approximately 100 Megapixels).

One image which we played with was a panoramic view of Delft. The resolution of this image: 78,797x 31,565 pixels. The size of data that makes up the image: 7.12 GB. As Bram Stolk of SARA explained it to me, the multiple photos that make this monster image were captured by a camera mounted on a robotic arm. Afterwards, the computer that controls the camera automatically stitches the multiple photos together into one image.

Another image presented by SARA on the EVL LambdaVision display was a visualization of a brain structure, also constructed from multiple image sources. As we navigated around the image, Bram explained to us what, in his view, is an important advantage of using wall-size displays: You can zoom into details while still maintaining the sense of the whole. In other words, since you continue to see the whole image while examining the details, you have the sense of context in which each detail fits.

SARA’s demo showed me one effect of scaling up existing imaging technologies – in this case, scaling up the size of an image and the size of a display. The same hi-res image presented on a wall-sized display functions in a new way. Although factual information in it does not change, we can now experience it and understand it differently. Pragmatically, it becomes a different image containing new knowledge.

## Redefining Existing Media

Consider the medium of digital video. Normal video flattens the world, rendering it prosaic, almost banal. The poetry of film – due to motion blur, depth of focus, and film grain – is replaced by the non-human objectivity of a surveillance viewpoint. 

On opening day of iGRID, I attended an event called “International Real-time Streaming of 4K Digital Cinema.” We were told the facts about what we are about to witness: “Live and pre-recorded 4K content, with four times the resolution of HDTV, is compressed using JPEG 2000 at 200-400Mb and streamed in real time via 1Gb IP networks, from Keio University in Tokyo to iGrid 2005 in San Diego.” In layman terms: digital video – computer animations, dynamic visualizations generated in real-time, digitally scanned film as well as a real-time teleconferencing session – all with the resolution of 4000 x 2000 pixels is being streamed from Tokyo to San Diego where it is projected using a 4000-line projector.

What I see has nothing to do visually with what I normally experience as streaming video. In fact, these moving images are unlike anything I’ve ever seen. Forget about the usual streaming artifacts – everything is perfect. The images contain much more detail than you can see with natural sight or capture with a film camera. Everything is in focus; the level of detail and sharpness can be compared to high-quality, large-format still photography. But this is not a print from a 4x5 color negative shot using a long exposure. What I am seeing, along with the stunned audience, is being captured in real time by a digital video camera in Tokyo, compressed, sent across the ocean, decompressed, then projected on a large screen in San Diego.

Watching the short film by a Japanese director that is beginning to explore the aesthetic possibilities of 4K digital video in relation to lighting, composition, and narrative, I was wondering if the pristine, super-clear, and poetic images of 4K digital video can be related to any visual tradition in the past. Surprisingly, if normal video flattens the world, rendering it prosaic and even banal, 4K digital video creates the opposite effect: Even most prosaic objects and boring, flat surfaces acquire a precious quality as the light captured and reflected by their micro-textures is rendered visible. The effect is as though seeing the world for the first time, after it has been washed clean by the rain. The comparison that comes to mind is with Dutch 17th-century paintings: portraits, still lives, and interiors. As analyzed by art historian Svetlana Alpers in her influential book _The Art of Describing_, in contrast to Italian Renaissance painters who recreated in their paintings soft Italian light that hides details and softens shapes, their Dutch counterparts delighted in presenting every detail and carefully rendering different surfaces, textures, and light effects. In the right hands, 4K digital video appears to be capable of creating a similar representation of the world. It achieves the poetic effect not by hiding the details in shadows or fog but rather by presenting them all – and letting our eyes delight in comparing different patterns and textures.

## Art after Compression

Here is another example of scaling – or rather, a new condition of media culture due to the massive scaling up of computer speed, memory, storage and at the same time continuing decrees in price of PCs, cameras, hard drives, bandwidth, etc. 

I will discuss in detail how artists, designers, filmmakers, and computer scientists already are taking advantage of this condition – in my lecture at PZI on November 15. To conclude this presentation today, let me simply summarize the initial idea.

Human art can be thought of as a form of compression - condensing individual and collective experiences, memory, and knowledge into symbols, icons, short narratives, and images. While there are many reasons for this, even if humans ever wanted to create significantly more detailed representations, the limitations of storage media would not allow this. 

This situation fundamentally changed over the last few years as the developments in IT now make possible for us for the first time to record (and consequently organize and access) as much data as we want. How does IT industry, computer science and engineering research, media design and art are responding to this new “post-compression” condition? What kind of media art can we create today when we can capture the world without any limitations.

In my talk on Nov 15 I will analyze developments across these different areas pointing towards a number of new cultural strategies which already becoming visible. The examples discussed will include Sokurov's film 'Russian Arc,' interactive virtual spaces by Masaki Fujihata, the rise of database art, visual search engines, Microsoft's MyLifeBits project, work on metadata standards, sensor networks.

---

# Introduction to Info-Aesthetics

_author: Lev Manovich_
_year: 2005_

To explain what I mean by _info-aesthetics_, let me start by noting something simple but nevertheless quite significant: the word “information” contains within it the word “form”. For a while now social theorists, economists, and politicians were telling us that we are living in a new “information society.” The term was first used already in the 1960s, even before computer revolution got underway. I will discuss in some detail the theories of information society as well as relayed concepts of post-industrial society, knowledge society, and network society – but I want to do this later. Since this book is about the culture of information society, what is important for me –as much as the arguments of economists and sociologists – are the changes in people’s everyday lives. What we do, what objects we use, how we communicate and interact with others and the kind of spaces where we dwell in or pass though - all this is bound to change exciting cultural patterns and aesthetic preferences as well as create new ones. So, if we can observe significant changes in all these dimensions of everyday human experience that are converging around “information,” this by itself is sufficient to justify going out to look at corresponding cultural responses. 

When the term “information society” was first introduced in the 1960s, most people even in America never saw a computer. (In my own case, having grown up in Moscow in the 1970s, I only came face to face with a working computer after I came to New York in 1981.) Of course, a few perceptive artists such as Jean-Luc Godard in his brilliant _Alphaville_ (1965) have already understood that a computer was becoming a new God of our times – but they were exceptions. Even such a visionary as Marshall McLuhan who seems to precisely predict most features of contemporary cyber-culture about three decades before they came into existence, has ignored computers. In _Understanding Media_ (1964), which presents a systematic analysis of all key historical and modern media technologies, McLuhan does devote the very last section to data processing, but in general computation plays no role in his theories. This is so probably because McLuhan was thinking of media as means of communication and/or representation – and in the 1960s computers were not yet involved in any of these functions in a way that would be visible to the public. 

If in the 1960s only a very small number of computer scientists – Joseph Licklider, Douglas Engelbart, Ted Nelson, Alan Kay, and a handful of others – understood that computer was bound to become an _engine of culture_ rather than being only a data processing machine, similarly only a few social scientists were able to perceive that dealing with information was replacing in importance industrial manufacturing. Today, however, what was once an academic hypothesis has become an everyday reality that can be easily observed by the majority of citizens leaving in the developed as well as developing countries. All kinds of work are reduced to manipulating data on one’s computer screen – in short, processing information. As you walk or drive past office buildings in any city, all offices regardless of what a company does look the same: rows of computer screens and keyboards. Regardless of their actual profession, financial analysts, city officials, secretaries, architects, accountants, and pretty much everybody else engaged in white-collar work are doing the same thing: processing information. And when we leave work, we don’t leave information society. In our everyday life, we use search engines; we retrieve data from databases; we rely on “personal information appliances” and “personal information managers.” We complain that there is too much information to keep track or make sense of – while the libraries and museums around the world constantly add to the global information pile by systematically digitizing everything they got. We turn our own lives into an information archive by storing all our emails, chats, SMS, digital photos, GPS tracks, favorite music tracks, favorite television shows, and other “digital traces” of our existence. One day we get tired from all this, so we start planning to take “e-mail free” holidays – but even this requires information work: searching for best deals on the internet, comparing fares, inputting credit card information into a reservation web site. And even on a most activity-free vacation, the moment we open a cell phone to make call or check messages, we enter the world of information. In short, “information society” is where most citizens of the developed and developing world live today, experiencing it in their everyday practice. (And while those living outside this world themselves are not using computers on a daily basis, the companies, NGOs and governments of the developed countries, which play the decisive role in deciding what happens to developing countries, are all of course computerized. Therefore, information processing shapes the lives and fates of citizens of these countries although they themselves may not experience it directly.)

Information processing has become the key dimension of our daily lives – but since we are physical beings, we have always required and continue to require various physical forms in order to house and transport our bodies, our information processing machines and information itself. These forms range from very big (buildings, bridges, airplanes) to very small (iPod, mobile phone), from rarely changing (architecture) to periodically updatable (clothes). If a person needs clothing, a computer needs a case to protect its insides and to allow us to enter and manipulate information in a convenient way (i.e., a human-computer interface – which is typically a keyboard and a screen). Text needs to be displayed in some way for us to be able to read it, be it a screen, paper or e-paper. [1]

In short, we need to design forms for ourselves – and also for information that we create, record, and manipulate. We may have become information-processing species, but we also remain to be form-creating species as well. If for Marx humans separated themselves from other species when they designed first tools for work, we can add that humans became humans by becoming _designers_ – the inventors and makers of forms. 

If information processing is the new defining characteristic of our world, what is then the effect of this on the forms we design today? This is the question that I have been interested in after finishing my previous book _The Language of New Media_ in 1999. While _Info-Aesthetics_ offers a discussion of multiple ways in which work with information shapes forms which we create, its important right away to differentiate between two lines of influence. On the one hand, we may think about how the centrality of dealing with information in our daily lives may affect our aesthetic preferences as manifested in trends in architecture, industrial design, graphic design, media design, cinema, music, fashion, theatre, dance, exhibition design, and other cultural fields. One the other hand, we also need to remember that most forms we encounter today are designed on computers. And this, of course, is likely to have at least as much of an effect on what forms the designers are going to come with. In summary, information processing acts both as a force outside a form, so to speak (i.e., the new habits of perception, behavior, work, and play) – as well as being the very method through which the forms are being designed. 

There is also another fundamental effect which is worth articulating right here in the introduction. In information society the design of forms becomes intricately linked with the concept of interface. First of all, as I already mentioned above, we need to give some visual form to what will appear on the screens of computers, mobile phones, PDAs, car navigation systems, and other devices – as well as to buttons, trackballs, microphones, and various other input tools. Therefore, human-computer interfaces which involve a set of visual conventions such as folders, icons, and menus (i.e., Graphical User Interface), audio conventions (as in voice recognition interface), and particular material articulations (such as the shape, color, material and texture of a mobile phone) – represent the whole new category of forms which need to be designed today. Even more importantly, as computation becomes incorporated in our lived environment (the trend which is described by such terms as “ubiquitous computing,” “pervasive computing,” “ambient intelligence,” “context-aware environments,” “smart objects”) the interfaces slowly leave the realm where they safely lived for a few decades – that is, stand-alone computers and electronics devices – and start appearing in all kinds of objects and on all kinds of surfaces, be it interior walls, furniture, benches, bags, clothing, posters, and so on. [2] Consequently, the forms of all these objects that previously lived “outside of information” have now address the likely presence of interfaces somewhere on them.

This does not mean that from now on “form follows interface” – rather, the two now have to accommodate each other. Beyond the traditional requirements that the material forms had to satisfy – a chair has to be comfortable for sitting, for example – their design is now also shaped by new requirements. For instance, at least so far, we are used to interact with text which is presented on flat and rectangular, and therefore if a screen is to be incorporated somewhere in the object, a part of it needs to be reasonably flat. Which is easy to do if an object is a table but not as easy if it is a piece of clothing or Gerry’s Disney Hall in Los Angeles specifically designed not to have a single flat area. (Of course, as new technologies such as Rapid Manufacturing may soon enable easy printing of an electronic display on any surface of any object while it is being produced, it’s possible that we will be able to quickly adjust our perceptual habits, so moving and change-shaping display surfaces will be accepted much easier than I can imagine. In fact, the computer-controlled graphic projections on the body of dancers as in _Apparition_ by Klaus Obermaier or in _Interactive Opera Stage_ system by Art+Com already show the aesthetic potential of displaying information over a changing non-flat not-rectangular form, i.e. a human body.) [3]

October 18, 5:04pm – 5:33pm. I am looking at the show of student projects from Department of Industrial Design at Eindhoven Technical University in Netherlands. The department is only three years old, so instead of designing traditional objects, students are working on “smart objects.” Every project in show starts with an everyday familiar object and ads some “magical” functions to it via electronics and computers. Which means that I see more examples of solid objects and media/interface surfaces coming together. In one project, a canopy placed diagonally over a child’s hospital in a bed becomes an electronic canvas. By tracking the position of a special pen that does not need to touch the drawing surface, the canvas allows the child to draw on it without having to move from the bed. In another project, a special mirror allows one person to leave a message for somebody else – for instance, a different member of a household. A rectangular block containing a camera is built into a mirror frame. You take the block out, record a video message and place the block back into the frame. After the video is automatically “loaded” into the magical mirror, a small picture appears somewhere on the mirror surface: when you click on the picture it plays a video message. Yet another project adds magical interactivity to a vertical plastic column. The lights inside the column turn it into an ambient light source. The column is covered with a special interface: a net. Depending on how you touch the net, the position, quality, and tint of the light changes. How exactly the light will change is not directly predictable, and this is what makes the interaction with light column fun. 

Together, these three projects show us different ways in an object, an interface and a display can be put together. The first two projects rely on already familiar behaviors – drawing with a pen or making a recording with a video camera. The last one calls for user to develop new vocabulary of movements and gestures to which the light will respond. And the ways in which a “smart object” talks back to us are also different: a canvas canopy shows a drawing, a mirror plays video, and a light glows in different ways. In short, the surface of an object can become both an output and input media, bringing together the physical and the screen-like – form and information - in surprising ways. There is indeed magic to these “smart objects”: we see familiar normally “passive objects literally coming to life and responding to our interactions with them.

The forms that are discussed in this book are not only material ones (as in design and architecture); they are also ways to structure data to make it meaningful and useful for human users by presenting it on some kind of display. So, a cinematic narrative, an interactive information visualization, a Web search engine, User Interface (UI) of Nokia phones, or Spotlight (a new search / file management tool in MAC OS) are also forms, which organize data, be it audio-visual recordings in the case of a film, or documents on a hard drive the case of Spotlight. To distinguish these kinds of forms from the material forms, I will refer to them as “screen forms” – keeping in mind that the actual displays can also include paper (as in illustrations and graphs which appear in journals), as well as augmented reality displays where information is seen superimposed against the real world.

Since this book is about form and information, I will be focusing on the new screen forms that either offer us fundamentally new ways to manage information or respond to the dramatic increase in its quantity. This last fact may appear trivial: we all know that the amount of data being created every year is growing very fast, that every day there are 15,000 new blogs are created [4]; that... All this is familiar and therefore not very interesting; and yet our daily habits of work and entertainment, the ways in which we understand ourselves, others, and the world around us are being deeply reshaped through this pure quantitative growth of information being produced, exchanged, stored, and made available. 

So, this is another reason why I chose the term “information society” over any other term as a context for this study. I believe that the exponential growth of information available to us is one of main pressure points on contemporary culture and this pressure will only continue to increase. The cultural effects of this information glut are diverse, and I investigate some of them in part three of this book. By situating my investigation within the context of “information society” I want to highlight new cultural dimension that so far has not been part of our critical vocabulary: scale. In other words, while normally we think of culture using qualitatively different categories such as authorship, collaboration, reception, media type, ideology, and so on, we also now need to start considering something purely quantitative: _the dramatic increase in the scale of media available_. We no longer deal with “old media” or “new media” – we now have to think through what it means to be living with “more media.”

Some effects of this quantitative change are already visible. Our new standard interface to culture is a search engine. Although we have now completely used to this, imagine if somebody was to tell you in the early 1990s that soon you will be searching first through millions of documents, and only then listening, watching, or reading. A related development is the shift from a single media object that physically existed as an object and was appreciated in isolation – to a sequence or a database of digital media. For instance, rather than fetishizing a particular physical music record or a particular photographic print, we deal with music playlists or catalogs of digital photographs. 

But what do these effects means? Has the increase in scale of available media and new tools and conventions used to access it led to the new aesthetics in works themselves and new patterns of reception? These kinds of question are much harder to answer. I don’t have these answers; instead in the third part of the book I discuss some new cultural practices and even new fields which all address this exponential growth of quantity of information in creative ways. 

I see this growth of information not as a cultural threat but as a cultural opportunity. New cultural strategies are often invented as a response to a real social crisis or simply a perceived change in social order. The industrialization of the nineteenth century provoked a number of creative responses such Art Nouveau and Arts and Crafts movement. World War I and revolutionary fever in Europe led to Constructivism, development of Russian montage school in cinema and photomontage, Surrealism, etc. Today “informationalization” puts pressure on society to invent new ways to interact with information, new ways to make sense of it, and new ways to represent it. Social software such as Wikipedia, work in information visualization and information design such as the projects by Benjamin Fry, exceptional database narratives such as _Bleeding Through: Layers of Los Angeles_ by Norman Klein, Rosemary Camella and Andreas Kratky, and cultural analysis such as _Rhythm Science_ by DJ Spooky are all examples of approaching new information environment creatively. Instead of trying to defend ourselves against information glut, we need to approach this situation as the opportunity to invent new forms appropriate for our world. In short, _we need to invent info-aesthetics_. 

## References:

[1] Therefore, although the word “information” contains the world “form” inside it, in reality it’s the other way around – information always has to be wrapped up in some external form in order to be useful to us.

[2] Takashi Hoshimo reports that “Posters in Japan are being embedded with tag readers that receive signals from the user’s ‘ID’ tag and send relevant information and free products back.” Takashi Hoshimo, “Bloom Time Out East,” _ME: Mobile Entertainment_, November 2005, issue 9, p. 25, [www.mobile-ent.biz](www.mobile-ent.biz))

[3] Apparition. Choreographer: Klaus Obermaier. Production: Ars Electronica Future Lab. Presented at Ars Electronica 2004 festival.

[4] Data for October 2005.

---

# Image Future

_author: Lev Manovich_
_year: 2006_

## Introduction

For the larger part of the twentieth century, different areas of commercial moving image culture maintained their distinct production methods and distinct aesthetics. Films and cartoons were produced completely differently, and it was easy to tell their visual languages apart. Today the situation is different. Softwarization of all areas of moving image production created a common pool of techniques that can be used regardless of whether one is creating motion graphics for television, a narrative feature, an animated feature, or a music video. The abilities to composite many layers of imagery with varied transparency, to place 2D and 3D visual elements within a shared 3D virtual space and then move a virtual camera through this space, to apply simulated motion blur and depth of field effect, to change over time any visual parameter of a frame are equally available to the creators of all forms of moving images.

The existence of this common vocabulary of software-based techniques does not mean that all films now look the same. What it means, however, is that while most live action films, animated features and motion graphics do look quite distinct today, this is the result of a deliberate choices rather than the inevitable consequence of differences in production methods and technology. 

Given that all techniques of previously distinct media are now available within a single software-based production environment, what is the meaning of the terms that were used to refer to these media in the twentieth century – such as “animation”? From the industry point of view, the answer is simple. Animation not only continues to exist as a distinct area of media industry but it’s also very successful – its success in no small part fueled by new efficiency of software-based global production workflow. 2D and 3D animated features, shorts and series are produced today in larger numbers than ever before; students can pursue careers in “animation”; Japanese anime and animated features continue to grow in popularity; China is building whole cities around mega-size animation and rendering studios and production facilities. 

Certainly, the aesthetics of many contemporary feature-length 3D animated features largely relies on the visual language of twentieth-century commercial animation. So, while everything may be modeled and animated in 3D computer animation program, the appearance of the characters, their movements, and the staging of scenes conceptually owe more to mid-20th century Disney than to 21st century Autodesk (producer of industry-standard Maya software). Similarly, hybrid looking short-form films (exemplified by but not limited to “motion graphics”) also often feature sequences or layers that look very much like character animation we know from the 20th century. 

The examples above illustrate just one, more obvious, role of animation in contemporary post-digital visual landscape. In this chapter I will explore its other role: as a generalized tool set that can be applied to any images, including film and video. Here, animation functions not as a medium but as a set of general-purpose techniques – used together with other techniques in the common pool of options available to a filmmaker/designer. Put differently, what has been “animation” has become a part of the computer metamedium.

I have chosen a particular example for my discussion that I think will illustrate well this new role of animation. It is an especially intricate method of combining live action and CG (a common abbreviation for “computer graphics.”) Called “Universal Capture” (U-cap) by their creators, it was first systematically used on a large scale by ESC Entertainment in _Matrix 2_ and _Matrix 3_ films from _The Matrix_ trilogy. I will discuss how this method is different from the now standard and older techniques of integrating live action and computer graphics elements. The use of Universal Capture also leads to visual hybrids – but they are quite distinct from the hybrids found in motion graphics and other short-form moving image productions being created today. With Universal Capture, different types of imagery are “fused” together to create a new kind of image. This image combines “the best of” qualities of two types of imagery that we normally understand as being ontologically the opposites: live action recording and 3D computer animation. I will suggest that such image hybrids are likely to play a large role in future visual culture while the place of “pure” images that are not fused or mixed with anything is likely to gradually diminish. 

## Uneven Development

What kinds of images would dominate visual culture a number of decades from now? Would they still be similar to the typical images that surround us today – photographs that are digitally manipulated and often combined with various graphical elements and type? Or would future images be completely different? Would photographic code fade away in favor of something else? 

There are good reasons to assume that the future images would be photograph-like. Like a virus, a photograph turned out to be an incredibly resilient representational code: it survived waves of technological change, including computerization of all stages of cultural production and distribution. One of the reason for this persistence of photographic code lies in its flexibility: photographs can be easily mixed with all other visual forms - drawings, 2D and 3D designs, line diagrams, and type. As a result, while photographs continue to dominate contemporary visual culture, most of them are not pure photographs but various mutations and hybrids: photographs which went through different filters and manual adjustments to achieve a more stylized look, a more flat graphic look, more saturated color, etc.; photographs mixed with design and type elements; photographs which are not limited to the part of the spectrum visible to a human eye (night vision, x-ray); simulated photographs created with 3D computer graphics; and so on. Therefore, while we can say that today we live in a “photographic culture,” we also need to start reading the word “photographic” in a new way. “Photographic” today is really photo-GRAPHIC, the photo providing only an initial layer for the overall graphical mix. (In the area of moving images, the term “motion graphics” captures perfectly the same development: the subordination of live action cinematography to the graphic code.)

One way in which change happens in nature, society, and culture is inside out. The internal structure changes first, and this change affects the visible skin only later. For instance, according to Marxist theory of historical development, infrastructure (i.e., mode of production in a given society – also called “base”) changes well before superstructure (i.e., ideology and culture in this society). To use a different example, think of the history of technology in the twentieth century. Typically, a new type of machine was at first fitted within old, familiar skin: for instance, early twentieth century cars emulated the form of horse carriage. The popular idea usually ascribed to Marshall McLuhan – that the new media first emulates old media – is another example of this type of change. In this case, a new mode of media production, so to speak, is first used to support old structure of media organization, before the new structure emerges. For instance, first typeset book was designed to emulate hand-written books; cinema first emulated theatre; and so on.

This concept of uneven development can be useful in thinking about the changes in contemporary visual culture. Since this process started in the middle of the 1950s, computerization of photography (and cinematography) has by now completely changed the internal structure of a photographic image. Yet its “skin,” i.e., the way a typical photograph looks, still largely remains the same. It is therefore possible that at some point in the future the “skin” of a photographic image would also become completely different, but this did not happen yet. So, we can say at present our visual culture is characterized by a new computer “base” and old photographic “superstructure.”

_The Matrix_ films provide us with a very rich set of examples perfect for thinking further about these issues. The trilogy is an allegory about how its visual universe is constructed. That is, the films tell us about the Matrix, a virtual universe that is maintained by computers – and of course, visually the images of _The Matrix_ trilogy that we the viewers see in the films were all indeed assembled using help software. (The animators sometimes used Maya but mostly relied on custom written programs). So, there is a perfect symmetry between us, the viewers of a film, and the people who live inside the Matrix – except while the computers running the Matrix are capable of doing it in real time, most scenes in each of _The Matrix_ films took months and even years to put together. (So _The Matrix_ can be also interpreted as the futuristic vision of computer games in the future when it would become possible to render _The Matrix_-style visual effects in real time.)

The key to the visual universe of _The Matrix_ is the new set of computer graphic techniques that over the years were developed by Paul Debevec, Georgi Borshukov, John Gaeta, and a number of other people both in academia and in the special effects industry. [1] Their inventors coined a number of names for these techniques: “virtual cinema,” “virtual human,” “virtual cinematography,” “universal capture.” Together, these techniques represent a true milestone in the history of computer-driven special effects. They take to their logical conclusion the developments of the 1990s such as motion capture, and simultaneously open a new stage. We can say that with _The Matrix_, the old “base” of photography has finally been completely replaced by a new computer-driven one. What remains to be seen is how the “superstructure” of a photographic image – what it represents and how – will change to accommodate this “base.”

## Reality Simulation versus Reality Sampling

Before proceeding, I should note that not all of special effects in _The Matrix_ rely on Universal Capture. Also, since _The Matrix_, other Hollywood films and video games (EA SPORT Tiger Woods 2007) already used some of the same strategies. However, in this chapter I decided to focus on the use of this process in the second and third films of _The Matrix_ for which the method of Universal Capture was originally developed. And while the complete credits for everybody involved in developing Universal Capture would run for a whole page, here I will identify it with Gaeta. The reason is not because, as a senior special effects supervisor for _The Matrix Reloaded_ and _The Matrix Revolutions_ he got most publicity. More importantly, in contrast to many others in the special effects industry, Gaeta has extensively reflected on the techniques he and his colleagues have developed, presenting it as a new paradigm for cinema and entertainment and coining useful terms and concepts for understanding it. 

In order to understand better the significance of Gaeta’s method, lets briefly run through the history of 3D photo-realistic image synthesis and its use in the film industry. In 1963 Lawrence G. Roberts (who later in the 1960s became one of the key people behind the development of Arpanet but at that time was a graduate student at MIT) published a description of a computer algorithm to construct images in linear perspective. These images represented the objects’ edges as lines; in contemporary language of computer graphics they would be called “wire frames.” Approximately ten years later computer scientists designed algorithms that allowed for the creation of shaded images (so-called Gouraud shading and Phong shading, named after the computer scientists who create the corresponding algorithms). From the middle of the 1970s to the end of the 1980s the field of 3D computer graphics went through rapid development. Every year new fundamental techniques were created: transparency, shadows, image mapping, bump texturing, particle system, compositing, ray tracing, radiosity, and so on. [2] By the end of this creative and fruitful period in the history of the field, it was possible to use combination of these techniques to synthesize images of almost every subject that often were not easily distinguishable from traditional cinematography. (“Almost” is important here since the creation of photorealistic moving images of human faces remained a hard to reach a goal – and this is in part what Total Capture method was designed to address.) 

All this research was based on one fundamental assumption: in order to re-create an image of visible reality identical to the one captured by a film camera, we need to systematically simulate the actual physics involved in construction of this image. This means simulating the complex interactions between light sources, the properties of different materials (cloth, metal, glass, etc.), and the properties of physical film cameras, including all their limitations such as depth of field and motion blur. Since it was obvious to computer scientists that if they exactly simulate all this physics, a computer would take forever to calculate even a single image, they put their energy in inventing various short cuts which would create sufficiently realistic images while involving fewer calculation steps. So, in fact each of the techniques for image synthesis I mentioned in the previous paragraph is one such “hack” – a particular approximation of a particular subset of all possible interactions between light sources, materials, and cameras. 

This assumption also meant that you are re-creating reality step-by-step starting from a blank canvas (or, more precisely, an empty 3D space.) Every time you want to make a still image or an animation of some object or a scene, the story of creation from The Bible is being replayed.

(I imagine God creating universe by going through the numerous menus of a professional 3D modeling, animation, and rendering program such as Maya. First, he has to make all the geometry: manipulating splines, extruding contours, adding bevels… Next, for every object and creature he has to choose the material properties: specular color, transparency level, image, bump and reflexion maps, and so on. He finishes one set of parameters, wipes his forehead, and starts working on the next set. Now on defining the lights: again, dozens of menu options need to be selected. He renders the scene, looks at the result, and admires his creation. But he is far from being done: the universe he has in mind is not a still image but an animation, which means that the water has to flow, the grass and leaves have to move under the blow of the wind, and all the creatures also have to move. He sights and opens another set of menus where he has to define the parameters of algorithms that simulate the physics of motion. And on, and on, and on. Finally, the world itself is finished and it looks good; but now God wants to create the Man so he can admire his creation. God sighs again, and takes from the shelf a particular Maya manual from the complete set which occupies the whole shelf…)

Of course, we are in somewhat better position than God was. He was creating everything for the first time, so he could not borrow things from anywhere. Therefore, everything had to be built and defined from scratch. But we are not creating a new universe but instead visually simulating a universe that already exists, i.e., physical reality. Therefore, computer scientists working on 3D computer graphics techniques have realized early on that in addition to approximating the physics involved they can also sometimes take another shortcut. Instead of defining something from scratch through the algorithms, they can simply _sample_ it from existing reality and incorporate these samples in the construction process. 

The examples of the application of this idea are the techniques of texture mapping and bump mapping which were introduced already in the second part of the 1970s. With texture mapping, any 2D digital image – which can be a close-up of some texture such as wood grain or bricks, but which can be also anything else, for instance a logo, a photograph of a face or of clouds – is wrapped around a 3D model. This is a very effective way to add visual richness of a real world to a virtual scene. Bump texturing works similarly, but in this case the 2D image is used as a way to quickly add complexity to the geometry itself. For instance, instead of having to manually model all the little cracks and indentations which make up the 3D texture of a concrete wall, an artist can simply take a photograph of an existing wall, convert into a grayscale image, and then feed this image to the rendering algorithm. The algorithm treats grayscale image as a depth map, i.e., the value of every pixel is being interpreted as relative height of the surface. So, in this example, light pixels become points on the wall that are a little in front while dark pixels become points that are a little behind. The result is enormous saving in the amount of time necessary to recreate a particular but very important aspect of our physical reality: a slight and usually regular 3D texture found in most natural and many human-made surfaces, from the bark of a tree to a weaved cloth. 

Other 3D computer graphics techniques based on the idea of sampling existing reality include reflection mapping and 3D digitizing. Despite the fact that all these techniques have been always widely used as soon as they were invented, many people in the computer graphics field always felt that they were cheating. Why? I think this feeling was there because the overall conceptual paradigm for creating photorealistic computer graphics was to simulate everything from scratch through algorithms. So, if you had to use the techniques based on directly sampling reality, you somehow felt that this was just temporary - because the appropriate algorithms were not yet developed or because the machines were too slow. You also had this feeling because once you started to manually sample reality and then tried to include these samples in your perfect algorithmically defined image, things rarely would fit exactly right, and painstaking manual adjustments were required. For instance, texture mapping would work perfectly if applied to a flat surface, but if the surface were curved, inevitable distortion would occur. 

Throughout the 1970s and 1980s the “reality simulation” paradigm and “reality sampling” paradigms co-existed side-by-side. More precisely, as I suggested above, sampling paradigm was “imbedded” within reality simulation paradigm. It was a common sense that the right way to create photorealistic images of reality is by simulating its physics as precisely as one could. Sampling existing reality and then adding these samples to a virtual scene was a trick, a shortcut within over wise honest game of mathematically simulating reality in a computer. 

## Building _The Matrix_

So far, we looked at the paradigms of 3D computer graphics field without considering the uses of the 3D images? So, what happens if you want to incorporate photorealistic images produced with CG into a film? This introduces a new constraint. Not only every simulated image has to be consistent internally, with the cast shadows corresponding to the light sources, and so on, but now it also has to be consistent with the cinematography of a film. The simulated universe and live action universe have to match perfectly (I am talking here about the “normal” use of computer graphics in narrative films and not the hybrid aesthetics of TV graphics, music videos, etc. which deliberately juxtaposes different visual codes). As can be seen in retrospect, this new constraint eventually changed the relationship between the two paradigms in favor of sampling paradigm. But this is only visible now, after films such as _The Matrix_ made the sampling paradigm the basis of its visual universe. [3]

At first, when filmmakers started to incorporate synthetic 3D images in films, this did not have any effect on how computer scientists thought about computer graphics. 3D computer graphics for the first time briefly appeared in a feature film in 1980 (_Looker_). Throughout the 1980s, a number of films were made which used computer images but always only as a small element within the overall film narrative. (One exception was _Tron_; released in 1982, it can be compared to _The Matrix_ since its narrative universe is situated inside computer and created through computer graphics – but this was an exception.) For instance, one of _Star Trek_ films contained a scene of a planet coming to life; it was created using CG. (In fact, now commonly used “particle system” was invented for to crate this effect.) But this was a single scene, and it had no interaction with all other scenes in the film. 

In the early 1990s the situation has started to change. With pioneering films such as _The Abyss_ (James Cameron, 1989), _Terminator 2_ (James Cameron, 1991), and _Jurassic Park_ (Steven Spielberg, 1993), computer generated characters became the key protagonists of feature films. This meant that they would appear in dozens or even hundreds of shots throughout a film, and that in most of these shots computer characters would have to be integrated with real environments and human actors captured via live action photography (such shots are called in the business “live plates.”) Examples are the T-100 cyborg character in _Terminator 2: Judgment Day_, or dinosaurs in _Jurassic Park_. These computer-generated characters are situated inside the live action universe that is the result of capturing physical reality via the lens of a film camera. The simulated world is located inside the captured world, and the two have to match perfectly. 

As I pointed out in _The Language of New Media_ in the discussion of compositing, perfectly aligning elements that come from different sources is one of fundamental challenges of computer-based realism. Throughout the 1990s filmmakers and special effects artists have dealt with this challenge using a variety of techniques and methods. What Gaeta realized earlier than others is that the best way to align the two universes of live action and 3D computer graphics was to build _a single new universe_. [4]

Rather than treating sampling reality as just one technique to be used along with many other “proper” algorithmic techniques of image synthesis, Gaeta and his colleagues turned it into the key foundation of Universal Capture process. The process systematically takes physical reality apart and then systematically reassembles the elements together to create a new software-based representation. The result is a new kind of image that has photographic / cinematographic appearance and level of detail yet internally is structured in a completely different way.

Universal Capture was developed and refined over a three-year period from 2000 to 2003. [5] How does the process work? There are actually more stages and details involved, but the basic procedure is the following. [6] An actor’s performance is recorded using five synchronized high-resolution video cameras. “Performance” in this case includes everything an actor will say in a film and all possible facial expressions. [7] (During the production the studio was capturing over 5 terabytes of data each day.) Next special algorithms are used to track each pixel’s movement over time at every frame. This information is combined with a 3D model of a neutral expression of the actor captured via a 3D scanner. The result is an animated 3D shape that accurately represents the geometry of the actor’s head as it changes during a particular performance. The shape is mapped with color information extracted from the captured video sequences. A separate very high-resolution scan of the actor’s face is used to create the map of small-scale surface details like pores and wrinkles, and this map is also added to the model. (How is that for hybridity?)

After all the data has been extracted, aligned, and combine, the result is what Gaeta calls a “virtual human” - a highly accurate reconstruction of the captured performance, now available as a 3D computer graphics data – with all the advantages that come from having such representation. For instance, because actor’s performance now exists as a 3D object in virtual space, the filmmaker can animate virtual camera and “play” the reconstructed performance from an arbitrary angle. Similarly, the virtual head can be also lighted in any way desirable. It can be also attached to a separately constructed CG body. [8] For example, all the characters which appeared the Burly Brawl scene in _The Matrix_ 2 were created by combining the heads constructed via Universal Capture done on the leading actors with CG bodies which used motion capture data from a different set of performers. Because all the characters along with the set were computer generated, this allowed the directors of the scene to choreograph the virtual camera, having it fly around the scene in a way not possible with real cameras on a real physical set.

The process was appropriately named Total Capture because it captures all the possible information from an object or a scene using a number of recording methods – or at least, whatever is possible to capture using current technologies. Different dimensions – color, 3D geometry, reflectivity and texture – are captured separately and then put back together to create a more detailed and realistic representation.

Total Capture is significantly different from the commonly accepted methods used to create computer-based special effects such as keyframe animation and physically based modeling. In the first method, an animator specifies the key positions of a 3D model, and the computer calculates in-between frames. With the second method, all the animation is automatically created by software that simulates the physics underlying the movement. (This method thus represents a particular instance of “reality simulation” paradigm.) For instance, to create a realistic animation of moving creature, the programmers model its skeleton, muscles, and skin, and specify the algorithms that simulate the actual physics involved. Often the two methods are combined: for instance, physically based modeling can be used to animate a running dinosaur while manual animation can be used for shots where the dinosaur interacts with human characters. 

When the third _Matrix_ film was being released, the most impressive achievement in physically based modeling was the battle in _The Lord of the Rings: Return of the King_ (Peter Jackson, 2003) which involved tens of thousands of virtual soldiers all driven by Massive software. [9] Similar to the Non-human Players (or bots) in computer games, each virtual soldier was given the ability to “see” the terrain and other soldiers, a set of priorities and an independent “brain,” i.e., a AI program which directs character’s actions based on the perceptual inputs and priorities. But in contrast to games AI, Massive software does not have to run in real time. Therefore, it can create the scenes with tens and even hundreds of thousands realistically behaving agents (one commercial created with the help of Massive software featured 146,000 virtual characters).

Universal Capture method uses neither manual animation nor simulation of the underlying physics. Instead, it directly samples physical reality, including color, texture, and the movement of the actors. Short sequences of an actor’s performances are encoded as 3D computer animations; these animations form a library from which the filmmakers can then draw as they compose a scene. The analogy with musical sampling is obvious here. As Gaeta pointed out, his team never used manual animation to try to tweak the motion of character’s face; however, just as a musician may do it, they would often “hold” particular expression before going to the next one. [10] This suggests another analogy – analog video editing. But this is a second-degree editing, so to speak: instead of simply capturing segments of reality on video and then joining them together, Gaeta’s method produces complete virtual recreations of particular phenomena – self-contained micro-worlds – which can be then further edited and embedded within a larger 3D simulated space. 

## Animation as an Idea

The brief overview of the methods of computer graphics that I presented above in order to explain Universal Capture offers good examples of the multiplicity of ways in which animation is used in contemporary moving image culture. If we consider this multiplicity, it is possible to come to a conclusion that “animation” as a separate medium in fact hardly exists anymore. At the same time, the general principles and techniques of putting objects and images into motion developed in nineteenth and twentieth century animation are used much more frequently now than before computerization. But they are hardly ever used by themselves – usually they are combined with other techniques drawn from live action cinematography and computer graphics. 

So where does animation start and end today? When you see a Disney or Pixar animated feature, or many graphics shorts it is obvious that you are seeing “animation.” Regardless of whether the process involves drawing images by hand or using 3D software, the principle is the same: somebody created the drawings or 3D objects, set keyframes and then created in-between positions. (Of course, in the course of commercial films, this is not one person but large teams.) The objects can be created in multiple ways and inbetweening can be done manually or automatically by the software, but this does not change the basic logic. The movement, or any other change over time, is defined manually – usually via keyframes (but not always). In retrospect, the definition of movement via keys probably was the essence of twentieth century animation. It was used in traditional cell animation by Disney and others, for stop motion animation by Starevich and Trnka, for the 3D animated shorts by Pixar, and it continues to be used today in animated features that combine traditional cell method and 3D computer animation. And while experimental animators such as Norman McLaren refused keys / in-betweens system in favor of drawing each frame on film by hand without explicitly defining the keys, this did not change the overall logic: the movement was created by hand. Not surprisingly, most animation artists exploited this key feature of animation in different ways, turning it into aesthetics: for instance, exaggerated squash and stretch in Disney, or the discontinuous jumps between frames in McLaren. 

What about other ways in which images and objects can be set into motion? Consider for example the methods developed in computer graphics: physically based modeling, particle systems, formal grammars, artificial life, and behavioral animation. In all these methods, the animator does not directly create the movement. Instead, it is created by the software that uses some kind of mathematical model. For instance, in the case of physically based modeling the animator may sets the parameters of a computer model which simulates a physical force such as wind which will deform a piece of cloth over a number of frames. Or she may instruct the ball to drop on the floor, and let the physics model control how the ball will bounce after it hits the floor. In the case of particle systems used to model everything from fireworks, explosions, water and gas to animal flocks and swarms, the animator only has to define initial conditions: a number of particles, their speed, their lifespan, etc.

In contrast to live action cinema, these computer graphics methods do not capture real physical movement. Does it mean that they belong to animation? If we accept that the defining feature of traditional animation was manual creation of movement, the answer will be no. But things are not so simple. With all these methods, the animator sets the initial parameters, runs the model, adjusts the parameters, and repeats this production loop until she is satisfied with the result. So, while the actual movement is produced not by hand by a mathematical model, the animator maintains significant control. In a way, the animator acts as a film director – only in this case she is directing not the actors but the computer model until it produces a satisfactory performance. Or we can also compare her to a film editor who is selecting among best performances of the computer model.

James Blinn, a computer scientist responsible for creating many fundamental techniques of computer graphics, once made an interesting analogy to explain the difference between manual keyframing method and physically based modeling. [11] He told the audience at a SIGGRAPH panel that the difference between the two methods is analogous to the difference between painting and photography. In Blinn’s terms, an animator who creates movement by manually defining keyframes and drawing in-between frames is like a painter who is observing the world and then making a painting of it. The resemblance between a painting and the world depends on painter’s skills, imagination, and intentions. Whereas an animator who uses physically based modeling is like a photographer who captures the world as it actually is. Blinn wanted to emphasize that mathematical techniques can create a realistic simulation of movement in the physical world and an animator only has to capture what is created by the simulation. 

Although this analogy is useful, I think it is not completely accurate. Obviously, the traditional photographer whom Blinn had in mind (i.e., before Photoshop) chooses composition, contrast, depth of field, and many other parameters. Similarly, an animator who is using physically based modeling also has control over a large number of parameters and it depends on her skills and perseverance to make the model produce a satisfying animation. Consider the following example from the related area of software art that uses some of the same mathematical methods. Casey Reas, an artist who is well-known both for his own still images and animations and for Processing graphics programming environment he helped to develop, told me that he may spend only a couple of hours writing a software program to create a new work – and then another two years working with the different parameters of the same program and producing endless test images until he is satisfied with the results. [12] So while at first physically based modeling appears to be opposite of traditional animation in that the movement is created by a computer, in fact it should be understood as a hybrid between animation and computer simulation. While the animator no longer directly draws each phase of movement, she is working with the parameters of the mathematical model that “draws” the actual movement. 

And what about Universal Capture method as used in _The Matrix_? Gaeta and his colleagues also banished keyframing animation – but they did not used any mathematical modes to automatically generate motion either. As we saw, their solution was to capture the actual performances of an actor (i.e., movements of actor’s face), and then reconstruct it as a 3D sequence. Together, these reconstructed sequences form a library of facial expressions. The filmmaker can then draw from this library, editing together a sequence of expressions (but not interfering with any parameters of separate sequences). It is important to stress that a 3D model has no muscles, or other controls traditionally used in animating computer graphics faces - it is used “as is.” 

Just as it is the case when animator employs mathematical models, this method avoids drawing individual movements by hand. And yet, its logic is that of animation rather than of cinema. The filmmaker chooses individual sequences of actors’ performances, edits them, blends them if necessary, and places them in a particular order to create a scene. In short, the scene is actually constructed by hand even though its components are not. So, while in traditional animation the animator draws each frame to create a short sequence (for instance, a character turning his head), here the filmmaker “draws” on a higher level: manipulating whole sequences as opposed to their individual frames. 

To create final movie scenes, Universal Capture is combined with Virtual Cinematography: staging the lighting, the positions and movement of a virtual camera that is “filming” the virtual performances. What makes this Virtual Cinematography as opposed to simply “computer animation” as we already know it? The reason is that the world as seen by a virtual camera is different from a normal world of computer graphics. It consists of  reconstructions of the actual set and the actual performers created via Universal Capture. The aim is to avoid manual processes usually used to create 3D models and sets. Instead, the data about the physical world is captured and then used to create a precise virtual replica.

Ultimately, ESC’s production method as used in _The Matrix_ is neither “pure” animation, nor cinematography, nor traditional special effects, nor traditional CG. Instead, it is “pure” example of hybridity in general, and “deep remixability” in particular. With its complex blend of the variety of media techniques and media formats, it is also typical of moving image culture today. When the techniques drawn from these different media traditions are brought together in a software environment, the result is not a sum of separate components but a variety of hybrid methods - such as Universal Capture. As I already noted more than once, I think that this how different moving image techniques function now in general. After computerization virtualized them – “extracting” them from their particular physical media to turn into algorithms – they start interacting and creating hybrids. While we have already encountered various examples of hybrid techniques, Total Capture and Virtual Cinematography illustrate how creative industries today develop whole production workflow based on hybridity.

It is worthwhile here to quote Gaeta who himself is very clear that what he and his colleagues have created is a new hybrid. In 2004 interview, he says: “If I had to define virtual cinema, I would say it is somewhere between a live-action film and a computer-generated animated film. It is computer generated, but it is derived from real world people, places, and things.” [13] Although Universal Capture offers a particularly striking example of such “somewhere between,” most forms of moving image created today are similarly “somewhere between,” with animation being one of the coordinate axes of this new space of hybridity. 

## “Universal Capture”: Reality Re-assembled

The method which came to be called “Universal Capture” combines the best of two worlds: visible reality as captured by lens-based cameras, and synthetic 3D computer graphics. While it is possible to recreate the richness of the visible world through manual painting and animation, as well as through various computer graphics techniques (texture mapping, bump mapping, physical modeling, etc.), it is expensive in terms of labor involved. Even with physically based modeling techniques endless parameters have to be tweaked before the animation looks right. In contrast, capturing visible reality via lens-based recording (the process which in the twentieth century was called “filming”) is cheap: just point the camera and press “record” button.

The disadvantage of such lens-based recordings is that they lack flexibility demanded by contemporary remix culture. _Remix culture demands not self-contained aesthetic objects or self-contained records of reality but smaller units - parts that can be easily changed and combined with other parts in endless combinations_. However, lens-based recording process flattens the semantic structure of reality. Instead of a set of unique objects which occupy distinct areas of a 3D physical space, we end up with a flat field of made from pixels (or film grains in the case of film-based capture) that do not carry any information of where they came from, i.e., which objects they correspond to. Therefore, any kind of spatial editing operation – deleting objects, adding new ones, compositing, etc. – becomes quite difficult. Before anything can be done with an object in the image, it has to be manually separated from the rest of the image by creating a mask. And unless an image shows an object that is properly lighted and shot against a special blue or green background, it is practically impossible to mask the object precisely.

In contrast, 3D computer generated worlds have the exact flexibility one would expect from media in information age. (It is not therefore accidental that 3D computer graphics representation – along with hypertext and other new computer-based data representation methods – was conceptualized in the same decade when the transformation of advanced industrialized societies into information societies became visible.) In a 3D computer generated worlds everything is discrete. The world consists of  a number of separate objects. Objects are defined by points described by their coordinates in a 3D space; other properties of objects such as color, transparency and reflectivity are similarly described in terms of discrete numbers. As a result, while a 3D CG representation may not have the richness of a lens-based recording, it does contain a semantic structure of the world. This structure is easily accessible at any time. A designer can directly select any object (or any object part) in the scene. Thus, to duplicate an object hundred times requires only a few mouse clicks or typing a short command; similarly, all other properties of a world can be always easily changed. And since each object itself consists of  discrete components (flat polygons or surface patches defined by splines), it is equally easy to change its 3D form by selecting and manipulating its components. In addition, just as a sequence of genes contains the code that is expanded into a complex organism, a compact description of a 3D world that contains only the coordinates of the objects can be quickly transmitted through the network, with the client computer reconstructing the full world (this is how online multi-player computer games and simulators work).

Universal Capture brings together the complementary advantages of lens-based capture and CG representation in an ingenious way. Beginning in the late 1970s when James Blinn introduced CG technique of texture mapping [14], computer scientists, designers and animators were gradually expanding the range of information that can be recorded in the real world and then incorporated into a computer model. Until the early 1990s this information mostly involved the appearance of the objects: color, texture, light effects. The next significant step was the development of motion capture. During the first half of the 1990s it was quickly adopted in the movie and game industries. Now computer synthesized worlds relied not only on sampling the visual appearance of the real world but also on sampling of movements of animals and humans in this world. Building on all these techniques, Gaeta’s method takes them to a new stage: capturing just about everything that at present can be captured and then reassembling the samples to create a digital - and thus completely malleable - recreation. Put in a larger context, the resulting 2D / 3D hybrid representation perfectly fits with the most progressive trends in contemporary culture which are all based on the idea of a hybrid. 

## The New Hybrid

It is my strong feeling that the emerging “information aesthetics” (i.e., the new cultural features specific to information society) already has or will have a very different logic from what modernism. The later was driven by a strong desire to erase the old - visible as much in the avant-garde artists’ (particularly the futurists) statements that museums should be burned, as well as in the dramatic destruction of all social and spiritual realities of many people in Russia after the 1917 revolution, and in other countries after they became Soviet satellites after 1945. Culturally and ideologically, modernists wanted to start with “tabula rasa,” radically distancing them from the past. It was only in the 1960s that this move started to feel inappropriate, as manifested both in loosening of ideology in communist countries and the beginnings of new post-modern sensibility in the West. To quote the title of a famous book by Robert Venturi, Denise Scott Brown, and Steven Izenour (published in 1972, it was the first systematic manifestation of new sensibility), _Learning from Las Vegas_ meant admitting that organically developing vernacular cultures involves bricolage and hybridity, rather than purity seen for instance in “international style” which was still practiced by architects world-wide at that time. Driven less by the desire to imitate vernacular cultures and more by the new availability of previous cultural artifacts stored on magnetic and soon digital media, in the 1980s commercial culture in the West systematically replaced purity by stylistic heterogeneity. Finally, when Soviet Empire collapsed, post-modernism has won world over. 

Today we have a very real danger of being imprisoned by new “international style” - something which we can call the new “global style” The cultural globalization, of which cheap airline flights, the web, and billions of mobile phones are two most visible carriers, erases some dimensions of the cultural specificity with the energy and speed impossible for modernism. Yet we also witness today a different logic at work: the desire to creatively place together old and new – local and transnational - in various combinations. It is this logic, for instance, which made cities such as Barcelona (where I talked with John Gaeta in the context of Art Futura 2003 festival which led to this article), such a “hip” and “in” place at the turn of the century (that is, 20th to 21st). All over Barcelona, architectural styles of many past centuries co-exist with new “cool” spaces of bars, lounges, hotels, new museums, and so on. Medieval meets multi-national, Gaudi meets Dolce and Gabbana, Mediterranean time meets global time. The result is the invigorating sense of energy which one feels physically just walking along the street. It is this hybrid energy, which characterizes in my view the most interesting cultural phenomena today. [15] The hybrid 2D / 3D image of _The Matrix_ is one such hybrids.

The historians of cinema often draw a contrast between the Lumières and Marey. Along with a number of inventors in other countries all working independently from each other, the Lumières created what we now know as cinema with its visual effect of continuous motion based on the perceptual synthesis of discrete images. Earlier Muybridge already developed a way to take successive photographs of a moving object such as horse; eventually the Lumières and others figured out how to take enough samples so when projected they perceptually fuse into continuous motion. Being a scientist, Marey was driven by an opposite desire: not to create a seamless illusion of the visible world but rather to be able to understand its structure by keeping subsequent samples discrete. Since he wanted to be able to easily compare these samples, he perfected a method where the subsequent images of moving objects were superimposed within a single image, thus making the changes clearly visible. 

The hybrid image of _The Matrix_ in some ways can be understand as the synthesis of these two approaches which for a hundred years ago remained in opposition. Like the Lumières, Gaeta’s goal is to create a seamless illusion of continuous motion. At the same time, like Marey, he also wants to be able to edit and sequence the individual recordings of reality. 

In the beginning of this chapter, I evoked the notion of uneven development, pointing that often the structure inside (“infrastructure”) completely changes before the surface (“superstructure”) catches up. What does this idea imply for the future of images and in particular 2D / 3D hybrids as developed by Gaeta and others? As Gaeta pointed out in 2003, while his method can be used to make all kinds of images, so far it was used in the service of realism as it is defined in cinema – i.e., anything the viewer will see has to obey the laws of physics. [16] So in the case of _The Matrix,_ its images still have traditional “realistic” appearance while internally they are structured in a completely new way. In short, we see the old “superstructure” which stills sits on top of “old” infrastructure. What kinds of images would we see then the superstructure” would finally catch up with the infrastructure? 

Of course, while the images of Hollywood special effects movies so far follow the constraint of realism, i.e., obeying the laws of physics, they are also continuously expanding the boundaries of what “realism” means. In order to sell movie tickets, DVDs, and all other merchandise, each new special effects film tries to top the previous one showing something that nobody has seen before. In _The Matrix_ 1 it was “bullet time”; in _The Matrix_ 2 it was the Burly Brawl scene where dozens of identical clones fight Neo; in _Matrix 3_ it was the Superpunch. [17] The fact that the image is constructed differently internally does allow for all kinds of new effects; listening to Gaeta it is clear that for him the key advantage of such image is the possibilities it offers for virtual cinematography. That is, if before camera movement was limited to a small and well-defined set of moves – pan, dolly, roll – now it can move in any trajectory imaginable for as long as the director wants. Gaeta talks about the Burly Brawl scene in terms of _virtual choreography_: both choreographing the intricate and long camera moves impossible in the real word and also all the bodies participating in the flight (all of them are digital recreations assembled using Total Capture method). According to Gaeta, creating this one scene took about three years. So, while in principle Total Capture represents one of the most flexible way to recreate visible reality in a computer so far, it will be years before this method is streamlined and standardized enough for these advantages to become obvious. But when it happens, the artists will have an extremely flexible hybrid medium at their disposal: completely virtualized cinema. Rather than expecting that any of the present pure forms will dominate the future of visual culture, I think this future belongs to such hybrids. In other words, the future images would probably be still photographic – although only on the surface. 

And what about animation? What will be its future? As I have tried to explain, besides animated films proper and animated sequences used as a part of other moving image projects, animation has become a set of principles and techniques which animators, filmmakers and designers employ today to create new techniques, new production methods and new visual aesthetics. Therefore, I think that it is not worthwhile to ask if this or that visual style or method for creating moving images which emerged after computerization is “animation” or not. It is more productive to say that most of these methods were born from animation and have animation DNA – mixed with DNA from other media. I think that such a perspective which considers “animation in an extended field” is a more productive way to think about animation today, and that it also applies to other modern media fields which “donated” their genes to a computer metamedium.

## References:

[1] For technical details of the method, see the publications of Georgi Borshukov: [www.virtualcinematography.org/publications.html](www.virtualcinematography.org/publications.html).

[2] Although not everybody would agree with this analysis, I feel that after the end of 1980s the field has significantly slowed down: on the other hand, all key techniques which can be used to create photorealistic 3D images have been already discovered; on the other hand, rapid development of computer hardware in the 1990s meant that computer scientists no longer had to develop new techniques to make the rendering faster, since the already developed algorithms would now run fast enough. 

[3] The terms “reality simulation” and “reality sampling” are made up by me; the terms “virtual cinema,” “virtual human,” “universal capture” and “virtual cinematography” come from John Gaeta. The term “image based rendering” appeared already in the 1990s – see the publication list at [http://www.debevec.org/Publications/](http://www.debevec.org/Publications/), accessed September 4, 2008.

[4] Therefore, while the article in _Wired_ which positioned Gaeta as a groundbreaking pioneer and as a rebel working outside of Hollywood contained the typical journalistic exaggeration, it was not that far from the truth. Steve Silberman, “Matrix 2,” _Wired_ 11.05 (May 2003), [http://www.wired.com/wired/archive/11.05/matrix2.html](http://www.wired.com/wired/archive/11.05/matrix2.html) 

[5] Georgi Borshukov, “Making of _The Superpunch_,” presentation at Imagina 2004, available at [www.virtualcinematography.org/publications/acrobat/Superpunch.pdf](www.virtualcinematography.org/publications/acrobat/Superpunch.pdf).

[6] The details can be found in George Borshukov, Dan Piponi, Oystein Larsen, J.P.Lewis, Christina Tempelaar-Lietz, “Universal Capture - Image-based Facial Animation for _The Matrix Reloaded_,” SIGGRAPH 2003 Sketches and Applications Program, available at [http://www.virtualcinematography.org/publications/acrobat/UCap-s2003.pdf](http://www.virtualcinematography.org/publications/acrobat/UCap-s2003.pdf).

[7] The method captures only the geometry and images of actor’s head; body movements are recorded separately using motion capture.

[8] Borshukov et al, “Universal Capture.”

[9] See [www.massivesoftware.com](www.massivesoftware.com).

[10] John Gaeta, presentation during a workshop on the making of _The  Matrix_, Art Futura 2003 festival, Barcelona, October 12, 2003.

[11] I don’t remember the exact year of SIGGRAPH conference where Blinn has spoken but I think it was end of the 1980s when physically based modeling was still a new concept.

[12] Casey Reas, private communication, April 2005.

[13] Catherine Feeny, “\_The Matrix\_ Revealed: An Interview with John Gaeta,” _VFXPro_, May 9, 2004, [www.uemedia.net/CPC/vfxpro/article\_7062.shtml](www.uemedia.net/CPC/vfxpro/article_7062.shtml).

[14] J. F. Blinn, "Simulation of Wrinkled Surfaces," _Computer Graphics_ (August 1978): 286-92.

[15] Seen in this perspective, my earlier book _The Language of New Media_ can be seen as a systematic investigation of a particular slice of contemporary culture driven by this hybrid aesthetics: the slice where the logic of digital networked computer intersects the numerous logics of already established cultural forms. Lev Manovich, _The Language of New Media_ (The MIT Press, 2001.)

[16] John Gaeta, making of _The Matrix_ workshop.

[17] Borshukov, “Making of _The Superpunch_.”

---

# Friendly Alien: Object and Interface

_author: Lev Manovich_
_year: 2006_

Since 1996, artist Miltos Manetas makes paintings that systematically portray the new essential objects of contemporary life: joysticks, computers, computer game consoles, and computer cables (lots of them). Manetas also paints people who are usually intensely engaged in the activities made possible by consumer electronics devices, such as playing a computer game. But he never shows what games they are playing or what images they are looking at. Instead, he focuses on human-computer interface: hands clutching a joystick, a body stretched across the floor in the intense concentration or, alternatively, relaxing besides a laptop, a computer console, or a TV.

Manetas paintings of the 1990s reflected the popular then views of the computer as an unfamiliar and foreign presence, even an alien; computer work as immersion and withdrawal from the physical surrounding; the laptop, the game console “sucking in” the user away from the immediate space (similar to the vision of TV in Cronenberg’s 1982 _Videodrome_). The orgy of electronic cables in these paintings which seem to grow and multiply bring the references of a cyborg and science fiction movies such as _Alien_ and _The Matrix_.

In contrast, his latest painting such as _Girls in Nike_ (2005) represent technology as being completely integrated and fused with the lived environment: items of fashionable clothing and computer cables become complementary; the atmosphere is decorative and festive. Technology is neither threatening nor it is some outside force that has been domesticated. Rather, it is playful and playable: it brings a party into the everyday. The sound which accompanying our interaction with the icons; the icons which playfully unfold into windows in MAC OS X; colorful desktop backgrounds; shiny reflective surfaces and anthropomorphic shapes – all this makes computers and consumer electronics devices stand out from the everyday grayness. Technology is a pet which surprises us, sometimes disobeying and even annoying us – but is always animated, always entertaining, always fun, and almost fashion.

My visit to the famous Collette store in Paris the same day in October 2005 when I saw _Girls in Nike_ in Manetas’s studio only confirmed this new identity of consumer technology today. Collette is a legendary store that in the middle of the 1990s introduced a new concept that today became an accepted genre - store as the collection of most interesting design objects currently being created around the world, with an obligatory cool café and changing art exhibitions.

Situated across the entrance was the new display positioned right in the center of the store. It housed latest cell phones, PDAs, and a portable SONY PlayStation. These “techno-jewels” came to dominate the store, taking the space away from albums, perfumes, clothes, and various design objects which all now were occupying the perimeter. But, just as in Manetas’s new paintings, the techno-objects in the case did not look dominating, threatening, or alien. They seemed to acquire the same status as perfume, photography books, clothes, and other items in the store. Put differently, they were no longer “technology.” Instead, they became simply “objects” and as such they now had the same right as other objects which we use daily to be beautiful and elegant, to have interesting shapes and textures; to reflect who we use and at the same time allow us to reinvent ourselves. In short, they now belonged to the world of design and fashion rather than engineering.

Yet, as another display in Collette made it clear, the integration was far from complete. SONY just commissioned 10 top fashion designers to design cases for PSP (Portable Sony PlayStation) and they were presented in the store. The cases were disappointing – although they used a variety of materials, patterns, colors, and designs, none of them felt integrated with PSP design: the refine and minimal logic of PSP menu screens, the way they slide horizontally, etc. What I saw in each case there two completely different design logics not talking to each other at all.

I feel similar unease in some of the recent attempts to make cell phones more “fashionable” by adding easily recognizable signs of fashion - encrustation, silver textures, ”art deco” patterns. The problem is that techno-objects are not ordinary objects. This applies equally to cell phones, PDAs, portable game players, portable music players, portable video players, etc. They all contain interfaces – most often a screen for output and input and a few buttons, and sometimes also a trackwheel, or a small built-in keyboard. And behind the screen lives a whole separate world with its logic, aesthetics, and dynamics. And when this electronic screen and the world it presents to us ends (I am talking about the physical boundary of the screen), this creates visual and psychological feeling of discontinuity. Suddenly we are in a different world – that of non-interactive, “dead” surfaces which enclose the screen. And, typically, the design of these surfaces does not have much to do with the design of the screen interface. The “fashion” cases for PSP exemplify this situation. All cases were nice by themselves but the associative worlds they invoked had nothing to do with the world inside a PSP screen.

Let me put these experiences in more general terms. _Today the design of forms becomes intricately linked with the question of interface_. First of all, we need to give some visual form to what will appear on the screens of computers, mobile phones, PDAs, car navigation systems, and other devices – as well as to buttons, trackwheels, microphones, and various other input tools. Therefore, human-computer interfaces which involve a set of visual conventions such as folders, icons, and menus (i.e., a Graphical User Interface), audio conventions (as in voice recognition interface), and particular material articulations (such as the shape, color, material and texture of a mobile phone) represent the whole new category of forms, which need to be designed today. Even more importantly, as computation becomes incorporated in our lived environment (the trend which is described by such terms as “ubiquitous computing,” “pervasive computing,” “ambient intelligence,” “context-aware environments,” “smart objects”) the interfaces slowly leave the realm where they safely lived for a few decades – that is, stand-alone computers and electronics devices – and start appearing in all kinds of objects and on all kinds of surfaces, be it interior walls, furniture, benches, bags, clothing, and so on. Consequently, _the forms of all these objects that previously lived “outside of information” now have to address the likely presence of interfaces somewhere on them_.

This does not mean that from now on “form follows interface.” Rather, a physical form and an interface have to learn how to accommodate each other. Beyond the traditional requirements that the material forms have to satisfy – a chair has to be comfortable for sitting, for example – their design is now being shaped by new requirements. For instance, at least so far, we are used to interact with text, which is presented on flat and rectangular surface, and therefore if a screen is to be incorporated somewhere in the object, a part of it needs to be reasonably flat. Which is easy to do if an object is a table but not as easy if it is a piece of clothing or Gerry’s Disney Hall in Los Angeles specifically designed not to have a single flat area. (Of course, as new technologies such as Rapid Manufacturing may soon enable easy printing of an electronic display on any surface of any object while it is being produced, it’s possible that we will be able to quickly adjust our perceptual habits, so moving and change-shaping display surfaces will be accepted much easier than I can imagine. In fact, the computer-controlled graphic projections on the body of dancers as in _Apparition_ by Klaus Obermaier or in _Interactive Opera Stage_ system by Art+Com already show the aesthetic potential of displaying information over a changing non-flat not-rectangular form, i.e., a human body.)

In short, today the interface and the material object that supports it still seem to come from different worlds. The interface is a "friendly alien," but it is still the alien. The task of rethinking both interface and objects together so they can be fused into a new unity is not an easy one and it will require lots of work and imagination before aesthetically satisfying solutions will be find.

In conclusion, let me describe my visit to a show of student projects from Department of Industrial Design at Eindhoven Technical University in Netherlands, which I saw during Dutch Design Week in the fall of 2005. The department is only three years old, so instead of designing traditional objects, students are working on “smart objects.” Every project in show starts with an everyday familiar object and ads some “magical” functions to it via electronics and computers. Which means that I see more examples of solid objects and media/interface surfaces coming together. In one project, a canopy placed diagonally over a child’s hospital in a bed becomes an electronic canvas. By tracking the position of a special pen that does not need to touch the drawing surface, the canvas allows the child to draw on it without having to move from the bed. In another project, a special mirror allows one person to leave a message for somebody else – for instance, a different member of a household. A rectangular block containing a camera is built into a mirror frame. You take the block out, record a video message and place the block back into the frame. After you do this, the video is automatically “loaded” into the magical mirror, and a small picture appears somewhere on the mirror surface. When you click on the picture it plays a video message. Yet another project adds magical interactivity to a vertical plastic column. The lights inside the column turn it into an ambient light source. The column is covered with a special interface: a net. Depending on how you touch the net, the position, quality, and tint of the light changes. How exactly the light will change is not directly predictable, and this is what makes the interaction with light column fun. There is real magic to all these “smart objects”: we see familiar normally passive objects literally coming to life and responding to our interactions with them.

Together, these three projects show us different ways in an object, an interface and a display can be put together. The first two projects rely on already familiar behaviors – drawing with a pen or making a recording with a video camera. The last one calls for user to develop new vocabulary of movements and gestures to which the light will respond. And the ways in which a “smart object” talks back to us are also different: a canvas canopy shows a drawing, a mirror plays video, and a light glows in different ways. In short, the surface of an object can become both an output and input media, bringing together the physical and the screen-like – form and information - in surprising ways. 

---

# Social Data Browsing

_author: Lev Manovich_
_year: 2006_

![The Dumpster](https://www.tate.org.uk/intermediaart/images/15386_golan_levin_dumpster_top.jpg)

_The Dumpster_ by Golan Levin, Kamal Nigam and Jonathan Feinberg, 2006.

Consider the following paradox. The same few decades of the nineteenth century that gave us the most detailed artistic representations of human emotions and inner feelings, including romantic love, also saw the rise of statistical and sociological imagination. While Flaubert and Tolstoy were putting the emotions of their heroines under the artistic microscope of their prose, a different paradigm was emerging in which the individuals were nothing but dots contributing to a social law, a pattern, or a distribution. In 1838 August Compte coined the term ”sociology” for the new discipline that was to study the laws governing the life of society. (He also proposed the term “social physics”). According to another founder of the discipline, Emile Durkheim, sociology is the science concerned with “social facts” – phenomena that have an independent and objective existence separate from the actions of the individuals. In his major work _Suicide_ (1897) Durkheim set out to demonstrate how such seemingly individual acts as suicides in fact follow general statistical patterns and can be explained in terms of structural forces that operate in society at large. Compare this to _Anna Karenina_ (1877) where Tolstoy meticulously follows the last hours and minutes of Anna’s life with a kind of anti-sociological gaze – looking at her not from the outside as a social scientist, but on the contrary, depicting how the outside world appears as seen by her.

In general, representational art has depicted individuals rather than social groups, classes, and institutions. Even in the case of modern realist literature and painting, including socialist realism, which consciously aimed to represent social types and classes, what the writers and painters actually show us are individual human beings. In other words, regardless of whether a painting or a sculpture is named “worker”, “farmer”, “miner,” etc., it shows a single concrete individual. And when artists have tried visually to represent really big groups, the typical result has been a crowd in which individual differences are hard to read. The same relationships between the zoom function and the level of detail holds today – consider the individual figures in Matthew Barney’s _The Cremaster Cycle_ versus the groups of veiled women in the films by Shirin Neshat, or the panoramic views of Andreas Gursky which reduce individuals to swirling dots.

It appears that we may be dealing with some essential characteristic of art. Or maybe this limitation is simply a general characteristic of all images in general – their inability to represent abstract concepts and logical relationships. After all, if in the course of evolution human species developed two different representations systems – one linguistic and one image-based – it would make sense that they should complement each other, and that images would not do what language does best.

But what if this limitation is simply a result of the representational techniques that artists had at their disposal? Consider, for instance, how the techniques of films invented in the first two decades of the twentieth century – editing and different types of shots – have allowed film directors to alternate between close-ups showing individuals and long shots showing the groups to which these individuals belong. Given this example, what can we expect from computers? Can computer media be used to create artistic representations that link the individual and the social without subsuming one in the other, i.e., the particular in the general? If we consider the range of computer techniques available for organizing and viewing data, things look quite encouraging. We can switch between multiple views of the same data, traverse the data at different scales, and move between multiple media linked together. And we can do this in near or close to real time. We can also instruct software to search through and mine very large amounts of data – such as the data produced by the millions of real people who engage in online chat, write blogs, send emails, upload their photos on Flickr and so on. What types of representation can be created if we combine these computer techniques and new ways of gathering data as well as of structuring and displaying it?

Although [*The Dumpster*](http://www2.tate.org.uk/netart/bvs/thedumpster.htm) by Golan Levin (working with Kamal Nigam and Jonathan Feinberg) can be related to traditional genres such as portraiture or documentary, as well as established new-media genres such as visualization and database art, it is something new and different. I would like to call it a “social data browser”. It allows you to navigate between the intimate details of people’s experiences and the larger social groupings. The particular and the general are presented simultaneously, without one being sacrificed to the other.

_The Dumpster_ application window shows a large “crowd” of circles at the same time. While in a typical painting individual differences would be lost at this scale, here you can click on any circle and read the corresponding blog fragment. And this is just a beginning. Consider the way in which Levin structures the navigation. In typical hypermedia you move horizontally between pages or scenes connected by links. In typical information visualization you “move upward”, so to speak – from the level of individual data to larger patterns that become visible when the numerous data points are turned into a single image or a shape. But in Levin’s group portrait, you are encouraged to navigate both horizontally, vertically, and diagonally between the particular and the general. You can, for example, simply click on different circles, jumping from one breakup case to another and randomly explore the overall data space. Or you can explore the circles that are similar in color – which means that the corresponding postings are similar in some ways. Or you can explore the circles that have an opposite color and thus belong to a different grouping. In short, the seemingly incompatible points of view of Tolstoy and Durkheim – the subjective experience and the social facts – are brought together via the particular information architecture and navigation design of _The Dumpster_.

But if we simply limit ourselves to describing the work as it appears visually, we will miss the crucial characteristics of the social data browser constructed by Levin. We need to consider how the data presented in _The Dumpster_ was obtained and processed before it was presented to us. Using a variety of methods, Levin and his collaborators have filtered the huge data space of online blogs isolating the postings from 2005 where teenagers narrated their breakups. The result was 20,000 postings describing “confirmed” breakups. These postings were subjected to further analysis in order to derive various metadata about them: reasons for the break-up, who broke up with whom, the age and sex of the author, as well as their emotional state. Most of this metadata was not explicitly contained in the postings but is inferred with a high degree of probability by the project’s authors.

The result is a group portrait appropriate for the age of data mining, large databases, and global surveillance programs such as Echelon. The group “painted” by *The Dumpster* did not commission this portrait itself but rather was created by the artist by searching though the digital traces that people leave online. The ordering of individual members within this very large group of 20,000 people is the result of mathematical analysis. As a result, each individual breakup experience becomes a point in a multi-dimensional space that we are invited to explore. In short, we are invited to mine the data prepared by the project’s authors who used sophisticated computer methods.

More than two decades ago, William Gibson accurately predicted the cyberculture of the 1990s with its idea of virtual navigation through data. By naming his recent novel _Pattern Recognition_, Gibson points to the new period we are living in now. It is a period when more prosaic but ultimately more consequential ways of exploring data have come to the forefront, including search engines available to the masses and data mining as used by companies and government agencies. _The Dumpster_ uses industrial strength data gathering and data analysis strategies that normally are not easily accessible for single individuals to show how they result in new kinds of social representations.

About the Dumpster:

 ![The Dumpster](https://www.tate.org.uk/intermediaart/images/15563_golanlevin_top_withtext.jpg)

(Text about Golan Levin, Kamal Nigam and Jonathan Feinberg, _The Dumpster_, 2006, commissioned by Tate Modern.)


_The Dumpster_ by Golan Levin in collaboration with Kamal Nigam and Jonathan Feinberg visualizes information using data from web logs to plot the romantic lives of teenagers, creating a dynamic, interactive map of relationship start-ups and break-downs.

Using data gathering and data analysis strategies the work creates a group portrait that illustrates relational formations, by drawing upon the popularity of social media platforms.

The viewer/reader is able to at once, view the components of the portrait through the examination of individual narratives and at the same time review their context within the larger group.

_The Dumpster_ has been commissioned in collaboration with [artport](http://artport.whitney.org/), the Whitney Museum of American Art's portal to net art.

---

# Import/Export: Design Workflow and Contemporary Aesthetics

_author: Lev Manovich_
_year: 2006_

Until the arrival of the software-based tools in the 1990s, to combine different types of time-based media together was either time consuming, or expensive, or in some cases simply impossible. Software tools such as Illustrator, Photoshop and After Effects have changed this situation in a fundamental way. Now a designer can import different media into her composition with just a few mouse clicks.

However, the contemporary software-based visual design does not simply involve combining elements from different sources within a single application. In this article we will look at the whole workflow typical of contemporary visual design (graphic design, web design. and motion graphics) and its effects on contemporary aesthetics. 

Although ”import”/”export” commands appear in most modern media authoring and editing software running under GUI, at first sight they do not seem to be very important for understanding software culture. When you “import,” you are not authoring new media or modifying media objects or accessing information across the globe, as in web browsing. All these two commands allow you to do is to move data around between different applications. In other words, they make data created in one application compatible with other applications. And that does not look so glamorous.

Think again. What is the largest part of the economy of greater Los Angeles area? It is not entertainment. (From movie production to museums and everything is between only accounts for 15%). It turns out that the largest part of the economy is import/export business - more than 60%. More generally, one commonly evoked characteristic of globalization is greater connectivity – places, systems, countries, organizations etc. becoming connected in more and more ways. And connectivity can only happen if you have certain level of compatibility: between business codes and procedures, between shipping technologies, between network protocols, between computer file formats, and so on. 

Let us take a closer look at import/export commands. As I will try to show below, these commands play a crucial role in software culture, and in particular in media design – regardless of what kind of project a design is working on.

Before they adopted software tools in the 1990s, graphic designers, filmmakers, and animators used completely different technologies. Therefore, as much as they were influenced by each other or shared the same aesthetic sensibilities, they inevitably created differently looking images. Filmmakers used camera and film technology designed to capture three-dimensional physical reality. Graphic designers were working with offset printing and lithography. Animators were working with their own technologies: transparent cells and an animation stand with a stationary film camera capable of making exposures one frame at a time as the animator changed cells and/or moved background. 

As a result, twentieth century cinema, graphic design, and animation (I am talking here about standard animation techniques used by most commercial studios) developed distinct artistic languages and vocabularies both in terms of form and content. For example, graphic designers worked with a two-dimensional space, film directors arranged compositions in three-dimensional space, and cell animators worked with a “two-and-a-half” dimensions. This holds for the overwhelming majority of works produced in each field, although of course exceptions do exist. For instance, Oscar Fischinger made one abstract film that consisted of  simple geometric objects moving in an empty space – but as far as I know, this is the only film in the whole history of abstract animation, which is taking place in three-dimensional space. 

The differences in technology influenced what kind of content would appear in different media. Cinema showed “photorealistic” images of nature, built environments and human forms articulated by special lighting. Graphic designs featured typography, abstract graphic elements, monochrome backgrounds and cutout photographs. And cartoons showed hand-drawn flat characters and objects animated over hand-drawn but more detailed backgrounds. The exceptions are rare. For instance, while architectural spaces frequently appear in films because directors could explore their three-dimensionality in staging scenes, they practically never appear in animated films in any detail – until animation studios start using 3D computer animation.

Why was it so difficult to cross boundaries? For instance, in theory one could imagine making an animated film in the following way: printing a series of slightly different graphics designs and then filming them as though they were a sequence of animated cells. Or a film where a designer simply made a series of hand drawings that used the exact vocabulary of graphic design and then filmed them one by one. And yet, to the best of my knowledge, such a film was never made. What we find instead are many abstract animated films that have certain connection to various styles of abstract painting. For example, Oscar Fischinger’s films and paintings share certain forms. We can also find abstract films and animated commercials and movie titles that have certain connection to graphic design aesthetics popular around the same times. For instance, some moving image sequences made by motion graphics pioneer Pablo Ferro around 1960s display psychedelic aesthetics which can be also found in posters, record covers, and other works of graphic design in the same period. 

And yet, despite these connections, works in different media never used exactly the same visual language. One reason is that projected film could not adequately show the subtle differences between typeface sizes, line widths, and grayscale tones crucial for modern graphic design. Therefore, when the artists were working on abstract art films or commercials that adopted design aesthetics (and most major 20th abstract animators worked both on their own films and commercials), they could not simply expand the language of a printed page into time dimension. They had to invent essentially a parallel visual language that used bold contrasts, more easily readable forms, and thick lines – which, because of their thickness, were in fact no longer lines but shapes. 

Although the limitations in resolution and contrast of film and television image in comparison to a printed page contributed to the distance between the languages used by abstract filmmakers and graphic designers for the most of the twentieth century, ultimately I do not think it was the decisive factor. Today the resolution, contrast and color reproduction between print, computer screens, television screens, and the screens of mobile phones are also substantially different – and yet we often see exactly the same visual strategies deployed across these different display media. If you want to be convinced, leaf through any book or a magazine on contemporary 2D design (i.e., graphic design for print, broadcast, and the web). When you look at pages featuring the works of a particular designer or a design studio, in most cases it’s impossible to identify the origins of the images unless you read the captions. Only then do you find that which image is a poster, which one is a still from a music video, and which one is magazine editorial. 

I am going to use Taschen’s _Graphic Design for the 21st Century: 100 of the World’s Best Graphic Designers_ (2001) for examples. Peter Anderson’s design showing a line of type against a cloud of hundreds of little letters in various orientations turns out to be the frames from the title sequence for Channel Four documentary. His other design which similarly plays on the contrast between jumping letters in a larger font against irregularly cut planes made from densely packed letters in much smaller fonts turns to be a spread from IT Magazine. Since the first design was made for broadcast while the second was made for print, we would expect that the first design would employ bolder forms - however, both designs use the same scale between big and small fonts, and feature texture fields composed from hundreds of words in such a small font that they clear need to be read. A few pages later we encounter a design by Philippe Apeloig that uses exactly the same technique and aesthetics as Anderson. In this case, tiny lines of text positioned at different angles form a 3D shape floating in space. On the next page another design by Apeloig creates a field in perspective - made not from letters but from hundreds of identical abstract shapes.

These design rely on software’s ability (or on the designer being influenced by software use and recreating what she did with software manually) to treat text as any graphical primitive and to easily create compositions made from hundreds of similar or identical elements positioned according to some pattern. And since an algorithm can easily modify each element in the pattern, changing its position, size, color, etc., instead of the completely regular grids of modernism we see more complex structures that are made from many variations of the same element. (This strategy is explored particularly imaginatively in Zaha Hadid’s designs such as Louis Vuitton Icone Bag, 2006, and in urban masterplans for Singapore and Turkey which use what Hadid calls a “variable grid.”)

Each designer included in the book was asked to provide a brief statement to accompany the portfolio of their work, and Lust studio has put this phrase as their motto: “Form-follows-process.” So, what is the nature of design process in the software age and how does it influence the forms we see today around us?

If you are practically involved in design or art today, you already know that contemporary designers use the same small set of software tools to design just about everything: InDesign, Dreamweaver, Photoshop, Illustrator, Flash, Premiere, After Effects, Maya. However, the crucial factor is not the tools themselves but the workflow process, enabled by “import” and “export” operations and related methods (“place,” “insert object,” “subscribe,” “smart object,” etc.), which ensure coordination between these tools.

When a particular media project is being put together, the software used at the final stage depends on the type of output media and the nature of the project – After Effects for motion graphics projects and video compositing, Illustrator or Freehand for print illustrations, InDesign for graphic design, Flash for interactive interfaces and web animations, 3ds Max or Maya for 3D computer models and animations, and so on. But these programs are rarely used alone to create a media design from start to finish. Typically, a designer may create elements in one program, import them into another program, add elements created in yet another program, and so on. This happens regardless of whether the final product is an illustration for print, a web site, or a motion graphics sequence; whether it is a still or a moving image, interactive or non-interactive, etc.

The very names which software companies give to the products for media design and production refer to this defining characteristic of software-based design process. Since 2005, Adobe has been selling its different media authoring applications bundled together into “Adobe Creative Suite.” Among the subheadings and phrases used to accompany this band name, one in particular is highly meaningful in the context of our discussion: “Design Across Media.” This phrase accurately describes both the capabilities of the applications collected in a suite, and their actual use in the real world. Each of the key applications collected in the suite – Photoshop, Illustrator, InDesign, Flash, Dreamweaver, After Effects, Premiere – has many special features geared for producing a design for particular output media. Illustrator is set up to work with professional-quality printers; After Effects and Premiere can output video files in a variety of standard video formats such as HDTV; Dreamweaver supports programming and scripting languages to enable creation of sophisticated and large-scale dynamic web sites. But while a design project is finished in one of these applications, most other applications in Adobe Creative Suite will be used in the process to create and edit its various elements. Thus is one of the ways in which Adobe Creative Suite enables “design across media.” The compatibility between applications also means that the elements (called in professional language “assets”) can be later re-used in new projects. For instance, a photograph edited in Photoshop can be first used in a magazine ad and later put in a video, a web site, etc. Or the 3D models and characters created for a feature film are reused for a video game based on the film. This ability to re-use the same design elements for very different projects types is very important because of the widespread practice in creative industries to create products across the range of media which share the same images, designs, characters, narratives, etc. An advertising campaign often works “across media” including web ads, TV ads, magazine ads, billboards, etc. And if turning movies into games and games into movies has been already popular in Hollywood for a while, a new trend since approximately middle of 2000s is to create a movie, a game, a web site or maybe other media products at the same time – and have all the products use the same digital assets both for economic reasons and to assure aesthetic continuity between these products. Thus, a studio may create 3D backgrounds and characters and put them both in a movie and in a game, which will be released simultaneously. If media authoring applications were not compatible, such practice would simply not be possible. 

All these examples illustrate the intentional reuse of design elements “across media.” However, the compatibility between media authoring applications also has a much broader and non-intentional effect on contemporary aesthetics. Given the production workflow I just described, we may expect that the same visual techniques and strategies will also appear in all types of media projects designed with software without this being consciously planned for. We may also expect that this will happen on a much more basic level. This is indeed the case. _The same software-enabled design strategies, the same software-based techniques and the same software-generated iconography are now found across all types of media, all scales, and all kinds of projects_. 

We have already encountered a few concrete examples. For instance, the three designs by Peter Anderson and Philippe Apeloig done for different media use the same basic computer graphic technique: automatic generation of a repeating pattern while varying the parameters which control the appearance of each element making up the pattern’s element – its size, position, orientation, curvature, etc. (The general principle behind this technique can also be used to generate 3D models, animations, textures, make plants and landscapes, etc. It is often referred to as “parametric design,” or “parametric modeling.”) The same technique is also used by Hadid’s studio for Louis Vuitton Icone Bag. In another example, which will be discussed below, Gregg Lynn used particle systems technique – which at that time was normally used to simulate fire, snow, waterfalls, and other natural phenomena in cinema – to generate the forms of a building. 

To use the biological metaphor, we can say that compatibility between design applications creates very favorable conditions for the propagation of media DNAs between species, families, and classes. And this propagation happens on all levels: the whole design, parts of a design, the elements making up the parts, and the “atoms” which make up the elements. Consider the following hypothetical example of propagation on a lower level. A designer can use Illustrator to create a 2D smooth curve (called in computer graphics field called a “spline.”) This curve becomes a building block that can be used in any project. It can form a part of an illustration or a book design. It can be imported into animation program where it can be set to motion or imported into 3D program where it can be extruded in 3D space to define a solid object. 

Over time software manufacturers worked to developed tighter ways of connecting their applications to make moving elements from one to another progressively easier and more useable. Over the years, it became possible to move a complex project between applications without losing anything (or almost anything). For example, in describing the integration between Illustrator CS3 and Photoshop CS3, Adobe’s web site states that a designer can “Preserve layers, layer comps, transparency, editable files when moving files between Photoshop and Illustrator.” [1] Another important development has been the concept that Microsoft Office calls “linked objects.” If you link all of a part of one file to another file (for instance, linking an excel document to a PowerPoint presentation), any time information changes in the first file, it automatically gets updated in the second file. Many media applications implement this feature. To use the same example of Illustrator CS3, a designer can “Import Illustrator files into Adobe Premiere Pro software, and then use Edit Original command to open the artwork in Illustrator, edit it, and see your changes automatically incorporated into your video project.” [2]

Each of the type of programs used by media designers – 3D graphics, vector drawing, image editing, animation, compositing – excel at particular design operations, i.e., particular ways of creating design elements or modifying already existing elements. These operations can be compared to the different types of blocks of a Lego set. You can create an infinite number of projects by just using the limited number of block types provided in the set. Depending on the project, these block types will play different functions and appear in different combinations. For example, a rectangular red block may become a part of the tabletop, a part of the head of a robot, etc.

Design workflow that uses a small number of compatible software programs works in a similar way – with one important difference. The building blocks used in contemporary design are not only different kinds of visual elements one can create – vector patterns, 3D objects, particle systems, etc. – but also _various ways of modifying these elements_: blur, skew, vectorize, change transparency level, spherisize, extrude, etc. This difference is crucial. If media creation and editing software did not include these and many other modification operations, we would have seen an altogether different visual language at work today. We would have seen “multimedia,” i.e., designs that simply combine elements from different media. Instead, we see “deep remixability” – the “deep” interactions between working methods and techniques of different media within a single project. 

In a “cross-over” use, the techniques which were previously specific in one media are applied to other media types (for example, a lens blur filter). This often can be done within a single application – for instance, applying After Effects’s blur filter to a composition which can contain graphic elements, video, 3D objects, etc. However, being able to move a whole project or its elements between applications opens many more possibilities because each application offers many unique techniques not available in other applications. As the media data travels from one application to the next, is being transformed and enhanced using the operations offered by each application. For example, a designer can take her project she has been editing in Adobe Premiere and import in After Effects where she can use advanced compositing features of this program. She can then import the result back into Premiere and continue editing. Or she can create artwork in Photoshop or Illustrator and import into Flash where it can be animated. This animation can be then imported into a video editing program and combined with video. A spline created in Illustrator becomes a basis for a 3D shape. And so on. 

The production workflow specific to the software era that I just illustrated has two major consequences. Its first result is the visual aesthetics of hybridity that dominates contemporary design universe. The second is the use of the same techniques and strategies across this universe - regardless of the output media and type of project. 

As I already stated more than once, a typical design today combines techniques coming from multiple media. We now in a better position to understand why this is the case. As a designer works on a project, she combines the results of the operations specific to different software programs that were originally created to imitate work with different physical media (Illustrator was created to make illustrations, Photoshop - to edit digitized photographs, Premiere – to edit video, etc.). While these operations continue to be used in relation to their original media, most of them are now also used as part of the workflow on any design job. 

The essential condition that enables this new design logic and the resulting aesthetics is compatibility between files generated by different programs. In other words, “import,” “export” and related functions and commands of graphics, animation, video editing, compositing, and modeling software are historically more important than the individual operations these programs offer. The ability to combine raster and vector layers within the same image, to place 3D elements into a 2D composition and vice versa, and so on is what enables the production workflow with its reuse of the same techniques, effects, and iconography across different media.

The consequences of this compatibility between software and file formats, which was gradually achieved during the 1990s, are hard to overestimate. Besides the hybridity of modern visual aesthetics and reappearance of exactly the same design techniques across all output media, there are also other effects. For instance, the whole field of motion graphics as it exists today came into existence to a large extent because of the integration between vector drawing software, specifically Illustrator, and animation/compositing software such as After Effects. A designer typically defines various composition elements in Illustrator and then imports them into After Effects where they are animated. This compatibility did not exist when the initial versions of different media authoring and editing software initially became available in the 1980s. It was gradually added in particular software releases. But when it was achieved around the middle of the 1990s [3], within a few years the whole language of contemporary graphical design was fully imported into the moving image area – both literally and metaphorically. 

In summary, the compatibility between graphic design, illustration, animation, video editing, 3D modeling and animation, and visual effects software plays the key role in shaping visual and spatial forms of the software age. On the one hand, never before have we witnessed such a variety of forms as today. On the other hand, exactly the same techniques, compositions and iconography can now appear in any media. 

## References:

[1] [http://www.adobe.com/products/illustrator/features/allfeatures/](http://www.adobe.com/products/illustrator/features/allfeatures/), accessed August 30, 2008.

[2] Ibid.

[3] In 1995, After Effects 3.0 enabled Illustrator import and Photoshop as comp import. [http://en.wikipedia.org/wiki/Adobe\_After\_Effects](http://en.wikipedia.org/wiki/Adobe_After_Effects), accessed August 28, 2008.

---

# After Effects, or Velvet Revolution. Part I

_author: Lev Manovich_
_year: 2006_

During the heyday of post-modern debates, at least one critic in America noticed the connection between post-modern pastiche and computerization. In his book _After the Great Divide_ (1986), Andreas Huyssen writes: “All modern and avantgardist techniques, forms and images are now stored for instant recall in the computerized memory banks of our culture. But the same memory also stores all of pre-modernist art as well as the genres, codes, and image worlds of popular cultures and modern mass culture.” [1] His analysis is accurate – except that these “computerized memory banks” did not really became commonplace for another fifteen years. Only when the Web absorbed enough of the media archives it became this universal cultural memory bank accessible to all cultural producers. But even for the professionals, the ability to easily integrate multiple media sources within the same project – multiple layers of video, scanned still images, animation, graphics, and typography – only came towards the end of the 1990s. 

In 1985 when Huyssen book was in preparation for publication I was working for one of the few computer animation companies in the world called _Digital Effects_. [2] Each computer animator had his own interactive graphics terminal that could show 3D models but only in wireframe and in monochrome; to see them fully rendered in color, we had to take turns as the company had only one color raster display which we all shared. The data was stored on bulky magnetic tapes about a feet in diameter; to find the data from an old job was a cumbersome process which involved locating the right tape in tape library, putting it on a tape drive and then searching for the right part of the tape. We did not have a color scanner, so getting “all modern and avantgardist techniques, forms and images” into the computer was far from trivial. And even if we had one, there was no way to store, recall and modify these images. The machine that could do that – Quantel Paintbox – cost over USD 160,000, which we could not afford. And when in 1986 Quantel introduced Harry, the first commercial non-linear editing system which allowed for digital compositing of multiple layers of video and special effects, its cost similarly made it prohibitive for everybody except network television stations and a few production houses. Harry could record only eighty seconds of broadcast quality video. In the realm of still images, things were not much better: for instance, digital still store Picturebox released by Quantel in 1990 could hold only 500 broadcast quality images and it cost was similarly very high.

In short, in the middle of the 1980s neither we nor other production companies had anything approachable to “computerized memory banks” imagined by Huyssen. And of course, the same was true for the visual artists that were then associated with post-modernism and the ideas of pastiche, collage and appropriation. In 1986 BBC produced documentary _Painting with Light_ for which half a dozen well-known painters including Richard Hamilton and David Hockney were invited to work with Quantel Paintbox. The resulting images were not so different from the normal paintings that these artists were producing without a computer. And while some artists were making references to “modern and avantgardist techniques, forms and images,” these references were painted rather than being directly loaded from “computerized memory banks.” Only in the middle of the 1990s, when relatively inexpensive graphics workstations and personal computers running image editing, animation, compositing, and illustration software became commonplace and affordable for freelance graphic designers, illustrators, and small post-production and animation studios, the situation described by Huyssen started to become a reality.

The results were dramatic. Within about five years, modern visual culture was fundamentally transformed. Previously separate media - live action cinematography, graphics, still photography, animation, 3D computer animation, and typography – started to be combined in numerous ways. By the end of the decade, the “pure” moving image media became an exception and hybrid media became the norm. However, in contrast to other computer revolutions such as the rise of World Wide Web around the same time, this revolution was not acknowledged by popular media or by cultural critics. What received attention were the developments that affected narrative filmmaking – the use of computer-produced special effects in Hollywood feature films or the inexpensive digital video and editing tools outside of it. But another process which happened on a larger scale - the transformation of the visual language used by all forms of moving images outside of narrative films – has not been critically analyzed. In fact, while the results of these transformations have become fully visible by about 1998, at the time of this writing (early 2006) I am not aware of a single theoretical article discussing them. 

One of the reasons is that in this revolution no new media per se were created. Just as ten years ago, the designers were making still images and moving images. But the aesthetics of these images was now very different. In fact, it was so new that, in retrospect, the post-modern imagery of just ten years ago that at the time looked strikingly different now appears as a barely noticeable blip on the radar of cultural history. 

## Visual Hybridity

This article is a first part of the series devoted to the analysis of the new hybrid visual language of moving images that emerged during the period of 1993-1998. Today this language dominates our visual culture. While narrative features mostly stick to live cinematography and video shot by ordinary people with consumer video cameras and cell phones is similarly usually left as is, everything else – commercials, music videos, motion graphics, TV graphics, and other types of short non-narrative films and moving image sequences being produced around the world by the media professionals including companies, individual designers and artists, and students – are hybrid. 

Of course, I could have picked the different dates, for instance starting a few years earlier - but since After Effects software which will play the key role in my account was released in 1993, I decided to pick this year as my first date. And while my second date also could have been different, I believe that by 1998 the broad changes in the aesthetics of moving image became visible. If you want to quickly see this for yourself, simply compare demo reels from the same visual effects companies made in early 1990s and late 1990s (a number of them are available online – look for instance at the work of Pacific Data Images. [3] In the work from the beginning of the decade, computer imagery in most cases appears by itself – that is, we see whole commercials and promotional videos done in 3D computer animation, and the novelty of this new media is foregrounded. By the end of the 1990s, computer animation becomes just one element integrated in the media mix that also includes live action, typography, and design.

Although these transformations happened only recently, the ubiquity of the new hybrid visual language today (2006) is such that it takes an effort to recall how different things looked before. Similarly, the changes in production processes and equipment that made this language possible also quickly fade from both the public and professional memory. As a way to quick evoke these changes as seen from the professional perspective, I am going to quote from 2004 interview with Mindi Lipschultz who has worked as an editor, producer and director in Los Angeles since 1979:

> If you wanted to be more creative [in the 1980s], you couldn’t just add more software to your system. You had to spend hundreds of thousands of dollars and buy a paintbox. If you wanted to do something graphic – an open to a TV show with a lot of layers – you had to go to an editing house and spend over a thousand dollars an hour to do the exact same thing you do now by buying an inexpensive computer and several software programs. Now with Adobe After Effects and Photoshop, you can do everything in one sweep. You can edit, design, animate. You can do 3D or 2D all on your desktop computer at home or in a small office. [4]

In the 1989 former Soviet satellites of Central and Eastern Europe have peacefully liberated themselves from the Soviet Union. In the case of Czechoslovakia, this event came to be referred as Velvet Revolution – to contrast it to typical revolutions in modern history that were always accompanied by bloodshed. To emphasize the gradual, almost invisible pace of the transformations which occurred in moving image aesthetics between approximately 1993 and 1998, I am going to appropriate the term Velvet Revolution to refer to this transformations. Although it may seem presumptuous to compare political and aesthetics transformations simply because they share the same non-violent quality, as we will see in the later article, the two revolutions are actually related. But we can only make this connection after we analyses in detail how the aesthetics and the very logic of moving images changed during this period. 

Although the Velvet Revolution I will be discussing involved many technological and social developments – hardware, software, production practices, new job titles and new professional fields – it is appropriate to highlight one software package as being in the center of the events. This software is After Effects. Introduced in 1993, After Effects was the first software designed to do animation, compositing, and special effects on the personal computer. [5] Its broad effect on moving image production can be compared to the effects of Photoshop and Illustrator on photography, illustration, and graphic design. Although today (2006) media design and post-production companies continue to rely on more expensive “high-end” software such as Flame, Inferno or Paintbox that run on specialized graphics workstations from SGI, because of its affordability and length of time on the market After Effects is the most popular and well-known application in this area. Consequently, After Effects will be given a privileged role in this text as both the symbol and the key material foundation which made Velvet Revolution in moving image culture possible – even though today other programs in the similar price category such as Apple’s Motion, Autodesk’s Combustion, and Adobe’s Flash have challenged After Effects dominance. 

Finally, before proceeding I should explain the use of examples in this article. The visual language I am analyzing is all around us today (this may explain why academics have remained blind to it). After globalization, this language spoken by all communication professionals around the world. You can see for yourself all the examples of various aesthetics I will be mentioning below by simply watching television in practically any country and paying attention to graphics or going to a club to see a VJ performance or visiting the web sites of motion graphics designers and visual effects companies or opening any book on contemporary design. Nevertheless, I have included references to particular projects below so the reader can see exactly what I am referring to. [6] But since my goal is to describe the new cultural language which by now has become practically universal, I want to emphasize that each of these examples can be substituted numerous others. 

## Examples

The use of After Effects is closely identified with a particular type of moving images which became commonplace to a large part because of this software – “motion graphics.” Concisely defined by Matt Frantz in his Master Thesis as “designed non-narrative, non-figurative based visuals that change over time,” [7] motion graphics today include film and television titles, TV graphics, dynamic menus, the graphics for mobile media content, and other animated sequences. Typically motion graphics appear as parts of longer pieces: commercials, music videos, training videos, narrative and documentary films, interactive projects. 

While motion graphics definitely exemplify the changes that took place during Velvet Revolution, these changes are more broad. Simply put, the result of Velvet Revolution is _a new hybrid visual language of moving images in general_. This language is not confined to particular media forms. And while today it manifests itself most clearly in non-narrative forms, it is also often present in narrative and figurative sequences and films. 

For example, a music video may use live action while also employing typography and a variety of transitions done with computer graphics (example: video for _Go_ by Common, directed by Convert / MK12 / Kanye West, 2005). Or it may embed the singer within the animated painterly space (video for Sheryl Crow’s _Good Is Good_, directed by Psyop, 2005.) A short film may mix typography, stylized 3D graphics, moving design elements, and video (_Itsu_ for Plaid, directed by Pleix collective, 2002. [8] 

In some cases, the juxtaposition of different media is clearly visible (examples: music video for _Don’t Panic_ by Coldplay; main title for _The Inside_ by Imaginary Forces, 2005). In other cases, a sequence may move between different media so quickly that the shifts are barely noticeable (GMC Denali “Holes” commercial by Imaginary Forces, 2005). Yet in other cases, a commercial or a movie title may feature continuous action shot on video or film, with the image being periodically changing from a more natural to a highly stylized look. 

While the particular aesthetic solutions vary from one piece to the next and from one designer to another, they all share the same logic: the appearance of multiple media simultaneously in the same frame. Whether these media are openly juxtaposed or almost seamlessly blended together is less important than the fact of this co-presence itself.

Today such hybrid visual language is also common to a large proportion of short “experimental” (i.e., non-commercial) films being produced for media festivals, the web, mobile media devices, and other distribution platforms. [9] The large percentage of the visuals created by VJs and Live Cinema artists are also hybrid, combining video, layers of 2D imagery, animation, and abstract imagery generated in real time. (For examples, consult _The VJ book_, _VJ: Live Cinema Unraveled_, or web sites such as [www.vjcentral.com](www.vjcentral.com) and [www.live-cinema.org](www.live-cinema.org). [10] In the case of feature narrative films and TV programs, while they are still rarely mix different graphical styles within the same frame, many now feature highly stylized aesthetics which would previously be identified with illustration rather than filmmaking – for instance, TV series _CSI_, George Lucas’s latest _Star Wars_ films, or Robert Rodriguez’s _Sin City_. 

## Media Remixability

What is the logic of this new hybrid visual language? _This logic is one of remixability: not only of the content of different media or simply their aesthetics, but their fundamental techniques, working methods, languages, and assumptions._ United within the common software environment, cinematography, animation, computer animation, special effects, graphic design, and typography have come to form a new metamedium. A work produced in this new metamedium can use all techniques which were previously unique to these different media, or any subset of these techniques. 

If we use the concept of “remediation” to describe this new situation, we will misrepresent this logic – or the logic of media computing in general. [11] The computer does not “remediate” particular media. Instead, _it simulates all media_. And what it simulates are not surface appearances of different media, but all the techniques used for their production and all the methods of viewing and interaction with the works in these media. 

Once all types of media met within the same digital environment – and this was accomplished by the middle of the 1990s - they started interacting in the ways that could never be predicted nor even imagined previously. For instance, while particular media techniques continue to be used in relation to their original media, they can also be applied to other media. (This is possible because the techniques are turned into algorithms, all media is turned into digital data stored in compatible file formats, and software is designed to read and write files produced by other programs.) Here are a few examples: motion blur is applied to 3D computer graphics, computer generated fields of particles are blended with live action footage to give it enhanced look, a virtual camera is made to move around the virtual space filled with 2D drawings, flat typography is animated as though it is made from a liquid like material (the liquid simulation coming from computer graphics field), and so on. And while this “cross-over” use by itself constitutes a fundamental shift in media history, today a typical short film or a sequence may combine many such pairings within the same frame. The result is a hybrid, intricate, complex, and rich visual language – or rather, numerous languages that share the basic logic of remixabilty.

I believe that “media remixability” which begins around middle of the 1990s constitutes a new fundamental stage in the history of media. It manifests itself in different areas of culture and not only moving images – although the later does offer a particularly striking example of this new logic at work. Here software such as After Effects became a Petri dish where computer animation, live cinematography, graphic design, 2D animation and typography started to interact together, creating new hybrids. And as the examples mentioned above demonstrate, the result of this process of remixability are new aesthetics and new media species which cannot be reduced to the sum of media that went into them. Put differently, the interactions of different media in the same software environment are cultural species.

Media remixability does not necessary lead to a collage-like aesthetics which foregrounds the juxtapositions of different media and different media techniques. As a very different example of what media remixability can result in, consider a more subtle aesthetics well captured by the name of the software under discussion – After Effects. If in the 1990s computers were used to create highly spectacular special effects or “invisible effects,” [12] by the end of this decade we see something else emerging: a new visual aesthetics which goes “beyond effects.” In this aesthetics, the whole project – music video, commercial, short film, or a large part of a feature film – displays a hyper-real look where the enhancement of live action material is not completely invisible but at the same time it does not call attention to itself the way special effects usually did (examples: Reebok I-Pimp _Black Basketball_ commercial, _The Legend of Zorro_ main title, both by Imaginary Forces, 2005.) This new hyper-real aesthetics is yet another example of how in the hands of designers the Petri dish of software containing all media creation and manipulation techniques created during human history is now produces new hybrids. In fact, it produces only hybrids. 

## Layers, Transparency, Compositing

Let us now look at the details of new visual language of moving images which emerged from the Velvet Revolution and the material and social conditions – software, user interface, design workflow - which make remixabilty possible. Probably the most dramatic among the changes that took place during 1993-1998 was the new ability to combine together _multiple levels of imagery with varying degree of transparency via digital compositing_. If you compare a typical music video or a TV advertising spot circa 1986 with their counterparts circa 1996, the differences are striking. (The same holds for still images.) As I already noted, in 1986 “computerized memory banks” were very limited in their storage capacity and prohibitively expensive, and therefore designers could not quickly and easily cut and paste multiple image sources. But even when they would assemble multiple visual references, a designer only could place them next to, or on top of each other. She could not modulate these juxtapositions by precisely adjusting transparency levels of different images. Instead, she had to resort to the same photocollage techniques popularized in the 1920s. In other words, the lack of transparency restricted the number of different images sources that can be integrated within a single composition without it starting to look like many photomontages of John Heartfield, Hannah Höch, or Robert Rauschenberg – a mosaic of fragments without any strong dominant. [13]

Compositing also made trivial another operation which was very cumbersome previously. Until the 1990s, different media types such as hand-drawn animation, lens-based recordings, i.e., film and video, and typography practically never appeared within the same frame. Instead, animated commercials, publicity shorts, industrial films, and some feature and experimental films that did include multiple media usually placed them in separate shots. A few directors have managed to build whole aesthetic systems out of such temporal juxtapositions – most notably, Jean-Luc Godard. In his 1960s films such as _Week End_ (1967) Godard cut bold typographic compositions in between live action creating what can be called “media montages.” In the same 1960s pioneering motion graphics designer Pablo Ferro who has appropriately called his company Frame Imagery created promotional shorts and TV graphics that played on juxtapositions of different media replacing each other in a rapid succession. [14] In a number of Ferro’s spots, static images of different letterforms, line drawings, original hand painted artwork, photographs, very short clips from newsreels, and other visuals would come after another with machine gun speed. 

Within cinema, the superimposition of different media within the same frame were usually limited to the two media placed on top of each other in a standardized manner – i.e., static letters appearing on top of still or moving lens-based images in feature film titles. Both Ferro and another motion graphics pioneer Saul Bass have created a few title sequences where visual elements of different origin were systematically overlaid together – such as the opening for Hitchcock’s _Vertigo_ designed by Bass (1958). But I think it is fair to say that such complex juxtapositions of media within the same frame (rather than in edited sequence) were rare exceptions in the overwise “unimedia” universe where filmed images appeared in feature films and hand drawn images appeared in animated films. The only twentieth century feature film director I know of who has built his unique aesthetics by systematically combining different media within the same shot is Czech Karel Zeman. A typical shot by Zeman may contain filmed human figures, an old engraving used for background, and a miniature model. [15]

The achievements of these directors and designers are particularly remarkable given the difficulty of combing different media within the same frame during film era. To do this required utilizing the services of a special effects departments or separate companies which used optical printers. The techniques that were cheap and more accessible such as double exposure were limited in their precision. So, while a designer of static images could at least cut and paste multiple elements within the same composition to create a photomontage, to create the equivalent effect with moving images was far from trivial.

To put this in general terms, we can say that before computerization of the 1990s, the designer’s capacities to access, manipulate, remix, and filter visual information, whether still of moving, were quite restricted. In fact, they were practically the same as hundred years earlier - regardless of whether filmmakers and designers used in-camera effects, optical printing, or video keying. In retrospect, we can see they were at odds with the flexibility, speed, and precision of data manipulation already available to most other professional fields which by that time were computerized – sciences, engineering, accounting, management, etc. Therefore, it was only a matter of time before all image media would be turned into digital data and illustrators, graphic designers, animators, film editors, video editors, and motion graphics designers start manipulating them via software instead of their traditional tools. But this is only obvious today – after Velvet Revolution has taken place.

In 1985 Jeff Stein directed a music video for the new wave band Cars. This video had a big attempt in the design world, and MTV gave it the first prize in its first annual music awards. [16] Stein managed to create a surreal world in which a video cutout of the singing head of the band member was animated over different video backgrounds. In other words, Stein took the aesthetics of animated cartoons – 2D animated characters superimposed over a 2D background – and recreated it using video imagery. In addition, simple computer animated elements were also added in some shots to enhance the surreal effect. This was shocking because nobody ever saw such juxtapositions this before. Suddenly, modernist photomontage came alive. But ten years later, such moving video collages not only became commonplace, but they also became more complex, more layered, and more subtle. Instead of two or three, a composition could now feature hundreds and even thousands of layers. And each layer could have its own level of transparency.

In short, digital compositing now allowed the designers to easily _mix any number of visual elements regardless of the media in which they originated_ and to control each element in the process. 

We can make an analogy between multitrack audio recording and digital compositing of moving images. In multitrack recording, each soundtrack can be manipulated individually to produce the desired result. Similarly, in digital compositing each visual element can be independently modulated in a variety of ways: resized, recolored, animated, etc. Just as the music artist can focus on a particular track while muting all other tracks, a designer often turns of all visual tracks except the one she is currently adjusting. Similarly, both a music artist and a designer can at any time substitute one element of a composition by another, delete any elements, and add new ones. Most importantly, just as multitrack recording redefined the sound of popular music from the 1960s onward, once digital compositing became widely available during the 1990s, it changed the visual aesthetics of moving images in popular culture. 

This brief discussion only scratched the surface of my subject in this section, i.e., layers and transparency. For instance, I have not analyzed the actual techniques of digital compositing and the fundamental concept of an alpha channel which deserves a separate and detailed treatment. I have also did not go into the possible media histories leading to digital compositing, nor its relationship to optical printing, video keying and video effects technology of the 1980s. These histories and relationships were discussed in “Compositing” chapter (1999) in my _The Language of New Media,_ but from a different perspective than the one used here. At that time, I was looking at compositing from the point of view of the questions of cinematic realism, practices of montage, and the construction of special effects in feature films. Today, however, it is clear to me that in addition to disrupting the regime of cinematic realism in favor of other visual aesthetics, compositing also had another, even more fundamental effect. 

By the end of the 1990s digital compositing has become the basic operation used in creating _all_ forms of moving images, and not only big budget features. So, while compositing was originally developed in the context of special effects production in the 1970s and early 1980s [17], it had a much broader effect on contemporary visual and media cultures. Compositing played the key part in turning digital computer into an experimental lab where different media can meet and there their aesthetics and techniques can be combined to create new species. In short, digital compositing was essential in enabling the development of a new hybrid visual language of moving images which we see everywhere today. In other words, compositing enabled media remixability in moving image.

Thus, compositing that was at first a particular digital technique designed to integrate two particular media of live action film and computer graphics become a “universal media integrator.” And although compositing was originally created to support the aesthetics of cinematic realism, over time it actually had an opposite effect. Rather that forcing different media to fuse seamlessly, compositing led to the flourishing of numerous media hybrids where the juxtapositions between live and algorithmically generated, two-dimensional and three-dimensional, raster and vector are made deliberately visible rather than being hidden. 

## From “Time-based” to a “Composition-based”

My thesis about media remixability applies both to the cultural forms and the software used to create them. Just as the moving image media made by designers today mix formats, assumptions, and techniques of different media, the toolboxes and interfaces of the software they use are also remixes. Let us see use again After Effects as the case study to see how its interface remixes previously distinct working methods of different disciplines. 

When moving image designers started to use compositing / animation software such as After Effects, its interface encouraged them think about moving images in a fundamentally new way. Film and video editing systems and their computer simulations that came to be known as non-linear editors (today exemplified by Avid and Final Cut [18]) have conceptualized a media project as a sequence of shots organized in time. Consequently, while NLE (the standard abbreviation for non-linear editing software) gave the editor many tools for adjusting the edits, they took for granted the constant of film language that came from its industrial organization – that all frames have the same size and aspect ratio. This is an example of a larger phenomenon: as physical media were simulated in a computer, often many of their fundamental properties, interface conventions and constraints were methodically re-created in software – even though software medium itself has no such limitations. In contrast, from the beginning After Effects interface put forward a new concept of moving image – as a composition organized both in time and 2D space. 

The center of this interface is a Composition window conceptualized as a large canvas that can contain visual elements of arbitrary sizes and proportions. When I first started using After Effects soon after it came out, I remember feeling shocked that software did not automatically resize the graphics I dragged into Composition window to make them fit the overall frame. The fundamental assumption of cinema that accompanied it throughout its whole history – that film consists of  many frames which all have the same size and aspect ratio – was gone. 

In film and video editing paradigms of the twentieth century, the minimal unit on which the editor works on is a frame. She can change the length of an edit, adjusting where one film or video segment ends and another begins, but she cannot interfere with the contents of a frame. The frame as whole functions as a kind of “black box” that cannot be “opened.” This was the job for special effects departments. But in After Effects interface, the basic unit is not a frame, but a visual element placed in the Composition window. Each element can be individually accessed, manipulated, and animated. In other words, each element is conceptualized as an independent object. Consequently, a media composition is understood as a set of independent objects that can change over time. The very word “composition” is important in this context as it references 2D media (drawing, painting, photography, design) rather than filmmaking – i.e., space as opposed to time. 

Where does After Effects interface came from? Given that this software is commonly used to create animated graphics (i.e., “motion graphics”) and visual effects, it is not surprising that we can find interface elements which can be traced to three separate fields: animation, graphic design, and special effects. In traditional cell animation practice, an animator places a number of transparent cells on top of each other. Each cell contains a different drawing – for instance, a body of a character on one cell, the head on another cell, eyes on the third cell. Because the cells are transparent, the drawings get automatically “composited” into a single composition. While After Effects interface does not use the metaphor of a stack of transparent cells directly, it is based on the same principle. Each element in the Composition window is assigned a “virtual depth” relative to all other elements. Together all elements form a virtual stack. At any time, the designer can change the relative position of an element within the stack, delete it, or add new elements. 

We can also see a connection between After Effects interface and stop motion that was another popular twentieth century animation technique. With stop motion technique, puppets or any other objects are positioned in front of a camera and manually animated one frame at a time. The animator exposes one frame of film, changes the objects a tiny bit, exposes another frame, and so on. 

Just as it was the case with both cell and stop-motion animation, After Effects does not make any assumptions about the size or positions of individual elements. Rather than dealing with standardized units of time, i.e., film frames containing fixed visual content, a designer now works with separate visual elements positioned in space and time. An element can be a digital video frame, a line of type, an arbitrary geometric shape, etc. The finished work is the result of a particular arrangement of these elements in space and time. In this paradigm we can compare the designer to a choreographer who creates a dance by “animating” the bodies of dancers - specifying their entry and exit points, trajectories through space of the stage, and the movements of their bodies. (In this respect it is relevant that while After Effects interface did not evoke this reference, Macromedia Director which was the key multimedia authoring software of the 1990s did directly use the metaphor of the theatre stage.) 

While we can link After Effects interface to traditional animation methods as used by commercial animation studios, the working method put forward by software is more close to graphic design. In commercial animation studio of the twentieth century all elements – drawings, sets, characters, etc. – were prepared beforehand. The filming itself was a mechanical process. Of course, we can find exceptions to this industrial-like separation of labor in experimental animation practice where a film was typically produced by one person. For instance, in 1947 Oscar Fischinger made an eleven-minute film _Motion Painting 1_ by continuously modifying a painting and exposing film one frame at a time after each modification. However, because Fischinger was shooting on film, he had to wait a long time before seeing the results of his work. As the historian of abstract animation William Moritz writes, "Fischinger painted every day for over five months without being able to see how it was coming out on film, since he wanted to keep all the conditions, including film stock, absolutely consistent in order to avoid unexpected variations in quality of image." [19] In other words, in the case of this project by Fischinger, creating a design and seeing the result were even more separated than in a commercial animation process. 

In contrast, a graphic designer works “in real time.” As the designer introduces new elements, adjusts their locations, colors, and other properties, tries different images, changes the size of the type, and so on, she can immediately see the result of her work. [20] After Effects simulates this working method by making Composition window the center of its interface. Like a traditional designer, After Effects user interactively arranges the elements in this window and can immediately see the result. In short, After Effects interface makes filmmaking into a design process, and a film is re-conceptualized as a graphic design that can change over time. 

When physical media are simulated in a computer, we do not simply end with the same media as before. By adding new properties and working methods, computer simulation fundamentally changes the identity of a given media. For example, in the case of “electronic paper” such as a Word document or a PDF file, we can do many things which were not possible with ordinary paper: zoom in and out of the document, search for a particular phrase, change fonts and line spacing, etc. Similarly, current (2006) online interactive maps services provided by Mapquest, Yahoo, and Google augment the traditional paper map in multiple and amazing ways – just take a look at Google Earth. [21] 

A significant proportion of contemporary software for creating, editing, and interacting with media developed in this way – by simulating a physical media and augmenting it with new properties. But if we consider media design software such as Maya (used for 3D modeling and computer animation) or After Effects (motion graphics, compositing, and visual effects), we encounter a different logic. These software applications _do not simulate any single physical media that existed previously_. Rather, they _borrow from a number of different media combining and mixing their working methods and specific techniques._ (And, of course, they also add new capabilities specific to computer – for instance, the ability to automatically calculate the intermediate values between a number of keyframes.) For example, 3D modeling software mixes form making techniques which previously were “hardwired” to different physical media: the ability to change the curvature of a rounded form as though it is made from clay, the ability to build a structure from simple geometric primitives the way a house can be built from identical rectangular building blocks, etc. 

Similarly, as we saw, After Effects original interface, toolkit, and workflow drew on the techniques of animation and the techniques of graphic design. (We can also find traces of filmmaking and 3D computer graphics.) But the result is not simply a mechanical sum of all elements that came from earlier media. Rather, as software remixes the techniques and working methods of various media they simulate, the result are new interfaces, tools, and workflow with their own distinct logic. In the case of After Effects, the working method which it puts forward is neither animation, nor graphic design, nor cinematography, even though it draws from all these fields. It is a new way to make moving image media. Similarly, the visual language of media produced with this and similar software is also different from the languages of moving images which existed previously.

In other words, the Velvet Revolution unleashed by After Effects and other software did not simply made more commonplace the animated graphics artists and designers – John and James Whitney, Norman McLaren, Saul Bass, Robert Abel, Harry Marks, R/Greenberg, and others – were creating previously using stop motion animation, optical printing, video effects hardware of the 1980s, and other custom techniques and technologies. Instead, it led to the emergence of numerous new visual aesthetics that did not exist before. This article only begun the discussion of the common logic shared by these aesthetics; subsequent articles will look at its other features.

## References:

[1] Andreas Huyssen, “Mapping the Postmodern,” in _After the Great Divide_ (Bloomington and Indianapolis: Indiana University Press, 1986), 196.

[2] See Wayne Carlson, _A Critical History of Computer Graphics and Animations. Section 2:_ _The Emergence of Computer Graphics Technology_ , [http://accad.osu.edu/%7Ewaynec/history/lesson2.html](http://accad.osu.edu/~waynec/history/lesson2.html).

[3] [http://accad.osu.edu/~waynec/history/lesson6.html](http://accad.osu.edu/~waynec/history/lesson6.html).

[4] Mindi Lipschultz, interviewed by The Compulsive Creative, May 2004, [http://www.compulsivecreative.com/interview.php?intid=12](http://www.compulsivecreative.com/interview.php?intid=12).

[5] Actually, The NewTeck Video Toaster released in 1990 was the first PC based video production system that included a video switcher, character generation, image manipulation, and animation. Because of their low costs, Video Toaster systems were extremely popular in the 1990s. However, in the context of my article, After Effects is more important because, as I will explain below, it introduced a new paradigm for moving image design that was different from the familiar video editing paradigm supported by systems such as Toaster. 

[6] I have drawn these examples from three published sources, so they are easy to trace. The first is a DVD _I Love Music Videos_ that contains a selection of forty music videos for well-known bands from the 1990s and early 2000s, published in 2002. The second is an _onedotzero\_select DVD_, a selection of sixteen independent short films, commercial work and a Live Cinema performance presented by onedotzero festival in London and published in 2003. The third is Fall 2005 sample work DVD from Imaginary Forces, which is among most well-known motion graphics production houses today. The DVD includes titles and teasers for feature films, and the TV shows titles, stations IDs and graphics packages for cable channels. Most of the videos I am referring to can be also found on the net.

[7] Matt Frantz (2003), “Changing Over Time: The Future of Motion Graphics,”  [http://www.mattfrantz.com/thesisandresearch/motiongraphics.html](http://www.mattfrantz.com/thesisandresearch/motiongraphics.html).

[8] Included on _onedotzero\_select DVD 1_. Online version at  [http://www.pleix.net/films.html](http://www.pleix.net/films.html).

[9] In December 2005 I attended Impact media festival in Utrecht and I asked the festival director what percentage of submissions they received this year featured hybrid visual language as opposed to “straight” video or film. His estimate was about one half. In January 2006 I was part of the review team that judged graduating projects of students in SCI-ARC, a well-known research-oriented architecture school in Los Angeles. According to my informal estimate, approximately half projects featured complex curved geometry made possible by Maya that is modeling software now commonly used by architects. Given that both After Effects and Maya’s predecessor Alias was introduced the same year – 1993 – I think that this quantitative similarity in the proportion of projects that use new languages made possible by these software is quite telling. 

[10] Paul Spinrad, ed.,\_The VJ Book: Inspirations and Practical Advice for Live Visuals Performance\_ (Feral House, 2005); Timothy Jaeger, _VJ: Live Cinema Unraveled_ , available from [www.vj-book.com](www.vj-book.com).

[11] Jay David Bolter and Richard Grusin, _Remediation: Understanding New Media_ (The MIT Press, 1999).

[12] An invisible effect is the standard industry term. For instance, in 1997 the film _Contact_ directed by Robert Zemeckis was nominated for [1997 VFX HQ Awards](http://www.vfxhq.com/awards/97awards.html) in the following categories: Best Visual Effects, Best Sequence (The Ride), Best Shot (Powers of Ten), Best Invisible Effects (Dish Restoration) and Best Compositing. See [www.vfxhq.com/1997/contact.html](www.vfxhq.com/1997/contact.html).

[13] In the case of video, one of the main reasons which made combining multiple visuals difficult was the rapid degradation of the video signal when an analog video tape was copied more than a couple of times. Such a copy would no longer meet broadcasting standards. 

[14] Jeff Bellantoni and Matt Woolman, _Type in Motion_ (Rizzoli, 1999), 22-29.

[15] While of course special effects in feature films often combined different media, they were used together to create a single illusionistic space, rather than juxtaposed for the aesthetic effect such as in films and titles by Godard, Zeman, Ferro, and Bass.

[16] See [http://dreamvalley-mlp.com/cars/vid\_heartbeat.html#you\_might] (http://dreamvalley-mlp.com/cars/vid\_heartbeat.html#you\_might).

[17] Thomas Porter and Tom Duff, “Compositing Digital Images,” ACM _Computer Graphics_ vol. 18, no. 3 (July 1984): 253-259.

[18] I should note that compositing functionality was gradually added over time to most NLE, so today the distinction between original After Effects or Flame interfaces and Avid and Final Cut interfaces is less pronounced.

[19] Qtd. in Michael Barrier, _Oscar Fischinger. Motion Painting No. 1_ , [www.michaelbarrier.com/Capsules/Fischinger/fischinger\_capsule.htm](www.michaelbarrier.com/Capsules/Fischinger/fischinger_capsule.htm)

[20] While graphic designer does not have to wait until film is developed or computer finished rendering the animation, the design has its own “rendering” stage – making proofs. With both digital and offset printing, after the design is finished, it is sent to the printer that produces the test prints. If the designer finds any problems such as incorrect colors, she adjusts the design and then asks for proofs again. 

[21] [http://earth.google.com/](http://earth.google.com/).

---

# After Effects, or Velvet Revolution. Part II

_author: Lev Manovich_
_year: 2006_

This article is a second part of the series devoted to the analysis of the new hybrid visual language of moving images that emerged during the period of 1993-1998. Used first in film titles and television graphics, this language slowly came to dominate our visual culture. Today we see it in short films, music videos, commercials, moving images sequences which appear in interactive projects and media interfaces, and web sites. Because this fundamental shift in the aesthetics of moving images did not receive any critical discussion while it was happening – in contrast to other aspects of Digital Revolution such interactivity and the Web – I have called it a “Velvet Revolution” in moving image culture. 

My thesis is that this new language can be understood with the help of the concept of remixability – if we use this concept in a new way. Let us call it “deep remixability.” For what gets remixed is not only the content of different media, but their fundamental techniques, working methods, and ways of representation and expression. United within the common software environment, cinematography, animation, computer animation, special effects, graphic design, and typography have come to form a new metamedium. A work produced in this new metamedium can use all techniques which were previously unique to these different media, or any subset of these techniques. 

In the first part I started the discussion of how the new software-based methods of production – specifically software such as After Effects - made this language possible. We analyzed compositing; we also discussed how the interface and production workflow in After Effects themselves mixed the production methods of twentieth century cinema, animation, and graphic design. In this part I will look at other aspects of software-based moving image production, and then use this discussion to refine my analysis of how _deep remixability_ functions. 

## Three-dimensional Space as a New Platform for Media Design

As I was researching what the users and industry reviewers has been saying about After Effects, I came across a somewhat condescending characterization of this software as “Photoshop with keyframes.” I think that this characterization is actually quite useful. [1] Think about all the different ways of manipulating images available in Photoshop and the degree of control provided by its multiple tools. Think also about its concept a visual composition as a stack of potentially hundreds of layers each with its transparency and multiple alpha channels. The ability to animate such a composition and continue using Photoshop tools to adjust visual elements over time on all layers independently indeed constitute a new paradigm for creating moving images. And this is what After Effects and other animation, visual effects and compositing software make possible today. [2] And while the paradigm of working with a number of layers placed on top of each other itself is not new – consider traditional cell animation, optical printing, photocollage, and graphic design – going from a few non-transparent layers to hundreds and even thousands, each with its controls, fundamentally changes not only how a moving image looks but also what it can say. 

But innovative as it was, by the beginning of the 2000s 2D digital compositing paradigm already came to be supplemented by a new one: 3D compositing. The new paradigm has even less connections to previous media than 2D compositing. Instead, it takes the relatively new media that was born with computers in the 1960s – 3D computer graphics – and transforms it into a general platform for moving media design.

The language used in professional production milieu today reflects an implicit understanding that 3D graphics is a new medium unique to a computer. When people use terms “computer visuals,” “computer imagery,” or “CGI” which is an abbreviation for “computer generated imagery,” everybody understands that they refer to 3D graphics as opposed to any other image source such as “digital photography.” But what is my own reason for thinking of 3D computer graphics as a new media – as opposed to considering it as an extension of architectural drafting, projection geometry, or set making? Because it offers a new method for representing physical reality - both what actually exists and what is imagined. This method is fundamentally different from what has been offered by main media of the industrial era: still photography, film recording, and audio recording. With 3D computer graphics, we can represent three-dimensional structure of the world – versus capturing only a perspectival image of the world, as in lens-based recording. We can also manipulate our representation using various tools with ease and precision which is qualitatively different of a much more limited “manipulability” of a model made from any physical material (although nanotechnology promises to change this in the future.) And, as the case of contemporary architecture makes it clear, 3D computer graphics is not simply a faster way of working with geometric representations such as plans and cross-sections used by draftsmen for centuries. When the generations of young architects and architectural students started to systematically work with 3D software such as Alias in the middle of the 1990s, the ability to directly manipulate a 3D shape (rather than only dealing with its projections as in traditional drafting) quickly led to a whole new language of complex non-rectangular shapes. In other words, designers working with the media of 3D computer graphics started to imagine different things.

To come back to our topic of discussion: When Velvet Revolution of the 1990s made possible to easily combine multiple media sources in a single moving image sequence via digital compositing, CGI was added to the mix. Today, 3D models are routinely used in media compositions created in After Effects and similar software, along with all other media sources. But in order to be a part of the mix, they need to be placed on their own 2D layers and thus treated as 2D images. This was the original After Effects paradigm: all image media can meet as long as they are reduced to 2D. [3]

In contrast, in 3D compositing paradigm all media types are placed within a single 3D space. This works as follows. A designer positions all image sources which are two inherently two dimensional – for instance, digital film or digitized film, hand-drawn elements, typography – on separate 2D planes. These planes are situated within the single virtual 3D space. One advantage of this representation is that since 3D space is “native” to 3D computer graphics, 3D models can stay as they are, i.e., three-dimensional. An additional advantage is that the designer can now use all the techniques of virtual cinematography as developed in 3D computer animation. She can define different kinds of lights, fly the virtual camera around and through the image planes at any trajectory, and use depth of field and motion blur effects. [4] 

## 3D Compositing and The Logic of Reversal

In 1995 I published the article _What is Digital Cinema?_ which was my first attempt to describe the changes in the logic of moving image production I was witnessing. In that article I proposed that the logic of hand-drawn animation, which throughout the twentieth century was marginal in relation to cinema, became dominant in a computer era. Because software allows the designer to manually manipulate any image regarding of its source as though it was drawn in the first place, the ontological differences between different image media become irrelevant. Both conceptually and practically, they all reduced to hand-drawn animation. 

Having discussed the use of layers in 2D compositing using the example of After Effects, I can now add that animation logic moves from the marginal to the dominant position also in another way. The paradigm of a composition as a stack of separate visual elements as practiced in cell animation becomes the default way of working with all images in a software environment – regardless of their origin and final output media. In short, a moving image in general is now understood as a composite of layers of imagery. A “single layer image” such as un-manipulated digital video becomes an exception.

The emergence of 3D compositing paradigm can be also seen as following the logic of temporal reversal. The new representational structure as developed within computer graphics field – a 3D virtual space containing 3D models – has gradually moved from a marginal to the dominant role. In the 1970s and 1980s computer graphics were used only occasionally in a dozen of feature films such as _Alien_ (1979), _Tron_ (1981), _The Last Starfighter_ (1984), and _Abyss_ (1989), and selected television commercials and broadcast graphics. But by the beginning of the 2000s, the representation structure of computer graphics, i.e., a 3D virtual space, came to function as an umbrella within can hold all other image types regardless of their origin. An example of an application which implements this paradigm is Flame, enthusiastically described by one user as “a full 3D compositing environment into which you can bring 3D models, create true 3D text and 3D particles, and distort layers in 3D space.” [5] 

This does not mean that 3D animation itself became visually dominant in moving image culture, or that the 3D structure of the space within which media compositions are now routinely constructed is necessarily made visible (usually it is not.) Rather, the way 3D computer animation organizes visual data – as objects positioned in a Cartesian space – became the way to work with all moving image media. As already stated above, a designer positions all the elements which go into a composition – 2D animated sequences, 3D objects, particle systems, video, and digitized film sequences, still images and photographs – inside the shared 3D virtual space. There these elements can be further animated, transformed, blurred, filtered, etc. So, while all moving image media has been reduced to the status of hand-drawn animation in terms of their manipulability, we can also state that all media have become layers in 3D space. In short, the new media of 3D computer animation has “eaten up” the dominant media of the industrial age – lens-based photo, film, and video recording.

## From a “Moving Image” to a “Media Composition”

This is a good moment to pause and reflect on the very term of our discussion – moving image. When cinema in its modern form was born in the end of the nineteenth century, the new medium was understood as an extension of already familiar one – that is, as photographic image which is now moving. This understanding can be found in the press accounts of the day and also in at least one of the official names given to the new medium - “moving pictures.” On the material level, a film indeed consisted of  separate photographic frames which when driven through projector created the effect of motion for the viewer. So, the concept used to understand it indeed fit with the material structure of the medium. 

But is this concept still appropriate today? When we record video and play it, we are still dealing with the same structure: a sequence of frames. But for the professional media designers, the terms have changed. The importance of these changes is not just academic and purely theoretical. Because designers understand their media differently, they are creating media that looks different and has a new logic. 

Consider the conceptual changes, or new paradigms – which at the same time are new ways of designing – we have discussed so far. Theoretically they are not necessary all compatible with each other, but in production practice these different paradigms are used together. A “moving image” became a hybrid which can combine all different visual media invented so far – rather than holding only one kind of data such as camera recording, hand drawing, etc. Rather than being understood as a singular flat plane – the result of light focused by the lens and captured by the recording surface – it is now understood as a stack of separate layers potentially infinite in number. And rather than “time-based,” it becomes “composition-based,” or “object oriented.” That of, instead of being treated as a sequence of frames arranged in time, a “moving image” is now thought of as a two-dimensional composition that consists of  a number of objects that can be manipulated independently. And finally, in yet another paradigm of 3D compositing, the designer is working in a three-dimensional space that holds both CGI and lens-recorded flat image sources.

Of course, frame-based representation did not disappear – but it became simply a recoding and output format rather than the space where the actual design is taking place. And while the term “moving image” can be still used as an appropriate description for how the output of a design process is experienced by the viewers, it is no longer captures how the designers think about what they create. They are thinking today very differently than twenty years ago. 

If we focus on what the different paradigms summarized above have in common, we can say that filmmakers, editors, special effects artists, animators, and motion graphics designers are working on _a composition in 2D or a 3D space that consists of  a number of separate objects_. The spatial dimension became as important as temporal dimension. From the concept of a “moving image” understood as a sequence of static photographs we have moved to a new concept: _a modular media composition_. 

## Motion Graphics

Let me invoke the figure of the inversion from marginal to mainstream in order to introduce yet one more paradigmatic shift. Another media type which until 1990s was even more marginal to live action filmmaking than animation – typography – has now become an equal player along with lens-based images and all other types of media. The term “motion graphics” has been used at least since 1960 when a pioneer of computer filmmaking John Whitney named his new company Motion Graphics. However, until Velvet Revolution only a handful of people and companies have systematically explored the art of animated typography: Norman McLaren, Saul Blass, Pablo Ferro, R/Greenberg, and a few others. [6] But in the middle of the 1990s moving image sequences or short films dominated by moving animated type and abstract graphical elements rather than by live action started to be produced in large numbers. The material cause for motion graphics take off? After Effects running on PCs and other software running on relatively inexpensive graphics workstations became affordable to smaller design, visual effects, post-production houses, and soon individual designers. Almost overnight, the term “motion graphics” became well known. The five-hundred-year-old Gutenberg universe came into motion. 

Along with typography, the whole language of twentieth graphical century design was “imported” into moving image design. This development did not receive a name of its own, but it is obviously at least as important. Today (2006) the term “motion graphics” is often used to refer to all moving image sequences which are dominated by typography and/or design and embedded in larger forms. But we should recall that while in the twentieth century typography was indeed often used in combination with other design elements, for five hundred years it formed its own word. Therefore, I think it is important to consider the two kinds of “import” operations that took place during Velvet Revolution – typography and twentieth century graphic design – as two distinct historical developments.

## Deep Remixability

Although the discussions in this and the first parts of this series of articles did not cover all the changes that took place during Velvet Revolution, the magnitude of the transformations should by now be clear. While we can name many social factors that all could have and probably did play some role – the rise of branding, experience economy, youth markets, and the Web as a global communication platform during the 1990s – I believe that these factors alone cannot account for the specific design and visual logics which we see today in media culture. Similarly, they cannot be explained by simply saying that contemporary consumption society requires constant innovation, constant novel aesthetics, and effects. This may be true – but why do we see these particular visual languages as opposed to others, and what is the logic that drives their evolution? I believe that to properly understand this, we need to carefully look at media creation, editing, and design software and their use in production environment (which can range from a single laptop to a number of production companies collaborating on the same large-scale project). 

The makers of software used in production usually do not set out to create a revolution. On the contrary, software is created to fit into already existing production procedures, job roles, and familiar tasks. But software is like species within the common ecology – in this case, a shared computer environment. Once “released,” they start interacting, mutating, and making hybrids. Velvet Revolution can therefore be understood as the period of systematic hybridization between different software species originally designed to do work in different media. In the beginning of the 1990s, we had – Illustrator for making vector-based drawings, Photoshop for editing of continuous tone images, Wavefront and Alias for 3D modeling and animation, After Effects for 2D animation, and so on. By the end of the 1990s, a designer could combine operations and representational formats such as a bitmapped still image, an image sequence, a vector drawing, a 3D model and digital video specific to these programs within the same design – regardless of its destination media. I believe that the hybrid visual language that we see today across “moving image” culture and media design in general is largely the outcome of this new production environment. While this language supports seemingly numerous variations as manifested in the particular media designs, its general logic can be summed up in one phrase: remixability of previously separate media languages.

As I stressed in this text, the result of this hybridization is not simply a mechanical sum of the previously existing parts but new species. This applies both to the visual language of particular designs, and to the operations themselves. When an old operation is integrated into the overall digital production environment, it often comes to function in a new way. I would like to conclude by analyzing in detail how this process works in the case of a particular operation - in order to emphasize once again that media remixability is not simply about adding the content of different media or adding together their techniques and languages. And since remix in contemporary culture is commonly understood as these kinds of additions, we may want to use a different term to talk about the kinds of transformations the example below illustrates. Let us call it _deep remixability_. 

What does it mean when we see depth of field effect in motion graphics, films and television programs which use neither live action footage nor photorealistic 3D graphics but have a more stylized look? Originally an artifact of lens-based recording, depth of field was simulated in a computer when the main goal of 3D compute graphics field was to create maximum “photorealism,” i.e., synthetic scenes not distinguishable from live action cinematography. [7] But once this technique became available, media designers gradually realized that it can be used regardless of how realistic or abstract the visual style is – as long as there is a suggestion of a 3D space. Typography moving in perspective through an empty space; drawn 2D characters positioned on different layers in a 3D space; a field of animated particles – any composition can be put through the simulated depth of field. 

The fact that this effect is simulated and removed from its original physical media means that a designer can manipulate it a variety of ways. The parameters which define what part of the space is in focus can be independently animated, i.e., set to change over time – because they are simply the numbers controlling the algorithm and not something built into the optics of a physical lens. So, while simulated depth of field can be said to maintain the memory of the particular physical media (lens-based photo and film recording) from which it came from, it became an essentially new technique which functions as a “character” in its own right. It has the fluidity and versatility not available previously. Its connection to the physical world is ambiguous at best. On the one hand, it only makes sense to use depth of field if you are constructing a 3D space even if it is defined in a minimal way by using only a few or even a single depth cue such as lines converging towards the vanishing point or foreshortening. On the other hand, the designer can be said to “draw” this effect in any way desirable. The axis controlling depth of field does not need to be perpendicular to the image plane, the area in focus can be anywhere in space, it can also quickly move around the space, etc. 

Following Velvet Revolution, the aesthetic charge of many media designs is often derived from more “simple” remix operations – juxtaposing different media in what can be called “media montage.” However, for me the essence of this Revolution is the more fundamental _deep remixability_ illustrated by the example analyzed above. Computerization virtualized practically all media creating and modification techniques, “extracting” them from their particular physical media and turning them into algorithms. (This means that in most cases, we will no longer find any of these techniques in their pure original state.) 

## Import/Export: Design Workflow and Contemporary Aesthetics

In our discussions of digital and After Effects interface and workflow (part 1) as well as the newer paradigm of 3D compositing (this part), we have already come across the crucial aspect of software-based media production process. Until the arrival of the software-based tools, to import media in different formats into a single space was either time consuming, or expensive, or in some cases simply impossible. As we saw, software tools such as After Effects have changed this situation in a fundamental way. 

However, the contemporary software-based design of moving images – and any other media, for that matter – does not simply involve combining elements from different sources within a single application. In this section we will look at the whole workflow typical of contemporary design – be it design of moving images, still illustrations, 3D objects and scenes, architecture, music, web sites, or any other media. (Of course, most of the analysis of software-based design we did so far in this and previous article also applies of other media besides moving images. However, in this section I want to make this explicit, and therefore my examples below will include not only moving images.)

Although ”import”/”export” commands appear in most modern media authoring and editing software running under GUI, at first sight they do not seem to be very important for understanding software culture. You are not authoring new media or modifying media objects or accessing information across the globe, as in web browsing. All these commands allow you to do is to move data around between different applications. In other words, they make data created in one application compatible with other applications. And that does not look so glamorous.

Think again. What is the largest part of the economy of greater Los Angeles area? It is not entertainment - from movie production to museums and everything is between (around 15%). It turns out that the largest part is import/export business (more than 60%). More generally, one commonly evoked characteristic of globalization is greater connectivity – places, systems, countries, organizations etc. becoming connected in more and more ways. And connectivity can only happen if you have certain level of compatibility: between business codes and procedures, between shipping technologies, between network protocols, and so on. 

Let us take a closer look at import/export commands. As I will try to show below, these commands play a crucial role in software culture, and in particular in media design. Because my own experience is in visual media, my examples will come from this area but the processes I describe apply now to all media designed with software. 

Before they adopted software tools in the 1990s, filmmakers, graphic designers, and animators used completely different technologies. Therefore, as much as they were influenced by each other or shared the same aesthetic sensibilities, they inevitably created differently looking images. Filmmakers used camera and film technology designed to capture three-dimensional physical reality. Graphic designers were working with offset printing and lithography. Animators were working with their own technologies: transparent cells and an animation stand with a stationary film camera capable of making exposures one frame at a time as the animator changed cells and/or moved background. 

As a result, twentieth century cinema, graphic design, and animation (I am talking here about standard animation techniques used by commercial studios) developed distinct artistic languages and vocabularies both in terms of form and content. For example, graphic designers worked with a two-dimensional space, film directors arranged compositions in three-dimensional space, and cell animators worked with a ”two-and-a-half” dimensional space. This holds for the overwhelming majority of works produced in each field, although of course exceptions do exist. For instance, Oscar Fischinger made one abstract film that involved moving three-dimensional shapes – but as far as I know, this is the only time in the whole history of abstract animation where we see an abstract three-dimensional space. 

The differences in technology influenced what kind of content would appear in different media. Cinema showed “photorealistic” images of nature, built environment and human forms articulated by special lighting. Graphic designs feature typography, abstract graphic elements, monochrome backgrounds, and cutout photographs. And cartoons show hand-drawn flat characters and objects animated over hand-drawn but more detailed backgrounds. The exceptions are rare. For instance, while architectural spaces frequently appear in films because they could explore their three dimensionality in staging scenes, they practically never appear in animated films in any detail – until animation studios start using 3D computer animation.

Why was it so difficult to cross boundaries? For instance, in theory one could imagine making an animated film in the following way: printing a series of slightly different graphics designs and then filming them as though they were a sequence of animated cells. Or a film where a designer simply made a series of hand drawings that used the exact vocabulary of graphic design and then filmed them one by one. And yet, to the best of my knowledge, such a film was never made. What we find instead are many abstract animated films that have certain connection to various styles of abstract painting. For example, Oscar Fischinger’s films and paintings share certain forms. We can find abstract films and animated commercials and movie titles that have certain connection to graphic design of the times. For instance, some moving image sequences made by motion graphics pioneer Pablo Ferro around 1960s display psychedelic aesthetics which can be also found in posters, record covers, and other works of graphic design in the same period. [8]

And yet, it is never exactly the same language. One reason is that projected film could not adequately show the subtle differences between typeface sizes, line widths, and grayscale tones crucial for modern graphic design. Therefore, when the artists were working on abstract art films or commercials that used design aesthetics (and most key abstract animators produced both), they could not simply expand the language of printed page into time dimension. They had to invent essentially a parallel visual language that used bold contrasts, more easily readable forms and thick lines - which because of their thickness were in fact no longer lines but shapes. 

Although the limitations in resolution and contrast of film and television image in contrast to printed page played the role in keeping the distance between the languages used by abstract filmmakers and graphic designers for the most of the twentieth century, ultimately I do not think it was the decisive factor. Today the resolution, contrast and color reproduction between print, computer screens and television screens are also substantially different – and yet we often see exactly the same visual strategies deployed across these different display media. If you want to be convinced, leaf through any book or a magazine on contemporary 2D design (i.e., graphic design for print, broadcast, and the web). When you look at a spread featuring the works of a particular designer or a design studio, in most cases it’s impossible to identify the origins of the images unless you read the captions. Only then do you find that this image is a poster, that one is a still from a music video, and this one is magazine editorial. 

I am going to use Taschen’s _Graphic Design for the 21st Century: 100 of the World’s Best Graphic Designers_ (2001) for examples. [9] Peter Anderson’s images showing a heading against a cloud of hundreds of little letters in various orientations turns out to be the frames from the title sequence for _Channel Four_ documentary. His other image which similarly plays on the contrast between jumping letters in a larger font against irregularly cut planes made from densely packed letters in much smaller fonts turns to be a spread from _IT Magazine_. Since the first design was made for broadcast while the second was made for print, we would expect that the first design would employ bolder forms - however, both designs use the same scale between big and small fonts, and feature texture fields composed from text that no longer need to be read. A few pages later we encounter a design by Philippe Apeloig that uses exactly the same technique and aesthetics as Anderson. In this case, tiny lines of text positioned at different angles form a 3D shape floating in space. On the next page another design by Apeloig also creates a field in perspective made from hundreds of identical abstract shapes.

These designers rely on software’s ability (or on the designer being influenced by software use and following the same logic while doing the design manually) to treat text as any graphical primitive and to easily create compositions made from hundreds of similar or identical elements positioned according to some pattern. And since an algorithm can easily modify each element in the pattern, changing its position, size, color, etc., instead of the completely regular grids of modernism we see more complex structures that are made from many variations of the same element. 

Each designer included in this book was asked to provide a brief statement to accompany the portfolio of their work, and Lust has put this phrase as their motto: “Form-follows-process.” So, what is the nature of design process in the software age and how does it influence the forms we see today around us? 

Everybody who is practically involved in design and art today knows that contemporary designers use the same set of software tools to design everything. However, the crucial factor is not the tools themselves but the workflow process, enabled by “import” and “export” operations. 

When a particular media project is being put together, the software used at the final stage depends on the type of output media and the nature of the project – for instance, After Effects for motion graphics projects and video compositing, Illustrator or Freehand for print illustrations, InDesign for graphic design, Flash for interactive interfaces and web animations, 3ds Max or Maya for 3D computer models and animations. But these programs are rarely used alone to create a media design from start to finish. Typically, a designer may create elements in one program, import them into another program, add elements created in yet another program, and so on. This happens regardless of whether the final product is an illustration for print, a web site, or a motion graphics sequence; whether it is a still or a moving image, interactive or non-interactive, etc. Given this production workflow, we may expect that the same visual techniques and strategies will appear in all media designed with computers. 

For instance, a designer can use Illustrator or Freehand to create a 2D curve (technically, a spline). This curve becomes a building block that can be used in any project. It can form a part of an illustration or a book design. It can be imported into animation program where it can be set to motion or imported into 3D program where it can be extruded in 3D space to define a solid form. 

Each of the type of programs used by media designers – 3D graphics, vector drawing, image editing, animation, compositing – excel at particular design operations, i.e., particular ways of creating a design element or modifying already existing element. These operations can be compared to the different blocks of a Lego set. While you can make an infinite number of projects out of these blocks, most of the blocks will be utilized in every project, although they will have different functions and appear in different combinations. For example, a rectangular red block may become a part of the tabletop, part of the head of a robot, etc.

Design workflow which uses multiple software programs works in a similar way, except in this case the building blocks are not just different kinds of visual elements one can create – vector patterns, 3D objects, particle systems, etc. – but also various ways of modifying these elements: blur, skew, vectorize, change transparency level, spherisize, extrude, etc. This difference is very important. If media creation and editing software did not include these and many other modification operations, we would have seen an altogether different visual language at work today. We would have seen “digital multimedia,” i.e., designs that simply combine elements from different media. Instead, we see what I call “metamedia” – the remixing of working methods and techniques of different media within a single project. 

Here are a few typical examples of this media remixability that can be seen in the majority of design projects done today around the world. Motion blur is applied to 3D computer graphics; computer generated fields of particles are blended with live action footage to give it enhanced look, flat drawings are placed into a virtual spaces where a virtual camera moves around them, flat typography is animated as though it is made from a liquid-like material (the liquid simulation coming from computer animation software). Today a typical short film or a sequence may combine many of such pairings within the same frame. The result is a hybrid, intricate, complex, and rich media language – or rather, numerous languages that share the basic logic of remixabilty.

As we can see, the production workflow specific to the software age has two major consequences: the hybridity of media language we see today across contemporary design universe, and the use of the similar techniques and strategies regardless of the output media and type of project. Like an object build from Lego blocks, a typical design today combines techniques coming from multiple media. More precisely, it combines the results of the operations specific to different software programs that were originally created to imitate work with different physical media, (Illustrator was created to make illustrations, Photoshop - to edit digitized photographs, After Effects - to create 2D animation, etc.) While these techniques continue to be used in relation to their original media, most of them are now also used as part of the workflow on any design job. 

The essential condition that enables this new design logic and the resulting aesthetics is _compatibility between files generated by different programs_. In other words, “import” and “export” commands of graphics, animation, video editing, compositing, and modeling software are historically more important than the individual operations these programs offer. The ability to combine raster and vector layers within the same image, to place 3D elements into a 2D composition and vice versa, and so on is what enables the production workflow with its reuse of the same techniques, effects, and iconography across different media.

The consequences of this compatibility between software and file formats which was gradually achieved during the 1990s are hard to overestimate. Besides the hybridity of modern visual aesthetics and reappearance of exactly the same design techniques across all output media, there are also other effects. For instance, the whole field of motion graphics as it exists today came into existence to a large extent because of the integration between vector drawing software, specifically Illustrator, and animation/compositing software such as After Effects. A designer typically defines various composition elements in Illustrator and then import them into After Effects where they are animated. This compatibility did not exist when the initial versions of different media authoring and editing software initially became available in the 1980s. It was gradually added in particular software releases. But when it was achieved around the middle of the 1990s, within a few years the whole language of contemporary graphical design was fully imported into the moving image area – both literally and metaphorically. 

In summary, _the compatibility between graphic design, illustration, animation, and visual effects software plays the key role in shaping visual and spatial forms of the software age_. On the one hand, never before have we witnessed such a variety of forms as today. On the other hand, exactly the same techniques, compositions and iconography can now appear in any media. And at the same time, any single design may combine multiple operations which previously only existed within distinct physical or computer media. 

And you thought that “import”/’export” commands did not matter that much?

## References:

[1] Soon after the initial release of After Effects in January 1993, the company that produced it was purchased by Adobe who was already selling Photoshop. 

[2] Photoshop and After Effects were designed originally by different people at different time, and even after both were purchased by Adobe (it released Photoshop in 1989 and After Effects in 1993), it took Adobe a number of years to build close links between After Effects and Photoshop eventually making it easy going back and forth between the two programs.

[3] I say “original” because in the later version of After Effects Adobe added the ability to work with 3D layers.

[4] If 2D compositing can be understood as an extension of twentieth century cell animation where a composition consists of  a stack of flat drawings, the conceptual source of 3D compositing paradigm is different. It comes out from the work on integrating live action footage and CGI in the 1980s done in the context of feature films production. Both film director and computer animator work in a three-dimensional space: the physical space of the set in the first case, the virtual space as defined by 3D modeling software in the second case. Therefore, conceptually it makes sense to use three-dimensional space as a common platform for the integration of these two worlds. It is not accidental that NUKE, one of the leading programs for 3D compositing today was developed in house at Digital Domain which was co-founded in 1993 by James Cameron – the Hollywood director who systematically advanced the integration of CGI and live action in his films such as _Abyss_ (1989), _Terminator 2_ (1991), and _Titanic_ (1997).

[5] Alan Okey, post to forums.creativecow.net, Dec 28, 2005,  [http://forums.creativecow.net/cgi-bin/dev\_read\_post.cgi?forumid=154&postid=855029](http://forums.creativecow.net/cgi-bin/dev_read_post.cgi?forumid=154&postid=855029).

[6] For a rare discussion of motion graphics prehistory as well as equally rare attempt to analyze the field by using a set of concepts rather than as the usual coffee table portfolio of individual designers, see Jeff Bellantoni and Matt Woolman, _Type in Motion_ (Rizzoli, 1999). 

[7] For more on this process, see the chapter “Synthetic Realism and its Discontents” in _The Language of New Media_.

[8] Jeff Bellantoni and Matt Woolman, _Type in Motion, innovations in digital graphics_ (London: Thames and Hudson, 2000), 26-27.

[9] Charlotte Fiell and [Peter Fiell](http://www.amazon.com/exec/obidos/search-handle-url/index=books&field-author-exact=Peter Fiell&rank=-relevance%2C%2Bavailability%2C-daterank/103-5662557-0277435), eds., _Graphic Design for the 21st Century: 100 of the World’s Best Graphic Designers_ (Cologne: Taschen, 2003).

---

# Alan Kay’s Universal Media Machine

_author: Lev Manovich_
_year: 2006_

Medium:
> Definition 8.
> 
> a. A specific kind of artistic technique or means of expression as determined by the materials used or the creative methods involved: the medium of lithography.
> b. The materials used in a specific artistic technique: oils as a medium.

 \> _American Heritage Dictionary_, 4th edition (Houghton Mifflin, 2000).
 \>
 \> \_“The best way to predict the future is to invent it.”
 \>
 \> Alan Kay

Today we routinely use computers to create, edit and display photography, film, video, writing, typing, sculpting, architectural designs, music, etc. Although we take this for granted today, it took decades to conceptualize and develop principles and techniques which make computer simulation of older media possible. In this chapter, I look at the work or writings of Alan Kay and other pioneers of computer media working in the 1960s and 1970s in order to understand their reasons for creating these simulations. 

Keywords: simulation, remediation, metamedium, new media theory, Alan Kay, Xerox PARC

## Appearance versus Function

As a result of the adoption of GUI (graphical user interface) in the 1980s, software has replaced many other tools and technologies for the creative professional and it has also given hundreds of millions of people the ability to create, manipulate, sequence, and share media – but has this led to the invention of fundamentally _new_ forms of culture? Today, computer scientists along with media companies are busy inventing electronic _books_ and interactive _television_; consumers are happily purchasing (or downloading for free) _music albums_ and _feature films_ distributed in digital form, as well as making _photographs_ and _video_ with their digital cameras and cell phones; office workers are reading PDF _documents which imitate paper_. And even at the futuristic edge of digital culture — inhabited by smart objects and ambient intelligence — traditional forms persist: Philips showcases “smart” _household mirrors_ which can hold electronic notes and videos, while its Director of Research dreams about a normal looking _vase_ which can hold digital photographs. 

In short, the revolution in means of production, distribution, and access to media has not been accompanied by a similar revolution in the syntax and semantics of media. It is Alan Kay and his collaborators at PARC (the Palo Alto Research Center, formerly “Xerox PARC”) that we must call to task for making digital computers imitate older media. By systematically developing easy to use GUI-based software to create and edit familiar media types, Kay and others appear to have locked the computer into being a simulation machine for “old media.” Technologies developed at PARC, such as the bitmapped color display used as the main computer screen, laser printing, and the first Page Description Language which eventually led to Postscript, were conceived to support the computer’s new role as a machine for the simulation of physical media. To put these developments in terms of Bolter and Grusin’s very influential book _Remediation: Understanding New Media_ (2000), we can say that GUI-based software turned digital computers into what they might call “remediation machines.” 

Bolter and Grusin define remediation as “the representation of one medium in another” (Bolter and Grusin, 2000). According to their argument, new media always remediate older forms and therefore, we should not expect that computers would function any differently. This perspective emphasizes the continuity between computational media and earlier media forms. Rather than being separated by different logics, all media — including computers — follow the same logic of remediation. The only difference between computers and other media lies in how and what they remediate. As Bolter and Grusin put this in the first chapter of their book, “What is new about digital media lies in their particular strategies for remediating television, film, photography, and painting.” In another place in the same chapter, they make an equally strong statement that leaves no ambiguity about their position: “We will argue that remediation is a defining characteristic of the new digital media.”

If we consider today all the digital media created both by consumers and by professionals — digital photography and video shot with inexpensive cameras and cell phones, the contents of personal blogs and online journals, illustrations created in Photoshop, feature films cut on AVID, etc. — in terms of its appearance, digital media indeed often looks to the casual observer exactly the same way it did before it became digital. Thus, if we limit ourselves to looking at the _surfaces_ of media, the remediation argument accurately describes much of what goes on with computational media. But rather than accepting this condition as an inevitable consequence of the universal logic of remediation, we should ask _why_ this is the case. In other words, if contemporary computational media imitates other media, how did this become possible? There was definitely nothing in the original theoretical formulations of digital computers by Turing or Von Neumann about computers imitating other media such as books, photography, or film.

The conceptual and technical gap that separates the first room-sized computers — used by the military to calculate the shooting tables of anti-aircraft guns or to crack German communication codes — versus the contemporary small desktop and laptop computers — used by ordinary people to store, edit, and share media — is vast. The contemporary identity of a computer as a media processor took about forty years to emerge, if we count from 1949 when MIT’s Lincoln Laboratory started to work on its first interactive computers, to 1989 when the first commercial version of Photoshop was released. It took generations of brilliant and very creative thinkers to invent the multitude of concepts and techniques that today make it possible for computers to “remediate” other media so well. What were their reasons for doing this? What was their thinking? In short, why did these people dedicate their careers to inventing the ultimate “remediation machine”? 

I cannot consider the thinking of each of the key figures in the history of media computing in the space of one article. However, we can take a closer look at one place where the identity of a computer as a “remediation machine” was largely put in place — Alan Kay’s Learning Research Group at Xerox PARC in operation during the first part of the 1970s. 

We can ask two questions: first, what exactly did Kay want to do, and second, how did he and his colleagues set about to achieve their aims? [1] The brief answer — which will be expanded below — is that Kay wanted to turn computers into a “personal dynamic media” which can be used for learning, discovery, and artistic creation. His group achieved this by systematically simulating most existing media within a computer, while simultaneously adding many new properties to these media. Kay and his collaborators also developed a new type of programming language that, at least in theory, would allow users to quickly invent new types of media using the set of general tools already provided for them. All these tools and simulations of already existing media were given a unified user interface designed to activate multiple mentalities and ways of learning, including the kinesthetic, the iconic, and the symbolic.

Kay conceived of “personal dynamic media” as a fundamentally new kind of media with a number of historically unprecedented properties, such as the ability to store all of the user’s information, simulate all types of media within a single machine, and, as Kay and Adele Goldberg put it, “involve the learner in a two-way conversation” (Kay and Goldberg, 2003:399). [2] These properties enable new relationships between the user and the media she may be creating, editing, or viewing on a computer. And this is essential if we want to understand the relationships between computers and earlier media. Briefly put, while visually computational media may closely mimic other media, these media now function in fundamentally different ways. 

For instance, consider digital photography that often does in fact imitate the appearance of traditional photography. For Bolter and Grusin, this is an example of how digital media “remediates” its predecessors. But rather than only paying attention to their appearance, let us think about how digital photographs can function. If a digital photograph is turned into a physical object in the world — an illustration in a magazine, a poster on the wall, a print on a t-shirt — it functions in the same ways as its predecessor. [3] But if we leave the same photograph inside its native computer environment — which may be a laptop, a network storage system, or any computer-enabled media device such as a cell phone which allows its user to edit and move it to other devices or the Internet — it can function in ways which, in my view, make it radically different from its traditional equivalent. To use a different term, we can say that a digital photograph offers its users many more affordances that its non-digital predecessor could. For example, a digital photograph can be quickly modified in numerous ways, and equally quickly combined with other images; it can be instantly moved around the world and shared with other people; and it can be inserted into a text document, or an architectural design. Furthermore, we can automatically — by running the appropriate algorithms— improve its contrast, make it sharper, and even in some situations remove blur.

Note that only some of these new properties are specific to a particular media – in this case, a digital photograph (i.e., an array of pixels represented as numbers). Many other properties are shared by a larger class of media species – for instance, at the current stage of digital culture, all types of media files can be attached to an email message. Still others display more general features of the current GUI paradigm (which was developed thirty years ago at PARC): for instance, the fast response of the computer to its user’s actions assure that there will be, as Kay and Goldberg put it, “no discernable pause between cause and effect” (Kay and Goldberg, 2003:394). Still other features are enabled by network protocols such as TCP-IP that allow all kinds of computers and other devices to be connected to the same network. In summary, we can say that only some of the “new DNA” of a digital photograph is due to its particular place of birth, i.e., inside a digital camera. Many of its other features are the result of the current paradigm of network computing in general. 

## “Simulation is the Central Notion of the Dynabook”

While Alan Kay has articulated his ideas in a number of articles and talks, his 1977 article co-authored with one of his main PARC collaborators, computer scientist Adele Goldberg, is a particularly useful resource if we want to understand contemporary computational media. In this article, Kay and Goldberg describe the vision of the Learning Research Group at PARC in the following way: to create “\_a personal dynamic medium\_ the size of a notebook (the Dynabook) which could be owned by everyone and would have the power to handle virtually all of its owner’s information-related needs” (Ibid.: 393). [4] Kay and Goldberg ask the readers to imagine that this device “had enough power to outrace your senses of sight and hearing, enough capacity to store for later retrieval thousands of page-equivalents of reference materials, poems, letters, recipes, records, drawings, animations, musical scores, waveforms, dynamic simulations and anything else you would like to remember and change”(Ibid.: 394). 

In my view, “all” in the first statement is important: it means that the Dynabook — or the computational media environment in general, regardless of the size or form of the device in which it is implemented — should support the viewing, creating, and editing of all possible media which have traditionally been used for human expression and communication. Accordingly, while separate programs to create works in different media were already in existence, Kay’s group for the first time implemented them all together within a single machine. In other words, Kay’s paradigm was not to simply create a new type of computer-based media which would co-exist with other physical media. Rather, the goal was to establish the computer as an umbrella, a platform for _all_ already existing expressive artistic media, which Kay and Goldberg dub the “metamedium.” This paradigm changes our understanding of what media is. From Lessing’s _Laocoon; or, On the Limits of Painting and Poetry_ (1766) to Nelson Goodman’s _Languages of Art_ (1968), the modern discourse about media depends on the assumption that different media have distinct properties and in fact should be understood in opposition to each other. Putting all media within a single computer environment does not necessarily erase all differences in what various media can represent and how they are perceived — but it does bring them closer to each other in a number of ways. Some of these new connections were already apparent to Kay and his colleagues; others became visible only decades later when the new logic of media set in place at PARC unfolded more fully; some are perhaps still not visible to us today because they have not been given practical realization. One obvious example of such connections is the emergence of multimedia as a standard form of communication: Web pages, PowerPoint presentations, multimedia artworks, mobile multimedia messages, media blogs, and other communication forms which combine media. Another is the rise of common interface conventions and tools which we use in working with different types of media regardless of their origin: for instance, a virtual camera, a magnifying lens, and, of course, the omnipresent copy, cut, and paste commands. [5] Yet another is the ability to map one media into another using appropriate software — images into sound, sound into images, quantitative data into a 3D shape or sound, etc. — used widely today in such areas as DJ/VJ/live cinema performance and information visualization. This situation is the direct opposite of the modernist media paradigm of the early twentieth century which was focused on discovering the unique language of each artistic medium. All in all, it is as though different media are actively trying to reach towards each other, exchanging properties and letting each other borrow their own unique features.

Alan Turing theoretically defined a computer as a machine that can simulate a very large class of other machines, and it is this simulation ability that is largely responsible for the proliferation of computers in modern society. But as I already mentioned, neither he nor other theorists and inventors of digital computers explicitly considered that this simulation could also include media. It was only Kay and his generation that extended the idea of simulation to media – thus turning the Universal Turing Machine into a Universal Media Machine, so to speak. 

Accordingly, Kay and Goldberg write: “In a very real sense, simulation is the central notion of the Dynabook” (Ibid.: 399). When we use computers to simulate some process in the real world — the behavior of a weather system, the processing of information in the brain, the deformation of a car in a crash — our concern is to correctly model the necessary features of this process or system. We want to be able to test how our model would behave in different conditions with different data, and the last thing we want is for the computer to introduce some new property into the model that we ourselves did not specify. In short, when we use computers as a general-purpose medium for simulation, we want this medium to be completely “transparent.” 

But what happens when we simulate different media in a computer? In this case, the appearance of new properties may be welcome as they can extend the expressive and communicative potential of these media. Appropriately, when Kay and his colleagues created computer simulations of existing physical media — i.e., the tools for representing, creating, editing, and viewing these media — they “added” many new properties. For instance, in the case of a book, Kay and Goldberg point out, “It need not be treated as a simulated paper book since this is _a new medium with new properties._ A dynamic search may be made for a particular context. The non-sequential nature of the file medium and the use of dynamic manipulation allows a story to have many accessible points of view”(Ibid.: 395). [6] Kay and his colleagues also added various other properties to the computer simulation of paper documents. As Kay has referred to this in another article, his idea was not to simply imitate paper, but rather to create “magical paper” (Kay, 1999:199). For instance, the PARC team gave users the ability to modify the fonts in a document and create new fonts. They also implemented another important idea that was already developed by Douglas Engelbart’s team in the 1960s: the ability to create different views of the same structure (I will discuss this in more detail below). And both Engelbart and Ted Nelson also already “added” something else: the ability to connect different documents or different parts of the same document through hyperlinking — i.e., what we now know as hypertext and hypermedia. Engelbart’s group also developed the ability for multiple users to collaborate on the same document. This list goes on and on: e-mail in 1965, newsgroups in 1979, the World Wide Web in 1991, and so on.

Each of these new properties has far-reaching consequences. Take “search,” for instance. Although the ability to search through a page-long text document does not sound like a very radical innovation, as the document gets longer, this ability becomes more and more important. It becomes absolutely crucial if we have a very large collection of documents — such as all the Web pages available today. Although current search engines are far from being perfect and new technologies will continue to evolve, imagine how different the culture of the Web would be without them. 

Or take the capacity to collaborate on the same document(s) by a number of users connected to the same network. While it was already widely used by companies in the 1980s and 1990s, it was not until early 2000s that the larger public saw the real cultural potential of this “addition” to print media. By harvesting the small amounts of labor and expertise contributed by a large number of volunteers, social software projects — most famously, Wikipedia — created vast and dynamically updatable pools of knowledge which would be impossible to create in traditional ways. In a less visible way, every time we do a search on the Web and then click on some of the results, we also contribute to a knowledge set used by everybody else — since in deciding the sequence in which to present the results of a particular search, Google’s algorithms take into account those among the results of previous searches for the same words people found most useful.

Studying the writings and public presentations of the people who invented interactive media computing — Sutherland, Engelbart, and Kay — makes it clear that they did not come with new properties of computational media as an after-thought. On the contrary, they knew that were turning physical media into new media. In 1968, Engelbart gave his famous demo at the Fall Joint Computer Conference in San Francisco before a few thousand people that included computer scientists, IBM engineers, people from other companies involved in computers, and funding officers from various government agencies (Waldrop, 2001:287). Although Engelbart had ninety minutes, he had a lot to show. Over the few previous years, his team at The Research Center for Augmenting Human Intellect had essentially developed the modern _office_ environment as it exists today (not be confused with the modern _media design_ environment which was developed later at PARC). Their computer system included word processing with outlining features, documents connected through hypertext, online collaboration (i.e., two people at remote locations working on the same document in real-time), online user manuals, an online project planning system, and other elements of what is now commonly called the “computer-supported collaborative work.” This team also developed the key elements of the modern user interface that were later refined at PARC: the mouse and multiple windows. 

Paying attention to the sequence of this demo reveals that while Engelbart had to make sure that his audience would be able to relate the new computer system to what they already knew and could use, his focus was on a completely new set of features available in computer simulated media. Engelbart devotes the first segment of the demo to word processing, but as soon as he briefly demonstrated text entry, cut, paste, insert, naming, and saving files – in other words, the set of tools which make a computer into a more versatile typewriter – he then goes on to show at more length the features of his system which no writing medium had before: “view control.” [7] As Engelbart points out, the new writing medium could switch at the user’s wish between _many different views of the same information_. A text file could be sorted in different ways. It could also be organized as a hierarchy with a number of levels which can be collapsed and expanded – like the outline tools included in modern word processors such as Microsoft Word. 

In his demo, Engelbart next shows another example of view control, which today, forty years later, is still not available in popular document management software. He makes a long “to do” list and organizes it by locations. He then instructs the computer to display these locations as a visual graph (i.e., a set of points connected by lines.) In front of our eyes, representation in one medium changes into another medium – text becomes a graph. But this is not all. The user can control this graph to display different amounts of information – something that no image in physical media can do. As Engelbart clicks on different points in a graph corresponding to particular locations, the graph shows the appropriate part of his “to do” list. This ability to interactively change how much and what information an image shows is particularly important in today’s information visualization applications.

Next, Engelbart presents “a chain of views” which he prepared beforehand. He switches between these views using “links,” which may look like hyperlinks the way they exist on the Web today — but they actually have a different function. Instead of creating a path between many different documents à la Vannevar Bush’s Memex (often seen as the precursor to modern hypertext), Engelbart is using links as a method for switching between different views of a single document organized hierarchically. He brings a line of words displayed in the upper part of the screen; when he clicks on these words, more detailed information is displayed in the lower part of the screen. This information can, in its turn, contain links to other views that show even more detail. 

In this way, rather than using links to drift through the textual universe associatively and “horizontally,” we move “vertically” between general and more detailed information. Appropriately, in Engelbart’s paradigm, we are not “navigating” — we are “switching views.” We can create many different views of the same information, and switch between these views in different ways. And this is what Engelbart systematically explains in this first part of his demo. He demonstrates that one can change views by issuing commands, by typing numbers that correspond to different parts of a hierarchy, by clicking on parts of a picture, or on links in the text.

Since new media theory and criticism emerged in the early 1990s, endless texts have been written about interactivity, hypertext, virtual space, cyberspace, cyborgs, and so on. But I have never seen anyone discuss “view control.” And yet this is one of the most fundamental and radical new techniques for working with information and media available to us today. “View control,” i.e., the ability to switch between many different views and kinds of views of the same information, is now implemented in multiple ways not only in word processors and email clients, but also in all “media processors” (i.e., media editing software) such as AutoCAD, Maya, After Effects, Final Cut, Photoshop, inDesign, and so on. For instance, in the case of 3D software, it can usually display the model in at least half a dozen different ways: in wireframe, fully rendered, etc. In the case of animation and visual effects software, since a typical project may contain dozens of separate objects, each having dozens of parameters, it is often displayed in a way similar to how outline processors can show text. In other words, the user can switch between more and less information. One can choose to see only those parameters on which the user is working at present. One can also zoom in and out of the composition. When this is done, parts of the composition do not simply get smaller or bigger — they show less or more information automatically. For instance, at a certain scale, the user may only see the names of different parameters; but when one zooms into the display, the program may also display the graphs which indicate how these parameters change over time.

As we can see from the examples analyzed above, the aim of the inventors of computational media — Engelbart, Nelson, Kay, and colleagues with whom they have worked — was not to simply create accurate simulations of physical media. Instead, in every case, the goal was to create “a new medium with new properties” which would allow people to communicate, learn, and create in new ways. So, while today, the content of these new media may often look the same as its predecessors, we should not be fooled by this similarity. The newness lies not in the content but in the software tools used to create, edit, view, distribute, and share this content. Therefore, rather than only looking at the “output” of software-based cultural practices, we need to consider the software itself — since it allows people to work with media in of a number of historically unprecedented ways. To summarize: while on the level of appearance, computational media indeed often remediate (i.e., re-presents) previous media, the software environment in which this media “lives” is very different. 

Let me add one more item to the examples discussed above – Ivan Sutherland’s _Sketchpad_ (1962). Created by Sutherland as a part of his PhD thesis at MIT, Sketchpad deeply influenced all subsequent work in computational media (including that of Kay) not only because it was the first interactive media authoring program, but also because it made it clear that computer simulations of physical media can add many exciting new properties to the media being simulated. Sketchpad was the first software that allowed its users to interactively create and modify line drawings. As Noah Wardrip-Fruin points out, it “moved beyond paper by allowing the user to work at any of 2000 levels of magnification — enabling the creation of projects that, in physical media, would either be unwieldy large or require detail work at an impractically small size” (Wardrip-Fruin, 2003:109). Sketchpad similarly redefined graphical elements of a design as objects which “can be manipulated, constrained, instantiated, represented ironically, copied, and recursively operated upon, even recursively merged” (Ibid). For instance, if the designer defines new graphical elements as instances of a master element and later makes a change to the master, all these instances would also change automatically.

Another new property that perhaps demonstrates most dramatically how computer-aided drafting and drawing differed from their physical counterparts was Sketchpad’s use of constraints. In Sutherland’s own words, “The major feature which distinguishes a Sketchpad drawing from a paper and pencil drawing is the user’s ability to specify to Sketchpad mathematical conditions on already drawn parts of his drawing which will be automatically satisfied by the computer to make the drawing take the exact shape desired” (Sutherland, 1963/2003). For instance, if a user drew a few lines, and then gave the appropriate command, Sketchpad automatically moves these lines until they are parallel to each other. If a user gives a different command and selects a particular line, Sketchpad moves the lines in such a way so they would be parallel to each other and perpendicular to the selected line.

Although we have not exhausted the list of new properties that Sutherland built into Sketchpad, it should be clear that this first interactive graphical editor was not only simulating existing media. Appropriately, Sutherland’s 1963 paper on Sketchpad repeatedly emphasizes the new graphical capacities of his system, marveling how it opens new fields of “graphical manipulation that has never been available before” (Ibid.: 123). The very title Sutherland gives to his PhD thesis foregrounds the novelty of his work: _Sketchpad: A man-machine graphical communication system_. Rather than conceiving of Sketchpad as simply another medium, Sutherland presents it as something else — a communication system between two entities: the human and the intelligent machine. Kay and Goldberg will later also foreground this dimension of communication by referring to it as “a two-way conversation” and calling the new “metamedium” “active” (Kay and Goldberg, 2003:394). We can also think of Sketchpad as a practical demonstration of J.C. Licklider ‘s idea of the “man-machine symbiosis” applied to image making and design (Licklider, 1960/2003).

## Permanent Extendibility

As we saw, Sutherland, Nelson, Engelbart, Kay and other pioneers of computational media have added many previously non-existent properties to media they have simulated in a computer. The subsequent generations of computer scientists, hackers, and designers have added many more properties — but this process is far from finished. And there is no logical or material reason why it will ever be finished. 

To add new properties to physical media requires modifying its physical substance. But since computational media exists as software, we can add new properties or even invent new types of media simply by changing existing, or writing new, software. Adding plug-ins and extensions, as programmers have been doing with Photoshop and Firefox, is another way to innovate. One can also combine existing software together For instance, at the moment of this writing in 2007, programmers keep extending the capacities of mapping media by creating software mash-ups which combine the services and data provided by Goggle Maps, Flickr, Amazon, other sites, and media uploaded by users. In short, _“new media” is “new” because new properties (i.e., new software techniques) can always be easily added to it._ Put differently, in industrial, i.e. mass-produced media technologies, “hardware” and “software” were one and the same thing. For example, the pages of a book were bound in a particular way that fixed the order of pages. The reader could change neither the order nor the level of detail displayed à la Engelbart’s “view control.” Similarly, the film projector combined hardware and what we now call “media player” software into a single machine. In the same way, the controls built into the twentieth-century mass-produced camera could not be modified at the user’s will. And although today the user of a digital camera similarly cannot easily modify the hardware of her camera, as soon as she transfers the pictures into a computer, she has access to an endless number of controls and options for modifying her pictures via software.

In the nineteenth and twentieth centuries, there were two types of situations when the normally rigidly fixed industrial media was more less fixed. The first type of situation was when a new media was being first developed: for instance, the invention of photography in the 1820s-1840s. The second type of situation was when artists would systematically experiment with and “open up” already industrialized media — such as the experiments with film and video during the 1960s, which came to be called “Expanded Cinema.” 

What used to be separate moments of experimentations with media during the industrial era, became the norm in software society. In other words, computers have legitimized experimentation with media. Why this is so? 

What differentiates a modern digital computer from any other machine – including industrial media machines for capturing and playing media — is the separation of hardware and software. It is because an endless number of different programs performing different tasks can be written to run on the same type of machine, this machine — i.e., a digital computer — is used so widely today. Consequently, the constant invention of new and the modification of existing media software are simply two examples of this general principle. In other words, experimentation is a default feature of computational media. In its very structure, it is “avant-garde” since it is constantly being extended and thus redefined.

If in modern culture “experimental” and “avant-garde” are opposed to the normalized and the stable, this opposition has largely disappeared in software culture. And the role of the media avant-garde is performed no longer by individual artists in their studios, but by a variety of players, from very big to very small — from companies such as Microsoft, Adobe, and Apple, to independent programmers, hackers, and designers.

But this process of the continual invention of new algorithms does not move in just any direction. If we look at contemporary media software – CAD, computer drawing and painting, image editing, audio and video remixing, word processing – we see that most of their fundamental principles were already developed by the generation of Sutherland and Kay. As new techniques continue to be invented, they are layered over the foundations that were gradually put in place by Sutherland, Engelbart, Kay and others in the 1960s and 1970s.

Of course, we are not dealing here only with the history of ideas. Social and economic factors — such as the dominance of the media software market by a handful of companies, or the wide adoption of particular file formats – also constrain possible directions of software evolution. Put differently, today software development is an industry and as such it is constantly balanced between stability and innovation, standardization, and exploration of new possibilities. But it is not just any industry. New programs can be written, and existing programs can be extended and modified (if the source code is available) by anybody who has programming skills and access to a computer, a programming language, and a compiler. In other words, today software is fundamentally modifiable in a way that physical industrially produced objects usually are not. Although Turing and Von Neumann already formulated this fundamental extendibility of software in theory, its contemporary practice —thousands of people daily involved in extending the capabilities of computational media — is a result of a long historical development. This development took us from the few early room-sized computers that were not easy to reprogram, to a wide availability of cheap computers and programming tools decades later. Such democratization of software development was at the core of Kay’s vision. Kay was particularly concerned with how to structure programming tools in such a way that would make development of media software possible for ordinary users. For instance, at the end of the 1977 article I have been already extensively quoting, Kay and Goldberg write: “We must also provide enough already-written general tools so that a user need not start from scratch for most things she or he may wish to do.”

Comparing the process of continuous media innovation via new software to the history of earlier, pre-computational media, reveals a new logic at work. According to a commonplace idea, when a new medium is invented, it first closely imitates already existing media before discovering its own language and aesthetics. Indeed, the Bibles first printed by Guttenberg closely imitated the look of handwritten manuscripts; early films produced in the 1890s and 1900s mimicked the presentational format of theatre by positioning the actors on an invisible shallow stage and having them face the “audience” represented by the fixed camera. Slowly, printed books developed a different way of presenting information; similarly, cinema also developed its own original concept of narrative space. Through repetitive shifts in points of view presented in subsequent shots, the viewers were placed inside this space – thus literally finding themselves inside the story. 

Can this logic apply to the history of computer media? As theorized by Turing and Von Neuman, the computer is a general-purpose simulation machine. This is its uniqueness and its difference from all other machines and previous media. This means that the commonplace idea — that a new medium gradually finds its own language cannot apply to computer media. If this were true, it would go against the very definition of the modern digital computer. This theoretical argument is supported by practice. The history of computer media so far has not been about arriving at some standardized language — the way this, for instance, happened with cinema — but rather, it seems to be about the gradual expansion of uses, techniques, and possibilities. Rather than arriving at a particular language, we are gradually discovering that the computer can speak more and more languages. 

If we are to look more closely at the early history of computer media — for instance, the way we have been looking at Kay’s ideas and work in this text, — we will discover another reason why the idea of a new medium gradually discovering its own language does not apply to computer media. The systematic practical work on making a computer simulate and extend existing media (e.g., Sutherland’s Sketchpad, the first interactive word processor developed by Engelbart’s group, etc.) came after computers were already put to multiple uses — performing different types of calculations, solving mathematical problems, controlling other machines in real time, running mathematical simulations, simulating some aspects of human intelligence, and so on. [8] Therefore, when the generation of Sutherland, Nelson and Kay started to create “new media,” they built it on top of, so to speak, what computers were already known to be capable of. Consequently, they added new properties into the physical media they were simulating right away. This can be very clearly seen in the case of Sketchpad. Understanding that one of the roles a computer can play is that of a problem solver, Sutherland built in a powerful new feature that never before existed in a graphical medium — satisfaction of constraints. To rephrase this example in more general terms, we can say that rather than moving from an imitation of older media to finding its own language, computational media was from the very beginning speaking a new language.

In other words, the pioneers of computational media did not have the goal of making the computer into a ”remediation machine” which would simply represent older media in new ways. Instead, well knowing the new capabilities provided by digital computers, they set out to create fundamentally new kinds of media for expression and communication. These new media would use as their raw “content” the older media which already served humans well for hundreds and thousands of years — written language, sound, line drawings and design plans, and continuous tone images like paintings and photographs. But this does not compromise the newness of new media. For computational media uses these traditional human media simply as building blocks to create previously unimaginable representational structures, creative and thinking tools, and communication options. 

Although Sutherland, Engelbart, Nelson, Kay, and others developed computational media on top of already existing developments in computational theory, programming languages, and computer engineering, it would be incorrect to conceive the history of such influences as only going in one — from already existing and more general computing principles to particular techniques of computational media. The inventors of computational media have had to question many, if not most of the already established ideas about computing. They have defined many new fundamental concepts and techniques of how both software and hardware work, thus making important contributions to hardware and software engineering. A good example is Kay’s development of Smalltalk, which for the first time, systematically established a paradigm of object-oriented programming. Kay’s rationale in developing this programming language was to give a unified appearance to all applications and the interface of the PARC system and, even more importantly, to enable its users to quickly program their own media tools. Kay cites an interesting example in which an object-oriented illustration program written in Smalltalk by a particularly talented twelve-year old girl was only a page long (Kay, 1987: v). Subsequently, the object-oriented programming paradigm became very popular and object-oriented features have been added to most popular languages such as C++ and Java.

Looking at the history of computer media and examining the thinking of its inventors makes it clear that we are dealing with the opposite of technological determinism. When Sutherland designed Sketchpad, and Nelson conceived hypertext, or Kay programmed a paint program, each new property of computer media had to be imagined, implemented, tested, and refined. In other words, these characteristics did not simply come as an inevitable result of a meeting between digital computers and modern media. Computational media had to be invented, step-by-step. And it was invented by people who were looking for inspiration in modern art, literature, cognitive and educational psychology, and theory of media as much as technology. For example, Kay recalls that reading McLuhan’s _Understanding Media_ led him to a realization that the computer can be a medium rather than a mere tool (Kay, 1990: 192-193).

So far, I have talked about the history of computational media as a series of consecutive “additions.” However, this history is not only a process of the accumulation of ever more options. Although, in general, we have more techniques at our disposal today than twenty or thirty years ago, it is also important to remember that many fundamentally new techniques were conceived but never given commercial implementation or were poorly implemented and did not become popular. Or perhaps they were not marketed properly. Sometimes the company making the software might go out of business. At other times, the company that created the software might be purchased by another company that in turn would “shelve” the software so it would not compete with its own products. And so on. In short, the reasons why many new techniques have not become commonplace are multiple and are not reducible to a single principle such as “the most easy to use techniques become most popular.”

For instance, one of the ideas developed at PARC was “project views.” Each view, according to Kay, “holds all the tools and materials for a particular project and is automatically suspended when you leave it” (Ibid.: 2000). Although currently (2007) there are some applications that implement this idea, it is not a part of most popular operating systems, that is, Windows, MAC OS X, and Linux. The same holds true for the contemporary World Wide Web implementation of hyperlinks. The links on the Web are static and one-directional. Ted Nelson, who is credited with inventing hypertext around 1964, conceived it from the beginning as having a variety of other link types. In fact, when Tim Berners-Lee submitted his paper about the Web to the ACM Hypertext 1991 conference, his paper was only accepted for a poster session rather than the main conference program. The reviewers saw his system as being inferior to many other hypertext systems that were already developed in the academic world over the previous two decades(Wardrip-Fruin and Monford, 1994/2003).

## Computer as Metamedium

As we have established, the development of computational media runs contrary to previous media history. But in a certain sense, the idea of a new media gradually discovering its own language actually might apply to the history of computational media after all. And just as it was the case with printed books and cinema, this process has taken a few decades. When the first computers were built in the middle of the 1940s, they could not be used as media for cultural representation, expression, and communication. Slowly, through the work of Sutherland, Engelbart, Nelson, Papert, and others in the 1960s, the ideas and techniques were developed which made computers into a “cultural machine.” One could create and edit text, make drawings, move around virtual objects, etc. And finally, when Kay and his colleagues at PARC systematized and refined these techniques, placing them under the umbrella of GUI, which made computers accessible to multitudes, a digital computer was finally, in cultural terms, given its own language.

Or rather, it became something that no other media has been before. For what has emerged was not yet another medium, but, as Kay and Goldberg insist in their article, something qualitatively different and historically unprecedented. To mark this difference, they introduce a new term – “metamedium.”

This metamedium is unique in a number of different ways. One of them we already discussed in detail — it could represent most other media while augmenting them with many new properties. Kay and Goldberg also name other properties that are equally crucial. The new metamedium is, they assert, “active — it can respond to queries and experiments — so that the messages may involve the learner in a two way conversation.” For Kay, who was strongly interested in children and learning, this property was particularly important since, as he puts it, it “has never been available before except through the medium of an individual teacher” (Ibid.: 394). Further, as noted above, the new metamedium can handle “virtually all of its owner’s information-related needs.” It can also “serve as a programming and problem solving tool,” and “an interactive memory for the storage and manipulation of data”( Ibid.: 393). But the property that is the most important from the point of view of media history is that the _computer metamedium is simultaneously a set of different media and a system for generating new media tools and new types of media._ In other words, a computer can be used to create _new tools for working in the media it already provides, as well as to develop new not-yet-invented media_.

Using the analogy with print literacy, Kay motivates this property in the following way: “The ability to ‘read’ a medium means you can _access_ materials and tools generated by others. The ability to write in a medium means you can _generate_ materials and tools for others. You must have both to be literate” (Kay, 1990: 193). [9] Accordingly, Kay’s key effort at PARC was the development of the Smalltalk programming language. All media editing applications and GUI itself were written in Smalltalk. This made all the interfaces of all applications consistent, facilitating quick learning of new programs. Even more importantly, according to Kay’s vision, the Smalltalk language would allow even beginning users to write their own tools and define their own media. In other words, all media editing applications, which would be provided with a computer, were to serve also as examples, inspiring users to modify them and to write their own applications. 

Accordingly, a large part of Kay and Goldberg’s paper is devoted to a description of software developed by the users of their system: “an animation system programmed by animators,” “a drawing and painting system programmed by a child,” “a hospital simulation programmed by a decision-theorist,” “an audio animation system programmed by musicians,” “a musical score capture system programmed by a musician,” “an electronic circuit designed by a high school student.” As can be seen from this list that corresponds to the sequence of examples in the article, Kay and Goldberg deliberately juxtapose different types of users — professionals, high school students, and children — in order to show that everybody can develop new tools using the Smalltalk programming environment. 

This sequence of examples also strategically juxtaposes media simulations with other kinds of simulations in order to emphasize that simulation of media is only a particular case of computer’s general ability to simulate all kinds of processes and systems. This juxtaposition of examples gives us an interesting way to think about computational media. Just as a scientist may use a simulation to test different conditions and play different what/if scenarios, a designer, writer, a musician, a filmmaker, or an architect working with computer media can quickly “test” different creative directions in which the project can be developed, as well as see how modifications of various “parameters” might affect the project. The latter option is particularly easy today, since the interfaces of most media editing software not only explicitly present these parameters, but also simultaneously give the user the controls for their modification. For instance, when the Formatting Palette in Microsoft Word shows the font used by the currently selected text, it is displayed in a column next to all the other fonts available. Trying a different font is as easy as scrolling down and selecting the name of a new font.

To give users the ability to write their own programs was a crucial part of Kay’s vision for the new “metamedium” he was inventing at PARC. According to Noah Wardrip-Fruin, Engelbart’s research program was focused on a similar goal: “Engelbart envisioned users creating tools, sharing tools, and altering the tools of others.” (Wardrip-Fruin, 2003:232). Unfortunately, when Apple shipped the first Macintosh in 1984, which was to become the first commercially successful personal computer modeled after PARC system, it did not have easy-to-use programming environment. HyperCard, written for the Macintosh in 1987 by Bill Atkinson (who was a PARC alumni) gave users the ability to quickly create certain kinds of applications – but it did not have the versatility and breadth envisioned by Kay. Only recently, as t general computer literacy has widened and many scripting languages have become available – Perl, PHP, Python, ActionScript, Vbscript, JavaScript, etc. – more people have started to create their own tools by writing software. A good example of a contemporary programming environment — which is currently very popular among artists and designers and which, in my view, is close to Kay’s vision — is Processing. [10] Built on top of the Java programming language, Processing features a simplified programming style and an extensive library of graphical and media functions. It can be used to develop complex media programs and also to quickly test ideas. Appropriately, the official name for Processing projects is “sketches.” [11] In the words of Processing initiators and main developers Ben Fry and Casey Reas, the language focuses “on the ‘process’ of creation rather than end results.” [12] Another popular programming environment that similarly enables quick development of media software is MAX/MSP and its successor PD developed by Miller Puckette. 

## Conclusion

The story I have just related could also be told differently. It is possible to put Sutherland’s work on Sketchpad in the center of computational media history; or Engelbart and his Research Center for Augmenting Human Intellect, which throughout the 1960s, developed hypertext (independently of Nelson), the mouse, the window, the word processor, mixed text/graphics displays, and a number of other “firsts.” Or we could shift focus to the work of the Architecture Machine Group at MIT, which since 1967 was headed by Nicholas Negroponte (and in 1985 became known as The Media Lab). We also need to recall that by the time Kay’s Learning Research Group at PARC flashed out the details of GUI and programmed various media editors in Smalltalk (a paint program, an illustration program, an animation program, etc.), artists, filmmakers, and architects were already using computers for more than a decade and a number of large-scale exhibitions of computer art were displayed in major museums such as the Institute of Contemporary Art, London, The Jewish Museum, New York, and the Los Angeles County Museum of Art. And certainly, in terms of advancing computer techniques for visual representation enabled by computers, other groups of computer scientists had already made important advancements. For instance, at the University of Utah, which became the main place for computer graphics research during the first part of the 1970s, scientists were producing 3D computer graphics much superior to the simple images that could be created on the computers being built at PARC. Next to the University of Utah, a company called Evans and Sutherland (headed by the same Ivan Sutherland who was also teaching at University of Utah) was already using 3D graphics for flight simulators — essentially pioneering the type of new media that is now called “navigable 3D virtual space.” [13]

The reason I decided to focus on Kay is his theoretical formulations that place computers in relation to other media and media history. While Vannevar Bush, J.C. Licklider, and Douglas Engelbart were primarily concerned with augmentation of intellectual and in particular scientific work, Kay was equally interested in computers as “a medium of expression through drawing, painting, animating pictures, and composing and generating music” (Ibid.: 393). Therefore, if we really want to understand how and why computers were redefined as a cultural media, and how the new computational media is different from earlier physical media, I think that Kay provides us with the best perspective. At the end of the 1977 article that served as the basis for our discussion, he and Goldberg summarize their arguments in the phrase, which in my view is the best formulation we have so far of what computational media is artistically and culturally. They call the computer _“a metamedium” whose content is “a wide range of already-existing and not-yet-invented media.”_ In another article published in 1984, Kay unfolds this definition. By way of conclusion, I would like to quote this longer definition, which is as accurate and inspiring today as it was when Kay wrote it:

> It [a computer] is a medium that can dynamically simulate the details of any other medium, including media that cannot exist physically. It is not a tool, though it can act like many tools. It is the first _metamedium_, and as such it has degrees of freedom for representation and expression never before encountered and as yet barely investigated. (Kay, 1984/1991: 225). 

## Notes:

[1] Kay has expressed his ideas in a few articles and a large number of interviews and public lectures. The following have been my main primary sources: Alan Kay and Adele Goldberg, _Personal Dynamic Media_, IEEE Computer, Vol. 10, No. 3 (March), 1977. My quotes are from the reprint of this article in _New Media Reader_, Eds. Noah Wardrip-Fruin and Nick Montfort (The MIT Press, 2003); Alan Kay, “The Early History of Smalltalk,” (HOPL-II/4/93/MA, 1993); Alan Kay, “A Personal Computer for Children of All Ages,” Proceedings of the ACM National Conference, Boston, August 1972; Alan Kay, _Doing with Images Makes Symbols_ (University Video Communications, 1987), videotape available at [www.archive.org](http://www.archive.org);  Alan Kay, “User Interface: A Personal View,” p. 193, in Brenda Laurel, ed., _The Art of Human-Computer Interface Design_ (Reading, Mass.: Addison-Wesley, 1990) 191-207; David Canfield Smith at al., “Designing the Star user Interface,” _Byte_, issue 4 (1982).

[2] Since the work of Kay’s group in the 1970s, computer scientists, hackers, and designers added many other unique properties. For instance, we can quickly move media around the Net and share it with millions of people using Flickr, YouTube, and other sites.

[3] However, consider the following examples of things to come: “Posters in Japan are being embedded with tag readers that receive signals from the user’s ‘ID’ tag and send relevant information and free products back.” (Hoshimo, 2005:25)

[4] The emphasis in this and all following quotes from this article in mine – L.M.

[5] This elevation of the techniques of particular media to a status of general interface conventions can be understood as the further unfolding of the principles developed at PARC in the 1970s. Firstly, the PARC team specifically wanted to have a unified interface for all new applications. Secondly, they developed the idea of “universal commands” such as “move,” “copy,” and “delete.” As described by the designers of Xerox Star personal computer released in 1981, “MOVE is the most powerful command in the system. It is used during text editing to rearrange letters in a word, words in a sentence, sentences in a paragraph, and paragraphs in a document. It is used during graphics editing to move picture elements, such as lines and rectangles, around in an illustration. It is used during formula editing to move mathematical structures, such as summations and integrals, around in an equation.” David Canfield Smith et al., “Designing the Star User Interface,” _Byte_, issue 4/1982, pp. 242-282. 

[6] Emphasis mine – L.M.

[7] The complete video of Douglas Engelbart’s 1968 demo is available at [http://sloan.stanford.edu/MouseSite/1968Demo.html](http://sloan.stanford.edu/MouseSite/1968Demo.html).

[8] We should also mention the work on SAGE by MIT Lincoln Laboratory, which by the middle of the 1950s, had already established the idea of interactive communication between a human and a computer via a screen with a graphical display and a pointing device. In fact, Sutherland developed Sketchpad on the TX-2, which was the new version of a larger computer MIT had constructed for SAGE.

[9] The emphasis is in the original.

[10] [www.processing.org](http://www.processing.org).

[11] [http://www.processing.org/reference/environment/](http://www.processing.org/reference/environment/).

[12] [http://processing.org/faq/](http://processing.org/faq/).

[13] For more on 3D virtual navigable space as a new media, or a “new cultural form,” see chapter “Navigable Space” in _The Language of New Media_.

## References:

[http://www.research.philips.com/newscenter/pictures/display-mirror.html/](http://www.research.philips.com/newscenter/pictures/display-mirror.html/), accessed January 20, 2005.

Bolter, Jay and Richard Grusin (1999). _Remediation: Understanding New Media_. Cambridge, MA.: The MIT Press.

Canfield Smith, David et al., (1982). Designing the Star User Interface. _Byte_, issue 4/1, pp. 242-282. 

Engelbart, Douglas C. (1968). Live public demonstration of the NLS at the Fall Joint Computer Conference, Convention Center in San Francisco,available online at [http://sloan.stanford.edu/MouseSite/1968Demo.html](http://sloan.stanford.edu/MouseSite/1968Demo.html).

Gassee, Jean-Louis with Howard Rheingold (1991). The Evolution of Thinking Tools. _The Art of Human-Computer Interface Design_. Ed., Brenda Laurel. Boston, MA.: Addison-Wesley Professional.

Hoshimo, Takashi (2005). Bloom Time Out East. _ME: Mobile Entertainment._ November, issue 9, p. 25 [www.mobile-ent.biz](www.mobile-ent.biz).

Kay, Alan and Adele Goldberg (1977/2003). Personal Dynamic Media. _New Media Reader_, eds. Noah Wardrip-Fruin and Nick Montfort. Cambridge, MA.: The MIT Press.

Kay, Alan (1993). The Early History of Smalltalk. (HOPL-II/4/93/MA.)

Kay, Alan (1972). A Personal Computer for Children of All Ages. _Proceedings of the ACM National Conference_. Boston, August. 

Kay, Alan (1987). _Doing with Images Makes Symbols_ (University Video Communications, 1987), videotape available at [www.archive.org](http://www.archive.org).

Kay, Alan (1990). User Interface: A Personal View. _The Art of Human-Computer Interface Design_, 191-207. Ed., Brenda Laurel. Boston, MA.: Addison-Wesley Professional.


Licklider, J.C. (1960/2003). Man-Machine Symbiosis. _New Media Reader_, eds. Noah Wardrip-Fruin and Nick Montfort. Cambridge, MA.: The MIT Press.

Manovich, Lev (2001). _The Language of New Media_. Cambridge, MA.: The MIT Press.

[www.processing.org](http://www.processing.org), accessed January 20, 2005.
[http://www.processing.org/reference/environment/](http://www.processing.org/reference/environment/), accessed January 20, 2005.

[http://processing.org/faq/](http://processing.org/faq/), accessed January 20, 2005.

Sutherland, Ivan (1963/2003). Sketchpad. A Man-Machine Graphical Communication System. _New Media Reader_, eds. Noah Wardrip-Fruin and Nick Montfort. Cambridge, MA.: The MIT Press.

Waldrop, M. Mitchell (2001). _The Dream Machine: J.C.R. Licklider and the Revolution That Made Computing Personal_. London: Viking.

Wardrip-Fruin, Noah and Nick Montfort, eds. (2003). _New Media Reader_. Cambridge, MA.: The MIT Press.

---

# Understanding Hybrid Media

_author: Lev Manovich_
_year: 2007_


## The Invisible Revolution

In the second part of the 1990s, moving-image culture went through a fundamental transformation. Previously separate media—live-action cinematography, graphics, still photography, animation, 3D computer animation, and typography—started to be combined in numerous ways. By the end of the decade, the “pure” moving-image media became an exception and hybrid media became the norm. 

Here are a few examples. [1] A music video may use live action while also employing typography and a variety of transitions done with computer graphics (video for “Go” by Common, directed by Convert/MK12/Kanye West, 2005). Or it may embed the singer within an animated painterly space (video for Sheryl Crow’s “Good Is Good,” directed by Psyop, 2005). A short film may mix typography, stylized 3D graphics, moving design elements, and video (_Itsu_ for Plaid, directed by the Pleix collective, 2002). [2]

In some cases, the juxtaposition of different media is clearly visible (video for “Don’t Panic” by Coldplay, 2001; main title for the television show _The Inside_ by Imaginary Forces, 2005). In other cases, a sequence may move between different media so quickly that the shifts are barely noticeable (GMC Denali “Holes” commercial by Imaginary Forces, 2005). Yet in other cases, a commercial or a movie title may feature continuous action shot on video or film, with the image periodically changing from a more natural to a highly stylized look. 

Such media hybridity does not necessary manifest itself in a collage-like aesthetics that foregrounds the juxtaposition of different media and different media techniques. As a very different example of what media remixability can result in, consider a more subtle aesthetics well captured by the name of the software that to a large extent made the hybrid visual language possible: After Effects (first released in 1993). If in the 1990s computers were used to create highly spectacular special effects or “invisible effects,” [3] toward the end of that decade we see something else emerging: a new visual aesthetics that goes “beyond effects.” In this aesthetics, the whole project—whether a music video, a TV commercial, a short film, or a large segment of a feature film—displays a hyper-real look in which the enhancement of live-action material is not completely invisible but at the same time it does not call attention to itself the way special effects usually tended to do (examples: Reebok I-Pump “Basketball Black” commercial and _The Legend of Zorro_ main title, both by Imaginary Forces, 2005).

Although the particular aesthetic solutions vary from one video to the next and from one designer to another, they all share the same logic: the simultaneous appearance of multiple media within the same frame. Whether these media are openly juxtaposed or almost seamlessly blended together is less important than the fact of this co-presence itself.

(Note that each of the examples above can be substituted by numerous others. You can easily find examples of all the various aesthetics I will be discussing in this essay by simply watching television in most countries and paying attention to the graphics, going to a club to see a VJ performance, visiting the websites of motion-graphics designers and visual-effects companies, or opening any book on contemporary design.) 

Today, hybrid visual language is also common to a large proportion of short “experimental” and “independent” (i.e., not commissioned by commercial clients) videos being produced for media festivals, the web, mobile media devices, and other distribution platforms. [4] Many visuals created by VJs and “live cinema” artists are also hybrid, combining video, layers of 2D imagery, animation, and abstract imagery generated in real time. [5] And as the animations of Jeremy Blake, Ann Lislegaard, and Takeshi Murata demonstrate, works created explicitly for art-world distribution similarly often choose to use the language of hybridity.

In contrast to other computer revolutions, such as the fast growth of World Wide Web in the second part of the 1990s, the revolution in moving-image culture that took place around the same time was not acknowledged by the popular media or by cultural critics. What received attention were the developments that affected narrative filmmaking—the use of computer-produced special effects in Hollywood feature films or the inexpensive digital video and editing tools outside of it. But another process that happened on a larger scale—the transformation of the visual language used by most forms of moving images outside of narrative films—has not been critically analyzed. In fact, although the results of these transformations were fully visible by about 1998, at the time of this writing (spring 2007), I am not aware of a single theoretical article discussing them. 

One reason is that in this revolution no new media per se were created. Designers were making still images and moving images just as they had in the previous decade, but the visual language of these images was now very different. In fact, it was so new that, in retrospect, the postmodern imagery of the 1980s, which at the time looked strikingly radical, now appears as a barely noticeable blip on the radar of cultural history. 

Since the end of the 1990s, the new hybrid visual language of moving images has dominated global visual culture. While narrative features still mostly use live-action footage, and videos shot by “consumers” and “prosumers” with commercial video cameras and cell phones are similarly usually left as is (at least, for now), almost everything else is hybrid. This includes commercials, music videos, motion graphics, TV graphics, dynamic menus, graphics for mobile media content, and other types of animated, short non-narrative films and moving-image sequences being produced around the world today by media professionals, including companies, individual designers and artists, and students. I believe that at least 80 percent of such sequences and films follow the aesthetics of hybridity. (This includes practically all “motion graphics,” i.e., animated non-narrative sequences that appear as parts of longer pieces.)

Today, narrative features rarely mix different graphical styles within the same frame. However, a number of recent films have featured the kind of highly stylized aesthetics that would have previously been identified with illustration rather than filmmaking: Larry and Andy Wachowski’s _Matrix_ series (1999–2003), Robert Rodriguez’s _Sin City_ (2005), and Zack Snyder’s _300_ (2007). These feature films are a part of a growing trend to shoot a large portion of the film using a “digital backlot” (green screen). [6] Consequently, most or all shots in such films are created by composing the footage of actors with [or: making a composite of the footage with actors and] computer-generated sets and other visuals. 

These films do not juxtapose their different media in as dramatic a way as what we commonly see in motion graphics. Nor do they strive for the seamless integration of CGI (computer-generated imagery) visuals and live action that characterized the earlier special-effects features of the 1990s, such as _Terminator 2_ (1991) and _Titanic_ (1997) (both by James Cameron). Instead, they explore the space in between juxtaposition and complete integration. 

_Matrix_, _Sin City_, _300_, and other films shot on a digital backlot combine multiple media to create a new stylized aesthetics that cannot be reduced to the already familiar look of live-action cinematography or 3D computer animation. Such films display exactly the same logic as motion graphics, which at first sight might appear to be very different. This logic is the same one we observe in the creation of new hybrids in biology. That is, the result of the hybridization process is not simply a mechanical sum of the previously existing parts but a new “species”—a new kind of visual aesthetics that did not exist previously.

## Media Hybridity in _Sodium Fox_ and _Untitled (Pink Dot)_

Blake’s _Sodium Fox_ and Murata’s _Untitled (Pink Dot)_ (both 2005) offer excellent examples of the new hybrid visual language that currently dominates moving-image culture. (To be more precise, we should call this language a _metalanguage_ since it includes numerous grammars and styles.) Among the many well-known artists working with moving images today, Blake was the earliest and most successful in developing his own style of hybrid media. His video _Sodium Fox_ is a sophisticated blend of drawings, paintings, 2D animation, photography, and effects available in software. Using a strategy commonly employed by artists in relation to commercial media in the twentieth century, Blake slows down the fast-paced rhythm of motion graphics as they are usually practiced today. However, despite the seemingly slow pace of his film, it is as informationally dense as the most frantically changing motion graphics such as one may find in clubs, music videos, television station IDs, and so on. _Sodium Fox_ creates this density by exploring in an original way the basic feature of the software-based production environment in general and programs such as After Effects in particular, namely, the construction of an image from potentially numerous layers. Of course, traditional cel animation as practiced in the twentieth century also involved building up an image from a number of superimposed transparent cels, with each one containing some of the elements that together make up the whole image. For instance, one cel could contain a face, another lips, a third hair, yet another a car, and so on. 

With computer software, however, designers can precisely control the transparency of each layer; they can also add different visual effects, such as blur, between layers. As a result, rather than creating a visual narrative based on the motion of visual elements through space (as was common in twentieth-century animation, both commercial and experimental), designers now have many new ways to create visual changes. Exploring these possibilities, Blake crafts his own visual language in which visual elements positioned on different layers are continuously and gradually “written over” each other. If we connect this new language to twentieth-century cinema rather than to cel animation, we can say that rather than fading in a new frame as a whole, Blake continuously fades in separate parts of an image. The result is an aesthetics that balances visual continuity with a constant rhythm of visual rewriting, erasing, and gradual superimposition. 

Like _Sodium Fox,_ Murata’s _Untitled (Pink Dot)_ also develops its own language within the general paradigm of media hybridity. Murata creates a pulsating and breathing image that has a distinctly biological feel to it. In the last decade, many designers and artists have used biologically inspired algorithms and techniques to create animal-like movements in their generative animations and interactives. However, in the case of _Untitled (Pink Dot)_, the image as a whole seems to come to life. 

To create this pulsating, breathing-like rhythm, Murata transforms live-action footage (scenes from one of the _Rambo_ films) into a flow of abstract color patches (sometimes they look like oversize pixels, and at other times they may be taken for artifacts of heavy image compression). But this transformation never settles into a final state. Instead, Murata constantly adjusts its degree. (In terms of the interfaces of media software, this would correspond to animating a setting of a filter or an effect.) One moment we see almost unprocessed live imagery; the next moment it becomes a completely abstract pattern; the following moment parts of the live image again become visible, and so on. 

In _Untitled (Pink Dot)_ the general condition of media hybridity is realized as a permanent metamorphosis. True, we still see some echoes of movement through space, which was the core method of predigital animation. (Here this is the movement of the figures in the live footage from _Rambo._) But now the real change that matters is the one between different media aesthetics: between the texture of a film and the pulsating abstract patterns of flowing patches of color, between the original “liveness” of human figures in action as captured on film and the highly exaggerated artificial liveness they generate when processed by a machine. 

Visually, _Untitled (Pink Dot)_ and _Sodium Fox_ do not have much in common_._ However, as we can see, both films share the same strategy: creating a visual narrative through continuous transformations of image layers, as opposed to discrete movements of graphical marks or characters, which was common to both the classic commercial animation of Disney and the experimental classics of Norman McLaren, Oskar Fischinger, and others. Although we can assume that neither Blake nor Murata has aimed to achieve this consciously, in different ways each artist stages for us the key technical and conceptual change that defines the new era of media hybridity. Media software allows the designer to combine any number of visual elements regardless of their original media and to control each element in the process. This basic ability can be explored through numerous visual aesthetics. The films of Blake and Murata, with their different temporal rhythms and different logics of media combination, exemplify this diversity. Blake layers over various still graphics, text, animation, and effects, dissolving elements in and out. Murata processes live footage to create a constant image flow in which the two layers—live footage and its processed result—seem to constantly push each other out.

## Deep Remixability

It is a truism that we live in a “remix culture.” Today, many cultural and lifestyle arenas—music, fashion, design, art, web applications, user-created media, food—are governed by remixes, fusions, collages, or mash-ups. If postmodernism defined 1980s, remix definitely dominates 2000s, and it will probably continue to rule the next decade as well. Following are just a few examples of the current diversity in remix practices. In his 2004–5 winter collection, John Galliano (a fashion designer for the house of Dior) mixed vagabond look, Yemenite traditions, Eastern European motifs, and other sources, which he collects during his extensive travels around the world. Over the last few years, DJ Spooky has been working on _Rebirth of a Nation_, a feature-length remix of D. W. Griffith’s 1915 film _The Birth of a Nation_. In April 2006, the Annenberg Center at the University of Southern California ran a two-day conference on “Networked Publics,” which devoted separate sessions to various types of remix cultures on the web: political remix videos, anime music videos, machinima, alternative news, infrastructure hacks, and the like. [7] (In addition to these, the web also houses a growing number of software mash-ups defined by Wikipedia as “a website or application that combines content from more than one source into an integrated experience.”) [8]

Remixing originally had a precise and narrow meaning specific to music. Although precedents of remixing can be found earlier, it was the introduction of multitrack mixers in the 1970s that made remixing a standard practice. With each element of a song—vocals, drums, etc.—available for separate manipulation, it became possible to remix the song—to change the volume of some tracks or substitute new tracks for the old ones. Gradually the term _remix_ became more and more broad, today referring to any reworking of already existing cultural work(s), whether visual projects, software, or literary texts. 

Can we understand the new hybrid language of moving image as a type of remix? I believe so—if we make one crucial distinction. Typical remix combines content within the same media or content from different media. For instance, a music remix may combine music elements from any number of artists; anime music videos may combine parts of anime films and music taken from a music video. Professionally produced motion graphics and other moving-image projects also routinely mix together content in the same media and/or from different media. For example, in the beginning of the “Go” music video, the video rapidly switches between live-action footage of a room and a 3D model of the same room. The live-action shots also incorporate a computer-generated plant and a still photographic image of mountain landscape. Later, shots of a female dancer are combined with elaborate animated typography. Throughout the video, we also see the characters being transformed into abstract animated patterns. And so on.

Such remixes of content from different media are definitely common today in moving-image culture. But for me, the essence of the “hybrid revolution” lies in something else altogether. Let’s call it “deep remixability.” _What gets remixed today is not only content from different media but also their fundamental techniques, working methods, and ways of representation and expression_. United within the common software environment, cinematography, animation, computer animation, special effects, graphic design, and typography have come to form a new _metamedium_. A work produced in this new metamedium can use all the techniques, or any subset of these techniques, that used to be unique to these different media. 

We may think of this new metamedium as a vast library of all previously known media techniques. But that is not all. Once all types of media met within the same digital environment—and this was accomplished in the second part of the 1990s—they started _interacting_ in ways that could never have been predicted or even imagined previously. 

For instance, while particular media techniques continue to be used in relation to their original media, they can also be applied to other media. Here are a few examples of this “crossover” logic: typography is choreographed to move in 3D space; motion blur is applied to CGI; algorithmically generated fields of particles are blended with live-action footage to give it an enhanced look; a virtual camera is made to move around a virtual space filled with 2D drawings. In each of these examples, the technique that was originally associated with a particular medium—cinema, cel animation, photorealistic computer graphics, typography, graphic design—is now applied to a different media.

Such interaction among virtualized media techniques is a key feature of moving-image culture today. Therefore, I have decided to introduce a special term — _deep remixability_ — to differentiate it from the simple remix of media content with which we are all familiar, be it music remixes, anime video remixes, 1980s postmodern art and architecture, and so on. 

## From Media to Algorithms

Why did the hybrid revolution take place? Why do the numerous moving-image sequences we see today use juxtapositions of media and hybrids of different media techniques as their basic aesthetic principle? We can identify many social and cultural factors that all could have played, and probably did play, some role since their emergence in the 1990s—for instance, branding, experience economy, youth markets, and the web. However, I believe that these factors alone cannot account for the specific design and visual logics that we see today in media culture. Similarly, they cannot be explained by simply saying that contemporary global consumption societies require constant innovation, novel aesthetics, and effects. This may be true—but why do we see these particular visual languages as opposed to others, and what is the logic that drives their evolution? I believe that to properly understand this, we need to carefully look at media design software and its use in production environments.

In the middle of the 1990s, relatively inexpensive graphics workstations and personal computers running image editing, graphic design, animation, video editing, compositing, special effects, and illustration software became commonplace and affordable for freelance graphic designers, illustrators, and small postproduction and animation studios. As we have seen, the results were dramatic. Within about five years, modern visual culture was fundamentally transformed. However, the makers of software used in production usually do not set out to create a revolution. On the contrary, software is usually created to fit into already existing production procedures, job roles, and familiar tasks. This applies to most media design software released in the 1990s. 

But software is like various species within the common ecology—in this case, a shared computer environment. Once “released,” they start interacting, mutating, and making hybrids. The invisible revolution that took place in the second part of the 1990s can therefore be understood as _the period of systematic hybridization between different software originally designed to be used by professionals working in different media_. By 1993, the key software applications were already available: Adobe Illustrator for making vector-based drawings, Adobe Photoshop for editing of continuous-tone images, Wavefront and Alias for 3D modeling and animation, and Adobe After Effects for 2D animation and visual effects. In the second part of that decade, software manufacturers gradually added technologies that made these programs compatible with one another. As a result, by the end of the 1990s, a designer could combine operations and representational formats such as a bitmapped still image, a vector image, a 3D model, and digital video within the same design project. I believe that the hybrid visual language we see today across moving-image culture and media design in general is largely the outcome of this new compatibility among key media design software. 

While this language supports seemingly numerous variations as manifested in the particular media designs, its general logic can be summed up in one phrase: hybridization, or deep remixability, of previously separate media techniques and media languages. The crossover effect is one manifestation of this deep remixability. Another crucial effect relates to the changes in the way that separate media techniques can function. Yet another effect is the transformation of what were previously unavoidable artifacts of media technologies into new techniques for media design. 

Let us look in detail at a particular example, which will illustrate the last two effects. What does it mean when we see depth-of-field effects in motion graphics, films, and television programs that use neither live-action footage nor photorealistic 3D graphics but have a more stylized look? Originally an artifact of lens-based recording, depth of field was simulated in 3D computer graphics when the goal was to create maximum “photorealism,” i.e., synthetic scenes that could not be distinguished from live-action cinematography. [9] But once this technique became available, media designers gradually realized that it could be used regardless of how realistic or abstract the overall visual style was—as long as there was a suggestion of 3D space. Typography moving in perspective through an empty space; drawn 2D characters positioned on different layers in a 3D space; a field of animated particles—any composition can be put through the simulated depth-of-field effect. 

The fact that this effect is simulated and removed from its original physical media means that a designer can manipulate it in a variety of ways. The parameters that define what part of the space is in focus can be animated independently—for example, set to change over time—because they are simply the numbers controlling the algorithm and not something built into the optics of a physical lens. So, while simulated depth of field can be said to maintain the memory of the particular physical media (lens-based photography and film recording) from which it came, it developed into an essentially new technique that functions as a “character” in its own right. It has a fluidity and versatility not available previously. Its connection to the physical world is ambiguous at best. On the one hand, it only makes sense to use depth of field if you are constructing a 3D space, even if it is defined in a minimal way by using only a few or even a single depth cue, such as lines converging toward the vanishing point or foreshortening. On the other hand, the designer can be said to “draw” this effect in any way desirable. The axis controlling depth of field does not need to be perpendicular to the image plane; the area in focus can be anywhere in space, and it can also move quickly around the space. 

As this example shows, computerization virtualized practically all media creating and modification techniques, “extracting” them from their particular physical medium of origin and turning them into algorithms. This means that, in most cases, we will no longer find any of these techniques in their pure original state. The media techniques became “supercharged” and amplified; their range and application were extended; and their controls were made explicit, formalized, quantifiable, and programmable. 

## The Variable Form

As the films of Blake and Murata illustrate, in contrast to twentieth-century animation, in contemporary motion graphics the transformations often affect the frame as a whole. Everything inside the frame keeps changing: visual elements, their transparency, the texture of the image, etc. In fact, if something stays the same for a while, that is an exception rather than the norm. 

Such _constant change on many visual dimensions_ is another key feature of animated sequences and short films produced today. Just as we did in the case of media hybridity, we can connect this preference for constant change to the particulars of software used in media design.

Digital computers allow us to represent any phenomenon or structure as a set of variables. In the case of design and animation software, this means that all possible forms—visual, temporal, spatial, interactive—are similarly represented as sets of variables that can change continuously. This new logic of form is deeply encoded in the interfaces of software packages and the tools they provide. In 2D animation/compositing software such as After Effects, each new object added to the scene by a designer shows up as a long list of variables—geometric position, color, transparency, and the like. Each variable is immediately assigned its own channel on the timeline used to create animation. [10] In this way, the software literally invites the designer to start animating various dimensions of each object in the scene. The same logic extends to the parameters that affect the scene as a whole, such as the virtual camera and the virtual lighting. If you add a light to the composition, this immediately creates half a dozen new animation channels describing the colors of the lights, their intensity, position, orientation, and so on. 

During the 1980s and 1990s, the general logic of computer representation—that is, representing everything as variables that can take on changing values—was systematically embedded throughout the interfaces of media design software. As a result, although a particular software application does not directly prescribe to its users what they can and cannot do, the structure of the interface strongly influences the designer’s thinking. In the case of moving-image design, the result of having a timeline interface with multiple channels all just waiting to be animated is that they usually do get animated by the designer. If previous constraints in animation technology—from the first optical toys in the early nineteenth century to the standard cel animation system in the twentieth century—resulted in an aesthetics of discrete and limited temporal change, then the interfaces of computer animation software quickly led to a new aesthetics: the continuous transformations of all image elements and often the image as a whole.

This change in animation aesthetics deriving from the interface design of animation software was paralleled by a change in another field—architecture. In the mid-1990s, when architects started to use software originally developed for computer animation and special effects, including Alias and later Maya, the logic of animated form entered architectural thinking as well. As already noted, animation software conceptualizes form as being inherently and infinitely variable. Even more crucial was the exposure of architects to the new generation of modeling tools in the animation software of the 1990s. For decades, the main technique for 3D modeling was to represent a virtual object as a collection of flat polygons. But by the mid-1990s, the faster processing speeds of computers and the increased size of computer memory made it practical to offer another technique on desktop workstations—spline-based modeling. This new technique for representing form pushed architectural thinking away from rectangular modernist geometry and toward the privileging of smooth and complex forms made from continuous curves. As a result, since the late 1990s, the aesthetics of “blobs” has come to dominate the thinking of many architecture students, young architects, and even already well-established “star” architects. 

But this was not the only consequence of the switch from traditional architectural tools and CAD software to animation/special effects software. Traditionally, architects created new projects on the basis of existing typology. A church, a private house, a railroad station all had their well-known types—the spatial templates determining the way space is to be organized. Similarly, when designing the details of a particular project, an architect would select from the various standard elements with well-known functions and forms: columns, doors, windows, etc. [11] In the twentieth century, mass-produced housing only further embraced this logic, which eventually became encoded in the interfaces of CAD software. 

But when, in the early 1990s, Gregg Lynn, Lars Spuybroek, the firm Asymptote, and other young architects started to use 3D software that had been created for other industries—computer animation, special effects, computer games, and industrial design—they found that this software came with none of the standard architectural templates or details. In addition, if CAD software for architects assumed that the basic building blocks of a structure are rectangular forms, 3D software came with different geometric primitives—smooth curves and 3D surfaces and solids made from such curves—which were appropriate for the creation of characters and products. 

As a result, rather than being understood as a composition made up of template-driven standardized parts, a building could now be imagined as a single continuous curved form that can vary infinitely. It could also be imagined as a number of continuous forms interacting together. In either case, the shape of each of these forms was not determined by any kind of a priori typology. 

(In retrospect, we can think of this highly productive “misuse” of 3D animation and modeling software by architects as another case of a crossover logic. In this case, it is a crossover between the conventions and the tools of one design field—character animation and special effects—and the ways of thinking and knowledge of another field, namely, architecture.)

Relating this discussion of architecture to our main subject here—animated graphics—we can see now that both fields were by the 1990s using computerization in a structurally similar way. In the case of animated images, until that decade, changes in an image over time were limited, discrete, and usually semantically driven (connected to the narrative). After the switch to software, moving images came to feature constant changes on many visual dimensions that were no longer limited by the semantics. As defined by numerous motion-graphics projects of the 2000s, contemporary temporal visual form constantly changes, pulsates, and mutates beyond the need to communicate meanings and narrative. (The films of Blake and Murata offer striking examples of this new aesthetics of a variable form; many other examples can easily be found by surfing websites that collect works by motion graphics studios and individual designers.)

A parallel process took place in architectural design. The differentiations in a traditional architectural form were connected to the need to communicate meaning and/or to fulfill the architectural program. An opening in a wall was either a window or a door; a wall was a boundary between functionally different spaces. Thus, just as in animation, the changes in the form were limited and they were driven by semantics. But today, the architectural form designed with modeling software can change continuously, and these changes no longer have to be justified by function. 

The Yokohama International Port Terminal, designed by Foreign Office Architects, illustrates very well the aesthetics of variable form in architecture. The building is a complex and continuous spatial volume without a single right angle and with no distinct boundaries that would break the form into parts or separate it from the ground plane. Visiting the building in December 2003, I spent four hours exploring the continuities between the exterior and the interior spaces and enjoying the constantly changing curvature of its surfaces. The building can be compared to a Mobius strip, except that it is much more complex, less symmetrical, and more unpredictable. It would be more appropriate to think of it as a whole set of such strips smoothly interlinked together. 

To summarize this discussion of how the shift to software-based representations affected the modern language of form: All constants were substituted by variables whose values can change continuously. As a result, culture went through what can be called the _continuity turn_. Both the temporal visual form of graphic cinema and the spatial form of architecture started to explore the new universe of continuous change and transformation. (The fields of product design and space design were similarly affected.) Previously, such an aesthetics of “total continuity” was imagined by only a few artists. For instance, in the 1950s, architect Friedrich Kiesler conceived a project titled _Continuous House_ that is, as the name implies, a single continuously curving form unconstrained by the usual divisions into rooms. But when architects started to work with the 3D modeling and animation software in the 1990s, such thinking became commonplace. Similarly, the understanding of a moving image as a continuously changing visual form, which previously could be found only in a small number of films made by experimental filmmakers throughout the twentieth century such as Fischinger’s _Motion Painting_ (1947), now became the norm.

## The Aesthetics of Continuity

Today, there are many successful short films under a few minutes and small-scale building projects based on the aesthetics of continuity, but the next challenge for both motion graphics and architecture is to discover ways to employ this aesthetics on a larger scale. In architecture, a number of architects have already begun to successfully address this challenge. Examples include already realized projects such as the Yokohama International Port Terminal or the Kunsthaus in Graz (2004), as well as those that have yet to be built, such as Zaha Hadid’s Performing Arts Centre on Saadiyat Island in Abu Dhabi, United Arab Emirates (2007). 

What about motion graphics? Blake is one of the few artists who have systematically explored how hybrid visual language can work in longer pieces. _Sodium Fox_ is 14 minutes; an earlier piece, _Mod Lang_ (2001), is 16 minutes. The three films that make up _Winchester Trilogy_ run for 21, 18, and 12 minutes. None of these films contain a single cut. 

_Sodium Fox_ and _Winchester Trilogy_ use a variety of visual sources, which include photography, old film footage, drawings, animation, type, and computer imagery. All these media are weaved together into a continuous flow. As I have already pointed out in relation to _Sodium Fox_, in contrast to shorter motion-graphics pieces with their frenzy of movement and animation, Blake’s films contain very little animation in a traditional sense. Instead, various still or moving images gradually fade in on top of each other. So, while each film moves through a vast terrain of different visuals—color and monochrome, completely abstract and figurative, ornamental and representational—it is impossible to divide the film into temporal units. In fact, even when I tried, I could not keep track of how the film got from one kind of image to a very different one just a couple of minutes later. And yet these changes were driven by some kind of logic, even if my brain could not compute it while I was watching each film. 

The hypnotic continuity of these films can be partly explained by the fact that all visual sources in the films were manipulated using graphics software. In addition, many images were slightly blurred. As a result, regardless of the origin of the images, they all acquired a certain visual coherence. So though the films skillfully play on the visual and semantic differences between live-action footage, drawings, photographs with animated filters on top of them, and other media, these differences do not create juxtaposition or stylistic montage. [12] Instead, various media seem to peacefully coexist, occupying the same space. Thus, Blake’s films can be said to stage for us the functioning of the digital _metamedium_ in general. 

According to computer scientist Alan Kay, who proposed this term in the 1970s, we should think of the digital computer as a metamedium containing all the different “already existing and non-yet-invented media.” [13] What does this imply for the aesthetics of digital projects? In my view, it does _not_ imply that the different media necessarily fuse together, or make up a new single hybrid, or result in “multimedia,” “intermedia,” or a totalizing _Gesamtkunstwerk_. As demonstrated by Blake’s films, _different media become compatible but at the same time they can preserve their distinct identities_. In his films, the visual elements in different media maintain their defining characteristics and unique appearances. 

Blake’s films expand our understanding of what the aesthetics of continuity can encompass. Different media are continuously added on top of each other, creating the experience of a continuous flow, which nevertheless preserves their differences. Ann Lislegaard also belongs to the “continuity generation.” Her recent films involve continuous navigation or an observation of imaginary architectural spaces. Visually, we may relate her films to the work of a number of twentieth-century painters and filmmakers: Giorgio de Chirico, Balthus, the Surrealists, Alan Resnais (_Last Year at Marienbad_), Andrei Tarkovsky (_Stalker_). However, the sensibility of Lislegaard’s films is unmistakably that of the early twenty-first century. The spaces are not clashing together as in, for instance, _Last Year at Marienbad_, nor are they made uncanny by the introduction of figures and objects (a practice of Réne Magritte and other Surrealists). Instead, like her fellow artists Blake and Murata, Lislegaard presents us with forms that continuously change before our eyes. She offers us yet another aesthetics of continuity made possible by software such as After Effects, which, as has already been noted, translates the general logic of computer representation—the substitution of all constants with variables—into concrete interfaces and tools. 

The visual changes in Lislegaard’s _Crystal World (after J. G. Ballard)_ happen right in front of us, and yet they are practically impossible to track. Within the space of a minute, one space is completely transformed into something very different. And it is impossible to say how exactly this happened. 

_Crystal World_ creates its own hybrid aesthetics that combines realistic spaces (done with 3D computer animation), completely abstract forms, and a digitized photograph of plants. Since everything is rendered in gray scale, the differences between media are not loudly announced. And yet they are there. It is this kind of subtle and at the same time precisely formulated distinction between different media that gives this video its unique beauty. In contrast to twentieth-century montage, which created meaning and effect through dramatic juxtapositions of semantics, compositions, spaces, and different media, Lislegaard’s aesthetics is in tune with other cultural forms. Today, the creators of minimal architecture and space design, web graphics, [14] generative animations and interactives, ambient electronic music, and progressive fashions similarly assume that a user is intelligent enough to make out and enjoy subtle distinctions and continuous modulations.

Lislegaard’s _Bellona (after Samuel R. Delany)_ (2005) takes the aesthetics of continuity in a different direction. We are moving through and around what appears to be a single set of spaces. (Historically, such continuous movement through a 3D space has its roots in the early uses of 3D computer animation in flight simulators and subsequently in first-person computer games and architectural walk-throughs.) Though we pass through the same spaces many times, each time the spaces are rendered in a different color scheme. The transparency and reflection levels also change. Lislegaard is playing a game with the viewer: while the overall structure of the film soon becomes clear, it is impossible to keep track of which space we are in at any given moment. We are never quite sure if we have already been there and it is now simply lighted differently, or if it is a space that we have not yet visited. 

_Bellona_ can be read as an allegory of “variable form.” In this case, variability is played out as seemingly endless color schemes and transparency settings. It does not matter how many times we have already seen the same space, it always can appear in a new way. 

To show us our world and ourselves in a new way is, of course, one of the key goals of all modern art regardless of the media. By substituting all constants with variables, media software institutionalizes this desire. Now everything can always change, and everything can be rendered in a new way. But, of course, simple changes in color or variations in a spatial form are not enough to create a new vision of the world. It takes talent to transform the possibilities offered by software into meaningful statements and original experiences. Lislegaard, Blake, and Murata — along with many other talented designers and artists working today — offer us distinct and original visions of our world in the stage of continuous transformation and metamorphosis: visions that are fully appropriate for our time of rapid social, technological, and cultural change. 

## References:

[1] I have drawn these examples from three published sources, so they are easy to trace. The first is a DVD, _I Love Music Videos_, which contains a selection of forty music videos for well-known bands from the 1990s and early 2000s, published in 2002. The second is a _onedotzero\_select_ DVD, a selection of sixteen independent short films, commercial work, and a live cinema performance presented by the onedotzero festival in London and published in 2003. The third is a fall 2005 sample work DVD from Imaginary Forces, which is among most well-known motion-graphics production houses today. The DVD includes titles and teasers for feature films, TV show titles, television station IDs, and graphics packages for cable channels. Most of the videos I am referring to can be also found on the Internet.

[2] Included on _onedotzero\_select DVD 1_. Online version at [http://www.pleix.net/films.html](http://www.pleix.net/films.html), accessed April 8, 2007.

[3] _Invisible effect_ is the standard industry term. For instance, the film _Contact_, directed by Robert Zemeckis, was nominated for [1997 VFX HQ Awards](http://www.vfxhq.com/awards/97awards.html) in the following categories: Best Visual Effects, Best Sequence (The Ride), Best Shot (Powers of Ten), Best Invisible Effects (Dish Restoration), and Best Compositing. See [www.vfxhq.com/1997/contact.html](www.vfxhq.com/1997/contact.html).

[4] In December 2005, I attended the Impact media festival in Utrecht and asked the festival director what percentage of the submissions they received that year featured hybrid visual language as opposed to “straight” video or film. His estimate was about 50 percent. In January 2006, I was part of the review team that judged the projects of students graduating from SCI-ARC, a well-known research-oriented architecture school in Los Angeles. According to my informal estimate, approximately one half of the projects featured complex curved geometry made possible by Maya, a modeling software now commonly used by architects. Given that both After Effects and Maya’s predecessor, Alias, was introduced in the same year — 1993 — I find this quantitative similarity in the percentage of projects that use new languages made possible by these software quite telling. 

[5] For examples, consult Paul Spinrad, ed., _The VJ Book: Inspirations and Practical Advice for Live Visuals Performance_ (Feral House, 2005); Timothy Jaeger, _VJ: Live Cinema Unraveled_, available from [www.vj-book.com](www.vj-book.com); and websites such as [www.vjcentral.com](www.vjcentral.com) and [www.live-cinema.org](www.live-cinema.org).

[6] [http://en.wikipedia.org/wiki/Digital\_backlot](http://en.wikipedia.org/wiki/Digital\_backlot), accessed April 8, 2007.

[7] [http://netpublics.annenberg.edu/](http://netpublics.annenberg.edu/), accessed February 4, 2007.

[8] [http://en.wikipedia.org/wiki/Mashup_%28web\_application\_hybrid%29](http://en.wikipedia.org/wiki/Mashup_(web\_application\_hybrid)), accessed February 4, 2007.

[9] For more on this process, see the chapter “Synthetic Realism and Its Discontents” in my book _The Language of New Media_ (Cambridge, MA: MIT Press, 2001). 

[10] Although the details vary among different software packages, the basic paradigm I am describing here is common to most of them. 

[11] I am grateful to Lars Spuybroek, the principal of Nox, for explaining to me how software-driven architectural design subverted traditional architectural thinking based on typologies. 

[12] In the “Compositing” chapter of _The Language of New Media,_ I have defined “stylistic montage” as “juxtapositions of stylistically diverse images in different media.” 

[13] Alan Kay and Adele Goldberg, “Personal Dynamic Media,” _IEEE Computer_ 10, no. 3 (March 1977). My quote is from the reprint of this article in _New Media Reader_, ed. Noah Wardrip-Fruin and Nick Montfort (Cambridge, MA: MIT Press, 2003).

[14] See my article “Generation Flash,” 2002, available at [www.manovich.net](www.manovich.net).

---

# Information as an Aesthetic Event

_author: Lev Manovich_
_year: 2007_

How do designers of information technology understand the interaction between the users and devices today? How do they design user interfaces? In this article I will analyze the shift in information technology design which took place between 1998 and 2007. Contrary to ten years ago, today the designers no longer try to make the interfaces invisible. Instead, the _interaction is treated as an event_ - as opposed to "non-event", as in the previous "invisible interface" paradigm. Put differently, using personal information devices is now conceived as a carefully orchestrated _experience_, rather than only a means to an end. I will discuss different aspects of this new interface paradigm using the examples of OSX, LG Chocolate, and iPhone. 

## Introduction

If you recall the very first mobile phone you owned – let's say at the end of the 1990s or maybe even the first years of 2000s – and compare to the phone you have (or wish to have) today, the difference in design is striking. 

The change in the design of mobile phones is just one example of larger trend which I call _aestheticization of information tools_. The trend begins around 1996-98 (1996: Wallpaper magazine [1] was launched and Collete [2], the first store for hip design products, opened in Paris; 1997: opening of Guggenheim Museum Bilbao; 1998: introduction of first iMac). It can certainly be connected with the democratization of design, the rise of branding, the competition in global economy and other larger socio-economic shifts. However, there are also particular reasons for it - non-reducible to these other forces.

Until mid-1990s only people working in particular jobs spent all their time interacting with information. In addition, these interactions were limited to work spaces and times; they were not spilling into leisure and other non-work activities. The rise of information society has greatly increased the proportion of people whose work involves information processing. At the same time, during the 1990s, interacting with information via computers and computer-based devices has gradually entered people’s lives outside of work. Because of its inherent multi-functionality and expandability, a computer and other devices build on top of it such as a mobile phone came to be used for all kinds of non-work activities: entertainment, culture, social life, communication with others. Consequently, work and non-work, professional and personal met within the same information processing machines - the same physical objects, same hardware and software interfaces, and in some cases even the same software. 

As these machines came to be redefined as consumer objects to be used in all areas of people’s lives, their aesthetics were altered accordingly. The associations with work and office culture and the emphasis on efficiency and functionality came to be replaced by new references and criteria. They include being friendly, playful, pleasurable, expressive, fashionable, signifying cultural identity, aesthetically pleasing, and designed for emotional satisfaction. Accordingly, the modernist design formula “form follows function” came to be replaced by new formulas such as “form follows emotion” – adopted by such as companies as world famous Frog design [3] which happened to design first Macintosh computers.

## Aestheticization of Interfaces

Something else has happened in this process. Until this decade the design of user interfaces was often ruled by the idea that the interface should be invisible. In fact, the really successful interface was supposed to be the one which the user does not notice. This paradigm made sense until the middle of the 1990s – that is, during the period when, outside of work, people used information devices on a limited basis. But what happens when the quantity of these interactions greatly increases, and information devices become intimate companions of people's lives? The more you use a mobile phone, a computer, a media player, or another personal information device, the more you "interact with an interface" itself. 

Regardless of whether the designers realize this consciously or not, today the design of user interaction reflects this new reality. The designers no longer try to hide the interfaces. Instead, the _interaction is treated as an event_ - as opposed to "non-event", as in the previous "invisible interface" paradigm. Put differently, using personal information devices is now conceived as a carefully orchestrated _experience_, rather than only a means to an end. The interaction explicitly calls attention to itself. The interface engages the user in a kind of game. The user is asked to devote significant emotional, perceptual and cognitive resources to the very act of operating the device. 

## OS X

Today a typical information device such as a mobile phone has two kinds of interfaces. One is a physical interface such as buttons and the phone cover. The second is a media interface: graphical icons, menus, and sounds. The new paradigm that treats interaction as an aesthetic and meaningful experience equally applies to both types of interfaces.

The most dramatic example of the historical shift in how interfaces are understood is the differences in user interface design between the successive generations of the operating system (OS) used in Apple computers – OS 9 and OS X. Released in October of 1999, OS 9 was the last version of Mac OS still based on the original system which came with the first Macintosh in 1984. Its look and feel – the strict geometry of horizontal and vertical lines, the similarly restrictive palette of grays and white, simple, and business-like icons – speaks of modernist design and "form follows function" ideology. It fits with grey suites, office buildings in International Style, and the whole twentieth century office culture. 

The next version of the operating system introduced in 2001 - OS X - was a radical departure. Its new user interface was called Aqua. Aqua's icons, buttons, windows, cursor, and other interface elements were colorful and three-dimensional. They used shadows and transparency. The programs animated when started. The icons in Dock playfully increased in size as the user moved a cursor over them. And if in OS 9 default desktop backgrounds were flat single-color monochrome, the backgrounds which came with Aqua were much more visually complex, more colorful, and assertive – drawing attention to themselves rather than trying to be invisible. 

In OS X, the interaction with the universal information processing machine of our time – a personal computer – was redefined as an explicitly aesthetic experience. This aesthetic experience became as important as the functionality (in technical terms, "usability"). The word aesthetics is commonly associated with beauty, but this is not the only meaning which is relevant here. Under \_OS X, user interface was aestheticized in a sense that it was now to explicitly appeal to and stimulate senses - rather than only users' cognitive processes.

The transformation of Apple from a company which was making hardware and software to a world leader in consumer product design – think of all design awards won by iMACs, Powerbooks, iPods and other Apple products – is itself the most clear example of what I called aestheticization of information tools. It is relevant here to recall another classical meaning of aesthetics: the coordination of all parts and details of an artwork or design – lines, forms, colors, textures, materials, movements, sounds. (I talk about classical aesthetics because twentieth century art has often aimed at opposite effects – shock, collision, and establishment of meaning and aesthetic experience through montage rather than unification of parts.) The critical and commercial success of Apple products and the truly fanatical feelings they evoke in many people to a large extent has to do with the degree of this integration which until now has not been seen in commercial products in this price range. In each new product or version, the details are refined until they all work together to create a rich, smooth, and consistent sensorial whole. This also applies to the way hardware and software work together. As an example, think of the coordination between the circular movement of user's finger on the track wheel of the original iPod and the corresponding horizontal movement of menus on the screen (which borrows from OS X column-view).

In the beginning of 2000s other personal technology companies had gradually begun to follow Apple in putting more and more emphasis on design of their products across all price categories. Sony started using the "Sony Style" phrase. In 2004 Nokia introduced its first line of "fashion phones" [4] declaring that personal technology can be "an object of desire" (two years later this became true for the whole mobile phone market). By investing in industrial designs of their consumer products, Samsung was able to move from an unknown supplier to a top world brand. Even the companies whose information products were almost exclusively used by professionals and business users started to compete in design of their products. For instance, the new 2006 version of BlackBerry smart phone popular with business people and professionals was introduced with this slogan: "BlackBerry Pearl – Small, Smart, and Stylish".

## Interaction as Theatre; Interaction as Experience

In retrospect we can see that aestheticization (or perhaps, “theatricization”) - of user interfaces of laptops, mobile phones, cameras and other mobile technology which took place between approximately 2001 and 2005 was conceptually prepared in previous decades. Based on the work done in the 1980s, computer designer and theorist Brenda Laurel published a groundbreaking book _Computers as Theatre_ in 1991 [5]. She called interface an expressive form and compared it with a theatrical performance. Using Aristotle's Poetics as her model, she suggested that interaction should lead to "pleasurable enjoyment". 

The notion of interaction as theatre brings an additional meaning to the idea that a mobile phone engages its user in a kind of game or a play which I put forward in the beginning. In suggesting this I was thinking of how the buttons on LG Chocolate suddenly appear glowing in red when you switch the phone on; or how when you select some option on the same phone it confirms your selection by replacing the current screen with a whole new graphic screen; or how pressing the cover of Motorola Pebble opens the phone in an expected and unique way. In over words, I was referring to a variety of ways in which the current generation of mobiles responds to user actions in a surprising and often seemingly exaggerated manner. (This applies to both physical interfaces and media interfaces.) The notion of interaction as theatre makes us notice another dimension of this play-like behavior. As I will describe in more detail below using the example of switching on LG Chocolate mobile, various sensorial responses which a mobile generates in response to our actions often are not single events but rather sequences of effects. As in a traditional theatre play, these sequences unfold in time. various sensorial effects play on each other, and it is their contrast as well as the differences between the senses being addressed – touch, vision, hearing – which together add up to a complex dramatic experience. 

In 1991 when Laurel published her book the use of technology products was still limited to particular professions but as designers of iMac have clearly recognized, at the end of the decade these products were becoming the mainstream items of consumer economy. And this economy as a whole was undergoing a fundamental change. In their 1999 book _Experience Economy: Work Is Theatre & Every Business a Stage_ Joseph Pine and James H. Gilmore argued that consumer economy was entering a new stage where the key to successful business was delivering experiences. [6] According to the authors, this was the new stage following the previous stages centered on goods themselves and later services. The authors stated that to be successful today, the company "must learn to stage a rich, compelling experience". If Laurel evoked theatre as a way to think about the particular case of human-computer interaction, authors of _Experience Economy_ suggested that it can be a metaphor for understanding the interaction between consumers and products in the new economy in general. 

Aestheticization (which is my preferred term) of hardware design and user interfaces of information products which took place throughout the industry in the following decade fits very well with the idea of "experience economy". Like any other interaction, _interaction with information devices became a designed experience._ In fact, we can say that the three stages in the development of user interfaces of computers – command-line interfaces, classical GUI of 1970s-1990s, and the new sensual and entertaining interfaces of post OS X era can be correlated to the three stages of consumer economy as a whole: goods, services, and experiences. Command-line interfaces "deliver the goods", that is, they focus on pure functionality and utility. GUI adds "service" to interfaces. And at next stage, interfaces become "experiences". 

## Experience Design in LG Chocolate

The idea of the experience economy works particularly well to explain how the physical interaction with technology objects - as opposed to their physical forms and screen interfaces only - was turned into the stage for delivering rich sensorial and often seductive experiences. For instance, early mobile phones did not have any covers at all. The screen and the key were always there, and they were always visible. By the middle of 2000s, the simple acts of opening a mobile phone or pressing its buttons were turned into real micro-plays: very short narratives complete with visual, tactile, and three-dimensional effects. In the short history of mobile phones, the examples of particular models whose commercial and critical popularity can to a significant degree be attributed to the innovative sensorial narratives of interaction with them are the Motorola RAZR V3 (2004) and LG Chocolate (2006; the actual model number is LG VX-8600). 

LG Chocolate sold over one million units in only eight weeks following its introduction. This phone offered a unique (from a 2006 point of view) interactive narrative which can be called a real Gesamtkunstwerk – directly engaging the three senses of sight, sound, and touch, and evoking the fourth sense of taste through the phone's name and color. When the phone is closed and off, it appears as a solid monochrome shape with its display and touchpad completely invisible. It is a mysterious Thing. When you switch the phone on, the whole multimedia drama unfolds. The Thing gradually awakens. Suddenly previously invisible buttons appear in a glowing red color. The screen lights up and it begins to play an animation. As the short animation unfolds towards its finale, the phone suddenly vibrates at exactly the same time when the LG logo comes into the screen.

Given that the process of aestheticization of information tools only started less than a decade ago, I am sure that what we have seen so far are just initial shy steps. More wild effects and experiences which we can’t even imagine today wait for us in the future. 

## Supermodernism: The Aesthetics of Disappearance

As iMac (1998) and OS X (2001) demonstrate, aestheticization of information technology paradigm was applied equally to designs of information products and their user interfaces – i.e. both “hardware” and “software.” In fact, although released at different time, the first iMacs (1998-1999) and OS X (2001-) share similar aesthetic features: bright clear colors, use of transparency / translucency, and rounded forms. And while both aim to remove the standard twentieth century associations of information technology - cold, indifferent to human presence, suited only for business - they at the same time cleverly exploit their technological identity. Both the translucency of iMAC plastic case, and the Dock magnification and Genie effects in Aqua interface similarly stage technology as magical and supernatural. 

In this respect it is relevant that a number of Ive’s subsequent designs of Apple products – Titanium and Aluminum PowerBooks (2001, 2003), iPod and iPod shuffle (2001, 2005), Mac Mini (2005), the accompanying power cables, earphones, and so on – adopted very different minimal aesthetics. In this aesthetics the technological object seems to want to disappear, fade into the background, and become ambient - rather than actively attracting attention to itself and its technological magic, like the original iMACs. Whether consciously or not, these Apple designs communicate, or rather foretell, the new identity of personal IT which today is still in development - the actual practical disappearance of technological objects as such as they become fully integrated into other objects, surfaces, spaces, and clothes. This is the stage of ubiquitous computing in which a technological fetish is dissolved into the overall fabric of material existence. The actual details of this potential future dematerialization will most probably be different from how it is imagined today, but the trend itself is clearly visible. But how to stage this future disappearance using technology available today? Apple designs of the first part of the 2000s can be understood as responses to this challenge. Historically, their particular aesthetics occupies an intermediate, transitional stage - between the stage of technology as a designed lifestyle object (exemplified by Apple iMacs from 1998 onward or Nokia’s Fashion collection of mobiles, 2004-) and its future stage as an invisible infrastructure implanted inside other objects, architectural forms and human body. 

In 1998 Dutch architecture theorist Hans Ibelings has published a slim but soon to become influential book _Supermodernism_ [7] in which he identified the similar aesthetics of disappearance in the architecture of the 1990s as exemplified by Foundation Cartier in Paris (Jean Nouvel, 1994), Railroad Switch Tower in Basel (Herzog & De Meuron, 1994-1997), or French National Library in Paris (Dominique Perrault, 1989). According to Ibelings, supermodern aesthetics “is characterized mainly by the absence of distinguishing marks, by neutrality.” [8] This aesthetics stands in opposition to previous architectural aesthetics of the 1980s and early 1990s: “Whereas postmodernist and deconstructionist architecture almost always contain a message, today architecture is increasingly conceived as an empty medium.” [9] But while architecture as “an empty medium” on purpose avoids communicating messages and over-signifying, it does instead something different and new. It creates unique sensorial experiences. The large, open, and empty interior volumes, the use of translucency and transparency, the employment of a variety of new materials and finishes which create finely focused sensorial effects – all these tactics have been used by supermodern architects to craft unique spatial experiences – where the experience one can have by being inside a particular building cannot be duplicated anywhere else. 

In retrospect, we can correlate supermodern aesthetics with the rise of “experience design” / “experience economy” in the second part of the 1990s. We can also see it as already partially employing the new logic of architecture which becomes fully operational in the next decade – that is, “signature” buildings by brand-name architects crucial for branding cities and companies alike. Canonical supermodern buildings used simple geometric volumes which offered subtle sensorial effects inside and tried to disappear when seen from a distance. Canonical brand architecture of 2000s appears to work differently – its easily identifiable and unique forms function as icons designed for media communication. But at the same time, just as supermodern buildings, signature iconic buildings also function as spatial destinations, i.e., they offer unique sensorial experiences inside. The complex and dynamic forms of Frank Gehry’s buildings such as Guggenheim Bilbao, Los Angeles Disney Hall, or Strata Center at MIT is a perfect example of this double function – they look dramatic and unique when photographed, and they simultaneously promise a unique spatial experience which requires a physical visit. 

Ibelings was looking only at architecture, but ten years later, we can say that the same supermodern aesthetics was put forward by Ives and his team in designing Apple products in the first half of the 2000s. The new developed materials and finishes, the flat largely empty surfaces uninterrupted by multiple buttons or screws (as it is the case in typical technological objects), the monochrome appearance which visually emphasizes the shape as a whole, the rounded corners, the glow of Apple logo which creates a three-dimensional effect, and the simplicity of the overall 3D form – all these techniques work together to create a powerful impression that an object is about to fade and completely dissolve. And at the same time, the same object – a laptop, monitor, or iPOD - creates another spatial experience which, despite the dramatic differences in size between these buildings and architecture, is a perfect analog of “a new spatial sensibility” that Ibelings found in supermodern buildings - “boundless and undefined space” which however “is not an emptiness but a safe contained, a flexible shell.” [10]

Ibelings has speculated about the different reasons for supermodern aesthetics in architecture, but in the case of personal information technologies, the spatial form which is simultaneously “boundless” and “undefined” and also “a safe contained, a flexible shell,” seem to me a perfect spatial metaphors for the meanings of these technologies as intended by Apple, Nokia and other progressive (i.e. attuned to lifestyle and cultural trends) technology/design companies in 2000s – mobility, flexibility, lack of predefined boundaries and limits. The last meaning, however, also happens to define a modern computer in theoretical terms – a universal simulation machine which via software can simulate unlimited number of other machines and tools and, again via software, is infinitely expandable. But how do you find a visual and/or spatial expression for such a meta-machine? This is one of the challenges of contemporary aesthetics. The supermodernist aesthetics of Apple products as designed by Ive and his team has so far been one of more successful solutions to this fundamental challenge.

## References:

[1] [www.wallpaper.com/](www.wallpaper.com/)

[2] [www.colette.fr/](www.colette.fr/)

[3] [http://www.design-emotion.com/2006/08/15/getting-emotional-with-hartmut-esslinger/](http://www.design-emotion.com/2006/08/15/getting-emotional-with-hartmut-esslinger/).

[4] [http://www.3g.co.uk/PR/Sept2004/8307.htm](http://www.3g.co.uk/PR/Sept2004/8307.htm).

[5] Brenda Laurel, _Computers as Theatre_ (Reading, MA: Addison-Wesley, 1991).

[6] Joseph Pine and James H. Gilmore, _Experience Economy: Work Is Theatre & Every Business a Stage_ (Harvard Business Press, 1999).

[7] Hans Ibelings, _Supermodernism: Architecture in the Age of Globalization_ (NAI Publishers, 1997).

[8] Ibid., p. 88.

[9] Ibid.

[10] _Supermodernism_, p. 62

---

# What Comes After Remix?

_author: Lev Manovich_
_year: 2007_

In 2000s the production of news, media, information, and knowledge underwent many important changes. One of the key changes is the widespread adoption of remix by producers and consumers alike. In parallel, a more capacious set of terms was introduced to characterize complex movements of content between people, locations, and devices. “Remix” is one of these new terms; others are “publish”, “upload”, “rip”, “share”, and “mix”.

If remix is defined as the production of any new content which combines elements from different sources – be they different online journals, web portals, news organizations, blogs, etc. – remix can be said to have become an intrinsic part of news communication on the Web. On the consumption side, RSS software allows anybody to curate their own news sources, with millions of web sites and blogs to choose from. The user can also select when and where to view her news – a phenomenon that has come to be known as “timeshifting” and “placeshifting”. On the production side, remix has been adopted as well. For instance, Google News algorithmically generates news, remixing material gathered from thousands of news publications. (Interestingly, there is no information on the Google News web site about the algorithm used, so we know nothing about its selection criteria or what counts as important and relevant news.) Another kind of “algorithmic remix” is performed by the web-art application [10x10](http://www.marcosweskamp.com/), designed and developed by Jonathan Harris, which presents a grid of news images based on the algorithmic analysis of news feeds from _The New York Times_, the BBC, and Reuters. [1]

It’s also possible to think of the so-called “user-generated content” (UGC) [2] as a set of varied remix operations. Firstly, the bloggers who commonly republish materials from other sources, adding their own comments, can be said to create a particular kind of remix. In these cases, the modification of the original content common to all remixes takes the form of commentary. Secondly, a significant part of user-generated content available online is actually the result of users remixing material produced by professionals. For instance, the popular genre of anime music videos involves creatively combining music ripped from professional music videos and pieces of anime edited together. Thirdly, in terms of its structure, a typical personal blog represents a remix of material drawn from other sources and assembled on one site; this remix is available to others creating their own remixes. (In regard to this, it is important that blog software allows for the modularity of each post – facilitating its reuse in other blogs. The same goes for other Web technologies such as “permalink” and the addition of “share” buttons to media content on social media sites such as YouTube. The availability of these tools around each piece of media content gives strong encouragement to users to include this content in their own remixes.) 

Having described some of the news communication practices on the Web in the 2000s that involve remix, I now want to place “remix epistemology” (the generation of knowledge about the world either for personal use or for that of other people by selecting and combining from the numerous sources available) within the wider phenomenon of “remix culture”. 

As the user-generated media content (video, photos, music) on the Web exploded around 2005-2006 an important semantic switch took place. The term “remix” (or “mix”) tended to be used in contexts where previously the term “editing” had been standard – for instance, when referring to a user editing a video. (Thus, when in the spring of 2007 Adobe released video editing software for users of the popular media sharing web site Photobucket, it named the software Remix. The software was a stripped-down version of one of the earliest video editing softwares for PCs called Premiere.) [3]

Today, many cultural and lifestyle arenas – music, fashion, design, art, Web applications, user created media, food – are mediated by remixes, fusions, collages, or mash-ups. If post-modernism defined the 1980s, remix definitely dominates the 2000s, and looks set to remain hegemonic over the next decade as well. (For an expanding resource on remix culture, visit remixtheory.net by Eduardo Navas.) Here are just a few examples of how remix continues to expand. For his 2004/2005-winter collection, John Galliano (a fashion designer for the House of Dior) mixed the vagabond look, Yemenite traditions, East European motifs, and other elements collected on his far-reaching travels. DJ Spooky created a feature-length remix of D.W. Griffith's 1912 _Birth of a Nation_ which he appropriately named _Rebirth of a Nation_. In April 2006, the Annenberg Center at the University of Southern California ran a two-day conference on “Networked Politics” that featured sessions on and presentations of a variety of remix cultures on the Web: political remix videos, anime music videos, machinima, alternative news, infrastructure hacks. [4] In addition to these cultures, which remix media content, there is a growing number of software applications that remix data – so-called software “mash-ups”. Wikipedia defines a mash-up as “a website or application that combines content from more than one source into an integrated experience.” [5] At  the time of writing (July 29, 2007), the web site [www.programmableweb.com](http://www.programmableweb.com/) lists a total of 2150 mash-ups, and it estimates that an average of 3.45 new mash-up Web applications are being published every day. [6]

Remix practice extends beyond culture and the Internet. _Wired_ magazine devoted its July 2005 issue to the theme Remix Planet. The introduction boldly stated: “From _Kill Bill_ to Gorillaz, from custom Nikes to _Pimp My Ride_, this is the age of the remix.” [7] Another of the world’s top IT trend watchers – the annual O’Reilly Emerging Technology Conferences (ETech) – similarly adopted remix as the theme for its 2005 conference. Attending the conference, I watched in amazement as top executives from Microsoft, Yahoo, Amazon, and other IT companies, not exactly known for their avant-garde aspirations, described their recent technologies and research projects by reference to remixing. If I had any doubts that we are living not simply in a Remix Culture but in a Remix Era, they evaporated totally at that conference.

Originally, “remixing” had a precise, narrow meaning. Gradually, its meaning became more diffuse. Although there were earlier precursors, it was through the introduction of multi-track mixers that remixing became a standard practice. With each element of a song – vocals, drums, etc. – available for individual manipulation, it became possible to “remix” the song: change the volume of some tracks or substitute new tracks for the old ones. Over time the term broadened, so that today it refers to any reworking of already existing cultural work(s). 

In his book _DJ Culture_, Ulf Poschardt identifies different stages in the evolution of remixing practice. In 1972 DJ Tom Moulton made his first disco remixes; as Poschardt points out, they “show a very chaste treatment of the original song. Moulton sought above all a different weighting of the various soundtracks, and worked the rhythmic elements of the disco songs even more clearly and powerfully…Moulton used the various elements of the sixteen or twenty-four track master tapes and remixed them.” [8] By 1987, “DJs started to ask other DJs for remixes” and the treatment of the original material became much more aggressive. For example, “Coldcut used the vocals from Ofra Hanza’s ‘Im Nin Alu’ and contrasted Rakim’s ultra-deep bass voice with her provocatively feminine voice. To this were added techno sounds and a house-inspired remix of a rhythm section that loosened the heavy, sliding beat of the rap piece, making it sound lighter and brighter.” [9]

Around the turn of the new millennium, people started to apply the term “remix” to other media besides music: visual projects, software, literary texts. Since, in my view, electronic music and software serve as the two key reservoirs of new metaphors for the rest of the culture today, this expansion of the term is understandable; one can only wonder why it did not happen earlier. Yet we are left with an interesting paradox: while in the realm of commercial music remixing is officially accepted, [10] in other cultural sectors it is seen as violation of copyright and hence as theft. So, while filmmakers, visual artists, photographers, architects and Web designers routinely remix already existing works, this is not openly acknowledged, and no proper terms equivalent to remixing in music exist to describe these practices.

One term that is sometimes used to talk about these practices in non-music areas is “appropriation”. The term was first used to refer to certain New York-based post-modern artists of the early 1980s who reworked older photographic images – [Sherrie Levine](http://en.wikipedia.org/wiki/Sherrie_Levine), Richard Prince, Barbara Kruger, and others. But the term “appropriation” never really caught on as “remixing” did. In fact, in contrast to “remix”, “appropriation” never quite transcended the original art world context in which it was coined. I think that “remixing” is a better term anyway, because it suggests the systematic reworking of a source, a meaning “appropriation” does not have. And indeed, the original “appropriation artists”, such as Richard Prince for example, simply copied the existing image in its entirety rather than remixing it. As with Duchamp’s famous urinal, the aesthetic effect derives from the transfer of a cultural sign from one sphere to another, rather than from any modification of it.

The other older term commonly used across the media is “quoting”, which I see it as describing a very different logic than remixing. If remixing implies systematically rearranging the whole text, quoting refers to the insertion of fragments from old text(s) into the new one. Thus, I don’t think we should see quoting as a historical precedent for remixing. Rather, we might think of it as a precedent for another new authorial practice which, like remixing, was made possible by electronic and digital technology – sampling. 

Music critic Andrew Goodwin defined sampling as “the uninhibited use of digital sound recording as a central element of composition. Sampling thus becomes an aesthetic program.” [11] We might say that with sampling technology, the practices of montage and collage that were always central to twentieth century culture, became industrialized. Yet we should be careful in applying the old terms to new technology driven cultural practices. The terms “montage” and “collage” pop up regularly in the writings of music theorists from Poschardt to Kodwo Eshun and DJ Spooky, who in 2004 published the brilliant book _Rhythm Science_. Ending up on a number of “best 10 books of 2004” lists, it proclaimed “unlimited remix” as _the_ artistic and political technique of our time. [12] In my view, the terms that have come down to us from the literary and visual modernism of the early twentieth century – think, for instance, of works by Moholy-Nagy, Hannah Höch or Raoul Hausmann – do not always adequately describe the new electronic music. Let us note just three differences. Firstly, musical samples are often arranged in loops. Secondly, the nature of sound allows musicians to mix existing sounds in a variety of ways, from clearly differentiating and contrasting individual samples (thus following the traditional modernist aesthetics of montage/collage), to melding them into an organic and coherent whole; borrowing terms used by Roland Barthes, we might say that if modernist collages always involved a “clash” of elements, electronic and software collages also accommodate a “blend”. [13] Thirdly, electronic musicians often take it for granted that their works will be remixed, sampled, taken apart and modified. 

It’s worth noting here that the revolution in electronic pop music that took place in the second part of the 1980s was paralleled by similar developments in the visual pop culture. The introduction of electronic editing equipment such as the switcher, keyer, paintbox, and image store made remixing and sampling a common practice in video production towards the end of the decade; while first pioneered in music videos, it came later to be used across the board in television. Other software tools such as Photoshop (1989) and After Effects (1993) had the same impact in the fields of graphic design, motion graphics, commercial illustration and photography. And, a few years later, the World Wide Web redefined an electronic document as a mix of other documents. Remix culture had arrived.

The question that at this point is really hard to answer is what comes after remix? Will we eventually get tired of cultural objects – be they dresses by Alexander McQueen, motion graphics by MK12 or songs by Aphex Twin – made from samples which come from the already existing database of culture? And if we do, is it even conceivable that we shall be able to create a new aesthetics that does not rely on copious sampling? When I emigrated from Russia to the U.S. in 1981, moving from a grey and red Communist Moscow to a vibrant and post-modern New York, I, along with others living in Russia, felt that the Communist regime would endure for at least another 300 years. But just ten years later, the Soviet Union no longer existed. Similarly, in the middle of the 1990s the euphoria unleashed by the Web, the collapse of Communist governments in Eastern Europe and early effects of globalization created the impression that we had finally left the Cold War culture behind – its heavily armed borders, massive spying, and the military-industrial complex. And now once again, only ten years on, we seem to find ourselves cast back into the darkest years of Cold War era, except that now we are being tracked with RFID chips, computer vision surveillance systems, data mining and the other new technologies of the twenty-first century. So, it is very possible that the remix culture, which right now appears to be so firmly entrenched as to resist challenges from any other cultural logic, will morph into something else sooner than we think. 

I don’t know what comes after remix. But if we apply our efforts to getting a grip – historically and theoretically – on the current remix era, we will be in a better position to recognize and understand whatever new era replaces it. 

## References:

[1] [www.tenbyten.org](http://www.tenbyten.org/), accessed July 29, 2007.

[2] A glance at the history of the Wikipedia page on “User-generated content” reveals that it was first created on January 28, 2006. According to the version of the article accessed at the time of writing (July 23, 2007), “[t]he term entered mainstream usage during 2005 after arising in web publishing and [new media](http://en.wikipedia.org/wiki/New_media) content production circles. It reflects the expansion of media production through new technologies that are accessible and affordable to the general public. These include [digital video](http://en.wikipedia.org/wiki/Digital_video), [blogging](http://en.wikipedia.org/wiki/Blogging), [podcasting](http://en.wikipedia.org/wiki/Podcasting), mobile phone photography and wikis.” [http://en.wikipedia.org/wiki/User-generated\_content](http://en.wikipedia.org/wiki/User-generated_content), accessed July 23, 2007.

[3] [http://www.webware.com/8301-1\_109-9689909-2.html](http://www.webware.com/8301-1\_109-9689909-2.html), accessed July 29, 2007.

[4] [http://netpublics.annenberg.edu/](http://netpublics.annenberg.edu/), accessed February 4, 2007.

[5] [http://en.wikipedia.org/wiki/Mashup_%28web\_application\_hybrid%29](http://en.wikipedia.org/wiki/Mashup_(web\_application\_hybrid)), accessed February 4, 2007.

[6] [http://www.programmableweb.com/mashups](http://www.programmableweb.com/mashups), accessed July 29, 2007.

[7] [http://www.wired.com/wired/archive/13.07/intro.html](http://www.wired.com/wired/archive/13.07/intro.html), accessed February 4, 2007.

[8] Ulf Poschardt, _DJ Culture_, trans. Shaun Whiteside (London: Quartet Books Ltd, 1998), 123.

[9] Ibid. 271.

[10] For instance, Web users are invited to remix Madonna songs at [http://madonna.acidplanet.com/default.asp?subsection=madonna](http://madonna.acidplanet.com/default.asp?subsection=madonna).

[11] Ibid. 280.

[12] Paul D. Miller aka Dj Spooky that Subliminal Kid. _Rhythm Science_. MIT Press, 2004.

[13] Roland Barthes, _Image, Music, Text_, translated by Stephen Heath (New York: Hill and Wang, 1977), 146.

---

# Designing Shanghai, or Why East Is the New West

_author: Lev Manovich_
_year: 2007_

I am finishing this essay in Barcelona's architecturally exceptional and at the same time serene Omm hotel. The city has been so completely covered with design hotels - there seems to be one on every corner - that sometimes I wonder if Catalonians sold their souls in the process. As my guide, I am using a little booklet from anothertravelguide.com which came compliments of my airBaltic flight I took to Tallinn (the capital of Estonia) a few weeks ago. In Tallinn every café and lounge plays Fashion TV and people dress accordingly - so I feel I can trust this guide. I am planning to go later to Ommsession which, according to my guide, is a cult nightclub of the moment, located at the Omm hotel. Interestingly, the guide which comes courtesy of a discount airline, also informs me about the dress code at Ommsession which is nothing but discount: gucci+prada+lanvin+balenciaga. However, it is Sunday, so I hope that I will be let in in my Calvin Klein and Hugo Boss clothes plus my leather jacket, which is at least as interesting as what the best luxury brands in the world can deliver. Except, just as some of the most cutting-edge design and thinking today, it does not come from an older Western metropolis. Instead, it comes from a place, which until this decade was definitely outside, on the margins of contemporary culture - Bulgaria. It also could have been from Peru, Thailand, Turkey, or another formerly "provincial" place. 

So, what about Shanghai? Where does it fit in relation to design wave and design innovation, which have swapped the planet in the last ten years? [1] When I started coming to China, making my first trip there in October 2004, one thing which I did not expect to find was that the new consumer spaces built recently - restaurants, multi-function entertainment/retail spaces, hotels, malls, etc. - not only featured stylish contemporary design but in fact often were at the very cutting edge of global design and lifestyle trends. For instance, if I think about the most remarkable meal I had in this decade, it was not in Madrid, New York, or Paris. It was in a restaurant located in Hangzhou, a city that is about three hours by train from Shanghai. The design of space and the design of food were both remarkable. And the most interesting café design I have experienced in this decade was Future Perfect in Shanghai. 

The city is also working hard to develop itself as a key player in design and other creative industries worldwide. So, while today the hotel in which you are staying after your Easyjet or Jetblue flight was probably designed somewhere else rather than Shanghai, this is likely to change in years to come. It is likely that before long made in Shanghai may acquire the same status as made in Denmark or made in Italy are today.

It is 10:35am, September 15, 2006. I am walking towards Shanghai Exhibition Center to attend 2006 Shanghai Design Biennial. Exhibition Center was built in the 1950s in the Stalinist style, and its architecture which is not unlike the nineteenth century eclecticism - Gothic meets Renaissance meets Arab Mosque meets Classicism - provides a surreal contrast to the Design Biennial which is completely Now and State-of the art: lectures on everything from sustainable architecture to brand management, and a separate show of New Lines in Italian Design. What makes the scene even more surreal is what outside Shanghai Exhibition Center. Imposing in its day, today it is overshadowed by high-rises of the 1980s and 1990s. They are arrogant, even aggressive, and some of them are fine examples of a particular unintentional Chinese post-modernism: steel and glass structures capped with some traditional Chinese motif apparently slapped there by the clients. [2]

Of course, you see such contrasts in China every day, but still, even after living in Shanghai for over two months, this particular Russian doll in a Chinese style - state of the art design products and trends circa 2007 - inside Communist mid-twentieth century eclecticism - inside the forest of steel and glass post-modern high-rises - remains one of the most striking real-life montages I have experienced in the city. The three layers of this doll summarize the three eras of consumer culture in China over last half a century: first, non-existing (during Mao era); second, when the architecture and goods have been imported from the West and other countries in Asia to a China (many of the goods actually being assembled in China); and third which is now, with China rapidly developing its own design and creative industries and beginning to export them. 

If we don't count Chinese food, first major Chinse cultural export of the second part of the 20th century was Chinese cinema (the Fifth Generation filmmakers of the 1980s). Next, in the 1990s, it was the turn of contemporary Chinese art. Using their academic training which artists in the West no longer have, Chinese realist painters were able to create unique cultural products not available anywhere else in the global cultural market. Given what I have seen in Shanghai, I expect that design, architecture, fashion, hi-level (as opposed to inexpensive and non-designed) cuisine, and media will start to be exported next. 

Currently US, Japan and Korea remain are three key exporters of media and lifestyle trends worldwide, including films, computer games, manga, street clothing styles, and music. It will probably take at least a few years before they will be joined by China. In architecture, for instance, MA Design, or MAD, has been recognized as the first Chinese architects to act internationally. [3] At the time of this writing (November 2007), none of their projects abroad were yet completed. These projects under construction or in proposal stage include The Absolute Towers in Toronto; Denmark Pavilion, a villa in Denmark; KBH Kunsthal (a large art space also in Denmark), Al Rostamai Group headquarters in Dubai, and Mongolia Private Meadow Club, a villa in inner Mongolia. 

I enter Shanghai Exhibition Center and go through the exhibition. At the one end of the hall, I encounter a big publicity poster which one may see only in China - Communist rhetoric and iconography which seemingly effortlessly were adopted to promote the new vision of China as the economic and cultural super-power of the new century. The texts on the poster read “EXPO 2010 Shanghai China” and “Better City, Better Life”; the background is a panorama of Shanghai. The panorama is rendered from a bird's view, and all we see are modern high-rises and blocks. No details from the past are visible. The panorama is rendered in this particular red color, which had become the communist brand since 1917 October Revolution in Russia. But, just as it is the case in China today in general, there no other visible traces of Communist iconography in exhibition itself. Instead, I see a mash-up of French, Japanese, Italian, Australian, and local design firms showing their products and services, all eager to make contacts and acquire new clients and contracts.

Off to 3rd floor of the Friendship Hall where Opening Ceremony and International Design Conference of Shanghai Design Biennial 2006 are in progress. The interior design of the conference hall, with its very large curtains arranged in decorative folds reminds me of Socialist Realist paintings from the middle of the twentieth century depicting Communist Party congresses. However, just as it was the case on the exhibition floor, there is nothing outdated or nostalgic in the speeches I am hearing today. The presentation from Hartmut Esslinger, the founder of Frog Design, is entitled “China 2010: From Production Champion to Global Brand Player.” I am also treated to “Hitachi Experience Design for the Next Society” (obviously, from Hitachi). But most telling are the presentations from the Chinese officials themselves who confidently talk about converting Shanghai from an industrial city to China's center for design and creative industries.

This is second decade of the globalization, and the countries which until a few years ago were thought of as "emerging markets" have fully emerged. For instance, India’s middle class is projected to increase from 50 million people in 2005 to 583 million people in 2025. As a percentage of India's population, middle class will grow from 5% to 41%. Accordingly, only between 2006 and 2008 the retail space in India' seven biggest cities has tripled. [4] By 2007, eight of the twenty largest companies in the world were Chinese, and only seven were American. [5] In the words of Brian Redican writing for December 2007 / January 2008 issue of Monocle, high-speed development "is now occurring in countries such as India, Brazil, Russia, Chile and the Czech Republic, and potentially even in Africa. And because this development has now reached a critical mass, it is this that is driving the global economy rather than what is happening in North America or Western Europe." [6]

Given most growth in consumer markets worldwide is now taking place in Asia, the stakes are very high. Not surprisingly, every major Asian city, including Shanghai, is now fiercely competing to become the center for creative industries in Asia. If you check the calendar of global design events on Core77 web site, you will see that it is dominated by design weeks and design conferences in Asia. [7] For instance, just for the period of November-December 2007, I found the following: Gwangju Design Biennale 2007 (South Korea), Pune Design Festival (India), Singapore Design Festival 2007 (Singapore), Guangzhou Design Week 2007 (China), Design Korea (Korea), UMO2007: Designing for User Experience (India), Business of Design Week 2007 (Hong Kong), EcoDesign 2007 (Japan), Design with India (India). This push to develop design and creative industries also includes building new museums of contemporary art and hosting biennales. Shanghai, for instance, has three separate museums of contemporary art: MOCA Shanghai at People's Park, Duolun Museum of Modern Art, and Zendai Museum of Modern Art in Pudong.

This emphasis on developing global creative industries throughout Asia goes hand in hand with the wide use of contemporary design at home. People living in Asia's big cities are surrounded by contemporary design. In this respect, Asia and Scandinavia are similar: what for us is design for them is a way of life. Of course, there is one very big difference: with the exception of Singapore and Japan, the rest of Asian countries have much smaller per capita income than Scandinavian countries. Consequently, significant parts of the populations can't afford to shop or eat in modern malls and other recently built urban spaces which feature contemporary design. In fact, in China the pricing of western chains such as MacDonald’s and Starbucks put them in the upmarket category. Thus, a visit to one of these places signifies status and prestige - opposite of what it means in the USA. And as far shopping in modern organized retail spaces in general, in 2007 only approximately 20% of the population of China were shopping in such places as opposed to neighborhood kiosks and traditional bazaars (in the same year this figure was 40% for Southeast Asia and only 5% in India). [8] And here lies the key difference between design culture in Scandinavia and China. While Sweden's global brands IKEA and H&M are bringing the local values - modern design affordable for everybody - to the rest of the world, you can also find them in most Swedish homes. In contrast, at present the majority of Chinese can't afford to have a lunch in one of the new shiny malls. 

Despite these economic differences, I have a feeling that Asian countries, including China, are ahead of both Europe and North America in understanding the importance of design for economic successes and branding. This seems to be understood equally well not only by the governments of these countries and city officials but also by owners of small businesses such as cafes and hair salons. Today in the West, with the visible exception of Scandinavian countries, contemporary design sensibility is still only used selectively, and it commands a significant price premium. But in Asia, be it Bangkok, Singapore, or Shanghai - contemporary design is used for all newly built urban spaces and consumer products sold at all price levels.

The actual design aesthetics are also different. The space design in the West - be in Madrid, Oslo, or Copenhagen - tries to project images of dignity, exclusivity, and middle age sophistication, i.e., lots of grey, black, and white and monochrome surfaces with no images. But in China and Southeast Asia the aesthetics which in the West is reserved for youth culture - bright colors, dynamic geometric patterns and lots of large, wall-size photographic images - are used everywhere. As a result, rather than signifying stability and exclusivity, new spaces in Asia speak of youth, dynamism, energy, and readiness for change. 

In Bangkok all taxis are painted in pink, yellow, or blue; Bangkok Airways brands itself as "Asia's boutique airline" and lets all customers use its lounges; a bank office, a post office or a copy center located in a new mall feature colorful and sometimes truly cutting-edge design - something you would expect to see at a design show rather than in a mall. (Yes, the famous Bed Supperclub is also a fabulous space definitely worth visiting, but with its all-white palette it is looking very 1990s in comparison to bright palette used in new Bangkok spaces.) And contemporary Thai home designs which are sold at special malls set up by governments to promote local design are, in my view, better than what you will find in Armani Casa or Alessi stores or on Philippe Starck or Karim Rashid web sites. 

A telling picture of how each county treats design is to compare the plastic trays used in its airports when you go through security. In the fall of 2007, I have passed through many airports in Europe, Australia, and Asia and the differences were quite clear. In San Diego where I started, the appearance of the trays was not too attractive. In Amsterdam Schiphol Airport the trays were in bright colors but still felt cheap. In Oslo the same plastic trays were much more sophisticated, their clear shapes and muted colors making a good advertising for Scandinavian design. But it was Singapore Changi Airport that topped it all. Its trays were clearly an avant-garde design statement - something you expect to find in MOMA design collection rather than the airport security area. Accordingly, this understanding of design was also extended to the whole airport experience which since its opening in received over 250 awards including best airport in the world. Taking my baggage through the security line was non-stressful and in fact, given the politeness and friendly smiles of the security personnel, I could say that it pleasant. The security personnel were smiling and welcoming - a complete opposite from the stress-producing, rude, and aggressive manners of security in USA airports. 

Another good way to see the difference in the use of design between East and West is to compare shopping malls. Similarly to Bangkok or Singapore, in Shanghai and other major cities in China which I visited new consumer spaces also have better than comparable spaces anywhere in North America or Europe. The businesses which in Europe and North America will be today given standard template design receive original and higher quality design in Shanghai - better materials and finishes, better ambience, more refined and coordinated relationships between colors and shapes, and interesting lights. For example, compare food courts in American malls with the restaurant design in new malls in Shanghai. In Shanghai you may mistake fast food places for fancy restaurants (and the quality of food is also much better than in USA malls). 

Unless people spend some time in Shanghai, they are usually surprised to hear when I tell them that in Shanghai design is used more widely and often more inventively that in the West. It seems that our unconscious image of China is still of grey "anti-design" Mao's China. Another reason is how Western media presents China. Media coverage - as well as cultural and artistic discussions and presentations of China in the West - focuses on the unprecedented scale and speed of economic development, rather on the concrete details of this development. And while Western art museums are truly obsessed with China, with yet another exhibition of contemporary Chinese art opening every month, visitors to these exhibitions are more likely to see artistic critique of this economic development by very skillful realist Chinese painters, rather than the actual photographs of a typical new mall, food court, a train station, or other newly built spaces. And even Wallpaper magazine which aims to cover the latest in design, interiors, fashion, art, and lifestyle trends around the world, sometimes follows the common stereotype. While talking about the growing economic importance of “creative industries” - media, design, the performing arts, an article in its October 2007 issue referred to China as powering along quite nicely with its old-fashioned non-creative economic motor. [9]

Why did Asia embrace "design" and "experience economy" to a larger extent than other parts of the world? The reasons for the emphasis on and wide acceptance of contemporary space design in Asia are multiple. (I don't want to pretend that I understand them all, but here are a few candidates.) One has to do with the importance of "face" in Asia, i.e., projecting the image of being successful, trendy, smart, etc. through material signifiers. For instance, according to the director of Nokia research center in Beijing, main motivators behind the sales of mobiles in India and China are "individuality, exclusivity, originality, uniqueness, glamour, status, distinction, esteem, prominence, celebrity." In his summary, "It's all about image and setting trends, having the latest designs, being unique and owning the most exclusive products they can" - and this goes for all consumers and not just the rich ones. [10] 

Another reason may have to do with the different living conditions in Asia and the West. Nowhere in the world people dress as imaginatively and obsessively as in Tokyo. Young people spend %90 of their income on clothes; and %40 of middle-aged women in Tokyo own at least one Vuitton bag (a real one). This correlates to people's particular living conditions in Tokyo - they live in tiny places, which do not offer possibilities to establish and present to others one's identity through home design and large possessions. Similarly, in most other Asian cities people's investment in clothes, accessories and other aspects of their appearances correlates well with their minimal living conditions. (This correlation may also explain why people in Russia and Eastern Europe also invested in dressing up more than people in the West. During the Soviet period people had no control over public spaces controlled by the governments, they could not compete via "big items" such as houses or cars, and their apartments were also usually quite small and of poor quality. Consequently, possessing and showing off expensive western clothing served as the main marker of prestige. This may explain why today in Riga all cafes show Fashion TV and why Moscow may be competing with Tokyo in terms of being obsessed with expensive Italian and French designer clothes and accessories.)

The importance of "face" and small living quarters, however, does not explain why Asia so willingly embraces design aesthetics that are new, hip, and non-traditional. Therefore, I think that the main reason is different, and it has to do with timing. In the West most spaces you encounter have been built before the present decade, and unless they have been recently renovated, their design reflects the style of the decade in which they were built. Which also means that they are likely to have standard template-design or no design at all - since until the middle of the 1990s design was not part of the mental landscape of most companies and businesses. It was not something you understood, cared about, or invested into.

In contrast, many countries in Asia including China (just as Eastern Europe) only started to fiercely build new leisure and consumer spaces in the second part of the 1990s. In other parts, they "came of age" at the same time when design and brandscaping (establishing a brand through unique designed spaces) started to get wide acceptance (Wallpaper magazine launched in 1996; Colette, the first "curated" design store, opened in Paris in the same year; Guggenheim Bilbao opened in 1997). At the same time, more citizens in these countries started to travel (just as in Europe, Asia now has dozens of new discount airlines) as well as more tourists started coming. Since a large proportion of consumer spaces in Asia only were built in 2000s, it is only to be expected that these spaces will feature latest technologies and latest sensibilities and values - which includes investments in original, "fresh," and "avant-garde" design (i.e., design which is conceptual, ironic, attracting attention to itself). 

This difference in design due to timing is even more visible when you compare the cities in Western Europe and Eastern Europe. The capitals of East European countries received infusion of capital in previous and this decade from Western companies and EU. Also, after the collapse of pro-Soviet communist governments in 1990, a younger generation was able to better take advantage of a new "wild west capitalism" in these countries; similarly, Western and newly emerged private companies would often prefer to hire younger people who don't have old "communist mentality." As a result, the decision makers, the politicians, and business owners in the East are often much younger than in the West. Therefore, since the new consumer and civil structures in Eastern Europe - airports, shopping malls, restaurants, offices, hotels, museums, theatres - are all just a few years old at best, and the decision makers, the clients, and the designers themselves are often younger than their counterparts in the West (the same also applies to China), new spaces in the East often look much more contemporary and innovative than their Western counterparts. 

For instance, when I visited Riga (the capital of Latvia) in May 2004, with the exception of the old town which already was highly polished, the rest of the city still looked to me more Soviet than Western. When I was in Riga again in January 2006, I could hardly recognize the city. Everything was shiny and new, and everything was built to the very latest standard - including bus station, which looked more like an airport. In contrast, when I visit Brussels or Basel, they look distinctly 1950s. And some parts, such as Brussels central train station, look and feel like a Communist Moscow when I was growing up there. (Brussels airport that I passed through in October 2007 was not so impressive either, even though it had enough signs proudly proclaiming "Welcome to Europe.")

The most beautiful new airport I have seen in this decade was Baiyun International Airport in Guangzhou (China) which opened in 2004, while the most depressing, non-modern, and ill-kept are some of the terminals at JFK and LAX (New York and Los Angeles, respectively). It seems that in terms of design and service, East and West have changed places. To put this briefly: "East is the new West." Or: "the margin is the new center." The typical food places in new shopping malls in Shanghai and other Asian cities have better design than the first-class lounges of many Western airports which appear to not have been updated since the 1980s. The same often goes for the use of IT technology: Singapore and Tallinn have free Wi-Fi practically everywhere in the city and it is set up by city agencies, i.e., top-down rather as bottom-up. In 2007 Estonia (which until 1990 was a part of the Soviet Union) was the first country ever to have successfully conducted national elections electronically (i.e., people were voting over Internet). In 2007 EU survey ranked Slovenia 2nd among 31 European countries for the efficiency of its e-government systems [11], while Singapore was ranked by Forbes as 1st among all countries the world for business.

In contrast, while spending a month in Sydney in the end of 2008 I could hardly find any cafes with Wi-Fi. The mid-level business hotel where I checked in after my arrival was charging $20/day for Internet use. Submitting Australian business visa application took many hours and multiple phone calls to the official in the immigration agency who himself had no idea why the government web site kept refusing to accept my application. When I went to take an obligatory chest X-Ray as a part of my application, I was asked to provide an address so they can send the results. I asked naively why they don't want to send the results directly to immigration office? If we send it to immigration, they will definitely loose it, confidently told me the clerk. 

Can we explain these differences in design, service, and infrastructure by comparing GDP per capita in the West (North America, Western Europe, Japan, Singapore) and the East (Eastern Europe, Russia, China, India, and Southeast Asia)? Indeed, the consumers who visit new urban malls in Asia have relatively higher incomes (in relation to other people in their countries) than those visiting the malls in West. That is, comparing to many others - often living in rural areas - they make good salaries and live consumer lifestyles. Therefore, we may conclude that the more upscale architecture of new shopping malls including their food areas is because they are geared towards middle and upper classes. Indeed, my Chinese friends told me the kids who hang out in Shanghai's new malls - having meals, drinking coffee, chatting - are usually the only children of their well-off parents who give them anything they want. 

But this reasoning does not explain things completely. For example, the largest and definitely quite luxurious and sophisticated shopping center in Riga's city center is built around the train station. In fact, it took me a while to find the actual train station as it was completely integrated into the shopping complex, which extends over the length of the largest square in Riga. Now, train stations supposedly receive people from all income levels, and in fact people with money are more likely to drive or fly. What was even more unexpected was Riga's bus station. Certainly, a typical bus station in North America of Western Europe is as far from cutting-edge design and urban sophistication as you can get. So, imagine how I was shocked than I entered a brightly lighted, very clean space featuring modern electronic display monitors, funky orange furniture, and other details, which you may expect to find in Scandinavian airports. I had to check the schedules to convince myself that this was indeed a bus terminal, with buses going between Riga and small towns elsewhere in Latvia. I think that these examples show the wider use of contemporary design and more upscale look of spaces in developing economies of the East (both Eastern Europe and Asia) in comparison to the similar places in the West is not just due to different demographics of the consumers whom developers these spaces. Often, it is a reflection of a different attitude to design shared by developers, people who work in these spaces, and consumers. This attitude is summarized well in the words of the organizers of The Great Indoors 2007 design conference: “The interior seems to be the only place in which people still dream of a better future. The interiors of hotels, shops and restaurants have evolved into the new epicenters of human imagination." [12]

Smart design, of course, is not just about using good lighting and quality materials, and thinking about how to create comfortable spaces, which have ambience and atmosphere using variety of means. And it is definitely not about simply putting iconic design objects in the interior - be they lights, chairs, or glassware (of course, we all have seen enough designed spaces which only do this). Ultimately, smart design is about fresh thinking: not taking anything for granted, and re-thinking every convention and every detail of space, an object, or a process. From this point of view, the best example of such fresh thinking which I encountered in my extensive travels over the last few years also involved a bus terminal in Bangkok - more precisely, it was a modern café near bus terminal. Once again, I did not expect to find anything sophisticated right next to the bus terminal. In this case, the café had very nice interior design and dozens of teas on offer, and the waiters had fashionable uniforms. But what was really amazing is that it featured a dozen of new large iMacs with free Internet access that was elegantly integrated into the interior. How come I have never seen such a café in California? It certainly does not cost much to buy a set of iMacs for a café - but why did nobody think about it? 

Shanghai certainly has many examples of great designed spaces for consumers built in the last few years - hotels, clubs, bars, restaurants, spas. My favorites spaces - as of September 2006 - were PIER 1 complex, Future Perfect café, the lobby area and the fitness room at The Regent hotel Shanghai, South Beauty restaurant across from Shanghai Exhibition Center, Japanese restaurant Shintori, and Bar Rouge at Bund 18. (If you visit Shanghai and want to check these and other new spaces that inevitably were built since my last visit, you can find information at [www.smartshanghai.com](www.smartshanghai.com).) A particular feature of Shanghai urban texture that separates from other Asian cites is the abundance of beautiful Art Deco villas in city central French Concession area - and designers and developers understand the uniqueness of this quite well. Some of most special spaces in Shanghai are located in these renovated villas. For instance, the owner of Yongfo Elite restaurant spent two years sourcing objects all over Asia to decorate the restaurant located in a beautiful French villa which a spacious garden. In 2004 Yongfo Elite was a runner up for the best club in a world in a rating by Wallpaper magazine.

Western media coverage in the West usually focuses on impressive super-large new construction such as CCTV building by Rem Koolhaas in Beijing or Shanghai World Financial Center in Pudong, and it is also fond of talking about the demolitions of old neighborhoods to create space for new developments. Such demolitions certainly took place. However, walking through the center of Shanghai you get a very different picture - every old villa is painstakingly restored and converted to a new use (a private residence, a restaurant, a café, a bar, a boutique, a spa, etc.).

Similar to Bangkok stores which sell the work of local designers, Shanghai has its own store Younik which sells innovative work by Shanghai-based fashion designers; it also has a design store/gallery which puts exhibitions of cutting edge furniture and design a la New York's Moss (both stores are located in Bund area on the river where a number of buildings from the beginning of the twentieth century were converted into upscale restaurants, bars, and boutiques). However, what in all my explorations of design culture in Shanghai was by far most interesting were not the consumer or exhibition spaces but the spaces which creative professionals themselves. Following the new emphasis on making Shanghai into a design center, every district in the city has created a hub for creative industries. (Shanghai also has technology parks focused on animation and other media industries but they located outside of the city's center.) The hubs which I visited - Bridge 8, X2 creative center, The New Factories - were all created by retrofitting existing older groups of buildings. In each case, the result was some of the most interesting urban design I ever saw. While keeping the original industrial buildings largely intact, the architects added smart details - unexpected passages, outdoor lighting systems, surfaces featuring interesting materials and patterns, bold signage, and over-size typography. They also added cafes and other social spaces. Certainly, in many cities in the West older buildings were similarly transformed into spaces for creative industries, but all the ones I visited in Berlin, New York, Moscow, Los Angeles, and elsewhere feature utilitarian design with minimal changes to the existing architecture. It seems that in the West the high-concept design and resources spend on reinterpreting older buildings destined for paying consumers usually does not extend to buildings which house creative industry professionals. (This is different in the case of some of the newly built architectural projects which house creative industries - for instance, a remarkable set of buildings in Culver City in Los Angeles by Eric Moss, or the recent Inter\_ActiveCorp headquarters, on West 18th Street in New York by Frank Gehry.)

In Shanghai, however, the same high-concept design and resources spend on reinterpreting older buildings as new consumer spaces have been applied for retrofitting the buildings which are to house designers themselves. In fact, design-wise, I find these creative hubs even more daring and original than my favorite cafes, restaurants, and hotel lobbies in the city. But what ultimately makes these hubs stand apart from even most innovative consumer spaces in Shanghai and elsewhere is not just their creative architecture. It is their content. The buildings are animated by all the activity and creative energy of the inhabitants inside. Rather than wondering customers and bored sales personnel, you see people working on computers behind the glass walls intensively working on an architectural design. In cafes as well, you are sitting next to designers, architects, photographers, and model agents discussing their current projects. These people are there to work rather than to serve you, and the energy of creative work animates the spaces in a way which - I am sorry to say this - is beyond anything architecture and design could do on their own.

Bridge 8 (converted over the course of 2004, it was by 2006 the most developed of the spaces I visited) houses over 50 creative companies in the areas of design, architecture, advertising, marketing, and consulting for creative industries (data from September 2006). The companies are from China, Hong Kong, Taiwan, Japan, UK, France, USA, Australia, and Italy. Ep700 also houses more than 50 companies with the focus on film, TV, and cartoons production, photography, advertising, and software design. Besides containing various design companies, New Factories complex also includes a spacious Anken Warehouse where individual creative professionals can rent desks, with all support functions - IT, printing, conference, secretarial and translation services, and travel - already provided.

These creative hubs in Shanghai are not only about production of culture, however. Today countless cities around the world feature descendants of Paris's Collette - design store / café / bar / gallery combos. In many cities you can also find the complexes of buildings - often former factories or other industrial buildings - which have been converted into the hubs for creative industries. What I have not encountered in all my travels is these two functions being combined a single place that is given an outstanding design and where professional, rather than consumer spaces, are the center of attention. But this is exactly what has been done at The New Factories, X2 creative center, and Bridge 8 in Shanghai. The companies devoted to architecture, commercial design (stores, boutiques, galleries, showrooms, clubs, hotels, restaurants, lounges, bars), marketing, catering, event management, etc. exist next to the kinds of spaces they design - i.e., galleries, restaurants, bars, and clubs. Architecturally, visually, experientially, the two types of spaces are placed on the same level. To put this in another way, we can say that in these buildings Wallpaper meets Frame. (Established in 1996, Wallpaper has become the most authoritative consumer guide to the new universe of designed experiences -the stuff which surrounds you, as the journal has originally called it. Frame, on other hand, is the equally influential magazine among professional designers that discussed latest outstanding examples of space design from the professional's perspective). [13]

In each of these complexes, the majority of spaces are given to professional clients but there are also at least a few spaces oriented to consumers. Along with dozens of creative companies and a large open space where individual designers can rent desks, The New Factories complex features restaurants, boutiques, a club, and the world's first Elite Bar by a biggest modeling agency in the world - Elite Models. X2 creative center also has an art galleries and a state-of-the art large club Absolute House. And the earliest and most developed (at the time of my research) Bridge 8 has two spas, a café, a restaurant, a number of design shops, and a large and architecturally interesting club Fabrique. 

Clearly, these included service economy spaces - spas, bars, restaurants - are first of all geared towards the professionals working in these complexes and the clients visiting them. Thus, the designers and other creative professionals also act as the earliest consumers who can appreciate and in fact (given their frequent international travel to other word centers for creative industries) expect smart bars, lounges, restaurants, spaces, and design galleries. These producers of design experiences, in other words, are also the default consumers of the products of their labor, with other tourists and locals to follow. 

It is interesting to compare the hubs for creative industries - Bridge 8, X2 creative center and The New Factories - with the spaces which house artist studios and art galleries: 798 art distinct in Beijing and "art industry park" (yes, this is the official name) at 50 Moganshan Road in Shanghai. The former area contains hundreds of artists’ studios and the latter also has dozens, and as result they became one-stop destinations for visiting curators and collectors of contemporary art. These spaces are also conversions of former factories, and they also house creative professionals - in this case, artists and craftsmen. But with many galleries and many artist studios deliberately keeping the doors open and welcoming visitors, these art factories are explicitly oriented towards outside consumers. The artists do not buy each other’s works - it is the visitors who do this. In contrast, Bridge 8 and other creative hubs in Shanghai are first of all spaces for professional work, with cafes, restaurants, spas, design art bookstores and spaces to service professionals themselves. 

Every year Business Week (which in my view provides the most comprehensive global coverage of design trends today via its web site [www.businessweek.com/innovate/](www.businessweek.com/innovate/)) gives prestigious industrial design awards. When in 2005 the editors looked at awards statistics over the previous five years, it was not surprising that Apple was well ahead of other companies. After all, Apple is commonly recognized today as the world leader in industrial design, and its head of design Jonathan Ives is often called the best industrial designer working today). However, another company turned out to be ahead of Apple. This company was Samsung. As Business Week writers noted, “No region of the world has embraced design more emphatically than Asia. Japanese companies first showed the power of design in the 1980s. Korean corporations [Samsung, LG] followed and began to brand themselves through design in the 1990s. And now Taiwanese and Chinese manufacturers are racing to use design to establish their names on the global scene... in 2005, Asian companies, led by Samsung, used design to leapfrog from invisible equipment suppliers to name brands on a global scale." [14]

Back in Omm hotel in Barcelona, feeling tired, I close my Apple Powerbook 2.2 GHz Intel Core Duo and put away my iPhone. The small but perfectly legible print imprinted on the Powerbook's bottom side says: Designed by Apple in California. Assembled in China. I wonder how many years it will be before my laptop label will have the same information in reverse: Designed in Shanghai. Assembled in the US. I imagine the future - maybe 25 years from now - where Asia and Eastern Europe dominate both knowledge and creative economies worldwide, with North America reduced to the role of a third world country used for manufacturing and outsourcing by Asians. (Of course, California where I live will do fine: since California is already a part of Pacific Rim, it will continue to prosper along with the rest of Asia.) I picture the former industrial buildings in the East Coast of the USA, which were converted to designed condos, restaurants, hotels, and malls in the 2000s; they are now being converted back to manufacturing plants. As for The Central Park in New York, it is now used to grow crops. 

Meanwhile, in Beijing, Koolhaas’s CCTV Headquarters skyscraper now appears as small and as archaic as the 1950s Shanghai Exhibition Center appears today in comparison to 1990s hi-rises next to it. But this last image is not mine. It comes from the project by MAD architects entitled Beijing 2050. They propose a gigantic (from today's point of view) floating island over CBD (Central Business District). In line with the current design sensibilities, MAD imagines their floating island as a kind of shining super-blob. The island is so big that CCTV building looks like a small mushroom under a tree. 

According to project description by MAD, the island will contain a variety of spaces: Digital Studios, multimedia business centers, theatres, restaurants, libraries, tourist attractions, exhibitions, gyms, and even a man-made lake are elevated above CBD, and interconnected horizontally. It is telling that this list begins with design studios. Clearly, in 2050 design is supposed to be central to the identity of a city. In fact, the floating island in Beijing 2050 appears to be a scaled-up model of X2 creative space, New Factories, or Bridge 8 as they already exist today - combining professional design offices with the spaces there the products, services and experiences being designed in these offices are offered for sale. And if today artists' studios function as highly desirable cultural commodities used to attract tourists, gentrify city areas, and raise their real estate values, it is easy to imagine that by 2050 the designer's workplaces may have even higher cultural prestige. With creative economy gradually becoming larger and larger part of the total economy, and designers being identified with the most important capital today - the ability to innovate - it will be only logical to imagine designers’ studios themselves being put on display. Thus, I can see digital studios in Beijing 2050 floating cloud having completely glass walls, with the people working inside being put on display as the most obvious sign of the China's leadership in creative industries. 

## References:

[1] This essay - both the analysis and particular buildings and space described - is based on my own research in Shanghai conducted in June 2005, June-July 2006, and September 2006 (10 weeks total). I have chosen the spaces, which in my view were most interesting - visually but also theoretically - as of end of September 2006. Given the continuous growth and development of Shanghai, certainly new equally interesting spaces exist now, and it is also likely that some of the spaces I am describing were given new functions or new design. 

[2] They house most hi-end shopping malls in the city, which are still (at least as of September 2006) are empty of customers - which is surprising given that China's market for luxury goods is now third in the world. (I wonder if they counted all the fake Louis Vuitton bags that are being carried by seemingly at least half of all women in Shanghai.)

[3] My list of MAD's projects is from the exhibition “MAD in China,” Danish Architecture Center, Copenhagen, November 2007.

[4] M. Mahanama, "Bazar Behavior," _CFO_ (September 2007), 32, 34.

[5] Brian Redican, "Embracing China," _Monocle_, volume 1, issue 9 (December 2007 / January 2008), p.82.

[6] Ibid.

[7] [http://www.core77.com/calendar/](http://www.core77.com/calendar/), accessed November 18, 2007.

[8] Mahanama, "Bazar Behavior," 33.

[9] Kevin Braddock, "The New Uncreativesm," _Wallpaper_ (October 2007), p. 189.

[10] Jyri Salomaa, presentation at the X|Media|Lab, School of Art, Design, and Media (ADM), Nanyang Technology University (NTU), September 28, 2007.

[11] Tobias Grey, "Slovenia," _Monocle_, volume 1, issue 9 (December 2007 / January 2008), p. 182.

[12] [http://www.the-great-indoors.com/conference/](http://www.the-great-indoors.com/conference/), accessed November 18, 2007.

[13] "Space design" is the term that in the middle of 2000s started to replace the older term "interior design."

[14] [http://www.businessweek.com/magazine/content/05\_27/b3941401.htm](http://www.businessweek.com/magazine/content/05\_27/b3941401.htm).

---

# The Practice of Everyday (Media) Life

_author: Lev Manovich_
_year: 2008_

## From Mass Consumption to Mass Cultural Production

The explosion of video content on the web (2005-) has unleashed a new media universe. On a practical level, this universe was made possible by free web platforms and inexpensive software tools which enable people to share their media and easily access media produced by others; rapidly fallen cost for professional-quality media capture devices such as HD video cameras; and addition of video capture to mobile phones. What is important, however, is that this new universe was not simply a scaled-up version of 20th century media culture. Instead, we moved from media to _social media_. [1] (Accordingly, we can also say that we graduated from 20th century _video/film_ to early 20th century _social video_.) What does this mean? This is the question this essay will engage with. 

Today “social media” is often discussed in relation to another term “Web 2.0” (coined by Tim O'Reilly in 2004.) While Web 2.0 refers to a number of different technical, economical, and social developments, most of them are directly relevant to our question: besides _social media_, other important concepts are _user-generated content_, _long tail_, _network as platform_, _folksonomy_, _syndication_, and _mass collaboration_. I will not be summarizing here all these concepts: Wikipedia, which itself is a great example of Web 2.0, does it better. My goal here is not to provide a detailed analysis of social and cultural effects of Web 2.0; rather, I would like to put forward a few questions and make a few points that I have not seen expressed by others and that directly relate to video and moving image cultures on the web. 

To get the discussion started, let us simply state two of the important the Web 2.0 themes. Firstly, in 2000s, we see a gradual shift from the majority of Internet users accessing content produced by a much smaller number of professional producers to users increasingly accessing content produced by other non-professional users. Secondly, if 1990s web was mostly a publishing medium, in 2000s it increasingly became a communication medium. (Communication between users, including conversations around user-generated content) take place through a variety of forms besides email: posts, comments, reviews, ratings, gestures and tokens, votes, links, badges, photo, and video.) [2] 

What do these trends mean for culture in general and for professional art in particular? First of all, it does not mean that every user has become a producer. According to 2007 statistics, only between 0.5 % – 1.5 % users of most popular social media sites (Flickr, YouTube, Wikipedia) contributed their own content. Others remained consumers of the content produced by this 0.5 - 1.5%. Does this imply that professionally produced content continues to dominate in terms of where people get their news and media? If by “content” we mean typical twentieth century mass media - news, TV shows, narrative films and videos, computer games, literature, and music – then the answer is often yes. For instance, in 2007 only 2 blogs made it into the list of 100 most read news sources. At the same time, we see emergence of “the long-tail” phenomenon on the net: not only “top 40” but most of the content available online - including content produced by individuals - finds some audiences. [3] These audiences can be tiny, but they are not 0. This is best illustrated by the following statistics: in the middle of 2000s every track out of a million of so available through iTunes sold at least once a quarter. In other words, every track, no matter how obscure, found at least one listener. This translates into new economics of media: as researchers who have studied the long tail phenomena demonstrated, in many industries the total volume of sales generated by such low popularity items exceeds the volume generated by “top forty.” [4]

Let us now consider another set of statistics that show that people increasingly get their information and media from social media sites. In January 2008, Wikipedia has ranked as number 9 most visited web site; Myspace was at number 6, Facebook was at 5, and MySpace was at 3. (According to the company that collects these statistics, it is more than likely that these numbers are U.S. biased, and that the rankings in other countries are different. [5] However, the general trend towards increasing use of social media sites – global, localized, or local - can be observed in most countries.)

The numbers of people participating in these social networks, sharing media, and creating “user generated content” are astonishing – at least from the perspective of early 2008. (It is likely that in 2012 or 2018 they will look trivial in comparison to what will be happening then.) MySpace: 300,000,000 users. [6] Cyworld, a Korean site similar to MySpace: 90 percent of South Koreans in their 20s, or 25 percent of the total population of South Korea. [7] Hi4, a leading social media site Central America: 100,000,000 users. [8] Facebook: 14,00,000 photo uploads daily. [9] The number of new videos uploaded to YouTube every 24 hours (as of July 2006): 65,000. [10] The number of Twitter-like sites internationally as of May 2007: 111. [11]

If these numbers are already amazing, consider a relatively new platform for media production and consumption: a mobile phone. In Early 2007, 2.2 billion people have mobile phones; by the end of the year this number is expected to be 3 billion. Obviously, today people in an Indian village all sharing one mobile phone do not make video blogs for global consumption – but this is today. Think of the following trend: in the middle of 2007, Flickr contained approximately 600 million images. By early 2008, this number has already doubled. 

These statistics are impressive. The more difficult question is: how to interpret them? First of all, they don’t tell us about the actual media diet of users (obviously these diets vary between places and demographics). For instance, we don’t have exact numbers (at least, they are not freely available) regarding what exactly people watch on sites such as YouTube – the percentage of user-generated content versus commercial content such as music videos, anime, game trailers, movie clips, etc. [12] Secondly, we also don’t have exact numbers regarding which percentage of peoples’ daily media/information intake comes from big news organization, TV, commercially realized films and music versus non-professional sources. 

These numbers are difficult to establish because today commercial information and media does not only arrive via its traditional channels such as newspapers, TV stations and movie theatres but also on the same channels which carry user-generated content: blogs, RSS feeds, Facebook’s posted items and notes, YouTube videos, etc. Therefore, simply counting how many people follow a particular communication channel is no longer telling you what they are watching.

But even if we knew precise statistics, it still would not be clear what are the relative roles between commercial sources and user-produced content in forming people understanding of the world, themselves, and others. Or, more precisely: what are the relative weights between the ideas expressed in large circulation media and alternative ideas available elsewhere? If one person gets all her news via blogs, does this automatically mean that her understanding of the world and important issues is different from a person who only reads mainstream newspapers? 

## The Practice of Everyday Media Life: Tactics as Strategies

For different reasons, media, businesses, consumer electronics and web industries, and academics converge in celebrating content created and exchanged by users. In academic discussions, in particular, the disproportional attention given to certain genres such as “youth media,” “activist media,” “political mash-ups” – which are indeed important but do not represent more typical usage of hundreds of millions of people.

In celebrating user-generated content and implicitly equating “user-generated” with “alternative” and “progressive,” academic discussions often stay away from asking certain basic critical questions. For instance: To what extent the phenomenon of user-generated content is driven by consumer electronics industry – the producers of digital cameras, video cameras, music players, laptops, and so on? Or: To what extent the phenomenon of user-generated content is also driven by social media companies themselves – who after are in the business of getting as much traffic to their sites as possible so they can make money by selling advertising and their usage data?

Here is another question: Given that the significant percentage of user-generated content either follows the templates and conventions set up by professional entertainment industry, or directly re-uses professionally produced content (for instance, anime music videos), does this means that people’s identities and imagination are now even more firmly colonized by commercial media than in the twentieth century? In other words: Is the replacement of _mass consumption of commercial culture_ in the 20th century by mass production of cultural objects by users in the early 21st century a progressive development? Or does it constitute a further stage in the development of “culture industry” as analyzed by [Theodor Adorno](http://en.wikipedia.org/wiki/Theodor_Adorno) and [Max Horkheimer](http://en.wikipedia.org/wiki/Max_Horkheimer) in their 1944 book _The Culture Industry: Enlightenment as Mass Deception_? Indeed, if the twentieth century subjects were simply consuming the products of culture industry, 21st century prosumers and “pro-ams” are passionately imitating it. That is, they now make their own cultural products that follow the templates established by the professionals and/or rely on professional content. 

The case in point is anime music videos (often abbreviated as AMV). My search for “anime music videos” on YouTube on February 7, 2008, returned 250,000 videos. [13] Animemusicvideos.org, the main web portal for anime music video makers (before the action moved to YouTube) contained 130,510 AMVs as of February 9, 2008. AMV are made by fans who edit together clips from one or more anime series to music, which comes from a different source such as professional music videos. Sometimes, AMV also use cut-scene footage from video games. In the last few years, AMV makers also started to increasingly add visual effects available in software such as After Effects. But regardless of the particular sources used and their combination, in the majority of AMV all video and music comes from commercial media products. AMVs makers see themselves as editors who re-edit the original material, rather than as filmmakers or animators who create from scratch. [14]

To help us analyze AMV culture, lets put to work the categories set up by Michel de Certeau in his 1980 book [_The Practice of Everyday Life_](http://en.wikipedia.org/wiki/The_Practice_of_Everyday_Life). [15] De Certeau makes a distinction between “strategies” used by institutions and power structures and “tactics” used by modern subjects in their everyday life. The tactics are the ways in which individuals negotiate strategies that were set for them. For instance, to take one example discussed by de Certeau, city’s layout, signage, driving and parking rules and official maps are strategies created by the government and companies. The ways an individual is moving through the city, taking shortcuts, wondering aimlessly, navigating through favorite routes, and adopting others are tactics. In other words, an individual can’t physically reorganize the city, but she can adopt itself to her needs by choosing how she moves through it. A tactic “expects to have to work on things in order to make them its own, or to make them ‘habitable’.” [16]

As De Certeau points out, in modern societies most of the objects which people use in their everyday life are mass produced goods; these goods are the expressions of strategies of designers, producers, and marketers. People build their worlds and identities out of these readily available objects by using different tactics: bricolage, assembly, customization, and – to use the term which was not a part of De Certeau’s vocabulary, but which has become important today – remix. For instance, people rarely wear every piece from one designer as they appear in fashion shows: they usually mix and match different pieces from different sources. They also wear clothing pieces in different ways than they were intended, and they customize the clothes themselves through buttons, belts, and other accessories. The same goes for the ways in which people decorate their living spaces, prepare meals, and in general construct their lifestyles.

While the general ideas of [_The Practice of Everyday Life_](http://en.wikipedia.org/wiki/The_Practice_of_Everyday_Life) still provide an excellent intellectual paradigm available for thinking about the vernacular culture, since the book was published in 1980s many things also have changed in important ways. These changes are less drastic in the area of governance, although even there we see moves towards more transparency and visibility. But in the area of consumer economy, the changes have been quite substantial. Strategies and tactics are now often closely linked in an interactive relationship, and often their features are reversed. This is particularly true for “born digital” industries and media such as software, computer games, web sites, and social networks. Their products are explicitly designed to be customized by the users. Think, for instance, of the original Graphical User Interface (popularized by Apple’s Macintosh in 1984), which allows the user to customize the appearance and functions of the computer and the applications to her liking. The same applies to recent web interfaces – for instance, iGoogle which allows the user to set up a custom home page selecting from many applications and information sources. Facebook, Flickr, Google, and other social media companies encourage others to write applications, which mash up their data and add new services (as of early 2008, Facebook hosted over 15,000 applications written by outside developers). The explicit design for customization is not limited to the web: for instance, many computer games ship with the level editor that allows the users to create their own levels. 

Although the industries dealing with the physical world are moving much slower, they are on the same trajectory. In 2003 Toyota introduced Scion cars. Scion marketing was centered on the idea of extensive customization. Nike, Adidas, and Puma all experimented with allowing the consumers to design and order their own shows by choosing from a broad range of show parts. (In the case of Puma Mongolian Barbeque concept, a few thousand unique shows can be constructed.) [17] In early 2008 Bug Labs introduced what they called “the Lego of gadgets”: open sourced consumer electronics platform consisting from a minicomputer and modules such as a digital camera or a LCD screen. [18] The recent celebration of DIY practice in various consumer industries is another example of this growing trend.

In short: during the time since the publication [_The Practice of Everyday Life_](http://en.wikipedia.org/wiki/The_Practice_of_Everyday_Life), companies have developed new kinds of strategies. These strategies mimic people’s tactics of bricolage, re-assembly, and remix. In other words: the logic of tactics has now become the logic of strategies.

Web 2.0 paradigm represents the most dramatic reconfiguration of strategies/tactics relationship to date. According to de Certeau original analysis from 1980, tactics do not necessary result in objects or anything stable or permanent; “Unlike the strategy, it [tactic] lacks the centralized structure and permanence that would enable it to set itself up as a competitor to some other entity… it renders its own activities an ‘unmappable’ form of subversion.” [19] Since 1980s, however, consumer and culture industries have started to systematically turn every subculture (particularly every youth subculture) into products. In short, the cultural tactics evolved by people were turned into strategies now sold to them. If you want to “oppose the mainstream,” you now had plenty of lifestyles available – with every subculture aspect, from music and visual styles to clothes and slang – available for purchase.

These adaptations, however, still focused on distinct subcultures: bohemians, hip hop and rap, Lolita fashion, rock, punk, skinhead, Goth, etc. [20] However, in 2000s, the transformation of people’s tactics into business strategies went into a new direction. The developments of the previous decade – the Web platform, the dramatically decreased costs of the consumer electronics devices for media capture and playback, increased global travel, and the growing consumer economies of many countries which after 1990 joined the “global world” – led to the explosion of user-generated “content” available in digital form: Web sites, blogs, forum discussions, short messages, digital photo, video, music, maps, etc. consumer industries. Responding to this explosion, web 2.0 companies created powerful platforms designed to host this content. MySpace, Facebook, Orkut, Livejournal, Blogger, Flickr, YouTube, h5 (Central America), Cyworld (Korea), Wretch (Taivan), Orkut (Brasil), Baidu (China), and thousands of other social media sites make this content instantly available worldwide (except, of course, the countries which block or filter these sites). Thus, not just particular features of particular subcultures but the details of everyday life of hundreds of millions of people who make and upload their media or write blog became public. 

What before was ephemeral, transient, unmappable, and invisible become permanent, mappable, and viewable. Social media platforms give users unlimited space for storage and plenty of tools to organize, promote, and broadcast their thoughts, opinions, behavior, and media to others. You can already directly stream video using your laptop or mobile phone, and it is only a matter of time before constant broadcasting of one’s live becomes as common as email. If you follow the evolution from MyLifeBits project (2001-) to Slife software (2007-) and Yahoo! Live personal broadcasting service (2008-), the trajectory towards constant capture and broadcasting of one’s everyday life is clear. 

According to de Certeau’s 1980 analysis, strategy “is engaged in the work of systematizing, of imposing order… its ways are set. It cannot be expected to be capable of breaking up and regrouping easily, something which a tactical model does naturally.” The strategies used by social media companies today, however, are the exact opposite: they are focused on flexibility and constant change. (Of course, all businesses in the age of globalization had to become adaptable, mobile, flexible, and ready to break up and regroup – but they rarely achieve the flexibility of web companies and developers.) [21] According to Tim O'Reilly who originally defined the term Web 2.0 in 2004, one important feature of Web 2.0 applications is “design for ‘hackability’ and remixability.” [22] Thus, most major Web 2.0 companies - Amazon, eBay, Flickr, Google, Microsoft, Yahoo and YouTube - make available their programming interfaces and some of their data to encourage others to create new applications using this data. [23]

In summary, today strategies used by social media companies often look more like tactics in the original formulation by de Certeau – while tactics look like strategies. Since the companies which create social media platforms make money from having as many users as possible visit them (they do so by serving ads, by selling data about usage to other companies, by selling add-on services, etc.), they have a direct interest in having users pour as much of their lives into these platforms as possible. Consequently, they give users unlimited storage space to store all their media, the ability to customize their “online lives” (for instance, by controlling what is seen by who) and expand the functionality of the platforms themselves. 

This, however, does not mean strategies and tactics have completely exchanged places. If we look at the actual media content produced by users, here strategies/tactics relationship is different. As I already mentioned, for many decades companies have been systematically turning the elements of various subcultures developed by people into commercial products. But these subcultures themselves, however, are rarely develop completely from scratch – rather, they are the result of cultural appropriation and/or remix of earlier commercial culture by people. [24] AMV subculture is a case in point. On the other hand, it exemplifies new “strategies as tactics” phenomenon: AMVs are hosted on mainstream social media sites such as YouTube, so they are not exactly “transient” or “unmappable” (since you can use search to find them, see how other users rated them, and so on). On the other hand, on the level of content, it is “practice of everyday life” as: the great majority of AMVs consist of segments lifted from commercial anime shows and commercial music. This does not mean that best AMVs are not creative or original – only that their creativity is different from the romantic/modernist model of “making it new.” To use de Certeau’s terms, we can describe it as _tactical creativity_ which “expects to have to work on things in order to make them its own, or to make them ‘habitable.’” 

## Media Conversations

So far, I discussed social media using the old familiar terms. However, the very terms, which I was evoking so far – content, a cultural object, cultural production, and cultural consumption – are redefined by Web 2.0 practices. 

We see new kinds of communication where content, opinion, and conversation often can’t be clearly separated. Blogs are a good example of this: lots of blog entries are comments by a blog writer about an item that s/he copied from another source. Or think about forums or comments below a web site entry where n original post may generate a long discussion which after goes into new and original directions, with the original item long forgotten. 

Often “content,” “news” or “media” become tokens used to initiate or maintain a conversation. Their original meaning is less important than their function as such tokens. I am thinking here of people posting pictures on each other’s pages on MySpace or exchanging gifts on Facebook. What kind of gift you get is less important than the act of getting a gift, or posting a comment or a picture. Although it may appear that such conversation simply foregrounds Roman Jakobson’s emotive and/or emphatic communication functions [25] described already in 1960, it is also possible that a detailed analysis will show them to being a genuinely new phenomenon. 

The beginnings of such analysis can be found in the work of Adrian Chan. As he points out, “All cultures practice the exchange of tokens that bear and carry meanings, communicate interest and count as personal and social transactions.” Token gestures “cue, signal, indicate users’ interests in one another.” While the use of tokens in not unique to networked social media, some of the features pointed by Chan do appear to be new. For instance, as Chan notes, the use of tokens is often “accompanied by ambiguity of intent and motive (the token's meaning may be codified while the user's motive for using it may not). This can double up the meaning of interaction and communication, allowing the recipients of tokens to respond to the token or to the user behind its use.” [26]

Consider another very interesting new communication situation: _a conversation around a piece of media_ – for instance comments added by users below somebody’s Flickr photo or YouTube video which do not only respond to the media object but also to each other. [27] (The same is often true to comments, reviews and discussions on the web in general – the object in question can be software, a film, a previous post, etc.) Of course, such conversation structures are also common in real life: think of a typical discussion in a graduate film studies class, for instance. However, web infrastructure and software allow such conversations to become distributed in space and time – people can respond to each other regardless of their location and the conversation can in theory go forever. (The web is millions of such conversations taking place at the same time.) These conversations are quite common: according to the report by Pew internet & American Life Project (12/19/2007), among U.S. teens who post photos online, %89 reported that people comment on these photos at least some of the time. [28]

Equally interesting are _conversations which take place through images or video_ – for instance, responding to a video with a new video. This, in fact, is a standard feature of YouTube interface. [29] (Note that all examples of interfaces, features, and common uses of social media sites refer to early 2008; obviously details may change by the time you read this.) Why social media sites contain huge numbers of such conversations through media, for me the most interesting case so far is a five-minute theoretical video “Web 2.0 ... The Machine is Us/ing Us” posted by a cultural anthropologist Michael Wesch on January 31, 2007. [30] A year later this video was watched 4,638,265 times. [31] It has also generated 28 video responses that range from short 30-second comments to long equally theoretical and carefully crafted long videos.

Just as it is the case with any other feature of contemporary digital culture, it is always possible to find some precedents for any of these communication situations. For instance, modern art can be understood as conversations between different artists or artistic schools. That is, one artist/movement is responding to the work of produced earlier by another artist/movement. Thus, modernists in general are reacting against classical nineteenth century culture; Jasper Johns and other pop-artists react to abstract expressionism; Godard reacts to Hollywood-style narrative cinema; and so on. To use the terms of YouTube, we can say that Godard posts his video response to one huge clip called “classical narrative cinema.” But the Hollywood studios do not respond – at least not for another 30 years. 

As can be seen from these examples, typically these conversations between artists and artistic schools were not full conversations. One artist/school produced something, another artist/school later responded with their own productions, and this was all. The first art/school usually did not respond. But beginning in the 1980s, professional media practices begin to respond to each other more quickly and the conversations are no longer go one way. Music videos affect the editing strategies of feature films and television; similarly, today the aesthetics of motion graphics is slipping into narrative features. Cinematography, which before only existed in films, is taken up in video games, and so on. But these conversations are still different from the _communication between individuals through media_ in a networked environment. In the case of Web 2.0, it is individuals directly talking to each other using media rather than only professional producers. 

## Is Art After Web 2.0 Still Possible?

Do professional artists (including video and media artists) benefit from the explosion of media content online being produced by regular users and the easy availability of media publishing platforms? Does the fact that we now have such platforms where anybody can publish their videos and charge for the downloads mean that artists have a new distribution channel for their works? Or does the world of social media – hundreds of millions of people daily uploading and downloading video, audio, and photographs; media objects produced by unknown authors getting millions of downloads; media objects fluently and rapidly moving between users, devices, contexts, and networks – make professional art irrelevant? In short, while modern artists have so far successfully met the challenges of each generation of media technologies, can professional art survive extreme democratization of media production and access? 

On one level, this question is meaningless. Surely, never in the history of modern art it has been doing so well commercially. No longer a pursuit for a few, contemporary art became another form of mass culture. Its popularity is often equal to that of other mass media. Most importantly, contemporary art has become a legitimate investment category, and with the all the money invested into it, it is unlikely that this market will ever collapse. (Of course, history has repeatedly shown that the most stable political regimes do eventually collapse.)

In a certain sense, since the beginnings of globalization in the early 1990s, the number of participants in the institution called “contemporary art” has experienced a growth, which parallels the rise of social media in 2000s. Since the early 1990s, many new countries entered the “global world” and adopted western values in their cultural politics. Which includes supporting, collecting, and promoting “contemporary art.” Thus, today Shanghai already has not just one but three museums of contemporary art plus more large-size spaces that show contemporary art than New York or London. A number of starchitects such as Frank Gehry and Zaha Hadid are now building museums and cultural centers on Saadiyat Island in Abu Dhabi. Rem Koolhaas is building new museum of contemporary art in Riga. I can continue this list, but you get the idea. 

In the case of social media, the unprecedented growth of numbers of people who upload and view each other media led to lots of innovation. While the typical diary video or anime on YouTube may not be that special, enough are. In fact, in all media where the technologies of productions were democratized (video, music, animation, graphic design, etc.), I have come across many projects which not only rival those produced by most well-known commercial companies and most well-known artists but also often explore the new areas not yet touched by those with lots of symbolic capital. 

Who is doing these projects? In my observations, while some of these projects do come from prototypical “amateurs,” “prosumers” and “pro-ams,” most are done by young professionals, or professionals in training. The emergence of the Web as the new standard communication medium in the 1990s means that today in most cultural fields, every professional or a company, regardless of its size and geo location, has a web presence and posts new works online. Perhaps most importantly, young design students can now put their works before a global audience, see what others are doing, and develop together new tools (for instance, processing.org community).

Note that we are not talking about “classical” social media or “classical” user-generated content here, since, at least at present, many of such portfolios, sample projects and demo reels are being uploaded on companies’ own web sites and specialized aggregation sites known to people in the field. Here are some examples of such sites that I consult regularly: xplsv.tv (motion graphics, animation), coroflot.com (design portfolios from around the world), archinect.com (architecture students projects), infosthetics.com (information visualization). In my view, the significant percentage of works you find on these web sites represents the most innovative cultural production done today. Or at least, they make it clear that the world of professional art has no special license on creativity and innovation. 

But perhaps the most conceptual innovation has been happening in the development of Web 2.0 medium itself. I am thinking about all the new creative software tools - web mash-ups, Firefox plug-ins, Facebook applications, etc. – coming out from both large companies such as Google and from individual developers who are creating and so on. 

Therefore, the true challenge posed to art by social media may be not all the excellent cultural works produced by students and non-professionals which are now easily available online – although I do think this is also important. The real challenge may lie in the dynamics of Web 2.0 culture – its constant innovation, its energy, and its unpredictability. 

## References:

[1] See Adrian Chan, _Social Media: Paradigm Shift?_ [http://www.gravity7.com/paradigm\_shift\_1.html](http://www.gravity7.com/paradigm_shift_1.html), accessed February 11, 20008.

[2] Ibid.

[3] “The Long Tail” was coined by Cris Anderson in 2004. See Cris Anderson, The Long Tail, _Wired_, 10.12 (October 2008), [http://www.wired.com/wired/archive/12.10/tail.html](http://www.wired.com/wired/archive/12.10/tail.html), accessed February 11, 2008.

[4] More “long tail” statistics can be found in Tom Michael, “The Long Tail of Search,” September 17, 2007, [http://www.zoekmachine-marketing-blog.com/artikels/white-paper-the-long-tail-of-search/](http://www.zoekmachine-marketing-blog.com/artikels/white-paper-the-long-tail-of-search/), accessed February 11, 2008.

[5] [http://www.alexa.com/site/help/traffic\_learn\_more](http://www.alexa.com/site/help/traffic\_learn\_more), accessed February 7, 2008.

[6] [http://en.wikipedia.org/wiki/Myspace](http://en.wikipedia.org/wiki/Myspace), accessed February 7, 2008.

[7] [http://en.wikipedia.org/wiki/Cyworld](http://en.wikipedia.org/wiki/Cyworld), accessed February 7, 2008.

[8] [http://www.pipl.com/statistics/social-networks/size-growth/](http://www.pipl.com/statistics/social-networks/size-growth/), accessed February 11, 2008.

[9] [http://en.wikipedia.org/wiki/Facebook](http://en.wikipedia.org/wiki/Facebook), accessed February 7, 2008.

[10] [http://en.wikipedia.org/wiki/Youtube](http://en.wikipedia.org/wiki/Youtube), accessed February 7, 2008.

[11] [http://en.wikipedia.org/wiki/Micro-blogging](http://en.wikipedia.org/wiki/Micro-blogging), accessed May 23, 2008.

[12] According to research conducted by Michael Wesch, in early 2007 YouTube contained approximately %14 commercially produced videos. Michael Wesch, presentation at panel 1, DIY Video Summit, University of Southern California, February 28, [http://www.video24-7.org/panels](http://www.video24-7.org/panels).

[13] [http://www.youtube.com](http://www.youtube.com/), accessed February 7, 2008.

[14] Conversation with Tim Park from animemusicvideos.org, February 9, 2009. 

[15] Michel de Certeau, _L'Invention du Quotidien. Vol. 1, Arts de Faire_. Union générale d'éditions 10-18. 1980. Translated into English as _The Practice of Everyday Life_. Translated by Steven Rendall. University of California Press. 1984.

[16] [http://en.wikipedia.org/wiki/The\_Practice\_of\_Everyday\_Life](http://en.wikipedia.org/wiki/The\_Practice\_of\_Everyday\_Life), accessed February 8, 2008.

[17] [https://www.puma.com/secure/mbbq/](https://www.puma.com/secure/mbbq/), accessed February 8.

[18] [http://buglabs.net/](http://buglabs.net/), accessed February 8.

[19] [http://en.wikipedia.org/wiki/The\_Practice\_of\_Everyday\_Life](http://en.wikipedia.org/wiki/The\_Practice\_of\_Everyday\_Life), accessed February 10, 2008.

[20] See [http://en.wikipedia.org/wiki/History\_of\_subcultures\_in\_the\_20th\_century](http://en.wikipedia.org/wiki/History_of_subcultures_in_the_20th_century), accessed February 10.

[21] Here is a typical statement coming from business community: “Competition is changing overnight, and product lifecycles often last for just a few months. Permanence has been torn asunder. We are in a time that demands a new agility and flexibility: and everyone must have the skill and insight to prepare for a future that is rushing at them faster than ever before.” Jim Caroll, The Masters of Business Imagination Manifesto aka The Masters of Business Innovation,” [http://www.jimcarroll.com/10s/10MBI.htm](http://www.jimcarroll.com/10s/10MBI.htm), accessed February 11, 2008.

[22] [http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html?page=4](http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html?page=4), accessed February 8.

[23] [http://en.wikipedia.org/wiki/Mashup_%28web\_application\_hybrid%29](http://en.wikipedia.org/wiki/Mashup_(web\_application\_hybrid)), accessed February 11, 2008.

[24] See very interesting feature in _Wired_ which describes a creative relationship between commercial manga publishers and fans in Japan. Wired story quotes Keiji Takeda, one of the main organizers of fan conventions in Japan as saying “This is where [convention floor] we're finding the next generation of authors. The publishers understand the value of not destroying that." Qtd. in Daniel H. Pink, Japan, Ink: Inside the Manga Industrial Complex, _Wired_, 15.11, 10.22.2007, [http://www.wired.com/techbiz/media/magazine/15-11/ff\_manga?currentPage=3](http://www.wired.com/techbiz/media/magazine/15-11/ff_manga?currentPage=3).

[25] See [http://www.signosemio.com/jakobson/a\_fonctions.asp](http://www.signosemio.com/jakobson/a_fonctions.asp), accessed February 7, 2008.

[26] [http://www.gravity7.com/paradigm\_shift\_1.html](http://www.gravity7.com/paradigm\_shift\_1.html), accessed February 11, 2008.

[27] According to a survey conducted in 2007, %13 of internet users who watch video also post comments about the videos. This number, however, does not tell how many of these comments are responses to other comments. See Pew/Internet & American Life Project, Technology and Media use Report, 7/25/2007,  [http://www.pewinternet.org/PPF/r/219/report\_display.asp](http://www.pewinternet.org/PPF/r/219/report_display.asp), accessed February 11, 2008.

[28] [http://www.pewinternet.org/PPF/r/230/report\_display.asp](http://www.pewinternet.org/PPF/r/230/report\_display.asp), accessed February 11, 2008.

[29] The phenomenon of “conversation through media” was first pointed to by Derek Lomas in 2006 in relation to comments on MySpace pages.

[30] [http://youtube.com/watch?v=6gmP4nk0EOE](http://youtube.com/watch?v=6gmP4nk0EOE), accessed February 8, 2008.

[31] Ibid.