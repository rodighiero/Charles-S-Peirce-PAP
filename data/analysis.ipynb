{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import codecs\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import markdown\n",
    "\n",
    "# with open('Manovich.md', 'r') as f: text = f.read()\n",
    "# texts = text.split(\"\\n# \")[2:] # Split and remove the first two elements\n",
    "\n",
    "with open('PAP.md', 'r') as f: text = f.read()\n",
    "texts = text.split(\"\\n\") # Split and remove the first two elements\n",
    "\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 "
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "import textacy.tm\n",
    "import textacy.preprocessing\n",
    "\n",
    "# en = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\n",
    "en = textacy.load_spacy_lang(\"en_core_web_lg\", disable=(\"parser\",))\n",
    "# en = textacy.load_spacy_lang(\"en_core_web_trf\", disable=(\"parser\",))\n",
    "\n",
    "docs = []\n",
    "titles = [] # List of titles\n",
    "\n",
    "for index, text in enumerate(texts):\n",
    "    titles.append(text.split('\\n\\n')[0])\n",
    "    docs.append(textacy.make_spacy_doc(text, lang=en))\n",
    "    print(index, end=' ')\n",
    "    # print(doc._.preview)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<56x466 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1003 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "stopwords = {'pap'}\n",
    "\n",
    "lemmas = []\n",
    "\n",
    "ngs = partial(textacy.extract.ngrams, n=[1], include_pos={'NOUN'})\n",
    "# ngs = partial(textacy.extract.ngrams, n=[1, 2], include_pos={'ADJ', 'NOUN'})\n",
    "# ngs = partial(textacy.extract.ngrams, n=[1], include_pos={\"NOUN\"})\n",
    "# ents = partial(textacy.extract.entities, include_types={\"PERSON\", \"ORG\", \"GPE\", \"LOC\"})\n",
    "\n",
    "for doc in docs:\n",
    "    # extraction = textacy.extract.keyterms.textrank(doc, normalize='lemma')\n",
    "    extraction = textacy.extract.terms(doc, ngs=ngs)\n",
    "    # extraction = textacy.extract.basics.words(doc, filter_stops=True, filter_nums=True)\n",
    "    lemmatization = textacy.extract.terms_to_strings(extraction, by=\"lemma\")\n",
    "    \n",
    "     # Remove strings containing stopwords\n",
    "    lemmatization = [l for l in lemmatization if not any(stopword in l for stopword in stopwords)]\n",
    "    lemmatization = [l for l in lemmatization if not '||||' in l]\n",
    "    \n",
    "    for index, l in enumerate(lemmatization):\n",
    "        if 'datum' in l : lemmatization[index] = l.replace('datum', 'data')\n",
    "        if 'medium' in l : lemmatization[index] = l.replace('medium', 'media')\n",
    "        \n",
    "    lemmas.append(list(lemmatization))\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "doc_term_matrix, dictionary = textacy.representations.build_doc_term_matrix(lemmas, tf_type=\"linear\", idf_type=\"smooth\")\n",
    "\n",
    "doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import umap\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from pointgrid import align_points_to_grid\n",
    "\n",
    "reducer = umap.UMAP(random_state=2, n_components=2, n_neighbors=5, min_dist=0.01, metric='cosine')\n",
    "# reducer = umap.UMAP(random_state=2, n_components=2, n_neighbors=3, min_dist=0.01, metric='hellinger')\n",
    "\n",
    "embedding = reducer.fit_transform(doc_term_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalization\n",
    "\n",
    "embedding = embedding * 100 # Multiply by 100\n",
    "embedding = align_points_to_grid(embedding)\n",
    "\n",
    "# Swap axes for horizontal position\n",
    "\n",
    "ptp = np.ptp(embedding, axis=0)\n",
    "if ptp[1]> ptp[0]:\n",
    "    embedding[:, [1, 0]] = embedding[:, [0, 1]]\n",
    "\n",
    "# Set value starting from 0\n",
    "\n",
    "embedding[:, 0] = embedding[:, 0] - embedding[:, 0].min()\n",
    "embedding[:, 1] = embedding[:, 1] - embedding[:, 1].min()\n",
    "\n",
    "# # Set origin at the middle\n",
    "\n",
    "ptp = np.ptp(embedding, axis=0)\n",
    "embedding[:, 0] = embedding[:, 0] - ptp[0] / 2\n",
    "embedding[:, 1] = embedding[:, 1] - ptp[1] / 2\n",
    "\n",
    "embedding = embedding.astype(int) # Set integer\n",
    "\n",
    "plt.figure(figsize=(10,10), dpi=50)\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], s=10)\n",
    "plt.axis('equal')\n",
    "\n",
    "# for i, title in enumerate(titles):\n",
    "#     text = plt.annotate(title, (x[i], y[i]))\n",
    "#     text.set_fontsize(5)\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmas Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 lexical pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.0, 87.5, ['icon', 'proposition']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while len(pairs) == 0:\n",
    "    i += .05\n",
    "    \n",
    "    for indexA, a in enumerate(embedding):\n",
    "        for indexB, b in enumerate(embedding):\n",
    "            if indexB > indexA:\n",
    "                distance = dist = math.sqrt((b[0] - a[0])**2 + (b[1] - a[1])**2)\n",
    "                if 0 < distance and distance < i:\n",
    "                    x = (b[0] + a[0]) / 2\n",
    "                    y = (b[1] + a[1]) / 2\n",
    "                    \n",
    "                    intersection = list(set(lemmas[indexA]) & set(lemmas[indexB]))\n",
    "\n",
    "                    wordfreq = []\n",
    "                    for lemma in intersection:\n",
    "                        wordfreq.append([lemma, lemmas[indexA].count(lemma) + lemmas[indexA].count(lemma)])\n",
    "                    \n",
    "                    wordfreq.sort(key=lambda x:x[1])\n",
    "                    wordfreq.reverse()\n",
    "\n",
    "                    pairs.append([x,y, [i[0] for i in wordfreq[:3]] ])\n",
    "\n",
    "\n",
    "print(len(pairs), 'lexical pairs')\n",
    "pairs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=4, min_samples=3, cluster_selection_epsilon=.2)\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=5)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=4, cluster_selection_method='leaf')\n",
    "# min_samples is to consier all the elements that owtherwide will be classified as noise\n",
    "# cluster_selection_epsilon extends clusters\n",
    "clusterer.fit(embedding)\n",
    "clusters = clusterer.labels_\n",
    "\n",
    "# Grouping by cluster\n",
    "\n",
    "values = set(clusters)\n",
    "if -1 in values: values.remove(-1)\n",
    "\n",
    "clusters = [[index for index, cluster in enumerate(clusters) if cluster==value] for value in values]\n",
    "\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(embedding.tolist(), codecs.open('../src/data/embedding.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "# json.dump(authors, codecs.open('../src/data/authors.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(lemmas, codecs.open('../src/data/lemmas.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(pairs, codecs.open('../src/data/pairs.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "# json.dump(topics, codecs.open('../src/data/topics.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(clusters, codecs.open('../src/data/clusters.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 - 1 2 - 2 2 - 3 2 - 4 1 - "
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, get_single_color_func\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# from planar import Polygon # It's not working anymore\n",
    "\n",
    "from os import path\n",
    "import multidict as multidict\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "for index, cluster in enumerate(clusters):\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    scale = 4\n",
    "    \n",
    "    min_X = min([i[0] for i in embedding[cluster]]) * scale\n",
    "    max_X = max([i[0] for i in embedding[cluster]]) * scale\n",
    "    min_Y = min([i[1] for i in embedding[cluster]]) * scale\n",
    "    max_Y = max([i[1] for i in embedding[cluster]]) * scale\n",
    "\n",
    "    width = max_X - min_X; height = max_Y - min_Y\n",
    "    \n",
    "    points = list(map(lambda i: (i[0] * scale - min_X, i[1] * scale - min_Y), embedding[cluster]))\n",
    "\n",
    "    # Hull\n",
    "\n",
    "    hull = ConvexHull(points)\n",
    "\n",
    "    x_hull = np.append(hull.points[hull.vertices,0], hull.points[hull.vertices,0][0])\n",
    "    y_hull = np.append(hull.points[hull.vertices,1], hull.points[hull.vertices,1][0])\n",
    "    \n",
    "    # Interpolation\n",
    "    \n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)    \n",
    "    interp_points = list(zip(interp_x, interp_y))\n",
    "\n",
    "    # Create mask\n",
    "\n",
    "    img = Image.new(mode = \"RGBA\", size = (width, height), color = (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # draw.polygon(points, fill=(0,0,0))\n",
    "    draw.polygon(interp_points, fill=(0,0,0))\n",
    "    mask = np.array(img)\n",
    "\n",
    "\n",
    "    # Collect text\n",
    "\n",
    "    text = []\n",
    "    for id in cluster:\n",
    "        text = text + lemmas[id]\n",
    "    text = ' '.join(map(str, text))\n",
    "    # text = text.replace('datum', 'data')\n",
    "    # text = text.replace('medium', 'media')\n",
    "\n",
    "    dictionary = multidict.MultiDict()\n",
    "    _dictionary = {}\n",
    "\n",
    "\n",
    "    # Frequency\n",
    "\n",
    "    for _word in text.split(\" \"):\n",
    "        val = _dictionary.get(_word, 0)\n",
    "        _dictionary[_word] = val + 1\n",
    "    for key in _dictionary:\n",
    "        dictionary.add(key, _dictionary[key])\n",
    "\n",
    "\n",
    "    # Wordcloud\n",
    "\n",
    "    max_words = math.ceil(len(dictionary)*.01)\n",
    "\n",
    "\n",
    "    wc = WordCloud(\n",
    "        mode = \"RGBA\",\n",
    "        color_func=lambda *args, **kwargs: (0, 0, 0),\n",
    "        font_path = path.join('Lato-Regular.ttf'),\n",
    "        mask=mask,\n",
    "        \n",
    "        normalize_plurals=False,\n",
    "        prefer_horizontal= 1,\n",
    "        \n",
    "        margin=40,\n",
    "\n",
    "        background_color=None,\n",
    "        # background_color='black',\n",
    "\n",
    "        # max_words=max_words,\n",
    "        \n",
    "        min_font_size= 10,\n",
    "        max_font_size= 100,\n",
    "        # collocation_threshold = 20,\n",
    "        relative_scaling = 0,\n",
    "    )\n",
    "\n",
    "    print(index, max_words, '-', end=' ')\n",
    "    \n",
    "    wc.generate_from_frequencies(dictionary) # generate word cloud\n",
    "    wc.to_file(path.join(\"../src/wordclouds/\" + f\"{index:02}\" + \".png\")) # store to file\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
