{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import codecs\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import markdown\n",
    "\n",
    "# with open('Manovich.md', 'r') as f: text = f.read()\n",
    "# texts = text.split(\"\\n# \")[2:] # Split and remove the first two elements\n",
    "\n",
    "with open('PAP.md', 'r') as f: text = f.read()\n",
    "texts = text.split(\"\\n\") # Split and remove the first two elements\n",
    "\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 "
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "import textacy.tm\n",
    "import textacy.preprocessing\n",
    "\n",
    "# en = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\n",
    "en = textacy.load_spacy_lang(\"en_core_web_lg\", disable=(\"parser\",))\n",
    "# en = textacy.load_spacy_lang(\"en_core_web_trf\", disable=(\"parser\",))\n",
    "\n",
    "docs = []\n",
    "titles = [] # List of titles\n",
    "\n",
    "for index, text in enumerate(texts):\n",
    "    titles.append(text.split('\\n\\n')[0])\n",
    "    docs.append(textacy.make_spacy_doc(text, lang=en))\n",
    "    print(index, end=' ')\n",
    "    # print(doc._.preview)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<56x466 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1003 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "stopwords = {'pap'}\n",
    "\n",
    "lemmas = []\n",
    "\n",
    "ngs = partial(textacy.extract.ngrams, n=[1], include_pos={'NOUN'})\n",
    "# ngs = partial(textacy.extract.ngrams, n=[1, 2], include_pos={'ADJ', 'NOUN'})\n",
    "# ngs = partial(textacy.extract.ngrams, n=[1], include_pos={\"NOUN\"})\n",
    "# ents = partial(textacy.extract.entities, include_types={\"PERSON\", \"ORG\", \"GPE\", \"LOC\"})\n",
    "\n",
    "for doc in docs:\n",
    "    # extraction = textacy.extract.keyterms.textrank(doc, normalize='lemma')\n",
    "    extraction = textacy.extract.terms(doc, ngs=ngs)\n",
    "    # extraction = textacy.extract.basics.words(doc, filter_stops=True, filter_nums=True)\n",
    "    lemmatization = textacy.extract.terms_to_strings(extraction, by=\"lemma\")\n",
    "    \n",
    "     # Remove strings containing stopwords\n",
    "    lemmatization = [l for l in lemmatization if not any(stopword in l for stopword in stopwords)]\n",
    "    lemmatization = [l for l in lemmatization if not '||||' in l]\n",
    "    \n",
    "    for index, l in enumerate(lemmatization):\n",
    "        if 'datum' in l : lemmatization[index] = l.replace('datum', 'data')\n",
    "        if 'medium' in l : lemmatization[index] = l.replace('medium', 'media')\n",
    "        \n",
    "    lemmas.append(list(lemmatization))\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "doc_term_matrix, dictionary = textacy.representations.build_doc_term_matrix(lemmas, tf_type=\"linear\", idf_type=\"smooth\")\n",
    "\n",
    "doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import umap\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from pointgrid import align_points_to_grid\n",
    "\n",
    "reducer = umap.UMAP(random_state=2, n_components=2, n_neighbors=5, min_dist=0.01, metric='cosine')\n",
    "# reducer = umap.UMAP(random_state=2, n_components=2, n_neighbors=3, min_dist=0.01, metric='hellinger')\n",
    "\n",
    "embedding = reducer.fit_transform(doc_term_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalization\n",
    "\n",
    "embedding = embedding * 100 # Multiply by 100\n",
    "embedding = align_points_to_grid(embedding)\n",
    "\n",
    "# Swap axes for horizontal position\n",
    "\n",
    "ptp = np.ptp(embedding, axis=0)\n",
    "if ptp[1]> ptp[0]:\n",
    "    embedding[:, [1, 0]] = embedding[:, [0, 1]]\n",
    "\n",
    "# Set value starting from 0\n",
    "\n",
    "# embedding[:, 0] = embedding[:, 0] - embedding[:, 0].min()\n",
    "# embedding[:, 1] = embedding[:, 1] - embedding[:, 1].min()\n",
    "\n",
    "# # Set origin at the middle\n",
    "\n",
    "# ptp = np.ptp(embedding, axis=0)\n",
    "# embedding[:, 0] = embedding[:, 0] - ptp[0] / 2\n",
    "# embedding[:, 1] = embedding[:, 1] - ptp[1] / 2\n",
    "\n",
    "embedding = embedding.astype(int) # Set integer\n",
    "\n",
    "plt.figure(figsize=(10,10), dpi=50)\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], s=10)\n",
    "plt.axis('equal')\n",
    "\n",
    "# for i, title in enumerate(titles):\n",
    "#     text = plt.annotate(title, (x[i], y[i]))\n",
    "#     text.set_fontsize(5)\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmas Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 lexical pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[855.0, 647.5, ['anthropomorphism', 'man']]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while len(pairs) == 0:\n",
    "    i += .05\n",
    "    \n",
    "    for indexA, a in enumerate(embedding):\n",
    "        for indexB, b in enumerate(embedding):\n",
    "            if indexB > indexA:\n",
    "                distance = dist = math.sqrt((b[0] - a[0])**2 + (b[1] - a[1])**2)\n",
    "                if 0 < distance and distance < i:\n",
    "                    x = (b[0] + a[0]) / 2\n",
    "                    y = (b[1] + a[1]) / 2\n",
    "                    \n",
    "                    intersection = list(set(lemmas[indexA]) & set(lemmas[indexB]))\n",
    "\n",
    "                    wordfreq = []\n",
    "                    for lemma in intersection:\n",
    "                        wordfreq.append([lemma, lemmas[indexA].count(lemma) + lemmas[indexA].count(lemma)])\n",
    "                    \n",
    "                    wordfreq.sort(key=lambda x:x[1])\n",
    "                    wordfreq.reverse()\n",
    "\n",
    "                    pairs.append([x,y, [i[0] for i in wordfreq[:3]] ])\n",
    "\n",
    "\n",
    "print(len(pairs), 'lexical pairs')\n",
    "pairs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=4, min_samples=3, cluster_selection_epsilon=.2)\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=5)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=4, cluster_selection_method='leaf')\n",
    "# min_samples is to consier all the elements that owtherwide will be classified as noise\n",
    "# cluster_selection_epsilon extends clusters\n",
    "clusterer.fit(embedding)\n",
    "clusters = clusterer.labels_\n",
    "\n",
    "# Grouping by cluster\n",
    "\n",
    "values = set(clusters)\n",
    "if -1 in values: values.remove(-1)\n",
    "\n",
    "clusters = [[index for index, cluster in enumerate(clusters) if cluster==value] for value in values]\n",
    "\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(embedding.tolist(), codecs.open('../src/data/embedding.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "# json.dump(authors, codecs.open('../src/data/authors.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(lemmas, codecs.open('../src/data/lemmas.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(pairs, codecs.open('../src/data/pairs.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "# json.dump(topics, codecs.open('../src/data/topics.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(clusters, codecs.open('../src/data/clusters.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 - 1 1 - 2 2 - 3 3 - "
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, get_single_color_func\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# from planar import Polygon # It's not working anymore\n",
    "\n",
    "from os import path\n",
    "import multidict as multidict\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "for index, cluster in enumerate(clusters):\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    scale = 4\n",
    "    \n",
    "    min_X = min([i[0] for i in embedding[cluster]]) * scale\n",
    "    max_X = max([i[0] for i in embedding[cluster]]) * scale\n",
    "    min_Y = min([i[1] for i in embedding[cluster]]) * scale\n",
    "    max_Y = max([i[1] for i in embedding[cluster]]) * scale\n",
    "\n",
    "    width = max_X - min_X; height = max_Y - min_Y\n",
    "    \n",
    "    points = list(map(lambda i: (i[0] * scale - min_X, i[1] * scale - min_Y), embedding[cluster]))\n",
    "\n",
    "    # Hull\n",
    "\n",
    "    hull = ConvexHull(points)\n",
    "\n",
    "    x_hull = np.append(hull.points[hull.vertices,0], hull.points[hull.vertices,0][0])\n",
    "    y_hull = np.append(hull.points[hull.vertices,1], hull.points[hull.vertices,1][0])\n",
    "    \n",
    "    # Interpolation\n",
    "    \n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)    \n",
    "    interp_points = list(zip(interp_x, interp_y))\n",
    "\n",
    "    # Create mask\n",
    "\n",
    "    img = Image.new(mode = \"RGBA\", size = (width, height), color = (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # draw.polygon(points, fill=(0,0,0))\n",
    "    draw.polygon(interp_points, fill=(0,0,0))\n",
    "    mask = np.array(img)\n",
    "\n",
    "\n",
    "    # Collect text\n",
    "\n",
    "    text = []\n",
    "    for id in cluster:\n",
    "        text = text + lemmas[id]\n",
    "    text = ' '.join(map(str, text))\n",
    "    # text = text.replace('datum', 'data')\n",
    "    # text = text.replace('medium', 'media')\n",
    "\n",
    "    dictionary = multidict.MultiDict()\n",
    "    _dictionary = {}\n",
    "\n",
    "\n",
    "    # Frequency\n",
    "\n",
    "    for _word in text.split(\" \"):\n",
    "        val = _dictionary.get(_word, 0)\n",
    "        _dictionary[_word] = val + 1\n",
    "    for key in _dictionary:\n",
    "        dictionary.add(key, _dictionary[key])\n",
    "\n",
    "\n",
    "    # Wordcloud\n",
    "\n",
    "    max_words = math.ceil(len(dictionary)*.01)\n",
    "\n",
    "\n",
    "    wc = WordCloud(\n",
    "        mode = \"RGBA\",\n",
    "        color_func=lambda *args, **kwargs: (0, 0, 0),\n",
    "        font_path = path.join('Lato-Regular.ttf'),\n",
    "        mask=mask,\n",
    "        \n",
    "        normalize_plurals=False,\n",
    "        prefer_horizontal= 1,\n",
    "        \n",
    "        margin=40,\n",
    "\n",
    "        background_color=None,\n",
    "        # background_color='black',\n",
    "\n",
    "        # max_words=max_words,\n",
    "        \n",
    "        min_font_size= 10,\n",
    "        max_font_size= 100,\n",
    "        # collocation_threshold = 20,\n",
    "        relative_scaling = 0,\n",
    "    )\n",
    "\n",
    "    print(index, max_words, '-', end=' ')\n",
    "    \n",
    "    wc.generate_from_frequencies(dictionary) # generate word cloud\n",
    "    wc.to_file(path.join(\"../src/wordclouds/\" + f\"{index:02}\" + \".png\")) # store to file\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
