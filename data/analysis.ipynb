{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import codecs\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud, get_single_color_func\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import textacy\n",
    "import textacy.tm\n",
    "import textacy.preprocessing\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from pointgrid import align_points_to_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import markdown\n",
    "\n",
    "with open('Lev_Manovich_All_Articles_1992_2007.md', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text.split(\"\\n# \")[2:] # Split and remove the first two elements\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 "
     ]
    }
   ],
   "source": [
    "# Parsing\n",
    "\n",
    "en = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\n",
    "# en = textacy.load_spacy_lang(\"en_core_web_lg\", disable=(\"parser\",))\n",
    "# en = textacy.load_spacy_lang(\"en_core_web_trf\", disable=(\"parser\",))\n",
    "\n",
    "docs = []\n",
    "\n",
    "for index, text in enumerate(texts):\n",
    "    doc = textacy.make_spacy_doc(text, lang=en)\n",
    "    # doc = nlp(text)\n",
    "    docs.append(doc)\n",
    "    # print(doc._.preview)\n",
    "    print(index, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<63x4234 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24823 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lematization\n",
    "\n",
    "stopwords = {'the'}\n",
    "\n",
    "lemmas = []\n",
    "\n",
    "# ngs = partial(textacy.extract.ngrams, n=[1, 2], include_pos={'ADJ', 'NOUN'})\n",
    "ngs = partial(textacy.extract.ngrams, n=[1], include_pos={\"NOUN\"})\n",
    "# ents = partial(textacy.extract.entities, include_types={\"PERSON\", \"ORG\", \"GPE\", \"LOC\"})\n",
    "\n",
    "for doc in docs:\n",
    "    # extraction = textacy.extract.keyterms.textrank(doc, normalize='lemma')\n",
    "    extraction = textacy.extract.terms(doc, ngs=ngs)\n",
    "    # extraction = textacy.extract.basics.words(doc, filter_stops=True, filter_nums=True)\n",
    "    lemmatization = textacy.extract.terms_to_strings(extraction, by=\"lemma\")\n",
    "    \n",
    "     # Remove strings containing stopwords\n",
    "    lemmatization = [l for l in lemmatization if not any(stopword in l for stopword in stopwords)]\n",
    "    \n",
    "    for index, l in enumerate(lemmatization):\n",
    "        if 'datum' in l : lemmatization[index] = l.replace('datum', 'data')\n",
    "        if 'medium' in l : lemmatization[index] = l.replace('medium', 'media')\n",
    "        \n",
    "    lemmas.append(list(lemmatization))\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "doc_term_matrix, dictionary = textacy.representations.build_doc_term_matrix(lemmas, tf_type=\"linear\", idf_type=\"smooth\")\n",
    "\n",
    "doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: media  computer  software\n",
      "topic 1: space  fl√¢neur  game\n",
      "topic 2: image  computer  cinema\n",
      "topic 3: database  narrative  computer\n",
      "topic 4: animation  media  effect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import textacy\n",
    "# from textacy.vsm import Vectorizer\n",
    "\n",
    "model = textacy.tm.TopicModel('nmf', n_topics=5)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "doc_topic_matrix.shape\n",
    "\n",
    "dictionary_inverted = {id: term for term, id in dictionary.items()}\n",
    "\n",
    "import operator\n",
    "\n",
    "topics = []\n",
    "\n",
    "for doc in model.get_doc_topic_matrix(doc_term_matrix):\n",
    "    index, value = max(enumerate(doc), key=operator.itemgetter(1))\n",
    "    # print(index, doc)\n",
    "    topics.append(index)\n",
    "\n",
    "# for weight in model.topic_weights(doc_topic_matrix):\n",
    "#     print(doc)\n",
    "for topic_idx, terms in model.top_topic_terms(dictionary_inverted, top_n=3):\n",
    "    print(f\"topic {topic_idx}: {'  '.join(terms)}\")\n",
    "# for i, val in enumerate(model.topic_weights(doc_topic_matrix)):\n",
    "#      print(i, val)\n",
    "\n",
    "model.termite_plot(doc_term_matrix, dictionary_inverted, n_terms=30, highlight_topics=[0,2,4,6,8,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UMAP\n",
    "\n",
    "# reducer = umap.UMAP(random_state=2, n_components=2, n_neighbors=8, min_dist=0.01, metric='cosine')\n",
    "reducer = umap.UMAP(random_state=2, n_components=2, n_neighbors=20, min_dist=0.01, metric='hellinger')\n",
    "\n",
    "embedding = reducer.fit_transform(doc_term_matrix)\n",
    "embedding = align_points_to_grid(embedding)\n",
    "\n",
    "x = embedding[:, 0]; y = embedding[:, 1]\n",
    "\n",
    "plt.figure(figsize=(20,20), dpi=300)\n",
    "plt.scatter(x, y, s=100, c=topics)\n",
    "\n",
    "# for i, txt in enumerate(authors):\n",
    "#     text = plt.annotate(' ' + authors[i], (x[i] + .03, y[i]))\n",
    "#     text.set_fontsize(3)\n",
    "\n",
    "plt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 lexical pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.692026138305664, 7.817766189575195, ['realism', 'computer', 'animation']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while len(pairs) == 0:\n",
    "    i += .05\n",
    "    \n",
    "    for indexA, a in enumerate(embedding):\n",
    "        for indexB, b in enumerate(embedding):\n",
    "            if indexB > indexA:\n",
    "                distance = dist = math.sqrt((b[0] - a[0])**2 + (b[1] - a[1])**2)\n",
    "                if 0 < distance and distance < i:\n",
    "                    x = (b[0] + a[0]) / 2\n",
    "                    y = (b[1] + a[1]) / 2\n",
    "                    \n",
    "                    intersection = list(set(lemmas[indexA]) & set(lemmas[indexB]))\n",
    "\n",
    "                    wordfreq = []\n",
    "                    for lemma in intersection:\n",
    "                        wordfreq.append([lemma, lemmas[indexA].count(lemma) + lemmas[indexA].count(lemma)])\n",
    "                    \n",
    "                    wordfreq.sort(key=lambda x:x[1])\n",
    "                    wordfreq.reverse()\n",
    "\n",
    "                    pairs.append([x,y, [i[0] for i in wordfreq[:3]] ])\n",
    "\n",
    "\n",
    "print(len(pairs), 'lexical pairs')\n",
    "pairs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering on embedding\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=4, min_samples=3, cluster_selection_epsilon=.2)\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=5)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=4, cluster_selection_method='leaf')\n",
    "# min_samples is to consier all the elements that owtherwide will be classified as noise\n",
    "# cluster_selection_epsilon extends clusters\n",
    "clusterer.fit(embedding)\n",
    "clusters = clusterer.labels_\n",
    "\n",
    "# Grouping by cluster\n",
    "\n",
    "values = set(clusters)\n",
    "if -1 in values: values.remove(-1)\n",
    "\n",
    "clusters = [[index for index, cluster in enumerate(clusters) if cluster==value] for value in values]\n",
    "\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "\n",
    "json.dump(embedding.tolist(), codecs.open('../src/data/embedding.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "# json.dump(authors, codecs.open('../src/data/authors.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(lemmas, codecs.open('../src/data/lemmas.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(pairs, codecs.open('../src/data/pairs.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(topics, codecs.open('../src/data/topics.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)\n",
    "json.dump(clusters, codecs.open('../src/data/clusters.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15 - 1 14 - 2 10 - 3 10 - 4 10 - 5 8 - "
     ]
    }
   ],
   "source": [
    "# Word clouds for clusters\n",
    "\n",
    "# from planar import Polygon\n",
    "from os import path\n",
    "import multidict as multidict\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "for index, cluster in enumerate(clusters):\n",
    "\n",
    "    scale = 500\n",
    "    \n",
    "    min_X = int(min([i[0] for i in embedding[cluster]]) * scale)\n",
    "    max_X = int(max([i[0] for i in embedding[cluster]]) * scale)\n",
    "    min_Y = int(min([i[1] for i in embedding[cluster]]) * scale)\n",
    "    max_Y = int(max([i[1] for i in embedding[cluster]]) * scale)\n",
    "\n",
    "    width = max_X - min_X; height = max_Y - min_Y\n",
    "    \n",
    "    points = list(map(lambda i: (int(i[0] * scale - min_X), int(i[1] * scale - min_Y)), embedding[cluster]))\n",
    "\n",
    "    # Hull\n",
    "\n",
    "    hull = ConvexHull(points)\n",
    "\n",
    "    x_hull = np.append(hull.points[hull.vertices,0], hull.points[hull.vertices,0][0])\n",
    "    y_hull = np.append(hull.points[hull.vertices,1], hull.points[hull.vertices,1][0])\n",
    "    \n",
    "    # Interpolation\n",
    "    \n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)    \n",
    "    interp_points = list(zip(interp_x, interp_y))\n",
    "\n",
    "    # Create mask\n",
    "\n",
    "    img = Image.new(mode = \"RGBA\", size = (width, height), color = (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # draw.polygon(points, fill=(0,0,0))\n",
    "    draw.polygon(interp_points, fill=(0,0,0))\n",
    "    mask = np.array(img)\n",
    "\n",
    "\n",
    "    # Collect text\n",
    "\n",
    "    text = []\n",
    "    for id in cluster:\n",
    "        text = text + lemmas[id]\n",
    "    text = ' '.join(map(str, text))\n",
    "    # text = text.replace('datum', 'data')\n",
    "    # text = text.replace('medium', 'media')\n",
    "\n",
    "    dictionary = multidict.MultiDict()\n",
    "    _dictionary = {}\n",
    "\n",
    "\n",
    "    # Frequency\n",
    "\n",
    "    for _word in text.split(\" \"):\n",
    "        val = _dictionary.get(_word, 0)\n",
    "        _dictionary[_word] = val + 1\n",
    "    for key in _dictionary:\n",
    "        dictionary.add(key, _dictionary[key])\n",
    "\n",
    "\n",
    "    # Wordcloud\n",
    "\n",
    "    max_words = math.ceil(len(dictionary)*.01)\n",
    "\n",
    "\n",
    "    wc = WordCloud(\n",
    "        mode = \"RGBA\",\n",
    "        color_func=lambda *args, **kwargs: (0, 0, 0),\n",
    "        font_path = path.join('Lato-Regular.ttf'),\n",
    "        mask=mask,\n",
    "        \n",
    "        normalize_plurals=False,\n",
    "        prefer_horizontal= 1,\n",
    "        \n",
    "        margin=40,\n",
    "\n",
    "        background_color=None,\n",
    "        # background_color='black',\n",
    "\n",
    "        # max_words=max_words,\n",
    "        \n",
    "        min_font_size= 10,\n",
    "        max_font_size= 100,\n",
    "        # collocation_threshold = 20,\n",
    "        relative_scaling = 0,\n",
    "    )\n",
    "\n",
    "    print(index, max_words, '-', end=' ')\n",
    "    \n",
    "    wc.generate_from_frequencies(dictionary) # generate word cloud\n",
    "    wc.to_file(path.join(\"../src/wordclouds/\" + f\"{index:02}\" + \".png\")) # store to file\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
